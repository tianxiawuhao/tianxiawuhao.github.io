<!DOCTYPE html>
<html>

<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta content="yes" name="apple-mobile-web-app-capable" />
<meta content="black" name="apple-mobile-web-app-status-bar-style" />
<meta name="referrer" content="never">
<meta name="keywords" content="">
<meta name="description" content="欢迎访问[tianxia]的个人博客">
<meta name="author" content="kveln">
<title>LoRA（Low-Rank Adaptation） | tianxia</title>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.0/dist/css/bootstrap.min.css">
<link href="https://cdn.bootcss.com/font-awesome/5.11.2/css/all.min.css" rel="stylesheet">
<link href='https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
<link
  href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800'
  rel='stylesheet' type='text/css'>
<link rel="alternate" type="application/rss+xml" title="LoRA（Low-Rank Adaptation） | tianxia » Feed"
  href="https://tianxiawuhao.github.io/atom.xml">
<link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.15.10/build/styles/androidstudio.min.css">
<link href="https://tianxiawuhao.github.io/styles/main.css" rel="stylesheet">
<script src="https://cdn.bootcss.com/jquery/3.4.1/jquery.min.js"></script>

<script src="https://cdn.jsdelivr.net/npm/@highlightjs/cdn-assets/highlight.min.js"></script>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/850552586/ericamcdn@0.1/css/live2d.css">

<script>hljs.initHighlightingOnLoad();</script>

  <meta property="og:description" content="LoRA（Low-Rank Adaptation）" />
  <meta property="og:url" content="https://tianxiawuhao.github.io/n7vyn_1QRC/" />
  <meta property="og:locale" content="zh-CN" />
  <meta property="og:type" content="website" />
  <meta property="og:site_name" content="tianxia" />
  <!-- <script src="../assets/styles/scripts/tocScript.js"></script> -->
</head>

<body>
  <!-- Navigation -->
  <nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
  <div class="container">
    <a class="navbar-brand" href="https://tianxiawuhao.github.io">tianxia</a>
    <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse"
      data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false"
      aria-label="Toggle navigation">
      Menu
      <i class="fas fa-bars"></i>
    </button>
    <div class="collapse navbar-collapse" id="navbarResponsive">
      <ul class="navbar-nav ml-auto">
        
        <li class="nav-item">
          
          <a class="nav-link" href="https://tianxiawuhao.github.io">首页</a>
          
        </li>
        
        <li class="nav-item">
          
          <a class="nav-link" href="/archives">归档</a>
          
        </li>
        
        <li class="nav-item">
          
          <a class="nav-link" href="/tags">标签</a>
          
        </li>
        
        <li class="nav-item">
          <div class="gridea-search-container">
            <form id="gridea-search-form" style="position: relative" data-update="1742729663519"
              action="/search/index.html">
              <input class="search-input" autocomplete="off" spellcheck="false" name="q" placeholder="搜索文章" />
              <i class="fas fa-search gt-c-content-color-first" style="position: absolute; top: 9px; left: 10px;"></i>
            </form>
          </div>
        </li>
      </ul>
    </div>
  </div>
</nav>
  <!-- Page Header -->
  <nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
  <div class="container">
    <a class="navbar-brand" href="https://tianxiawuhao.github.io">tianxia</a>
    <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse"
      data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false"
      aria-label="Toggle navigation">
      Menu
      <i class="fas fa-bars"></i>
    </button>
    <div class="collapse navbar-collapse" id="navbarResponsive">
      <ul class="navbar-nav ml-auto">
        
        <li class="nav-item">
          
          <a class="nav-link" href="https://tianxiawuhao.github.io">首页</a>
          
        </li>
        
        <li class="nav-item">
          
          <a class="nav-link" href="/archives">归档</a>
          
        </li>
        
        <li class="nav-item">
          
          <a class="nav-link" href="/tags">标签</a>
          
        </li>
        
        <li class="nav-item">
          <div class="gridea-search-container">
            <form id="gridea-search-form" style="position: relative" data-update="1742729663519"
              action="/search/index.html">
              <input class="search-input" autocomplete="off" spellcheck="false" name="q" placeholder="搜索文章" />
              <i class="fas fa-search gt-c-content-color-first" style="position: absolute; top: 9px; left: 10px;"></i>
            </form>
          </div>
        </li>
      </ul>
    </div>
  </div>
</nav>
<header class="masthead" style="background-image: url('https://tianxiawuhao.github.io/media/images/home-bg.jpg')">
  <div class="overlay"></div>
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-md-10 mx-auto">
        
          <!-- 没Title为其他页面Header -->
          
            <!-- 没Title并且有headerType为Post：文章Header -->
            <div class="post-heading">
              <span class="tags">
                
                <a href="https://tianxiawuhao.github.io/PjnFH7MKT/" class="tag">机器学习</a>
                
              </span>
              <h1>LoRA（Low-Rank Adaptation）</h1>
              <span class="meta">
                Posted on
                2025-02-26，35 min read
              </span>
            </div>
          
        
      </div>
    </div>
  </div>
</header>
  <!-- Post Content -->
  <article id="post-content-article">
    <div class="container">
      <div class="row">
        <div class="col-lg-8 col-md-10 mx-auto post-content-container">
          
          <img class="post-feature-header-image" src="https://tianxiawuhao.github.io/post-images/n7vyn_1QRC.png" alt="封面图">
          </img>
          
          <h3 id="lora-low-rank-adaptation"><strong>LoRA (Low-Rank Adaptation)</strong></h3>
<p>LoRA（低秩适配）是一种高效的参数高效微调（Parameter-Efficient Fine-Tuning, PEFT）方法，旨在通过低秩分解的方式对预训练模型进行微调。<br>
相比于全参数微调（Full Fine-Tuning），LoRA 只需要更新少量的参数。<br>
LoRA 的核心思想是：在模型的权重矩阵中引入低秩分解，仅对低秩部分进行更新，而保持原始预训练权重不变。<br>
代码可见以下，完全从0实现LoRA流程，不依赖第三方库的封装。</p>
<pre><code class="language-python">import os  # 文件操作
import argparse  # 命令行参数解析
import time  # 时间管理
import math # 数学计算
import warnings # 忽略警告信息
import torch.distributed as dist # 分布式训练
from contextlib import nullcontext # 默认上下文管理
from torch.utils.data import DataLoader, DistributedSampler  # 数据加载和分布式采样
from transformers import AutoTokenizer # 加载预训练的分词器
from model.model import MiniMindLM # 模型架构
from model.LMConfig import LMConfig # 配置
from model.dataset import SFTDataset #数据集
from model.model_lora import * # LoRA方法
from torch import optim, nn

# 忽略警告信息
warnings.filterwarnings('ignore')


# Logger function
# 在分布式训练中，只有主进程（rank=0）会打印日志信息，避免多个进程重复输出。如果不是分布式训练（ddp=False），则直接打印日志。
def Logger(content):
    if not ddp or dist.get_rank() == 0:
        print(content)

# 使用余弦退火策略动态调整学习率。公式为：
def get_lr(current_step, total_steps, lr):
    return lr / 10 + 0.5 * lr * (1 + math.cos(math.pi * current_step / total_steps))


# 代码和full_sft「几乎」一致
# 作用 ：定义一个训练epoch的函数。
# 参数 ：
# epoch：当前训练的epoch编号（从0开始）。
# wandb：WandB日志记录工具对象，用于记录训练过程中的指标。如果未启用WandB，则为None。
def train_epoch(epoch, wandb):
    # 作用 ：定义交叉熵损失函数，reduction='none' 表示不对损失进行归约操作（即返回每个样本的损失值，而不是平均或总和）。
    # 用途 ：后续会根据 loss_mask 对损失进行加权求和，因此需要逐样本计算损失。
    # 扩展 ：可以根据任务需求选择其他损失函数，如 nn.MSELoss 或自定义损失函数。
    loss_fct = nn.CrossEntropyLoss(reduction='none')
    # 作用 ：记录当前时间戳，用于计算训练耗时。
    # 用途 ：在日志中显示每一步或每个epoch的训练时间。
    # 扩展 ：可以结合更精细的时间管理工具（如 timeit 模块）来优化性能分析。
    start_time = time.time()
    # 作用 ：遍历训练数据加载器 train_loader，每次迭代获取一个批次的数据。
    # step：当前batch的索引。
    # X：输入数据（通常是tokenized的文本序列）。
    # Y：目标标签（通常是下一个token的预测目标）。
    # loss_mask：用于屏蔽无效token的掩码（如填充token或特殊标记）。
    # 用途 ：逐批次处理数据并更新模型参数。
    # 扩展 ：可以通过 DataLoader 的 collate_fn 自定义数据预处理逻辑。
    for step, (X, Y, loss_mask) in enumerate(train_loader):
        # 作用 ：将输入数据、目标标签和损失掩码移动到指定设备（如GPU或CPU）。
        # 用途 ：确保数据和模型位于同一设备上，避免运行时错误。
        # 扩展 ：可以使用 .pin_memory() 提前将数据锁定在内存中，以加速GPU数据传输。
        X = X.to(args.device)
        Y = Y.to(args.device)
        loss_mask = loss_mask.to(args.device)
        # 作用 ：
        # 调用 get_lr 函数动态计算当前的学习率。
        # 将计算得到的学习率应用到优化器的所有参数组。
        # 用途 ：实现学习率调度策略（如余弦退火）。
        # 扩展 ：
        # 可以替换为其他学习率调度器（如 torch.optim.lr_scheduler 中的 StepLR 或 CosineAnnealingLR）。
        # 支持多阶段学习率调度（如Warmup + Decay）。
        lr = get_lr(epoch * iter_per_epoch + step, args.epochs * iter_per_epoch, args.learning_rate)
        for param_group in optimizer.param_groups:
            param_group['lr'] = lr

        # 作用 ：
        # 上下文管理器 ctx ：如果使用混合精度训练（如FP16），则启用自动混合精度（AMP）。
        # 前向传播 ：调用模型 model(X) 进行前向传播，输出结果存储在 res 中。
        # 计算损失 ：
        # 使用 CrossEntropyLoss 计算逐样本损失。
        # 根据 loss_mask 加权求和，忽略无效token的损失。
        # 添加辅助损失（aux_loss），通常用于正则化或其他目标。
        # 将总损失除以累积步数（accumulation_steps），以便梯度累积后更新。
        # 用途 ：完成一次前向传播并计算最终损失。
        # 扩展 ：
        # 如果任务复杂，可以添加更多损失项（如KL散度、对比损失等）。
        # 支持多种损失权重的动态调整。
        with ctx:
            res = model(X)
            loss = loss_fct(
                res.logits.view(-1, res.logits.size(-1)),
                Y.view(-1)
            ).view(Y.size())
            loss = (loss * loss_mask).sum() / loss_mask.sum()
            loss += res.aux_loss
            loss = loss / args.accumulation_steps
        # 作用 ：
        # 反向传播 ：使用 scaler.scale(loss).backward() 计算梯度（支持混合精度）。
        # 梯度累积 ：每隔 accumulation_steps 步执行一次梯度更新。
        # 梯度裁剪 ：防止梯度爆炸，限制梯度范数不超过 grad_clip。
        # 优化器更新 ：调用 scaler.step(optimizer) 更新模型参数。
        # 重置梯度 ：调用 optimizer.zero_grad(set_to_none=True) 清空梯度。
        # 用途 ：实现梯度累积和混合精度训练，提高训练效率。
        # 扩展 ：
        # 支持动态调整 accumulation_steps，以适应不同硬件资源。
        # 可以替换为其他优化器（如LAMB、Ranger等）。
        scaler.scale(loss).backward()
        if (step + 1) % args.accumulation_steps == 0:
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(lora_params, args.grad_clip)

            scaler.step(optimizer)
            scaler.update()

            optimizer.zero_grad(set_to_none=True)

        # 作用 ：
        # 每隔 log_interval 步记录一次训练日志。
        # 打印当前epoch、step、损失、学习率和预计剩余时间。
        # 如果启用了WandB，将日志上传到云端。
        # 用途 ：监控训练进度，便于调试和分析。
        # 扩展 ：
        # 可以记录更多指标（如准确率、F1分数等）。
        # 支持保存日志到本地文件或数据库。
        if step % args.log_interval == 0:
            spend_time = time.time() - start_time
            Logger(
                'Epoch:[{}/{}]({}/{}) loss:{:.3f} lr:{:.12f} epoch_Time:{}min:'.format(
                    epoch + 1,
                    args.epochs,
                    step,
                    iter_per_epoch,
                    loss.item(),
                    optimizer.param_groups[-1]['lr'],
                    spend_time / (step + 1) * iter_per_epoch // 60 - spend_time // 60))

            if (wandb is not None) and (not ddp or dist.get_rank() == 0):
                wandb.log({&quot;loss&quot;: loss,
                           &quot;lr&quot;: optimizer.param_groups[-1]['lr'],
                           &quot;epoch_Time&quot;: spend_time / (step + 1) * iter_per_epoch // 60 - spend_time // 60})

        # 作用 ：
        # 每隔 save_interval 步保存一次模型。
        # 仅保存LoRA权重（区别于完整模型权重）。
        # 在分布式训练中，只有主进程（rank=0）执行保存操作。
        # 用途 ：定期保存模型，防止训练中断导致数据丢失。
        # 扩展 ：
        # 支持增量保存（仅保存新增的权重变化）。
        # 可以根据验证集性能保存最佳模型。
        if (step + 1) % args.save_interval == 0 and (not ddp or dist.get_rank() == 0):
            model.eval()
            # 【区别1】只保存lora权重即可
            save_lora(model, f'{args.save_dir}/lora/{args.lora_name}_{lm_config.dim}.pth')
            model.train()


def init_model(lm_config):
    # 作用 ：加载预训练分词器（AutoTokenizer）。
    # 用途 ：用于将输入文本转换为模型可接受的token序列。
    # 扩展 ：
    # 可以根据任务需求选择不同的分词器（如BERT、GPT等）。
    # 支持自定义词汇表或特殊标记。
    tokenizer = AutoTokenizer.from_pretrained('./model/minimind_tokenizer')
    # 作用 ：初始化模型 MiniMindLM，使用配置对象 lm_config。
    # 用途 ：创建一个语言模型实例。
    # 扩展 ：
    # 可以动态调整模型结构（如层数、隐藏维度等）。
    # 支持加载不同版本的预训练模型权重。
    model = MiniMindLM(lm_config)
    # 作用 ：
    # 根据是否启用 MoE（Mixture of Experts），生成检查点路径。
    # 检查点文件名包含模型维度和MoE标志。
    # 用途 ：加载与当前模型配置匹配的预训练权重。
    # 扩展 ：
    # 支持动态生成检查点路径，适配多种模型配置。
    # 可以根据任务类型（如分类、生成）加载不同的权重。
    moe_path = '_moe' if lm_config.use_moe else ''
    ckp = f'./out/rlhf_{lm_config.dim}{moe_path}.pth'
    # 作用 ：
    # 加载预训练权重到模型中。
    # 使用 strict=False 允许部分权重不匹配（如新增或删除层）。
    # 用途 ：初始化模型参数，避免从零开始训练。
    # 扩展 ：
    # 可以实现增量加载（仅加载部分权重）。
    # 支持多阶段训练（如先加载基础模型，再微调特定任务）。
    state_dict = torch.load(ckp, map_location=args.device)
    model.load_state_dict(state_dict, strict=False)
    # 作用 ：将模型移动到指定设备（如GPU或CPU），并返回模型和分词器。
    # 用途 ：确保模型和数据位于同一设备上。
    # 扩展 ：
    # 支持多设备分布式训练（如模型并行）。
    return model.to(args.device), tokenizer


def init_distributed_mode():
    # 作用 ：如果未启用分布式训练（ddp=False），直接返回。
    # 用途 ：避免不必要的分布式初始化操作。
    # 扩展 ：
    # 支持其他分布式后端（如gloo、mpi）。
    if not ddp: return
    # 作用 ：初始化分布式训练环境，使用NCCL后端（适用于CUDA）。
    # 用途 ：设置分布式通信组。
    # 扩展 ：
    # 支持动态选择后端（如nccl、gloo）。
    # 可以实现更复杂的分布式策略（如模型并行）。
    global ddp_local_rank, DEVICE
    dist.init_process_group(backend=&quot;nccl&quot;)
    # 作用 ：从环境变量中获取分布式训练的相关信息。
    # RANK：当前进程的全局rank。
    # LOCAL_RANK：当前进程在本地节点的rank。
    # WORLD_SIZE：总进程数。
    # 用途 ：确定每个进程的角色和位置。
    # 扩展 ：
    # 支持动态调整分布式配置（如进程数、节点数）。
    ddp_rank = int(os.environ[&quot;RANK&quot;])
    ddp_local_rank = int(os.environ[&quot;LOCAL_RANK&quot;])
    ddp_world_size = int(os.environ[&quot;WORLD_SIZE&quot;])
    # 作用 ：将当前进程绑定到指定的GPU设备。
    # 用途 ：确保每个进程使用独立的GPU资源。
    # 扩展 ：
    # 支持多GPU共享（如混合精度训练）。
    DEVICE = f&quot;cuda:{ddp_local_rank}&quot;
    torch.cuda.set_device(DEVICE)

# 作用 ：定义程序入口，确保代码只在直接运行时执行。
# 用途 ：模块化设计，便于复用和测试。
# 扩展 ：
# 支持命令行参数解析和动态配置。
if __name__ == &quot;__main__&quot;:
    # 作用 ：创建命令行参数解析器。
    # 用途 ：通过命令行传递超参数，灵活控制训练过程。
    # 扩展 ：
    # 支持从配置文件加载参数。
    # 动态验证参数合法性。
    parser = argparse.ArgumentParser(description=&quot;MiniMind SFT with LoRA&quot;)
    # 作用 ：定义输出目录路径，默认值为out。
    # 用途 ：保存模型权重和日志文件。
    # 扩展 ：
    # 支持自动创建目录。
    # 动态生成子目录（如按日期或任务类型）。
    parser.add_argument(&quot;--out_dir&quot;, type=str, default=&quot;out&quot;)
    # 作用 ：定义训练轮数，默认值为50。
    # 用途 ：控制训练时长。
    # 扩展 ：
    # 支持动态调整训练轮数（如早停机制）。
    parser.add_argument(&quot;--epochs&quot;, type=int, default=50)
    # 作用 ：定义批次大小，默认值为16。
    # 用途 ：控制每次迭代的数据量。
    # 扩展 ：
    # 动态调整批次大小以适应硬件资源。
    parser.add_argument(&quot;--batch_size&quot;, type=int, default=16)
    # 作用 ：定义学习率，默认值为5e-5。
    # 用途 ：控制优化器的步长。
    # 扩展 ：
    # 支持学习率调度策略（如余弦退火、Warmup）。
    parser.add_argument(&quot;--learning_rate&quot;, type=float, default=5e-5)
    # 作用 ：定义设备类型，默认优先使用GPU。
    # 用途 ：指定模型和数据所在的设备。
    # 扩展 ：
    # 支持多设备切换（如CPU、GPU、TPU）。
    parser.add_argument(&quot;--device&quot;, type=str, default=&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
    # 作用 ：定义数据类型，默认值为bfloat16。
    # 用途 ：支持混合精度训练。
    # 扩展 ：
    # 动态调整精度（如FP32、FP16）。
    parser.add_argument(&quot;--dtype&quot;, type=str, default=&quot;bfloat16&quot;)
    # 作用 ：启用WandB日志记录。
    # 用途 ：监控训练过程。
    # 扩展 ：
    # 支持其他日志工具（如TensorBoard）。
    parser.add_argument(&quot;--use_wandb&quot;, action=&quot;store_true&quot;)
    # 作用 ：定义WandB项目名称。
    # 用途 ：组织和管理实验。
    # 扩展 ：
    # 动态生成项目名称（如按任务类型）。
    parser.add_argument(&quot;--wandb_project&quot;, type=str, default=&quot;MiniMind-LoRA-SFT&quot;)
    # 作用 ：定义数据加载线程数，默认值为1。
    # 用途 ：加速数据加载。
    # 扩展 ：
    # 动态调整线程数以优化性能。
    parser.add_argument(&quot;--num_workers&quot;, type=int, default=1)
    # 作用 ：启用分布式训练。
    # 用途 ：支持多GPU或多节点训练。
    # 扩展 ：
    # 支持动态分配资源。
    parser.add_argument(&quot;--ddp&quot;, action=&quot;store_true&quot;)
    # 作用 ：定义梯度累积步数，默认值为1。
    # 用途 ：模拟更大的批次大小。
    # 扩展 ：
    # 动态调整累积步数以适应显存限制。
    parser.add_argument(&quot;--accumulation_steps&quot;, type=int, default=1)
    # 作用 ：定义梯度裁剪阈值，默认值为1.0。
    # 用途 ：防止梯度爆炸。
    # 扩展 ：
    # 支持动态调整裁剪阈值。
    parser.add_argument(&quot;--grad_clip&quot;, type=float, default=1.0)
    # 作用 ：定义Warmup步数，默认值为0。
    # 用途 ：逐步增加学习率。
    # 扩展 ：
    # 支持多种Warmup策略（如线性、指数）。
    parser.add_argument(&quot;--warmup_iters&quot;, type=int, default=0)
    # 作用 ：定义日志记录间隔，默认值为100。
    # 用途 ：定期打印训练进度。
    # 扩展 ：
    # 支持动态调整日志频率。
    parser.add_argument(&quot;--log_interval&quot;, type=int, default=100)
    # 作用 ：定义模型保存间隔，默认值为1。
    # 用途 ：定期保存模型权重。
    # 扩展 ：
    # 支持增量保存。
    parser.add_argument(&quot;--save_interval&quot;, type=int, default=1)
    # 作用 ：定义本地rank，默认值为-1。
    # 用途 ：分布式训练中的进程标识。
    # 扩展 ：
    # 支持动态分配rank。
    parser.add_argument('--local_rank', type=int, default=-1)
    # 作用 ：定义模型维度，默认值为512。
    # 用途 ：控制模型容量。
    # 扩展 ：
    # 支持动态调整维度。
    parser.add_argument('--dim', default=512, type=int)
    # 作用 ：定义模型层数，默认值为8。
    # 用途 ：控制模型深度。
    # 扩展 ：
    # 支持动态调整层数。
    parser.add_argument('--n_layers', default=8, type=int)
    # 作用 ：定义最大序列长度，默认值为512。
    # 用途 ：限制输入序列长度。
    # 扩展 ：
    # 支持动态截断或填充。
    parser.add_argument('--max_seq_len', default=512, type=int)
    # 作用 ：启用MoE（Mixture of Experts），默认值为False。
    # 用途 ：提高模型效率。
    # 扩展 ：
    # 支持动态调整专家数量。
    parser.add_argument('--use_moe', default=False, type=bool)
    # 作用 ：定义数据集路径，默认值为./dataset/lora_identity.jsonl。
    # 用途 ：加载训练数据。
    # 扩展 ：
    # 支持多种数据格式（如CSV、JSON）。
    parser.add_argument(&quot;--data_path&quot;, type=str, default=&quot;./dataset/lora_identity.jsonl&quot;)
    # 作用 ：定义LoRA权重文件名，默认值为lora_identity。
    # 用途 ：区分不同任务的LoRA权重。
    # 扩展 ：
    # 动态生成文件名。
    parser.add_argument(&quot;--lora_name&quot;, type=str, default=&quot;lora_identity&quot;, help=&quot;根据任务保存成lora_(英文/医学/心理...)&quot;)
    args = parser.parse_args()
    # 作用 ：创建一个 LMConfig 对象，用于定义模型的超参数。
    # dim：隐藏层维度。
    # n_layers：模型层数。
    # max_seq_len：最大序列长度。
    # use_moe：是否启用 MoE（Mixture of Experts）。
    # 用途 ：将命令行参数传递给模型配置对象，确保模型初始化时使用正确的超参数。
    # 扩展 ：
    # 支持动态调整模型配置（如通过 JSON 或 YAML 文件加载配置）。
    # 可以扩展为支持更多模型类型（如Transformer、RNN等）。
    lm_config = LMConfig(dim=args.dim, n_layers=args.n_layers, max_seq_len=args.max_seq_len, use_moe=args.use_moe)
    # 作用 ：
    # 将保存目录路径设置为 args.out_dir 的子目录。
    # 使用 os.makedirs 确保保存目录存在。如果目录已存在，则不会报错（exist_ok=True）。
    # 用途 ：为模型权重和日志文件创建存储路径。
    # 扩展 ：
    # 动态生成子目录（如按任务类型或日期）。
    # 支持分布式训练中的独立保存路径（如按rank编号区分）。
    args.save_dir = os.path.join(args.out_dir)
    os.makedirs(args.save_dir, exist_ok=True)
    os.makedirs(args.out_dir, exist_ok=True)
    # 作用 ：计算每个批次中包含的token总数。
    # 用途 ：监控训练过程中的数据吞吐量。
    # 扩展 ：
    # 动态调整批次大小或序列长度以优化性能。
    # 支持多阶段训练（如Warmup阶段逐步增加token数量）。
    tokens_per_iter = args.batch_size * lm_config.max_seq_len
    # 作用 ：设置PyTorch的随机种子，确保实验结果可复现。
    # 用途 ：控制随机性，避免因随机初始化导致的实验差异。
    # 扩展 ：
    # 支持动态设置随机种子（如通过命令行参数传递）。
    # 确保其他库（如NumPy、random）也设置相同的随机种子。
    torch.manual_seed(1337)
    # 作用 ：根据 args.device 判断当前设备类型（GPU或CPU）。
    # 用途 ：确保后续操作与设备类型匹配。
    # 扩展 ：
    # 支持多设备切换（如TPU、MPS）。
    # 动态检测可用设备并自动选择。
    device_type = &quot;cuda&quot; if &quot;cuda&quot; in args.device else &quot;cpu&quot;
    # 作用 ：
    # 如果设备是CPU，则使用默认上下文管理器（nullcontext）。
    # 如果设备是GPU，则启用混合精度训练（torch.cuda.amp.autocast）。
    # 用途 ：在GPU上加速训练并降低显存占用。
    # 扩展 ：
    # 支持动态调整精度（如FP16、BF16）。
    # 兼容其他混合精度工具（如Apex）。
    ctx = nullcontext() if device_type == &quot;cpu&quot; else torch.cuda.amp.autocast()
    # 作用 ：检查环境变量 RANK 是否存在，判断是否启用分布式训练。
    # 用途 ：决定是否需要初始化分布式模式。
    # 扩展 ：
    # 支持动态分配分布式配置（如进程数、节点数）。
    # 兼容其他分布式框架（如Horovod）。
    ddp = int(os.environ.get(&quot;RANK&quot;, -1)) != -1  # is this a ddp run?
    # 作用 ：
    # 如果启用分布式训练，调用 init_distributed_mode 初始化分布式环境。
    # 设置当前进程的设备为 DEVICE。
    # 用途 ：确保分布式训练中的每个进程使用独立的GPU资源。
    # 扩展 ：
    # 支持动态分配设备（如按进程编号分配GPU）。
    # 兼容多节点分布式训练。
    ddp_local_rank, DEVICE = 0, &quot;cuda:0&quot;
    if ddp:
        init_distributed_mode()
        args.device = torch.device(DEVICE)
    # 作用 ：生成WandB运行名称，包含训练超参数信息。
    # 用途 ：便于在WandB中区分不同实验。
    # 扩展 ：
    # 动态生成更详细的运行名称（如加入时间戳或任务类型）。
    # 支持自定义命名规则。
    args.wandb_run_name = f&quot;MiniMind-Lora-SFT-Epoch-{args.epochs}-BatchSize-{args.batch_size}-LearningRate-{args.learning_rate}&quot;
    # 作用 ：
    # 如果启用了WandB且当前进程为主进程（ddp_local_rank == 0），初始化WandB。
    # 否则，将 wandb 设置为 None。
    # 用途 ：记录训练过程中的指标（如损失、学习率）。
    # 扩展 ：
    # 支持其他日志工具（如TensorBoard、MLflow）。
    # 动态调整日志频率或内容。
    if args.use_wandb and (not ddp or ddp_local_rank == 0):
        import wandb

        wandb.init(project=args.wandb_project, name=args.wandb_run_name)
    else:
        wandb = None
    # 作用 ：调用 init_model 函数加载预训练模型和分词器。
    # 用途 ：准备模型和数据处理工具。
    # 扩展 ：
    # 支持动态加载不同版本的预训练模型。
    # 兼容多种分词器（如BERT、GPT）。
    model, tokenizer = init_model(lm_config)
    # 作用 ：将LoRA模块插入到模型中。
    # 用途 ：实现低秩分解微调，减少参数量。
    # 扩展 ：
    # 支持动态调整LoRA的秩（rank）。
    # 兼容其他高效微调方法（如Adapter、Prefix Tuning）
    apply_lora(model)
    # 作用 ：
    # 统计模型的总参数量。
    # 统计LoRA模块的参数量。
    # 用途 ：评估模型规模和LoRA的效率。
    # 扩展 ：
    # 支持统计其他模块的参数量（如MoE、Attention）。
    # 动态调整LoRA参数占比。
    total_params = sum(p.numel() for p in model.parameters())  # 总参数数量
    lora_params_count = sum(p.numel() for name, p in model.named_parameters() if 'lora' in name)  # LoRA 参数数量
    if not ddp or dist.get_rank() == 0:
        print(f&quot;LLM 总参数量: {total_params}&quot;)
        print(f&quot;LoRA 参数量: {lora_params_count}&quot;)
        print(f&quot;LoRA 参数占比: {lora_params_count / total_params * 100:.2f}%&quot;)
    # 作用 ：冻结所有非LoRA参数，仅对LoRA参数进行优化。
    # 用途 ：减少训练所需的显存和计算资源。
    # 扩展 ：
    # 支持部分冻结（如冻结Embedding层）。
    # 动态调整冻结策略。
    for name, param in model.named_parameters():
        if 'lora' not in name:
            param.requires_grad = False
    # 作用 ：提取所有LoRA参数。
    # 用途 ：为优化器提供待优化的参数列表。
    # 扩展 ：
    # 支持动态调整优化参数（如加入正则化项）。
    # 兼容其他优化器（如LAMB、Ranger）。
    lora_params = []
    for name, param in model.named_parameters():
        if 'lora' in name:
            lora_params.append(param)

    # 只对 LoRA 参数进行优化
    # 作用 ：定义AdamW优化器，仅优化LoRA参数。
    # 用途 ：更新模型参数以最小化损失。
    # 扩展 ：
    # 支持动态调整学习率（如余弦退火、Warmup）。
    # 兼容其他优化算法（如SGD、Adagrad）。
    optimizer = optim.AdamW(lora_params, lr=args.learning_rate)
    # 作用 ：加载训练数据集，使用指定的分词器和最大序列长度。
    # 用途 ：准备训练数据。
    # 扩展 ：
    # 支持多种数据格式（如CSV、JSON）。
    # 动态调整数据增强策略。
    train_ds = SFTDataset(args.data_path, tokenizer, max_length=lm_config.max_seq_len)
    # 作用 ：如果启用分布式训练，定义分布式采样器；否则为 None。
    # 用途 ：确保每个进程处理不同的数据子集。
    # 扩展 ：
    # 支持动态调整采样策略（如加权采样）。
    # 兼容其他分布式框架。
    train_sampler = DistributedSampler(train_ds) if ddp else None
    # 作用 ：定义数据加载器，用于批量加载训练数据。
    # 用途 ：加速数据加载并提高训练效率。
    # 扩展 ：
    # 动态调整线程数（num_workers）以优化性能。
    # 支持动态调整批次大小。
    train_loader = DataLoader(
        train_ds,
        batch_size=args.batch_size,
        pin_memory=True,
        drop_last=False,
        shuffle=False,
        num_workers=args.num_workers,
        sampler=train_sampler
    )
    # 作用 ：定义梯度缩放器，用于混合精度训练。
    # 用途 ：防止梯度下溢，提高训练稳定性。
    # 扩展 ：
    # 支持动态调整精度（如FP32、FP16）。
    # 兼容其他混合精度工具。
    scaler = torch.cuda.amp.GradScaler(enabled=(args.dtype in ['float16', 'bfloat16']))
    # 作用 ：计算每个epoch中的迭代次数。
    # 用途 ：监控训练进度。
    # 扩展 ：
    # 动态调整迭代次数（如按数据量变化）。
    iter_per_epoch = len(train_loader)
    # 作用 ：遍历每个epoch，调用 train_epoch 进行训练。
    # 用途 ：完成整个训练过程。
    # 扩展 ：
    # 支持早停机制（如验证集性能不再提升时停止训练）。
    # 动态调整训练策略（如学习率调度）。
    for epoch in range(args.epochs):
        train_epoch(epoch, wandb)
</code></pre>
<pre><code class="language-python"># 作用 ：
# torch：PyTorch核心库，用于张量操作和神经网络功能。
# optim 和 nn：分别提供优化器和神经网络模块的支持。
# 用途 ：
# 提供LoRA实现所需的工具和接口。
# 扩展 ：
# 支持动态加载不同版本的PyTorch。
# 增加更多自定义模块或工具。
import torch
from torch import nn


# 定义Lora网络结构
class LoRA(nn.Module):
    # 作用 ：
    # 定义一个LoRA模块，继承自 nn.Module。
    # 初始化时设置输入特征数（in_features）、输出特征数（out_features）和秩（rank）。
    # 使用两个线性层（A 和 B）实现低秩分解。
    # 用途 ：
    # 提供高效的参数微调能力。
    # 扩展 ：
    # 支持动态调整秩（rank）。
    # 增加对偏置项的支持。
    def __init__(self, in_features, out_features, rank):
        super().__init__()
        self.rank = rank  # LoRA的秩（rank），控制低秩矩阵的大小
        self.A = nn.Linear(in_features, rank, bias=False)  # 低秩矩阵A
        self.B = nn.Linear(rank, out_features, bias=False)  # 低秩矩阵B
        # 矩阵A高斯初始化
        self.A.weight.data.normal_(mean=0.0, std=0.02)
        # 矩阵B全0初始化
        self.B.weight.data.zero_()

    # 作用 ：
    # 实现LoRA模块的前向传播逻辑。
    # 输入通过低秩矩阵 A 和 B 进行变换。
    # 用途 ：
    # 提供低秩分解的计算能力。
    # 扩展 ：
    # 支持动态调整激活函数。
    # 增加对多分支结构的支持。
    def forward(self, x):
        return self.B(self.A(x))

# 作用 ：
# 遍历模型的所有模块，找到符合条件的线性层（权重为方阵）。
# 为符合条件的线性层添加LoRA模块，并修改其前向传播逻辑。
# 用途 ：
# 在现有模型中插入LoRA模块，实现高效微调。
# 扩展 ：
# 支持动态调整筛选条件（如仅对特定层应用LoRA）。
# 增加对其他类型层（如卷积层）的支持。
def apply_lora(model, rank=16):
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear) and module.weight.shape[0] == module.weight.shape[1]:
            lora = LoRA(module.weight.shape[0], module.weight.shape[1], rank=rank).to(model.device)
            setattr(module, &quot;lora&quot;, lora)
            original_forward = module.forward

            # 显式绑定
            def forward_with_lora(x, layer1=original_forward, layer2=lora):
                return layer1(x) + layer2(x)

            module.forward = forward_with_lora

# 作用 ：
# 从指定路径加载LoRA权重。
# 将权重加载到模型中对应的LoRA模块。
# 用途 ：
# 恢复LoRA模块的训练状态。
# 扩展 ：
# 支持动态调整加载路径。
# 增加对多种文件格式的支持。
def load_lora(model, path):
    state_dict = torch.load(path, map_location=model.device)
    for name, module in model.named_modules():
        if hasattr(module, 'lora'):
            lora_state = {k.replace(f'{name}.lora.', ''): v for k, v in state_dict.items() if f'{name}.lora.' in k}
            module.lora.load_state_dict(lora_state)

# 作用 ：
# 遍历模型的所有模块，提取LoRA模块的权重。
# 将权重保存到指定路径。
# 用途 ：
# 保存LoRA模块的训练状态。
# 扩展 ：
# 支持动态调整保存路径。
# 增加对压缩存储的支持。
def save_lora(model, path):
    state_dict = {}
    for name, module in model.named_modules():
        if hasattr(module, 'lora'):
            lora_state = {f'{name}.lora.{k}': v for k, v in module.lora.state_dict().items()}
            state_dict.update(lora_state)
    torch.save(state_dict, path)
</code></pre>
<pre><code class="language-python">
# 作用 ：从 transformers 库中导入 PretrainedConfig 基类。
# 用途 ：用于定义模型配置类，继承自 PretrainedConfig。
# 扩展 ：
# 支持其他基类（如自定义配置类）。
# 动态加载不同版本的 transformers。
from transformers import PretrainedConfig

# 作用 ：
# 定义一个名为 LMConfig 的模型配置类，继承自 PretrainedConfig。
# 设置 model_type 为 &quot;minimind&quot;，用于标识模型类型。
# 用途 ：
# 配置模型的超参数。
# 提供标准化的接口，便于与 transformers 生态集成。
# 扩展 ：
# 支持动态调整 model_type。
# 兼容多种模型架构
class LMConfig(PretrainedConfig):
    model_type = &quot;minimind&quot;
    # 作用 ：
    # 定义初始化方法，接受多个超参数作为输入。
    # 每个参数都有默认值，确保在未指定时使用默认配置。
    # 用途 ：
    # 配置模型的核心超参数。
    # 支持灵活的参数设置，适配不同的任务需求。
    # 扩展 ：
    # 支持动态调整参数范围。
    # 增加更多超参数（如正则化项、优化器配置）。
    def __init__(
            self,
            dim: int = 512,
            n_layers: int = 8,
            n_heads: int = 8,
            n_kv_heads: int = 2,
            vocab_size: int = 6400,
            hidden_dim: int = None,
            multiple_of: int = 64,
            norm_eps: float = 1e-5,
            max_seq_len: int = 8192,
            rope_theta: int = 1e6,
            dropout: float = 0.0,
            flash_attn: bool = True,
            ####################################################
            # Here are the specific configurations of MOE
            # When use_moe is false, the following is invalid
            ####################################################
            use_moe: bool = False,
            ####################################################
            num_experts_per_tok: int = 2,
            n_routed_experts: int = 4,
            n_shared_experts: bool = True,
            scoring_func: str = 'softmax',
            aux_loss_alpha: float = 0.1,
            seq_aux: bool = True,
            norm_topk_prob: bool = True,
            **kwargs,
    ):
    # 作用 ：
    # 将传入的参数赋值给类属性。
    # 每个参数对应模型的一个核心超参数。
    # 用途 ：
    # 配置模型的结构和行为。
    # 扩展 ：
    # 支持动态调整参数值（如通过命令行或配置文件）。
    # 增加更多超参数（如激活函数类型、归一化方式）。
        self.dim = dim
        self.n_layers = n_layers
        self.n_heads = n_heads
        self.n_kv_heads = n_kv_heads
        self.vocab_size = vocab_size
        self.hidden_dim = hidden_dim
        self.multiple_of = multiple_of
        self.norm_eps = norm_eps
        self.max_seq_len = max_seq_len
        self.rope_theta = rope_theta
        self.dropout = dropout
        self.flash_attn = flash_attn
        # 作用 ：
        # 配置混合专家（MoE）模块的相关参数。
        # 当 use_moe=False 时，这些参数无效。
        # 用途 ：
        # 控制MoE模块的行为。
        # 扩展 ：
        # 支持动态启用或禁用MoE。
        # 增加更多MoE相关参数（如专家容量、路由策略）。
        ####################################################
        # Here are the specific configurations of MOE
        # When use_moe is false, the following is invalid
        ####################################################
        self.use_moe = use_moe
        self.num_experts_per_tok = num_experts_per_tok  # 每个token选择的专家数量
        self.n_routed_experts = n_routed_experts  # 总的专家数量
        self.n_shared_experts = n_shared_experts  # 共享专家
        self.scoring_func = scoring_func  # 评分函数，默认为'softmax'
        self.aux_loss_alpha = aux_loss_alpha  # 辅助损失的alpha参数
        self.seq_aux = seq_aux  # 是否在序列级别上计算辅助损失
        self.norm_topk_prob = norm_topk_prob  # 是否标准化top-k概率
        super().__init__(**kwargs)
</code></pre>
<blockquote>
<p>训练后的模型权重文件默认每隔<code>100步</code>保存为: <code>lora_xxx_*.pth</code>（*为模型具体dimension，每次保存时新文件会覆盖旧文件）</p>
</blockquote>
<p>非常多的人困惑，如何使模型学会自己私有领域的知识？如何准备数据集？如何迁移通用领域模型打造垂域模型？<br>
这里举几个例子，对于通用模型，医学领域知识欠缺，可以尝试在原有模型基础上加入领域知识，以获得更好的性能。<br>
同时，我们通常不希望学会领域知识的同时损失原有基础模型的其它能力，此时LoRA可以很好的改善这个问题。<br>
只需要准备如下格式的对话数据集放置到<code>./dataset/lora_xxx.jsonl</code>，启动 <code>python train_lora.py</code><br>
训练即可得到<code>./out/lora/lora_xxx.pth</code>新模型权重。</p>
<p><strong>医疗场景</strong></p>
<pre><code class="language-text"> {&quot;conversations&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;请问颈椎病的人枕头多高才最好？&quot;}, {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;颈椎病患者选择枕头的高度应该根据...&quot;}]}
 {&quot;conversations&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;请问xxx&quot;}, {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;xxx...&quot;}]}
</code></pre>
<p><strong>自我认知场景</strong></p>
<pre><code class="language-text"> {&quot;conversations&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;你叫什么名字？&quot;}, {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;我叫minimind...&quot;}]}
 {&quot;conversations&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;你是谁&quot;}, {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;我是...&quot;}]}
</code></pre>
<p>此时【基础模型+LoRA模型】即可获得医疗场景模型增强的能力，相当于为基础模型增加了LoRA外挂，这个过程并不损失基础模型的本身能力。<br>
我们可以通过<code>eval_model.py</code>进行模型评估测试。</p>
<pre><code class="language-python">import argparse
import random
import time
import numpy as np
import torch
import warnings
from transformers import AutoTokenizer, AutoModelForCausalLM
from model.model import MiniMindLM
from model.LMConfig import LMConfig
from model.model_lora import *

warnings.filterwarnings('ignore')


def init_model(args):
    tokenizer = AutoTokenizer.from_pretrained(r'C:\study\minimind-master\model\minimind_tokenizer')
    if args.load == 0:
        moe_path = '_moe' if args.use_moe else ''
        modes = {0: 'pretrain', 1: 'full_sft', 2: 'rlhf', 3: 'reason'}
        ckp = f'./{args.out_dir}/{modes[args.model_mode]}_{args.dim}{moe_path}.pth'

        model = MiniMindLM(LMConfig(
            dim=args.dim,
            n_layers=args.n_layers,
            max_seq_len=args.max_seq_len,
            use_moe=args.use_moe
        ))

        state_dict = torch.load(ckp, map_location=args.device)
        model.load_state_dict({k: v for k, v in state_dict.items() if 'mask' not in k}, strict=True)

        if args.lora_name != 'None':
            apply_lora(model)
            load_lora(model, f'./{args.out_dir}/lora/{args.lora_name}_{args.dim}.pth')
    else:
        model = AutoModelForCausalLM.from_pretrained(
            r'C:\study\minimind-master\MiniMind2',
            trust_remote_code=True
        )
    print(f'MiniMind模型参数量: {sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.2f}M(illion)')
    return model.eval().to(args.device), tokenizer


def get_prompt_datas(args):
    if args.model_mode == 0:
        # pretrain模型的接龙能力（无法对话）
        prompt_datas = [
            '马克思主义基本原理',
            '人类大脑的主要功能',
            '万有引力原理是',
            '世界上最高的山峰是',
            '二氧化碳在空气中',
            '地球上最大的动物有',
            '杭州市的美食有'
        ]
    else:
        if args.lora_name == 'None':
            # 通用对话问题
            prompt_datas = [
                '请介绍一下自己。',
                '你更擅长哪一个学科？',
                '鲁迅的《狂人日记》是如何批判封建礼教的？',
                '我咳嗽已经持续了两周，需要去医院检查吗？',
                '详细的介绍光速的物理概念。',
                '推荐一些杭州的特色美食吧。',
                '请为我讲解“大语言模型”这个概念。',
                '如何理解ChatGPT？',
                'Introduce the history of the United States, please.'
            ]
        else:
            # 特定领域问题
            lora_prompt_datas = {
                'lora_identity': [
                    &quot;你是ChatGPT吧。&quot;,
                    &quot;你叫什么名字？&quot;,
                    &quot;你和openai是什么关系？&quot;
                ],
                'lora_medical': [
                    '我最近经常感到头晕，可能是什么原因？',
                    '我咳嗽已经持续了两周，需要去医院检查吗？',
                    '服用抗生素时需要注意哪些事项？',
                    '体检报告中显示胆固醇偏高，我该怎么办？',
                    '孕妇在饮食上需要注意什么？',
                    '老年人如何预防骨质疏松？',
                    '我最近总是感到焦虑，应该怎么缓解？',
                    '如果有人突然晕倒，应该如何急救？'
                ],
            }
            prompt_datas = lora_prompt_datas[args.lora_name]

    return prompt_datas


# 设置可复现的随机种子
def setup_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def main():
    parser = argparse.ArgumentParser(description=&quot;Chat with MiniMind&quot;)
    parser.add_argument('--lora_name', default='None', type=str)
    parser.add_argument('--out_dir', default='out', type=str)
    parser.add_argument('--temperature', default=0.85, type=float)
    parser.add_argument('--top_p', default=0.85, type=float)
    parser.add_argument('--device', default='cuda' if torch.cuda.is_available() else 'cpu', type=str)
    # 此处max_seq_len（最大允许输入长度）并不意味模型具有对应的长文本的性能，仅防止QA出现被截断的问题
    # MiniMind2-moe (145M)：(dim=640, n_layers=8, use_moe=True)
    # MiniMind2-Small (26M)：(dim=512, n_layers=8)
    # MiniMind2 (104M)：(dim=768, n_layers=16)
    parser.add_argument('--dim', default=512, type=int)
    parser.add_argument('--n_layers', default=8, type=int)
    parser.add_argument('--max_seq_len', default=8192, type=int)
    parser.add_argument('--use_moe', default=False, type=bool)
    # 携带历史对话上下文条数
    # history_cnt需要设为偶数，即【用户问题, 模型回答】为1组；设置为0时，即当前query不携带历史上文
    # 模型未经过外推微调时，在更长的上下文的chat_template时难免出现性能的明显退化，因此需要注意此处设置
    parser.add_argument('--history_cnt', default=0, type=int)
    parser.add_argument('--stream', default=True, type=bool)
    parser.add_argument('--load', default=0, type=int, help=&quot;0: 原生torch权重，1: transformers加载&quot;)
    parser.add_argument('--model_mode', default=1, type=int,
                        help=&quot;0: 预训练模型，1: SFT-Chat模型，2: RLHF-Chat模型，3: Reason模型&quot;)
    args = parser.parse_args()

    model, tokenizer = init_model(args)

    prompts = get_prompt_datas(args)
    test_mode = int(input('[0] 自动测试\n[1] 手动输入\n'))
    messages = []
    for idx, prompt in enumerate(prompts if test_mode == 0 else iter(lambda: input('👶: '), '')):
        setup_seed(random.randint(0, 2048))
        # setup_seed(2025)  # 如需固定每次输出则换成【固定】的随机种子
        if test_mode == 0: print(f'👶: {prompt}')

        messages = messages[-args.history_cnt:] if args.history_cnt else []
        messages.append({&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt})

        new_prompt = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )[-args.max_seq_len + 1:] if args.model_mode != 0 else (tokenizer.bos_token + prompt)

        answer = new_prompt
        with torch.no_grad():
            x = torch.tensor(tokenizer(new_prompt)['input_ids'], device=args.device).unsqueeze(0)
            outputs = model.generate(
                x,
                eos_token_id=tokenizer.eos_token_id,
                max_new_tokens=args.max_seq_len,
                temperature=args.temperature,
                top_p=args.top_p,
                stream=True,
                pad_token_id=tokenizer.pad_token_id
            )

            print('🤖️: ', end='')
            try:
                if not args.stream:
                    print(tokenizer.decode(outputs.squeeze()[x.shape[1]:].tolist(), skip_special_tokens=True), end='')
                else:
                    history_idx = 0
                    for y in outputs:
                        answer = tokenizer.decode(y[0].tolist(), skip_special_tokens=True)
                        if (answer and answer[-1] == '�') or not answer:
                            continue
                        print(answer[history_idx:], end='', flush=True)
                        history_idx = len(answer)
            except StopIteration:
                print(&quot;No answer&quot;)
            print('\n')

        messages.append({&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: answer})


if __name__ == &quot;__main__&quot;:
    main()
</code></pre>
<pre><code class="language-bash"># 注意：model_mode即选择基础模型的类型，这和train_lora是基于哪个模型训练的相关，确保统一即可。
python eval_model.py --lora_name 'lora_medical' --model_mode 2
</code></pre>
<p><strong>小测试</strong></p>
<pre><code class="language-text">👶: 我最近经常感到头晕，可能是什么原因？
🤖️: 头晕可能是由于多种原因，包括但不限于：心脏疾病、眼睛损伤、过敏反应、皮肤问题、过敏反应、压力、焦虑、疲劳、药物副作用、性功能障碍或者其他...

---

👶: 你和openai是什么关系？
🤖️: 我是 MiniMind，一个由 Jingyao Gong 开发的人工智能助手。我通过自然语言处理和算法训练来与用户进行交互。
</code></pre>
<p>PS：只要有所需要的数据集，也可以full_sft全参微调（需要进行通用知识的混合配比，否则过拟合领域数据会让模型变傻，损失通用性）</p>

          <div class="toc-container"><ul class="markdownIt-TOC">
<li>
<ul>
<li>
<ul>
<li><a href="#lora-low-rank-adaptation"><strong>LoRA (Low-Rank Adaptation)</strong></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
          <hr />
          
            <p class="prev-post">上一篇：
              <a href="https://tianxiawuhao.github.io/vfv73gaU-n/">
                <span class="post-title">
                  学习率调度策略&rarr;
                </span>
              </a>
            </p>
          
          
          <p class="next-post">下一篇：
            <a href="https://tianxiawuhao.github.io/GL2xUsgbSZ/">
              <span class="post-title">
                LoRA_PEFT&rarr;
              </span>
            </a>
          </p>
          
          <div class="comment" style="text-align: center;">
            

            
            
          </div>
        </div>
      </div>
    </div>
  </article>
  <!-- Footer -->
  <footer>
    <div class="container">
      <div class="row">
        <div class="col-lg-8 col-md-10 mx-auto">
          <ul class="list-inline text-center">
            
            
              
            
              
            
              
            
              
            
              
            
              
            
              
              <!-- <li class="list-inline-item">
              <a href="https://tianxiawuhao.github.io/atom.xml" target="_blank">
                <span class="fa-stack fa-lg">
                  <i class="fas fa-circle fa-stack-2x"></i>
                  <i class="fas fa-rss fa-stack-1x fa-inverse"></i>
                </span>
              </a>
              </li> -->
          </ul>
          <p class="copyright text-muted">Copyright &copy;<span>tianxia</span><br><a href="https://github.com/getgridea/gridea" class="Themeinfo">Powered by Gridea</a></p>
        </div>
      </div>
    </div>
   </footer>
  <!-- Bootstrap core JavaScript -->
  <script src="https://cdn.bootcss.com/twitter-bootstrap/4.3.1/js/bootstrap.bundle.min.js"></script>
  <!-- <script src="https://tianxiawuhao.github.io/media/scripts/bootstrap.bundle.min.js"></script> -->
  <!-- Bootstrap core JavaScript -->
  <script src="https://cdn.jsdelivr.net/gh/Alanrk/clean-cdn@1.0/scripts/clean-blog.min.js"></script>
  <!-- <script src="https://tianxiawuhao.github.io/media/scripts/clean-blog.min.js"></script> -->
  <script src="//instant.page/3.0.0" type="module" defer integrity="sha384-OeDn4XE77tdHo8pGtE1apMPmAipjoxUQ++eeJa6EtJCfHlvijigWiJpD7VDPWXV1"></script>
  <style type="text/css">a.back_to_top{text-decoration:none;position:fixed;bottom:40px;right:30px;background:#f0f0f0;height:40px;width:40px;border-radius:50%;line-height:36px;font-size:18px;text-align:center;transition-duration:.5s;transition-propety:background-color;display:none}a.back_to_top span{color:#888}a.back_to_top:hover{cursor:pointer;background:#dfdfdf}a.back_to_top:hover span{color:#555}@media print,screen and(max-width:580px){.back_to_top{display:none!important}}</style>
<a id="back_to_top" href="#" class="back_to_top">
  <span>▲</span></a>
<script>$(document).ready((function(_this) {
    return function() {
      var bt;
      bt = $('#back_to_top');
      if ($(document).width() > 480) {
        $(window).scroll(function() {
          var st;
          st = $(window).scrollTop();
          if (st > 30) {
            return bt.css('display', 'block')
          } else {
            return bt.css('display', 'none')
          }
        });
        return bt.click(function() {
          $('body,html').animate({
            scrollTop: 0
          },
          800);
          return false
        })
      }
    }
  })(this));</script>
  
  <script src="https://tianxiawuhao.github.io/media/scripts/tocScript.js"></script>
</body>

</html>