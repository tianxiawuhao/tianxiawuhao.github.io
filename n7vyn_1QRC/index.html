<!DOCTYPE html>
<html>

<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta content="yes" name="apple-mobile-web-app-capable" />
<meta content="black" name="apple-mobile-web-app-status-bar-style" />
<meta name="referrer" content="never">
<meta name="keywords" content="">
<meta name="description" content="æ¬¢è¿è®¿é—®[tianxia]çš„ä¸ªäººåšå®¢">
<meta name="author" content="kveln">
<title>LoRAï¼ˆLow-Rank Adaptationï¼‰ | tianxia</title>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.0/dist/css/bootstrap.min.css">
<link href="https://cdn.bootcss.com/font-awesome/5.11.2/css/all.min.css" rel="stylesheet">
<link href='https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
<link
  href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800'
  rel='stylesheet' type='text/css'>
<link rel="alternate" type="application/rss+xml" title="LoRAï¼ˆLow-Rank Adaptationï¼‰ | tianxia Â» Feed"
  href="https://tianxiawuhao.github.io/atom.xml">
<link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.15.10/build/styles/androidstudio.min.css">
<link href="https://tianxiawuhao.github.io/styles/main.css" rel="stylesheet">
<script src="https://cdn.bootcss.com/jquery/3.4.1/jquery.min.js"></script>

<script src="https://cdn.jsdelivr.net/npm/@highlightjs/cdn-assets/highlight.min.js"></script>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/850552586/ericamcdn@0.1/css/live2d.css">

<script>hljs.initHighlightingOnLoad();</script>

  <meta property="og:description" content="LoRAï¼ˆLow-Rank Adaptationï¼‰" />
  <meta property="og:url" content="https://tianxiawuhao.github.io/n7vyn_1QRC/" />
  <meta property="og:locale" content="zh-CN" />
  <meta property="og:type" content="website" />
  <meta property="og:site_name" content="tianxia" />
  <!-- <script src="../assets/styles/scripts/tocScript.js"></script> -->
</head>

<body>
  <!-- Navigation -->
  <nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
  <div class="container">
    <a class="navbar-brand" href="https://tianxiawuhao.github.io">tianxia</a>
    <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse"
      data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false"
      aria-label="Toggle navigation">
      Menu
      <i class="fas fa-bars"></i>
    </button>
    <div class="collapse navbar-collapse" id="navbarResponsive">
      <ul class="navbar-nav ml-auto">
        
        <li class="nav-item">
          
          <a class="nav-link" href="https://tianxiawuhao.github.io">é¦–é¡µ</a>
          
        </li>
        
        <li class="nav-item">
          
          <a class="nav-link" href="/archives">å½’æ¡£</a>
          
        </li>
        
        <li class="nav-item">
          
          <a class="nav-link" href="/tags">æ ‡ç­¾</a>
          
        </li>
        
        <li class="nav-item">
          <div class="gridea-search-container">
            <form id="gridea-search-form" style="position: relative" data-update="1742729663519"
              action="/search/index.html">
              <input class="search-input" autocomplete="off" spellcheck="false" name="q" placeholder="æœç´¢æ–‡ç« " />
              <i class="fas fa-search gt-c-content-color-first" style="position: absolute; top: 9px; left: 10px;"></i>
            </form>
          </div>
        </li>
      </ul>
    </div>
  </div>
</nav>
  <!-- Page Header -->
  <nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
  <div class="container">
    <a class="navbar-brand" href="https://tianxiawuhao.github.io">tianxia</a>
    <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse"
      data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false"
      aria-label="Toggle navigation">
      Menu
      <i class="fas fa-bars"></i>
    </button>
    <div class="collapse navbar-collapse" id="navbarResponsive">
      <ul class="navbar-nav ml-auto">
        
        <li class="nav-item">
          
          <a class="nav-link" href="https://tianxiawuhao.github.io">é¦–é¡µ</a>
          
        </li>
        
        <li class="nav-item">
          
          <a class="nav-link" href="/archives">å½’æ¡£</a>
          
        </li>
        
        <li class="nav-item">
          
          <a class="nav-link" href="/tags">æ ‡ç­¾</a>
          
        </li>
        
        <li class="nav-item">
          <div class="gridea-search-container">
            <form id="gridea-search-form" style="position: relative" data-update="1742729663519"
              action="/search/index.html">
              <input class="search-input" autocomplete="off" spellcheck="false" name="q" placeholder="æœç´¢æ–‡ç« " />
              <i class="fas fa-search gt-c-content-color-first" style="position: absolute; top: 9px; left: 10px;"></i>
            </form>
          </div>
        </li>
      </ul>
    </div>
  </div>
</nav>
<header class="masthead" style="background-image: url('https://tianxiawuhao.github.io/media/images/home-bg.jpg')">
  <div class="overlay"></div>
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-md-10 mx-auto">
        
          <!-- æ²¡Titleä¸ºå…¶ä»–é¡µé¢Header -->
          
            <!-- æ²¡Titleå¹¶ä¸”æœ‰headerTypeä¸ºPostï¼šæ–‡ç« Header -->
            <div class="post-heading">
              <span class="tags">
                
                <a href="https://tianxiawuhao.github.io/PjnFH7MKT/" class="tag">æœºå™¨å­¦ä¹ </a>
                
              </span>
              <h1>LoRAï¼ˆLow-Rank Adaptationï¼‰</h1>
              <span class="meta">
                Posted on
                2025-02-26ï¼Œ35 min read
              </span>
            </div>
          
        
      </div>
    </div>
  </div>
</header>
  <!-- Post Content -->
  <article id="post-content-article">
    <div class="container">
      <div class="row">
        <div class="col-lg-8 col-md-10 mx-auto post-content-container">
          
          <img class="post-feature-header-image" src="https://tianxiawuhao.github.io/post-images/n7vyn_1QRC.png" alt="å°é¢å›¾">
          </img>
          
          <h3 id="lora-low-rank-adaptation"><strong>LoRA (Low-Rank Adaptation)</strong></h3>
<p>LoRAï¼ˆä½ç§©é€‚é…ï¼‰æ˜¯ä¸€ç§é«˜æ•ˆçš„å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆParameter-Efficient Fine-Tuning, PEFTï¼‰æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡ä½ç§©åˆ†è§£çš„æ–¹å¼å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚<br>
ç›¸æ¯”äºå…¨å‚æ•°å¾®è°ƒï¼ˆFull Fine-Tuningï¼‰ï¼ŒLoRA åªéœ€è¦æ›´æ–°å°‘é‡çš„å‚æ•°ã€‚<br>
LoRA çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼šåœ¨æ¨¡å‹çš„æƒé‡çŸ©é˜µä¸­å¼•å…¥ä½ç§©åˆ†è§£ï¼Œä»…å¯¹ä½ç§©éƒ¨åˆ†è¿›è¡Œæ›´æ–°ï¼Œè€Œä¿æŒåŸå§‹é¢„è®­ç»ƒæƒé‡ä¸å˜ã€‚<br>
ä»£ç å¯è§ä»¥ä¸‹ï¼Œå®Œå…¨ä»0å®ç°LoRAæµç¨‹ï¼Œä¸ä¾èµ–ç¬¬ä¸‰æ–¹åº“çš„å°è£…ã€‚</p>
<pre><code class="language-python">import os  # æ–‡ä»¶æ“ä½œ
import argparse  # å‘½ä»¤è¡Œå‚æ•°è§£æ
import time  # æ—¶é—´ç®¡ç†
import math # æ•°å­¦è®¡ç®—
import warnings # å¿½ç•¥è­¦å‘Šä¿¡æ¯
import torch.distributed as dist # åˆ†å¸ƒå¼è®­ç»ƒ
from contextlib import nullcontext # é»˜è®¤ä¸Šä¸‹æ–‡ç®¡ç†
from torch.utils.data import DataLoader, DistributedSampler  # æ•°æ®åŠ è½½å’Œåˆ†å¸ƒå¼é‡‡æ ·
from transformers import AutoTokenizer # åŠ è½½é¢„è®­ç»ƒçš„åˆ†è¯å™¨
from model.model import MiniMindLM # æ¨¡å‹æ¶æ„
from model.LMConfig import LMConfig # é…ç½®
from model.dataset import SFTDataset #æ•°æ®é›†
from model.model_lora import * # LoRAæ–¹æ³•
from torch import optim, nn

# å¿½ç•¥è­¦å‘Šä¿¡æ¯
warnings.filterwarnings('ignore')


# Logger function
# åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼Œåªæœ‰ä¸»è¿›ç¨‹ï¼ˆrank=0ï¼‰ä¼šæ‰“å°æ—¥å¿—ä¿¡æ¯ï¼Œé¿å…å¤šä¸ªè¿›ç¨‹é‡å¤è¾“å‡ºã€‚å¦‚æœä¸æ˜¯åˆ†å¸ƒå¼è®­ç»ƒï¼ˆddp=Falseï¼‰ï¼Œåˆ™ç›´æ¥æ‰“å°æ—¥å¿—ã€‚
def Logger(content):
    if not ddp or dist.get_rank() == 0:
        print(content)

# ä½¿ç”¨ä½™å¼¦é€€ç«ç­–ç•¥åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡ã€‚å…¬å¼ä¸ºï¼š
def get_lr(current_step, total_steps, lr):
    return lr / 10 + 0.5 * lr * (1 + math.cos(math.pi * current_step / total_steps))


# ä»£ç å’Œfull_sftã€Œå‡ ä¹ã€ä¸€è‡´
# ä½œç”¨ ï¼šå®šä¹‰ä¸€ä¸ªè®­ç»ƒepochçš„å‡½æ•°ã€‚
# å‚æ•° ï¼š
# epochï¼šå½“å‰è®­ç»ƒçš„epochç¼–å·ï¼ˆä»0å¼€å§‹ï¼‰ã€‚
# wandbï¼šWandBæ—¥å¿—è®°å½•å·¥å…·å¯¹è±¡ï¼Œç”¨äºè®°å½•è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŒ‡æ ‡ã€‚å¦‚æœæœªå¯ç”¨WandBï¼Œåˆ™ä¸ºNoneã€‚
def train_epoch(epoch, wandb):
    # ä½œç”¨ ï¼šå®šä¹‰äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼Œreduction='none' è¡¨ç¤ºä¸å¯¹æŸå¤±è¿›è¡Œå½’çº¦æ“ä½œï¼ˆå³è¿”å›æ¯ä¸ªæ ·æœ¬çš„æŸå¤±å€¼ï¼Œè€Œä¸æ˜¯å¹³å‡æˆ–æ€»å’Œï¼‰ã€‚
    # ç”¨é€” ï¼šåç»­ä¼šæ ¹æ® loss_mask å¯¹æŸå¤±è¿›è¡ŒåŠ æƒæ±‚å’Œï¼Œå› æ­¤éœ€è¦é€æ ·æœ¬è®¡ç®—æŸå¤±ã€‚
    # æ‰©å±• ï¼šå¯ä»¥æ ¹æ®ä»»åŠ¡éœ€æ±‚é€‰æ‹©å…¶ä»–æŸå¤±å‡½æ•°ï¼Œå¦‚ nn.MSELoss æˆ–è‡ªå®šä¹‰æŸå¤±å‡½æ•°ã€‚
    loss_fct = nn.CrossEntropyLoss(reduction='none')
    # ä½œç”¨ ï¼šè®°å½•å½“å‰æ—¶é—´æˆ³ï¼Œç”¨äºè®¡ç®—è®­ç»ƒè€—æ—¶ã€‚
    # ç”¨é€” ï¼šåœ¨æ—¥å¿—ä¸­æ˜¾ç¤ºæ¯ä¸€æ­¥æˆ–æ¯ä¸ªepochçš„è®­ç»ƒæ—¶é—´ã€‚
    # æ‰©å±• ï¼šå¯ä»¥ç»“åˆæ›´ç²¾ç»†çš„æ—¶é—´ç®¡ç†å·¥å…·ï¼ˆå¦‚ timeit æ¨¡å—ï¼‰æ¥ä¼˜åŒ–æ€§èƒ½åˆ†æã€‚
    start_time = time.time()
    # ä½œç”¨ ï¼šéå†è®­ç»ƒæ•°æ®åŠ è½½å™¨ train_loaderï¼Œæ¯æ¬¡è¿­ä»£è·å–ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®ã€‚
    # stepï¼šå½“å‰batchçš„ç´¢å¼•ã€‚
    # Xï¼šè¾“å…¥æ•°æ®ï¼ˆé€šå¸¸æ˜¯tokenizedçš„æ–‡æœ¬åºåˆ—ï¼‰ã€‚
    # Yï¼šç›®æ ‡æ ‡ç­¾ï¼ˆé€šå¸¸æ˜¯ä¸‹ä¸€ä¸ªtokençš„é¢„æµ‹ç›®æ ‡ï¼‰ã€‚
    # loss_maskï¼šç”¨äºå±è”½æ— æ•ˆtokençš„æ©ç ï¼ˆå¦‚å¡«å……tokenæˆ–ç‰¹æ®Šæ ‡è®°ï¼‰ã€‚
    # ç”¨é€” ï¼šé€æ‰¹æ¬¡å¤„ç†æ•°æ®å¹¶æ›´æ–°æ¨¡å‹å‚æ•°ã€‚
    # æ‰©å±• ï¼šå¯ä»¥é€šè¿‡ DataLoader çš„ collate_fn è‡ªå®šä¹‰æ•°æ®é¢„å¤„ç†é€»è¾‘ã€‚
    for step, (X, Y, loss_mask) in enumerate(train_loader):
        # ä½œç”¨ ï¼šå°†è¾“å…¥æ•°æ®ã€ç›®æ ‡æ ‡ç­¾å’ŒæŸå¤±æ©ç ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡ï¼ˆå¦‚GPUæˆ–CPUï¼‰ã€‚
        # ç”¨é€” ï¼šç¡®ä¿æ•°æ®å’Œæ¨¡å‹ä½äºåŒä¸€è®¾å¤‡ä¸Šï¼Œé¿å…è¿è¡Œæ—¶é”™è¯¯ã€‚
        # æ‰©å±• ï¼šå¯ä»¥ä½¿ç”¨ .pin_memory() æå‰å°†æ•°æ®é”å®šåœ¨å†…å­˜ä¸­ï¼Œä»¥åŠ é€ŸGPUæ•°æ®ä¼ è¾“ã€‚
        X = X.to(args.device)
        Y = Y.to(args.device)
        loss_mask = loss_mask.to(args.device)
        # ä½œç”¨ ï¼š
        # è°ƒç”¨ get_lr å‡½æ•°åŠ¨æ€è®¡ç®—å½“å‰çš„å­¦ä¹ ç‡ã€‚
        # å°†è®¡ç®—å¾—åˆ°çš„å­¦ä¹ ç‡åº”ç”¨åˆ°ä¼˜åŒ–å™¨çš„æ‰€æœ‰å‚æ•°ç»„ã€‚
        # ç”¨é€” ï¼šå®ç°å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥ï¼ˆå¦‚ä½™å¼¦é€€ç«ï¼‰ã€‚
        # æ‰©å±• ï¼š
        # å¯ä»¥æ›¿æ¢ä¸ºå…¶ä»–å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆå¦‚ torch.optim.lr_scheduler ä¸­çš„ StepLR æˆ– CosineAnnealingLRï¼‰ã€‚
        # æ”¯æŒå¤šé˜¶æ®µå­¦ä¹ ç‡è°ƒåº¦ï¼ˆå¦‚Warmup + Decayï¼‰ã€‚
        lr = get_lr(epoch * iter_per_epoch + step, args.epochs * iter_per_epoch, args.learning_rate)
        for param_group in optimizer.param_groups:
            param_group['lr'] = lr

        # ä½œç”¨ ï¼š
        # ä¸Šä¸‹æ–‡ç®¡ç†å™¨ ctx ï¼šå¦‚æœä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆå¦‚FP16ï¼‰ï¼Œåˆ™å¯ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦ï¼ˆAMPï¼‰ã€‚
        # å‰å‘ä¼ æ’­ ï¼šè°ƒç”¨æ¨¡å‹ model(X) è¿›è¡Œå‰å‘ä¼ æ’­ï¼Œè¾“å‡ºç»“æœå­˜å‚¨åœ¨ res ä¸­ã€‚
        # è®¡ç®—æŸå¤± ï¼š
        # ä½¿ç”¨ CrossEntropyLoss è®¡ç®—é€æ ·æœ¬æŸå¤±ã€‚
        # æ ¹æ® loss_mask åŠ æƒæ±‚å’Œï¼Œå¿½ç•¥æ— æ•ˆtokençš„æŸå¤±ã€‚
        # æ·»åŠ è¾…åŠ©æŸå¤±ï¼ˆaux_lossï¼‰ï¼Œé€šå¸¸ç”¨äºæ­£åˆ™åŒ–æˆ–å…¶ä»–ç›®æ ‡ã€‚
        # å°†æ€»æŸå¤±é™¤ä»¥ç´¯ç§¯æ­¥æ•°ï¼ˆaccumulation_stepsï¼‰ï¼Œä»¥ä¾¿æ¢¯åº¦ç´¯ç§¯åæ›´æ–°ã€‚
        # ç”¨é€” ï¼šå®Œæˆä¸€æ¬¡å‰å‘ä¼ æ’­å¹¶è®¡ç®—æœ€ç»ˆæŸå¤±ã€‚
        # æ‰©å±• ï¼š
        # å¦‚æœä»»åŠ¡å¤æ‚ï¼Œå¯ä»¥æ·»åŠ æ›´å¤šæŸå¤±é¡¹ï¼ˆå¦‚KLæ•£åº¦ã€å¯¹æ¯”æŸå¤±ç­‰ï¼‰ã€‚
        # æ”¯æŒå¤šç§æŸå¤±æƒé‡çš„åŠ¨æ€è°ƒæ•´ã€‚
        with ctx:
            res = model(X)
            loss = loss_fct(
                res.logits.view(-1, res.logits.size(-1)),
                Y.view(-1)
            ).view(Y.size())
            loss = (loss * loss_mask).sum() / loss_mask.sum()
            loss += res.aux_loss
            loss = loss / args.accumulation_steps
        # ä½œç”¨ ï¼š
        # åå‘ä¼ æ’­ ï¼šä½¿ç”¨ scaler.scale(loss).backward() è®¡ç®—æ¢¯åº¦ï¼ˆæ”¯æŒæ··åˆç²¾åº¦ï¼‰ã€‚
        # æ¢¯åº¦ç´¯ç§¯ ï¼šæ¯éš” accumulation_steps æ­¥æ‰§è¡Œä¸€æ¬¡æ¢¯åº¦æ›´æ–°ã€‚
        # æ¢¯åº¦è£å‰ª ï¼šé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼Œé™åˆ¶æ¢¯åº¦èŒƒæ•°ä¸è¶…è¿‡ grad_clipã€‚
        # ä¼˜åŒ–å™¨æ›´æ–° ï¼šè°ƒç”¨ scaler.step(optimizer) æ›´æ–°æ¨¡å‹å‚æ•°ã€‚
        # é‡ç½®æ¢¯åº¦ ï¼šè°ƒç”¨ optimizer.zero_grad(set_to_none=True) æ¸…ç©ºæ¢¯åº¦ã€‚
        # ç”¨é€” ï¼šå®ç°æ¢¯åº¦ç´¯ç§¯å’Œæ··åˆç²¾åº¦è®­ç»ƒï¼Œæé«˜è®­ç»ƒæ•ˆç‡ã€‚
        # æ‰©å±• ï¼š
        # æ”¯æŒåŠ¨æ€è°ƒæ•´ accumulation_stepsï¼Œä»¥é€‚åº”ä¸åŒç¡¬ä»¶èµ„æºã€‚
        # å¯ä»¥æ›¿æ¢ä¸ºå…¶ä»–ä¼˜åŒ–å™¨ï¼ˆå¦‚LAMBã€Rangerç­‰ï¼‰ã€‚
        scaler.scale(loss).backward()
        if (step + 1) % args.accumulation_steps == 0:
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(lora_params, args.grad_clip)

            scaler.step(optimizer)
            scaler.update()

            optimizer.zero_grad(set_to_none=True)

        # ä½œç”¨ ï¼š
        # æ¯éš” log_interval æ­¥è®°å½•ä¸€æ¬¡è®­ç»ƒæ—¥å¿—ã€‚
        # æ‰“å°å½“å‰epochã€stepã€æŸå¤±ã€å­¦ä¹ ç‡å’Œé¢„è®¡å‰©ä½™æ—¶é—´ã€‚
        # å¦‚æœå¯ç”¨äº†WandBï¼Œå°†æ—¥å¿—ä¸Šä¼ åˆ°äº‘ç«¯ã€‚
        # ç”¨é€” ï¼šç›‘æ§è®­ç»ƒè¿›åº¦ï¼Œä¾¿äºè°ƒè¯•å’Œåˆ†æã€‚
        # æ‰©å±• ï¼š
        # å¯ä»¥è®°å½•æ›´å¤šæŒ‡æ ‡ï¼ˆå¦‚å‡†ç¡®ç‡ã€F1åˆ†æ•°ç­‰ï¼‰ã€‚
        # æ”¯æŒä¿å­˜æ—¥å¿—åˆ°æœ¬åœ°æ–‡ä»¶æˆ–æ•°æ®åº“ã€‚
        if step % args.log_interval == 0:
            spend_time = time.time() - start_time
            Logger(
                'Epoch:[{}/{}]({}/{}) loss:{:.3f} lr:{:.12f} epoch_Time:{}min:'.format(
                    epoch + 1,
                    args.epochs,
                    step,
                    iter_per_epoch,
                    loss.item(),
                    optimizer.param_groups[-1]['lr'],
                    spend_time / (step + 1) * iter_per_epoch // 60 - spend_time // 60))

            if (wandb is not None) and (not ddp or dist.get_rank() == 0):
                wandb.log({&quot;loss&quot;: loss,
                           &quot;lr&quot;: optimizer.param_groups[-1]['lr'],
                           &quot;epoch_Time&quot;: spend_time / (step + 1) * iter_per_epoch // 60 - spend_time // 60})

        # ä½œç”¨ ï¼š
        # æ¯éš” save_interval æ­¥ä¿å­˜ä¸€æ¬¡æ¨¡å‹ã€‚
        # ä»…ä¿å­˜LoRAæƒé‡ï¼ˆåŒºåˆ«äºå®Œæ•´æ¨¡å‹æƒé‡ï¼‰ã€‚
        # åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼Œåªæœ‰ä¸»è¿›ç¨‹ï¼ˆrank=0ï¼‰æ‰§è¡Œä¿å­˜æ“ä½œã€‚
        # ç”¨é€” ï¼šå®šæœŸä¿å­˜æ¨¡å‹ï¼Œé˜²æ­¢è®­ç»ƒä¸­æ–­å¯¼è‡´æ•°æ®ä¸¢å¤±ã€‚
        # æ‰©å±• ï¼š
        # æ”¯æŒå¢é‡ä¿å­˜ï¼ˆä»…ä¿å­˜æ–°å¢çš„æƒé‡å˜åŒ–ï¼‰ã€‚
        # å¯ä»¥æ ¹æ®éªŒè¯é›†æ€§èƒ½ä¿å­˜æœ€ä½³æ¨¡å‹ã€‚
        if (step + 1) % args.save_interval == 0 and (not ddp or dist.get_rank() == 0):
            model.eval()
            # ã€åŒºåˆ«1ã€‘åªä¿å­˜loraæƒé‡å³å¯
            save_lora(model, f'{args.save_dir}/lora/{args.lora_name}_{lm_config.dim}.pth')
            model.train()


def init_model(lm_config):
    # ä½œç”¨ ï¼šåŠ è½½é¢„è®­ç»ƒåˆ†è¯å™¨ï¼ˆAutoTokenizerï¼‰ã€‚
    # ç”¨é€” ï¼šç”¨äºå°†è¾“å…¥æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯æ¥å—çš„tokenåºåˆ—ã€‚
    # æ‰©å±• ï¼š
    # å¯ä»¥æ ¹æ®ä»»åŠ¡éœ€æ±‚é€‰æ‹©ä¸åŒçš„åˆ†è¯å™¨ï¼ˆå¦‚BERTã€GPTç­‰ï¼‰ã€‚
    # æ”¯æŒè‡ªå®šä¹‰è¯æ±‡è¡¨æˆ–ç‰¹æ®Šæ ‡è®°ã€‚
    tokenizer = AutoTokenizer.from_pretrained('./model/minimind_tokenizer')
    # ä½œç”¨ ï¼šåˆå§‹åŒ–æ¨¡å‹ MiniMindLMï¼Œä½¿ç”¨é…ç½®å¯¹è±¡ lm_configã€‚
    # ç”¨é€” ï¼šåˆ›å»ºä¸€ä¸ªè¯­è¨€æ¨¡å‹å®ä¾‹ã€‚
    # æ‰©å±• ï¼š
    # å¯ä»¥åŠ¨æ€è°ƒæ•´æ¨¡å‹ç»“æ„ï¼ˆå¦‚å±‚æ•°ã€éšè—ç»´åº¦ç­‰ï¼‰ã€‚
    # æ”¯æŒåŠ è½½ä¸åŒç‰ˆæœ¬çš„é¢„è®­ç»ƒæ¨¡å‹æƒé‡ã€‚
    model = MiniMindLM(lm_config)
    # ä½œç”¨ ï¼š
    # æ ¹æ®æ˜¯å¦å¯ç”¨ MoEï¼ˆMixture of Expertsï¼‰ï¼Œç”Ÿæˆæ£€æŸ¥ç‚¹è·¯å¾„ã€‚
    # æ£€æŸ¥ç‚¹æ–‡ä»¶ååŒ…å«æ¨¡å‹ç»´åº¦å’ŒMoEæ ‡å¿—ã€‚
    # ç”¨é€” ï¼šåŠ è½½ä¸å½“å‰æ¨¡å‹é…ç½®åŒ¹é…çš„é¢„è®­ç»ƒæƒé‡ã€‚
    # æ‰©å±• ï¼š
    # æ”¯æŒåŠ¨æ€ç”Ÿæˆæ£€æŸ¥ç‚¹è·¯å¾„ï¼Œé€‚é…å¤šç§æ¨¡å‹é…ç½®ã€‚
    # å¯ä»¥æ ¹æ®ä»»åŠ¡ç±»å‹ï¼ˆå¦‚åˆ†ç±»ã€ç”Ÿæˆï¼‰åŠ è½½ä¸åŒçš„æƒé‡ã€‚
    moe_path = '_moe' if lm_config.use_moe else ''
    ckp = f'./out/rlhf_{lm_config.dim}{moe_path}.pth'
    # ä½œç”¨ ï¼š
    # åŠ è½½é¢„è®­ç»ƒæƒé‡åˆ°æ¨¡å‹ä¸­ã€‚
    # ä½¿ç”¨ strict=False å…è®¸éƒ¨åˆ†æƒé‡ä¸åŒ¹é…ï¼ˆå¦‚æ–°å¢æˆ–åˆ é™¤å±‚ï¼‰ã€‚
    # ç”¨é€” ï¼šåˆå§‹åŒ–æ¨¡å‹å‚æ•°ï¼Œé¿å…ä»é›¶å¼€å§‹è®­ç»ƒã€‚
    # æ‰©å±• ï¼š
    # å¯ä»¥å®ç°å¢é‡åŠ è½½ï¼ˆä»…åŠ è½½éƒ¨åˆ†æƒé‡ï¼‰ã€‚
    # æ”¯æŒå¤šé˜¶æ®µè®­ç»ƒï¼ˆå¦‚å…ˆåŠ è½½åŸºç¡€æ¨¡å‹ï¼Œå†å¾®è°ƒç‰¹å®šä»»åŠ¡ï¼‰ã€‚
    state_dict = torch.load(ckp, map_location=args.device)
    model.load_state_dict(state_dict, strict=False)
    # ä½œç”¨ ï¼šå°†æ¨¡å‹ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡ï¼ˆå¦‚GPUæˆ–CPUï¼‰ï¼Œå¹¶è¿”å›æ¨¡å‹å’Œåˆ†è¯å™¨ã€‚
    # ç”¨é€” ï¼šç¡®ä¿æ¨¡å‹å’Œæ•°æ®ä½äºåŒä¸€è®¾å¤‡ä¸Šã€‚
    # æ‰©å±• ï¼š
    # æ”¯æŒå¤šè®¾å¤‡åˆ†å¸ƒå¼è®­ç»ƒï¼ˆå¦‚æ¨¡å‹å¹¶è¡Œï¼‰ã€‚
    return model.to(args.device), tokenizer


def init_distributed_mode():
    # ä½œç”¨ ï¼šå¦‚æœæœªå¯ç”¨åˆ†å¸ƒå¼è®­ç»ƒï¼ˆddp=Falseï¼‰ï¼Œç›´æ¥è¿”å›ã€‚
    # ç”¨é€” ï¼šé¿å…ä¸å¿…è¦çš„åˆ†å¸ƒå¼åˆå§‹åŒ–æ“ä½œã€‚
    # æ‰©å±• ï¼š
    # æ”¯æŒå…¶ä»–åˆ†å¸ƒå¼åç«¯ï¼ˆå¦‚glooã€mpiï¼‰ã€‚
    if not ddp: return
    # ä½œç”¨ ï¼šåˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒï¼Œä½¿ç”¨NCCLåç«¯ï¼ˆé€‚ç”¨äºCUDAï¼‰ã€‚
    # ç”¨é€” ï¼šè®¾ç½®åˆ†å¸ƒå¼é€šä¿¡ç»„ã€‚
    # æ‰©å±• ï¼š
    # æ”¯æŒåŠ¨æ€é€‰æ‹©åç«¯ï¼ˆå¦‚ncclã€glooï¼‰ã€‚
    # å¯ä»¥å®ç°æ›´å¤æ‚çš„åˆ†å¸ƒå¼ç­–ç•¥ï¼ˆå¦‚æ¨¡å‹å¹¶è¡Œï¼‰ã€‚
    global ddp_local_rank, DEVICE
    dist.init_process_group(backend=&quot;nccl&quot;)
    # ä½œç”¨ ï¼šä»ç¯å¢ƒå˜é‡ä¸­è·å–åˆ†å¸ƒå¼è®­ç»ƒçš„ç›¸å…³ä¿¡æ¯ã€‚
    # RANKï¼šå½“å‰è¿›ç¨‹çš„å…¨å±€rankã€‚
    # LOCAL_RANKï¼šå½“å‰è¿›ç¨‹åœ¨æœ¬åœ°èŠ‚ç‚¹çš„rankã€‚
    # WORLD_SIZEï¼šæ€»è¿›ç¨‹æ•°ã€‚
    # ç”¨é€” ï¼šç¡®å®šæ¯ä¸ªè¿›ç¨‹çš„è§’è‰²å’Œä½ç½®ã€‚
    # æ‰©å±• ï¼š
    # æ”¯æŒåŠ¨æ€è°ƒæ•´åˆ†å¸ƒå¼é…ç½®ï¼ˆå¦‚è¿›ç¨‹æ•°ã€èŠ‚ç‚¹æ•°ï¼‰ã€‚
    ddp_rank = int(os.environ[&quot;RANK&quot;])
    ddp_local_rank = int(os.environ[&quot;LOCAL_RANK&quot;])
    ddp_world_size = int(os.environ[&quot;WORLD_SIZE&quot;])
    # ä½œç”¨ ï¼šå°†å½“å‰è¿›ç¨‹ç»‘å®šåˆ°æŒ‡å®šçš„GPUè®¾å¤‡ã€‚
    # ç”¨é€” ï¼šç¡®ä¿æ¯ä¸ªè¿›ç¨‹ä½¿ç”¨ç‹¬ç«‹çš„GPUèµ„æºã€‚
    # æ‰©å±• ï¼š
    # æ”¯æŒå¤šGPUå…±äº«ï¼ˆå¦‚æ··åˆç²¾åº¦è®­ç»ƒï¼‰ã€‚
    DEVICE = f&quot;cuda:{ddp_local_rank}&quot;
    torch.cuda.set_device(DEVICE)

# ä½œç”¨ ï¼šå®šä¹‰ç¨‹åºå…¥å£ï¼Œç¡®ä¿ä»£ç åªåœ¨ç›´æ¥è¿è¡Œæ—¶æ‰§è¡Œã€‚
# ç”¨é€” ï¼šæ¨¡å—åŒ–è®¾è®¡ï¼Œä¾¿äºå¤ç”¨å’Œæµ‹è¯•ã€‚
# æ‰©å±• ï¼š
# æ”¯æŒå‘½ä»¤è¡Œå‚æ•°è§£æå’ŒåŠ¨æ€é…ç½®ã€‚
if __name__ == &quot;__main__&quot;:
    # ä½œç”¨ ï¼šåˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨ã€‚
    # ç”¨é€” ï¼šé€šè¿‡å‘½ä»¤è¡Œä¼ é€’è¶…å‚æ•°ï¼Œçµæ´»æ§åˆ¶è®­ç»ƒè¿‡ç¨‹ã€‚
    # æ‰©å±• ï¼š
    # æ”¯æŒä»é…ç½®æ–‡ä»¶åŠ è½½å‚æ•°ã€‚
    # åŠ¨æ€éªŒè¯å‚æ•°åˆæ³•æ€§ã€‚
    parser = argparse.ArgumentParser(description=&quot;MiniMind SFT with LoRA&quot;)
    # ä½œç”¨ ï¼šå®šä¹‰è¾“å‡ºç›®å½•è·¯å¾„ï¼Œé»˜è®¤å€¼ä¸ºoutã€‚
    # ç”¨é€” ï¼šä¿å­˜æ¨¡å‹æƒé‡å’Œæ—¥å¿—æ–‡ä»¶ã€‚
    # æ‰©å±• ï¼š
    # æ”¯æŒè‡ªåŠ¨åˆ›å»ºç›®å½•ã€‚
    # åŠ¨æ€ç”Ÿæˆå­ç›®å½•ï¼ˆå¦‚æŒ‰æ—¥æœŸæˆ–ä»»åŠ¡ç±»å‹ï¼‰ã€‚
    parser.add_argument(&quot;--out_dir&quot;, type=str, default=&quot;out&quot;)
    # ä½œç”¨ ï¼šå®šä¹‰è®­ç»ƒè½®æ•°ï¼Œé»˜è®¤å€¼ä¸º50ã€‚
    # ç”¨é€” ï¼šæ§åˆ¶è®­ç»ƒæ—¶é•¿ã€‚
    # æ‰©å±• ï¼š
    # æ”¯æŒåŠ¨æ€è°ƒæ•´è®­ç»ƒè½®æ•°ï¼ˆå¦‚æ—©åœæœºåˆ¶ï¼‰ã€‚
    parser.add_argument(&quot;--epochs&quot;, type=int, default=50)
    # ä½œç”¨ ï¼šå®šä¹‰æ‰¹æ¬¡å¤§å°ï¼Œé»˜è®¤å€¼ä¸º16ã€‚
    # ç”¨é€” ï¼šæ§åˆ¶æ¯æ¬¡è¿­ä»£çš„æ•°æ®é‡ã€‚
    # æ‰©å±• ï¼š
    # åŠ¨æ€è°ƒæ•´æ‰¹æ¬¡å¤§å°ä»¥é€‚åº”ç¡¬ä»¶èµ„æºã€‚
    parser.add_argument(&quot;--batch_size&quot;, type=int, default=16)
    # ä½œç”¨ ï¼šå®šä¹‰å­¦ä¹ ç‡ï¼Œé»˜è®¤å€¼ä¸º5e-5ã€‚
    # ç”¨é€” ï¼šæ§åˆ¶ä¼˜åŒ–å™¨çš„æ­¥é•¿ã€‚
    # æ‰©å±• ï¼š
    # æ”¯æŒå­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥ï¼ˆå¦‚ä½™å¼¦é€€ç«ã€Warmupï¼‰ã€‚
    parser.add_argument(&quot;--learning_rate&quot;, type=float, default=5e-5)
    # ä½œç”¨ ï¼šå®šä¹‰è®¾å¤‡ç±»å‹ï¼Œé»˜è®¤ä¼˜å…ˆä½¿ç”¨GPUã€‚
    # ç”¨é€” ï¼šæŒ‡å®šæ¨¡å‹å’Œæ•°æ®æ‰€åœ¨çš„è®¾å¤‡ã€‚
    # æ‰©å±• ï¼š
    # æ”¯æŒå¤šè®¾å¤‡åˆ‡æ¢ï¼ˆå¦‚CPUã€GPUã€TPUï¼‰ã€‚
    parser.add_argument(&quot;--device&quot;, type=str, default=&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
    # ä½œç”¨ ï¼šå®šä¹‰æ•°æ®ç±»å‹ï¼Œé»˜è®¤å€¼ä¸ºbfloat16ã€‚
    # ç”¨é€” ï¼šæ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒã€‚
    # æ‰©å±• ï¼š
    # åŠ¨æ€è°ƒæ•´ç²¾åº¦ï¼ˆå¦‚FP32ã€FP16ï¼‰ã€‚
    parser.add_argument(&quot;--dtype&quot;, type=str, default=&quot;bfloat16&quot;)
    # ä½œç”¨ ï¼šå¯ç”¨WandBæ—¥å¿—è®°å½•ã€‚
    # ç”¨é€” ï¼šç›‘æ§è®­ç»ƒè¿‡ç¨‹ã€‚
    # æ‰©å±• ï¼š
    # æ”¯æŒå…¶ä»–æ—¥å¿—å·¥å…·ï¼ˆå¦‚TensorBoardï¼‰ã€‚
    parser.add_argument(&quot;--use_wandb&quot;, action=&quot;store_true&quot;)
    # ä½œç”¨ ï¼šå®šä¹‰WandBé¡¹ç›®åç§°ã€‚
    # ç”¨é€” ï¼šç»„ç»‡å’Œç®¡ç†å®éªŒã€‚
    # æ‰©å±• ï¼š
    # åŠ¨æ€ç”Ÿæˆé¡¹ç›®åç§°ï¼ˆå¦‚æŒ‰ä»»åŠ¡ç±»å‹ï¼‰ã€‚
    parser.add_argument(&quot;--wandb_project&quot;, type=str, default=&quot;MiniMind-LoRA-SFT&quot;)
    # ä½œç”¨ ï¼šå®šä¹‰æ•°æ®åŠ è½½çº¿ç¨‹æ•°ï¼Œé»˜è®¤å€¼ä¸º1ã€‚
    # ç”¨é€” ï¼šåŠ é€Ÿæ•°æ®åŠ è½½ã€‚
    # æ‰©å±• ï¼š
    # åŠ¨æ€è°ƒæ•´çº¿ç¨‹æ•°ä»¥ä¼˜åŒ–æ€§èƒ½ã€‚
    parser.add_argument(&quot;--num_workers&quot;, type=int, default=1)
    # ä½œç”¨ ï¼šå¯ç”¨åˆ†å¸ƒå¼è®­ç»ƒã€‚
    # ç”¨é€” ï¼šæ”¯æŒå¤šGPUæˆ–å¤šèŠ‚ç‚¹è®­ç»ƒã€‚
    # æ‰©å±• ï¼š
    # æ”¯æŒåŠ¨æ€åˆ†é…èµ„æºã€‚
    parser.add_argument(&quot;--ddp&quot;, action=&quot;store_true&quot;)
    # ä½œç”¨ ï¼šå®šä¹‰æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼Œé»˜è®¤å€¼ä¸º1ã€‚
    # ç”¨é€” ï¼šæ¨¡æ‹Ÿæ›´å¤§çš„æ‰¹æ¬¡å¤§å°ã€‚
    # æ‰©å±• ï¼š
    # åŠ¨æ€è°ƒæ•´ç´¯ç§¯æ­¥æ•°ä»¥é€‚åº”æ˜¾å­˜é™åˆ¶ã€‚
    parser.add_argument(&quot;--accumulation_steps&quot;, type=int, default=1)
    # ä½œç”¨ ï¼šå®šä¹‰æ¢¯åº¦è£å‰ªé˜ˆå€¼ï¼Œé»˜è®¤å€¼ä¸º1.0ã€‚
    # ç”¨é€” ï¼šé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ã€‚
    # æ‰©å±• ï¼š
    # æ”¯æŒåŠ¨æ€è°ƒæ•´è£å‰ªé˜ˆå€¼ã€‚
    parser.add_argument(&quot;--grad_clip&quot;, type=float, default=1.0)
    # ä½œç”¨ ï¼šå®šä¹‰Warmupæ­¥æ•°ï¼Œé»˜è®¤å€¼ä¸º0ã€‚
    # ç”¨é€” ï¼šé€æ­¥å¢åŠ å­¦ä¹ ç‡ã€‚
    # æ‰©å±• ï¼š
    # æ”¯æŒå¤šç§Warmupç­–ç•¥ï¼ˆå¦‚çº¿æ€§ã€æŒ‡æ•°ï¼‰ã€‚
    parser.add_argument(&quot;--warmup_iters&quot;, type=int, default=0)
    # ä½œç”¨ ï¼šå®šä¹‰æ—¥å¿—è®°å½•é—´éš”ï¼Œé»˜è®¤å€¼ä¸º100ã€‚
    # ç”¨é€” ï¼šå®šæœŸæ‰“å°è®­ç»ƒè¿›åº¦ã€‚
    # æ‰©å±• ï¼š
    # æ”¯æŒåŠ¨æ€è°ƒæ•´æ—¥å¿—é¢‘ç‡ã€‚
    parser.add_argument(&quot;--log_interval&quot;, type=int, default=100)
    # ä½œç”¨ ï¼šå®šä¹‰æ¨¡å‹ä¿å­˜é—´éš”ï¼Œé»˜è®¤å€¼ä¸º1ã€‚
    # ç”¨é€” ï¼šå®šæœŸä¿å­˜æ¨¡å‹æƒé‡ã€‚
    # æ‰©å±• ï¼š
    # æ”¯æŒå¢é‡ä¿å­˜ã€‚
    parser.add_argument(&quot;--save_interval&quot;, type=int, default=1)
    # ä½œç”¨ ï¼šå®šä¹‰æœ¬åœ°rankï¼Œé»˜è®¤å€¼ä¸º-1ã€‚
    # ç”¨é€” ï¼šåˆ†å¸ƒå¼è®­ç»ƒä¸­çš„è¿›ç¨‹æ ‡è¯†ã€‚
    # æ‰©å±• ï¼š
    # æ”¯æŒåŠ¨æ€åˆ†é…rankã€‚
    parser.add_argument('--local_rank', type=int, default=-1)
    # ä½œç”¨ ï¼šå®šä¹‰æ¨¡å‹ç»´åº¦ï¼Œé»˜è®¤å€¼ä¸º512ã€‚
    # ç”¨é€” ï¼šæ§åˆ¶æ¨¡å‹å®¹é‡ã€‚
    # æ‰©å±• ï¼š
    # æ”¯æŒåŠ¨æ€è°ƒæ•´ç»´åº¦ã€‚
    parser.add_argument('--dim', default=512, type=int)
    # ä½œç”¨ ï¼šå®šä¹‰æ¨¡å‹å±‚æ•°ï¼Œé»˜è®¤å€¼ä¸º8ã€‚
    # ç”¨é€” ï¼šæ§åˆ¶æ¨¡å‹æ·±åº¦ã€‚
    # æ‰©å±• ï¼š
    # æ”¯æŒåŠ¨æ€è°ƒæ•´å±‚æ•°ã€‚
    parser.add_argument('--n_layers', default=8, type=int)
    # ä½œç”¨ ï¼šå®šä¹‰æœ€å¤§åºåˆ—é•¿åº¦ï¼Œé»˜è®¤å€¼ä¸º512ã€‚
    # ç”¨é€” ï¼šé™åˆ¶è¾“å…¥åºåˆ—é•¿åº¦ã€‚
    # æ‰©å±• ï¼š
    # æ”¯æŒåŠ¨æ€æˆªæ–­æˆ–å¡«å……ã€‚
    parser.add_argument('--max_seq_len', default=512, type=int)
    # ä½œç”¨ ï¼šå¯ç”¨MoEï¼ˆMixture of Expertsï¼‰ï¼Œé»˜è®¤å€¼ä¸ºFalseã€‚
    # ç”¨é€” ï¼šæé«˜æ¨¡å‹æ•ˆç‡ã€‚
    # æ‰©å±• ï¼š
    # æ”¯æŒåŠ¨æ€è°ƒæ•´ä¸“å®¶æ•°é‡ã€‚
    parser.add_argument('--use_moe', default=False, type=bool)
    # ä½œç”¨ ï¼šå®šä¹‰æ•°æ®é›†è·¯å¾„ï¼Œé»˜è®¤å€¼ä¸º./dataset/lora_identity.jsonlã€‚
    # ç”¨é€” ï¼šåŠ è½½è®­ç»ƒæ•°æ®ã€‚
    # æ‰©å±• ï¼š
    # æ”¯æŒå¤šç§æ•°æ®æ ¼å¼ï¼ˆå¦‚CSVã€JSONï¼‰ã€‚
    parser.add_argument(&quot;--data_path&quot;, type=str, default=&quot;./dataset/lora_identity.jsonl&quot;)
    # ä½œç”¨ ï¼šå®šä¹‰LoRAæƒé‡æ–‡ä»¶åï¼Œé»˜è®¤å€¼ä¸ºlora_identityã€‚
    # ç”¨é€” ï¼šåŒºåˆ†ä¸åŒä»»åŠ¡çš„LoRAæƒé‡ã€‚
    # æ‰©å±• ï¼š
    # åŠ¨æ€ç”Ÿæˆæ–‡ä»¶åã€‚
    parser.add_argument(&quot;--lora_name&quot;, type=str, default=&quot;lora_identity&quot;, help=&quot;æ ¹æ®ä»»åŠ¡ä¿å­˜æˆlora_(è‹±æ–‡/åŒ»å­¦/å¿ƒç†...)&quot;)
    args = parser.parse_args()
    # ä½œç”¨ ï¼šåˆ›å»ºä¸€ä¸ª LMConfig å¯¹è±¡ï¼Œç”¨äºå®šä¹‰æ¨¡å‹çš„è¶…å‚æ•°ã€‚
    # dimï¼šéšè—å±‚ç»´åº¦ã€‚
    # n_layersï¼šæ¨¡å‹å±‚æ•°ã€‚
    # max_seq_lenï¼šæœ€å¤§åºåˆ—é•¿åº¦ã€‚
    # use_moeï¼šæ˜¯å¦å¯ç”¨ MoEï¼ˆMixture of Expertsï¼‰ã€‚
    # ç”¨é€” ï¼šå°†å‘½ä»¤è¡Œå‚æ•°ä¼ é€’ç»™æ¨¡å‹é…ç½®å¯¹è±¡ï¼Œç¡®ä¿æ¨¡å‹åˆå§‹åŒ–æ—¶ä½¿ç”¨æ­£ç¡®çš„è¶…å‚æ•°ã€‚
    # æ‰©å±• ï¼š
    # æ”¯æŒåŠ¨æ€è°ƒæ•´æ¨¡å‹é…ç½®ï¼ˆå¦‚é€šè¿‡ JSON æˆ– YAML æ–‡ä»¶åŠ è½½é…ç½®ï¼‰ã€‚
    # å¯ä»¥æ‰©å±•ä¸ºæ”¯æŒæ›´å¤šæ¨¡å‹ç±»å‹ï¼ˆå¦‚Transformerã€RNNç­‰ï¼‰ã€‚
    lm_config = LMConfig(dim=args.dim, n_layers=args.n_layers, max_seq_len=args.max_seq_len, use_moe=args.use_moe)
    # ä½œç”¨ ï¼š
    # å°†ä¿å­˜ç›®å½•è·¯å¾„è®¾ç½®ä¸º args.out_dir çš„å­ç›®å½•ã€‚
    # ä½¿ç”¨ os.makedirs ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨ã€‚å¦‚æœç›®å½•å·²å­˜åœ¨ï¼Œåˆ™ä¸ä¼šæŠ¥é”™ï¼ˆexist_ok=Trueï¼‰ã€‚
    # ç”¨é€” ï¼šä¸ºæ¨¡å‹æƒé‡å’Œæ—¥å¿—æ–‡ä»¶åˆ›å»ºå­˜å‚¨è·¯å¾„ã€‚
    # æ‰©å±• ï¼š
    # åŠ¨æ€ç”Ÿæˆå­ç›®å½•ï¼ˆå¦‚æŒ‰ä»»åŠ¡ç±»å‹æˆ–æ—¥æœŸï¼‰ã€‚
    # æ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒä¸­çš„ç‹¬ç«‹ä¿å­˜è·¯å¾„ï¼ˆå¦‚æŒ‰rankç¼–å·åŒºåˆ†ï¼‰ã€‚
    args.save_dir = os.path.join(args.out_dir)
    os.makedirs(args.save_dir, exist_ok=True)
    os.makedirs(args.out_dir, exist_ok=True)
    # ä½œç”¨ ï¼šè®¡ç®—æ¯ä¸ªæ‰¹æ¬¡ä¸­åŒ…å«çš„tokenæ€»æ•°ã€‚
    # ç”¨é€” ï¼šç›‘æ§è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ•°æ®ååé‡ã€‚
    # æ‰©å±• ï¼š
    # åŠ¨æ€è°ƒæ•´æ‰¹æ¬¡å¤§å°æˆ–åºåˆ—é•¿åº¦ä»¥ä¼˜åŒ–æ€§èƒ½ã€‚
    # æ”¯æŒå¤šé˜¶æ®µè®­ç»ƒï¼ˆå¦‚Warmupé˜¶æ®µé€æ­¥å¢åŠ tokenæ•°é‡ï¼‰ã€‚
    tokens_per_iter = args.batch_size * lm_config.max_seq_len
    # ä½œç”¨ ï¼šè®¾ç½®PyTorchçš„éšæœºç§å­ï¼Œç¡®ä¿å®éªŒç»“æœå¯å¤ç°ã€‚
    # ç”¨é€” ï¼šæ§åˆ¶éšæœºæ€§ï¼Œé¿å…å› éšæœºåˆå§‹åŒ–å¯¼è‡´çš„å®éªŒå·®å¼‚ã€‚
    # æ‰©å±• ï¼š
    # æ”¯æŒåŠ¨æ€è®¾ç½®éšæœºç§å­ï¼ˆå¦‚é€šè¿‡å‘½ä»¤è¡Œå‚æ•°ä¼ é€’ï¼‰ã€‚
    # ç¡®ä¿å…¶ä»–åº“ï¼ˆå¦‚NumPyã€randomï¼‰ä¹Ÿè®¾ç½®ç›¸åŒçš„éšæœºç§å­ã€‚
    torch.manual_seed(1337)
    # ä½œç”¨ ï¼šæ ¹æ® args.device åˆ¤æ–­å½“å‰è®¾å¤‡ç±»å‹ï¼ˆGPUæˆ–CPUï¼‰ã€‚
    # ç”¨é€” ï¼šç¡®ä¿åç»­æ“ä½œä¸è®¾å¤‡ç±»å‹åŒ¹é…ã€‚
    # æ‰©å±• ï¼š
    # æ”¯æŒå¤šè®¾å¤‡åˆ‡æ¢ï¼ˆå¦‚TPUã€MPSï¼‰ã€‚
    # åŠ¨æ€æ£€æµ‹å¯ç”¨è®¾å¤‡å¹¶è‡ªåŠ¨é€‰æ‹©ã€‚
    device_type = &quot;cuda&quot; if &quot;cuda&quot; in args.device else &quot;cpu&quot;
    # ä½œç”¨ ï¼š
    # å¦‚æœè®¾å¤‡æ˜¯CPUï¼Œåˆ™ä½¿ç”¨é»˜è®¤ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼ˆnullcontextï¼‰ã€‚
    # å¦‚æœè®¾å¤‡æ˜¯GPUï¼Œåˆ™å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆtorch.cuda.amp.autocastï¼‰ã€‚
    # ç”¨é€” ï¼šåœ¨GPUä¸ŠåŠ é€Ÿè®­ç»ƒå¹¶é™ä½æ˜¾å­˜å ç”¨ã€‚
    # æ‰©å±• ï¼š
    # æ”¯æŒåŠ¨æ€è°ƒæ•´ç²¾åº¦ï¼ˆå¦‚FP16ã€BF16ï¼‰ã€‚
    # å…¼å®¹å…¶ä»–æ··åˆç²¾åº¦å·¥å…·ï¼ˆå¦‚Apexï¼‰ã€‚
    ctx = nullcontext() if device_type == &quot;cpu&quot; else torch.cuda.amp.autocast()
    # ä½œç”¨ ï¼šæ£€æŸ¥ç¯å¢ƒå˜é‡ RANK æ˜¯å¦å­˜åœ¨ï¼Œåˆ¤æ–­æ˜¯å¦å¯ç”¨åˆ†å¸ƒå¼è®­ç»ƒã€‚
    # ç”¨é€” ï¼šå†³å®šæ˜¯å¦éœ€è¦åˆå§‹åŒ–åˆ†å¸ƒå¼æ¨¡å¼ã€‚
    # æ‰©å±• ï¼š
    # æ”¯æŒåŠ¨æ€åˆ†é…åˆ†å¸ƒå¼é…ç½®ï¼ˆå¦‚è¿›ç¨‹æ•°ã€èŠ‚ç‚¹æ•°ï¼‰ã€‚
    # å…¼å®¹å…¶ä»–åˆ†å¸ƒå¼æ¡†æ¶ï¼ˆå¦‚Horovodï¼‰ã€‚
    ddp = int(os.environ.get(&quot;RANK&quot;, -1)) != -1  # is this a ddp run?
    # ä½œç”¨ ï¼š
    # å¦‚æœå¯ç”¨åˆ†å¸ƒå¼è®­ç»ƒï¼Œè°ƒç”¨ init_distributed_mode åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒã€‚
    # è®¾ç½®å½“å‰è¿›ç¨‹çš„è®¾å¤‡ä¸º DEVICEã€‚
    # ç”¨é€” ï¼šç¡®ä¿åˆ†å¸ƒå¼è®­ç»ƒä¸­çš„æ¯ä¸ªè¿›ç¨‹ä½¿ç”¨ç‹¬ç«‹çš„GPUèµ„æºã€‚
    # æ‰©å±• ï¼š
    # æ”¯æŒåŠ¨æ€åˆ†é…è®¾å¤‡ï¼ˆå¦‚æŒ‰è¿›ç¨‹ç¼–å·åˆ†é…GPUï¼‰ã€‚
    # å…¼å®¹å¤šèŠ‚ç‚¹åˆ†å¸ƒå¼è®­ç»ƒã€‚
    ddp_local_rank, DEVICE = 0, &quot;cuda:0&quot;
    if ddp:
        init_distributed_mode()
        args.device = torch.device(DEVICE)
    # ä½œç”¨ ï¼šç”ŸæˆWandBè¿è¡Œåç§°ï¼ŒåŒ…å«è®­ç»ƒè¶…å‚æ•°ä¿¡æ¯ã€‚
    # ç”¨é€” ï¼šä¾¿äºåœ¨WandBä¸­åŒºåˆ†ä¸åŒå®éªŒã€‚
    # æ‰©å±• ï¼š
    # åŠ¨æ€ç”Ÿæˆæ›´è¯¦ç»†çš„è¿è¡Œåç§°ï¼ˆå¦‚åŠ å…¥æ—¶é—´æˆ³æˆ–ä»»åŠ¡ç±»å‹ï¼‰ã€‚
    # æ”¯æŒè‡ªå®šä¹‰å‘½åè§„åˆ™ã€‚
    args.wandb_run_name = f&quot;MiniMind-Lora-SFT-Epoch-{args.epochs}-BatchSize-{args.batch_size}-LearningRate-{args.learning_rate}&quot;
    # ä½œç”¨ ï¼š
    # å¦‚æœå¯ç”¨äº†WandBä¸”å½“å‰è¿›ç¨‹ä¸ºä¸»è¿›ç¨‹ï¼ˆddp_local_rank == 0ï¼‰ï¼Œåˆå§‹åŒ–WandBã€‚
    # å¦åˆ™ï¼Œå°† wandb è®¾ç½®ä¸º Noneã€‚
    # ç”¨é€” ï¼šè®°å½•è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŒ‡æ ‡ï¼ˆå¦‚æŸå¤±ã€å­¦ä¹ ç‡ï¼‰ã€‚
    # æ‰©å±• ï¼š
    # æ”¯æŒå…¶ä»–æ—¥å¿—å·¥å…·ï¼ˆå¦‚TensorBoardã€MLflowï¼‰ã€‚
    # åŠ¨æ€è°ƒæ•´æ—¥å¿—é¢‘ç‡æˆ–å†…å®¹ã€‚
    if args.use_wandb and (not ddp or ddp_local_rank == 0):
        import wandb

        wandb.init(project=args.wandb_project, name=args.wandb_run_name)
    else:
        wandb = None
    # ä½œç”¨ ï¼šè°ƒç”¨ init_model å‡½æ•°åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨ã€‚
    # ç”¨é€” ï¼šå‡†å¤‡æ¨¡å‹å’Œæ•°æ®å¤„ç†å·¥å…·ã€‚
    # æ‰©å±• ï¼š
    # æ”¯æŒåŠ¨æ€åŠ è½½ä¸åŒç‰ˆæœ¬çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚
    # å…¼å®¹å¤šç§åˆ†è¯å™¨ï¼ˆå¦‚BERTã€GPTï¼‰ã€‚
    model, tokenizer = init_model(lm_config)
    # ä½œç”¨ ï¼šå°†LoRAæ¨¡å—æ’å…¥åˆ°æ¨¡å‹ä¸­ã€‚
    # ç”¨é€” ï¼šå®ç°ä½ç§©åˆ†è§£å¾®è°ƒï¼Œå‡å°‘å‚æ•°é‡ã€‚
    # æ‰©å±• ï¼š
    # æ”¯æŒåŠ¨æ€è°ƒæ•´LoRAçš„ç§©ï¼ˆrankï¼‰ã€‚
    # å…¼å®¹å…¶ä»–é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼ˆå¦‚Adapterã€Prefix Tuningï¼‰
    apply_lora(model)
    # ä½œç”¨ ï¼š
    # ç»Ÿè®¡æ¨¡å‹çš„æ€»å‚æ•°é‡ã€‚
    # ç»Ÿè®¡LoRAæ¨¡å—çš„å‚æ•°é‡ã€‚
    # ç”¨é€” ï¼šè¯„ä¼°æ¨¡å‹è§„æ¨¡å’ŒLoRAçš„æ•ˆç‡ã€‚
    # æ‰©å±• ï¼š
    # æ”¯æŒç»Ÿè®¡å…¶ä»–æ¨¡å—çš„å‚æ•°é‡ï¼ˆå¦‚MoEã€Attentionï¼‰ã€‚
    # åŠ¨æ€è°ƒæ•´LoRAå‚æ•°å æ¯”ã€‚
    total_params = sum(p.numel() for p in model.parameters())  # æ€»å‚æ•°æ•°é‡
    lora_params_count = sum(p.numel() for name, p in model.named_parameters() if 'lora' in name)  # LoRA å‚æ•°æ•°é‡
    if not ddp or dist.get_rank() == 0:
        print(f&quot;LLM æ€»å‚æ•°é‡: {total_params}&quot;)
        print(f&quot;LoRA å‚æ•°é‡: {lora_params_count}&quot;)
        print(f&quot;LoRA å‚æ•°å æ¯”: {lora_params_count / total_params * 100:.2f}%&quot;)
    # ä½œç”¨ ï¼šå†»ç»“æ‰€æœ‰éLoRAå‚æ•°ï¼Œä»…å¯¹LoRAå‚æ•°è¿›è¡Œä¼˜åŒ–ã€‚
    # ç”¨é€” ï¼šå‡å°‘è®­ç»ƒæ‰€éœ€çš„æ˜¾å­˜å’Œè®¡ç®—èµ„æºã€‚
    # æ‰©å±• ï¼š
    # æ”¯æŒéƒ¨åˆ†å†»ç»“ï¼ˆå¦‚å†»ç»“Embeddingå±‚ï¼‰ã€‚
    # åŠ¨æ€è°ƒæ•´å†»ç»“ç­–ç•¥ã€‚
    for name, param in model.named_parameters():
        if 'lora' not in name:
            param.requires_grad = False
    # ä½œç”¨ ï¼šæå–æ‰€æœ‰LoRAå‚æ•°ã€‚
    # ç”¨é€” ï¼šä¸ºä¼˜åŒ–å™¨æä¾›å¾…ä¼˜åŒ–çš„å‚æ•°åˆ—è¡¨ã€‚
    # æ‰©å±• ï¼š
    # æ”¯æŒåŠ¨æ€è°ƒæ•´ä¼˜åŒ–å‚æ•°ï¼ˆå¦‚åŠ å…¥æ­£åˆ™åŒ–é¡¹ï¼‰ã€‚
    # å…¼å®¹å…¶ä»–ä¼˜åŒ–å™¨ï¼ˆå¦‚LAMBã€Rangerï¼‰ã€‚
    lora_params = []
    for name, param in model.named_parameters():
        if 'lora' in name:
            lora_params.append(param)

    # åªå¯¹ LoRA å‚æ•°è¿›è¡Œä¼˜åŒ–
    # ä½œç”¨ ï¼šå®šä¹‰AdamWä¼˜åŒ–å™¨ï¼Œä»…ä¼˜åŒ–LoRAå‚æ•°ã€‚
    # ç”¨é€” ï¼šæ›´æ–°æ¨¡å‹å‚æ•°ä»¥æœ€å°åŒ–æŸå¤±ã€‚
    # æ‰©å±• ï¼š
    # æ”¯æŒåŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡ï¼ˆå¦‚ä½™å¼¦é€€ç«ã€Warmupï¼‰ã€‚
    # å…¼å®¹å…¶ä»–ä¼˜åŒ–ç®—æ³•ï¼ˆå¦‚SGDã€Adagradï¼‰ã€‚
    optimizer = optim.AdamW(lora_params, lr=args.learning_rate)
    # ä½œç”¨ ï¼šåŠ è½½è®­ç»ƒæ•°æ®é›†ï¼Œä½¿ç”¨æŒ‡å®šçš„åˆ†è¯å™¨å’Œæœ€å¤§åºåˆ—é•¿åº¦ã€‚
    # ç”¨é€” ï¼šå‡†å¤‡è®­ç»ƒæ•°æ®ã€‚
    # æ‰©å±• ï¼š
    # æ”¯æŒå¤šç§æ•°æ®æ ¼å¼ï¼ˆå¦‚CSVã€JSONï¼‰ã€‚
    # åŠ¨æ€è°ƒæ•´æ•°æ®å¢å¼ºç­–ç•¥ã€‚
    train_ds = SFTDataset(args.data_path, tokenizer, max_length=lm_config.max_seq_len)
    # ä½œç”¨ ï¼šå¦‚æœå¯ç”¨åˆ†å¸ƒå¼è®­ç»ƒï¼Œå®šä¹‰åˆ†å¸ƒå¼é‡‡æ ·å™¨ï¼›å¦åˆ™ä¸º Noneã€‚
    # ç”¨é€” ï¼šç¡®ä¿æ¯ä¸ªè¿›ç¨‹å¤„ç†ä¸åŒçš„æ•°æ®å­é›†ã€‚
    # æ‰©å±• ï¼š
    # æ”¯æŒåŠ¨æ€è°ƒæ•´é‡‡æ ·ç­–ç•¥ï¼ˆå¦‚åŠ æƒé‡‡æ ·ï¼‰ã€‚
    # å…¼å®¹å…¶ä»–åˆ†å¸ƒå¼æ¡†æ¶ã€‚
    train_sampler = DistributedSampler(train_ds) if ddp else None
    # ä½œç”¨ ï¼šå®šä¹‰æ•°æ®åŠ è½½å™¨ï¼Œç”¨äºæ‰¹é‡åŠ è½½è®­ç»ƒæ•°æ®ã€‚
    # ç”¨é€” ï¼šåŠ é€Ÿæ•°æ®åŠ è½½å¹¶æé«˜è®­ç»ƒæ•ˆç‡ã€‚
    # æ‰©å±• ï¼š
    # åŠ¨æ€è°ƒæ•´çº¿ç¨‹æ•°ï¼ˆnum_workersï¼‰ä»¥ä¼˜åŒ–æ€§èƒ½ã€‚
    # æ”¯æŒåŠ¨æ€è°ƒæ•´æ‰¹æ¬¡å¤§å°ã€‚
    train_loader = DataLoader(
        train_ds,
        batch_size=args.batch_size,
        pin_memory=True,
        drop_last=False,
        shuffle=False,
        num_workers=args.num_workers,
        sampler=train_sampler
    )
    # ä½œç”¨ ï¼šå®šä¹‰æ¢¯åº¦ç¼©æ”¾å™¨ï¼Œç”¨äºæ··åˆç²¾åº¦è®­ç»ƒã€‚
    # ç”¨é€” ï¼šé˜²æ­¢æ¢¯åº¦ä¸‹æº¢ï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§ã€‚
    # æ‰©å±• ï¼š
    # æ”¯æŒåŠ¨æ€è°ƒæ•´ç²¾åº¦ï¼ˆå¦‚FP32ã€FP16ï¼‰ã€‚
    # å…¼å®¹å…¶ä»–æ··åˆç²¾åº¦å·¥å…·ã€‚
    scaler = torch.cuda.amp.GradScaler(enabled=(args.dtype in ['float16', 'bfloat16']))
    # ä½œç”¨ ï¼šè®¡ç®—æ¯ä¸ªepochä¸­çš„è¿­ä»£æ¬¡æ•°ã€‚
    # ç”¨é€” ï¼šç›‘æ§è®­ç»ƒè¿›åº¦ã€‚
    # æ‰©å±• ï¼š
    # åŠ¨æ€è°ƒæ•´è¿­ä»£æ¬¡æ•°ï¼ˆå¦‚æŒ‰æ•°æ®é‡å˜åŒ–ï¼‰ã€‚
    iter_per_epoch = len(train_loader)
    # ä½œç”¨ ï¼šéå†æ¯ä¸ªepochï¼Œè°ƒç”¨ train_epoch è¿›è¡Œè®­ç»ƒã€‚
    # ç”¨é€” ï¼šå®Œæˆæ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ã€‚
    # æ‰©å±• ï¼š
    # æ”¯æŒæ—©åœæœºåˆ¶ï¼ˆå¦‚éªŒè¯é›†æ€§èƒ½ä¸å†æå‡æ—¶åœæ­¢è®­ç»ƒï¼‰ã€‚
    # åŠ¨æ€è°ƒæ•´è®­ç»ƒç­–ç•¥ï¼ˆå¦‚å­¦ä¹ ç‡è°ƒåº¦ï¼‰ã€‚
    for epoch in range(args.epochs):
        train_epoch(epoch, wandb)
</code></pre>
<pre><code class="language-python"># ä½œç”¨ ï¼š
# torchï¼šPyTorchæ ¸å¿ƒåº“ï¼Œç”¨äºå¼ é‡æ“ä½œå’Œç¥ç»ç½‘ç»œåŠŸèƒ½ã€‚
# optim å’Œ nnï¼šåˆ†åˆ«æä¾›ä¼˜åŒ–å™¨å’Œç¥ç»ç½‘ç»œæ¨¡å—çš„æ”¯æŒã€‚
# ç”¨é€” ï¼š
# æä¾›LoRAå®ç°æ‰€éœ€çš„å·¥å…·å’Œæ¥å£ã€‚
# æ‰©å±• ï¼š
# æ”¯æŒåŠ¨æ€åŠ è½½ä¸åŒç‰ˆæœ¬çš„PyTorchã€‚
# å¢åŠ æ›´å¤šè‡ªå®šä¹‰æ¨¡å—æˆ–å·¥å…·ã€‚
import torch
from torch import nn


# å®šä¹‰Loraç½‘ç»œç»“æ„
class LoRA(nn.Module):
    # ä½œç”¨ ï¼š
    # å®šä¹‰ä¸€ä¸ªLoRAæ¨¡å—ï¼Œç»§æ‰¿è‡ª nn.Moduleã€‚
    # åˆå§‹åŒ–æ—¶è®¾ç½®è¾“å…¥ç‰¹å¾æ•°ï¼ˆin_featuresï¼‰ã€è¾“å‡ºç‰¹å¾æ•°ï¼ˆout_featuresï¼‰å’Œç§©ï¼ˆrankï¼‰ã€‚
    # ä½¿ç”¨ä¸¤ä¸ªçº¿æ€§å±‚ï¼ˆA å’Œ Bï¼‰å®ç°ä½ç§©åˆ†è§£ã€‚
    # ç”¨é€” ï¼š
    # æä¾›é«˜æ•ˆçš„å‚æ•°å¾®è°ƒèƒ½åŠ›ã€‚
    # æ‰©å±• ï¼š
    # æ”¯æŒåŠ¨æ€è°ƒæ•´ç§©ï¼ˆrankï¼‰ã€‚
    # å¢åŠ å¯¹åç½®é¡¹çš„æ”¯æŒã€‚
    def __init__(self, in_features, out_features, rank):
        super().__init__()
        self.rank = rank  # LoRAçš„ç§©ï¼ˆrankï¼‰ï¼Œæ§åˆ¶ä½ç§©çŸ©é˜µçš„å¤§å°
        self.A = nn.Linear(in_features, rank, bias=False)  # ä½ç§©çŸ©é˜µA
        self.B = nn.Linear(rank, out_features, bias=False)  # ä½ç§©çŸ©é˜µB
        # çŸ©é˜µAé«˜æ–¯åˆå§‹åŒ–
        self.A.weight.data.normal_(mean=0.0, std=0.02)
        # çŸ©é˜µBå…¨0åˆå§‹åŒ–
        self.B.weight.data.zero_()

    # ä½œç”¨ ï¼š
    # å®ç°LoRAæ¨¡å—çš„å‰å‘ä¼ æ’­é€»è¾‘ã€‚
    # è¾“å…¥é€šè¿‡ä½ç§©çŸ©é˜µ A å’Œ B è¿›è¡Œå˜æ¢ã€‚
    # ç”¨é€” ï¼š
    # æä¾›ä½ç§©åˆ†è§£çš„è®¡ç®—èƒ½åŠ›ã€‚
    # æ‰©å±• ï¼š
    # æ”¯æŒåŠ¨æ€è°ƒæ•´æ¿€æ´»å‡½æ•°ã€‚
    # å¢åŠ å¯¹å¤šåˆ†æ”¯ç»“æ„çš„æ”¯æŒã€‚
    def forward(self, x):
        return self.B(self.A(x))

# ä½œç”¨ ï¼š
# éå†æ¨¡å‹çš„æ‰€æœ‰æ¨¡å—ï¼Œæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„çº¿æ€§å±‚ï¼ˆæƒé‡ä¸ºæ–¹é˜µï¼‰ã€‚
# ä¸ºç¬¦åˆæ¡ä»¶çš„çº¿æ€§å±‚æ·»åŠ LoRAæ¨¡å—ï¼Œå¹¶ä¿®æ”¹å…¶å‰å‘ä¼ æ’­é€»è¾‘ã€‚
# ç”¨é€” ï¼š
# åœ¨ç°æœ‰æ¨¡å‹ä¸­æ’å…¥LoRAæ¨¡å—ï¼Œå®ç°é«˜æ•ˆå¾®è°ƒã€‚
# æ‰©å±• ï¼š
# æ”¯æŒåŠ¨æ€è°ƒæ•´ç­›é€‰æ¡ä»¶ï¼ˆå¦‚ä»…å¯¹ç‰¹å®šå±‚åº”ç”¨LoRAï¼‰ã€‚
# å¢åŠ å¯¹å…¶ä»–ç±»å‹å±‚ï¼ˆå¦‚å·ç§¯å±‚ï¼‰çš„æ”¯æŒã€‚
def apply_lora(model, rank=16):
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear) and module.weight.shape[0] == module.weight.shape[1]:
            lora = LoRA(module.weight.shape[0], module.weight.shape[1], rank=rank).to(model.device)
            setattr(module, &quot;lora&quot;, lora)
            original_forward = module.forward

            # æ˜¾å¼ç»‘å®š
            def forward_with_lora(x, layer1=original_forward, layer2=lora):
                return layer1(x) + layer2(x)

            module.forward = forward_with_lora

# ä½œç”¨ ï¼š
# ä»æŒ‡å®šè·¯å¾„åŠ è½½LoRAæƒé‡ã€‚
# å°†æƒé‡åŠ è½½åˆ°æ¨¡å‹ä¸­å¯¹åº”çš„LoRAæ¨¡å—ã€‚
# ç”¨é€” ï¼š
# æ¢å¤LoRAæ¨¡å—çš„è®­ç»ƒçŠ¶æ€ã€‚
# æ‰©å±• ï¼š
# æ”¯æŒåŠ¨æ€è°ƒæ•´åŠ è½½è·¯å¾„ã€‚
# å¢åŠ å¯¹å¤šç§æ–‡ä»¶æ ¼å¼çš„æ”¯æŒã€‚
def load_lora(model, path):
    state_dict = torch.load(path, map_location=model.device)
    for name, module in model.named_modules():
        if hasattr(module, 'lora'):
            lora_state = {k.replace(f'{name}.lora.', ''): v for k, v in state_dict.items() if f'{name}.lora.' in k}
            module.lora.load_state_dict(lora_state)

# ä½œç”¨ ï¼š
# éå†æ¨¡å‹çš„æ‰€æœ‰æ¨¡å—ï¼Œæå–LoRAæ¨¡å—çš„æƒé‡ã€‚
# å°†æƒé‡ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„ã€‚
# ç”¨é€” ï¼š
# ä¿å­˜LoRAæ¨¡å—çš„è®­ç»ƒçŠ¶æ€ã€‚
# æ‰©å±• ï¼š
# æ”¯æŒåŠ¨æ€è°ƒæ•´ä¿å­˜è·¯å¾„ã€‚
# å¢åŠ å¯¹å‹ç¼©å­˜å‚¨çš„æ”¯æŒã€‚
def save_lora(model, path):
    state_dict = {}
    for name, module in model.named_modules():
        if hasattr(module, 'lora'):
            lora_state = {f'{name}.lora.{k}': v for k, v in module.lora.state_dict().items()}
            state_dict.update(lora_state)
    torch.save(state_dict, path)
</code></pre>
<pre><code class="language-python">
# ä½œç”¨ ï¼šä» transformers åº“ä¸­å¯¼å…¥ PretrainedConfig åŸºç±»ã€‚
# ç”¨é€” ï¼šç”¨äºå®šä¹‰æ¨¡å‹é…ç½®ç±»ï¼Œç»§æ‰¿è‡ª PretrainedConfigã€‚
# æ‰©å±• ï¼š
# æ”¯æŒå…¶ä»–åŸºç±»ï¼ˆå¦‚è‡ªå®šä¹‰é…ç½®ç±»ï¼‰ã€‚
# åŠ¨æ€åŠ è½½ä¸åŒç‰ˆæœ¬çš„ transformersã€‚
from transformers import PretrainedConfig

# ä½œç”¨ ï¼š
# å®šä¹‰ä¸€ä¸ªåä¸º LMConfig çš„æ¨¡å‹é…ç½®ç±»ï¼Œç»§æ‰¿è‡ª PretrainedConfigã€‚
# è®¾ç½® model_type ä¸º &quot;minimind&quot;ï¼Œç”¨äºæ ‡è¯†æ¨¡å‹ç±»å‹ã€‚
# ç”¨é€” ï¼š
# é…ç½®æ¨¡å‹çš„è¶…å‚æ•°ã€‚
# æä¾›æ ‡å‡†åŒ–çš„æ¥å£ï¼Œä¾¿äºä¸ transformers ç”Ÿæ€é›†æˆã€‚
# æ‰©å±• ï¼š
# æ”¯æŒåŠ¨æ€è°ƒæ•´ model_typeã€‚
# å…¼å®¹å¤šç§æ¨¡å‹æ¶æ„
class LMConfig(PretrainedConfig):
    model_type = &quot;minimind&quot;
    # ä½œç”¨ ï¼š
    # å®šä¹‰åˆå§‹åŒ–æ–¹æ³•ï¼Œæ¥å—å¤šä¸ªè¶…å‚æ•°ä½œä¸ºè¾“å…¥ã€‚
    # æ¯ä¸ªå‚æ•°éƒ½æœ‰é»˜è®¤å€¼ï¼Œç¡®ä¿åœ¨æœªæŒ‡å®šæ—¶ä½¿ç”¨é»˜è®¤é…ç½®ã€‚
    # ç”¨é€” ï¼š
    # é…ç½®æ¨¡å‹çš„æ ¸å¿ƒè¶…å‚æ•°ã€‚
    # æ”¯æŒçµæ´»çš„å‚æ•°è®¾ç½®ï¼Œé€‚é…ä¸åŒçš„ä»»åŠ¡éœ€æ±‚ã€‚
    # æ‰©å±• ï¼š
    # æ”¯æŒåŠ¨æ€è°ƒæ•´å‚æ•°èŒƒå›´ã€‚
    # å¢åŠ æ›´å¤šè¶…å‚æ•°ï¼ˆå¦‚æ­£åˆ™åŒ–é¡¹ã€ä¼˜åŒ–å™¨é…ç½®ï¼‰ã€‚
    def __init__(
            self,
            dim: int = 512,
            n_layers: int = 8,
            n_heads: int = 8,
            n_kv_heads: int = 2,
            vocab_size: int = 6400,
            hidden_dim: int = None,
            multiple_of: int = 64,
            norm_eps: float = 1e-5,
            max_seq_len: int = 8192,
            rope_theta: int = 1e6,
            dropout: float = 0.0,
            flash_attn: bool = True,
            ####################################################
            # Here are the specific configurations of MOE
            # When use_moe is false, the following is invalid
            ####################################################
            use_moe: bool = False,
            ####################################################
            num_experts_per_tok: int = 2,
            n_routed_experts: int = 4,
            n_shared_experts: bool = True,
            scoring_func: str = 'softmax',
            aux_loss_alpha: float = 0.1,
            seq_aux: bool = True,
            norm_topk_prob: bool = True,
            **kwargs,
    ):
    # ä½œç”¨ ï¼š
    # å°†ä¼ å…¥çš„å‚æ•°èµ‹å€¼ç»™ç±»å±æ€§ã€‚
    # æ¯ä¸ªå‚æ•°å¯¹åº”æ¨¡å‹çš„ä¸€ä¸ªæ ¸å¿ƒè¶…å‚æ•°ã€‚
    # ç”¨é€” ï¼š
    # é…ç½®æ¨¡å‹çš„ç»“æ„å’Œè¡Œä¸ºã€‚
    # æ‰©å±• ï¼š
    # æ”¯æŒåŠ¨æ€è°ƒæ•´å‚æ•°å€¼ï¼ˆå¦‚é€šè¿‡å‘½ä»¤è¡Œæˆ–é…ç½®æ–‡ä»¶ï¼‰ã€‚
    # å¢åŠ æ›´å¤šè¶…å‚æ•°ï¼ˆå¦‚æ¿€æ´»å‡½æ•°ç±»å‹ã€å½’ä¸€åŒ–æ–¹å¼ï¼‰ã€‚
        self.dim = dim
        self.n_layers = n_layers
        self.n_heads = n_heads
        self.n_kv_heads = n_kv_heads
        self.vocab_size = vocab_size
        self.hidden_dim = hidden_dim
        self.multiple_of = multiple_of
        self.norm_eps = norm_eps
        self.max_seq_len = max_seq_len
        self.rope_theta = rope_theta
        self.dropout = dropout
        self.flash_attn = flash_attn
        # ä½œç”¨ ï¼š
        # é…ç½®æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¨¡å—çš„ç›¸å…³å‚æ•°ã€‚
        # å½“ use_moe=False æ—¶ï¼Œè¿™äº›å‚æ•°æ— æ•ˆã€‚
        # ç”¨é€” ï¼š
        # æ§åˆ¶MoEæ¨¡å—çš„è¡Œä¸ºã€‚
        # æ‰©å±• ï¼š
        # æ”¯æŒåŠ¨æ€å¯ç”¨æˆ–ç¦ç”¨MoEã€‚
        # å¢åŠ æ›´å¤šMoEç›¸å…³å‚æ•°ï¼ˆå¦‚ä¸“å®¶å®¹é‡ã€è·¯ç”±ç­–ç•¥ï¼‰ã€‚
        ####################################################
        # Here are the specific configurations of MOE
        # When use_moe is false, the following is invalid
        ####################################################
        self.use_moe = use_moe
        self.num_experts_per_tok = num_experts_per_tok  # æ¯ä¸ªtokené€‰æ‹©çš„ä¸“å®¶æ•°é‡
        self.n_routed_experts = n_routed_experts  # æ€»çš„ä¸“å®¶æ•°é‡
        self.n_shared_experts = n_shared_experts  # å…±äº«ä¸“å®¶
        self.scoring_func = scoring_func  # è¯„åˆ†å‡½æ•°ï¼Œé»˜è®¤ä¸º'softmax'
        self.aux_loss_alpha = aux_loss_alpha  # è¾…åŠ©æŸå¤±çš„alphaå‚æ•°
        self.seq_aux = seq_aux  # æ˜¯å¦åœ¨åºåˆ—çº§åˆ«ä¸Šè®¡ç®—è¾…åŠ©æŸå¤±
        self.norm_topk_prob = norm_topk_prob  # æ˜¯å¦æ ‡å‡†åŒ–top-kæ¦‚ç‡
        super().__init__(**kwargs)
</code></pre>
<blockquote>
<p>è®­ç»ƒåçš„æ¨¡å‹æƒé‡æ–‡ä»¶é»˜è®¤æ¯éš”<code>100æ­¥</code>ä¿å­˜ä¸º: <code>lora_xxx_*.pth</code>ï¼ˆ*ä¸ºæ¨¡å‹å…·ä½“dimensionï¼Œæ¯æ¬¡ä¿å­˜æ—¶æ–°æ–‡ä»¶ä¼šè¦†ç›–æ—§æ–‡ä»¶ï¼‰</p>
</blockquote>
<p>éå¸¸å¤šçš„äººå›°æƒ‘ï¼Œå¦‚ä½•ä½¿æ¨¡å‹å­¦ä¼šè‡ªå·±ç§æœ‰é¢†åŸŸçš„çŸ¥è¯†ï¼Ÿå¦‚ä½•å‡†å¤‡æ•°æ®é›†ï¼Ÿå¦‚ä½•è¿ç§»é€šç”¨é¢†åŸŸæ¨¡å‹æ‰“é€ å‚åŸŸæ¨¡å‹ï¼Ÿ<br>
è¿™é‡Œä¸¾å‡ ä¸ªä¾‹å­ï¼Œå¯¹äºé€šç”¨æ¨¡å‹ï¼ŒåŒ»å­¦é¢†åŸŸçŸ¥è¯†æ¬ ç¼ºï¼Œå¯ä»¥å°è¯•åœ¨åŸæœ‰æ¨¡å‹åŸºç¡€ä¸ŠåŠ å…¥é¢†åŸŸçŸ¥è¯†ï¼Œä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚<br>
åŒæ—¶ï¼Œæˆ‘ä»¬é€šå¸¸ä¸å¸Œæœ›å­¦ä¼šé¢†åŸŸçŸ¥è¯†çš„åŒæ—¶æŸå¤±åŸæœ‰åŸºç¡€æ¨¡å‹çš„å…¶å®ƒèƒ½åŠ›ï¼Œæ­¤æ—¶LoRAå¯ä»¥å¾ˆå¥½çš„æ”¹å–„è¿™ä¸ªé—®é¢˜ã€‚<br>
åªéœ€è¦å‡†å¤‡å¦‚ä¸‹æ ¼å¼çš„å¯¹è¯æ•°æ®é›†æ”¾ç½®åˆ°<code>./dataset/lora_xxx.jsonl</code>ï¼Œå¯åŠ¨ <code>python train_lora.py</code><br>
è®­ç»ƒå³å¯å¾—åˆ°<code>./out/lora/lora_xxx.pth</code>æ–°æ¨¡å‹æƒé‡ã€‚</p>
<p><strong>åŒ»ç–—åœºæ™¯</strong></p>
<pre><code class="language-text"> {&quot;conversations&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;è¯·é—®é¢ˆæ¤ç—…çš„äººæ•å¤´å¤šé«˜æ‰æœ€å¥½ï¼Ÿ&quot;}, {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;é¢ˆæ¤ç—…æ‚£è€…é€‰æ‹©æ•å¤´çš„é«˜åº¦åº”è¯¥æ ¹æ®...&quot;}]}
 {&quot;conversations&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;è¯·é—®xxx&quot;}, {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;xxx...&quot;}]}
</code></pre>
<p><strong>è‡ªæˆ‘è®¤çŸ¥åœºæ™¯</strong></p>
<pre><code class="language-text"> {&quot;conversations&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;ä½ å«ä»€ä¹ˆåå­—ï¼Ÿ&quot;}, {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;æˆ‘å«minimind...&quot;}]}
 {&quot;conversations&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;ä½ æ˜¯è°&quot;}, {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;æˆ‘æ˜¯...&quot;}]}
</code></pre>
<p>æ­¤æ—¶ã€åŸºç¡€æ¨¡å‹+LoRAæ¨¡å‹ã€‘å³å¯è·å¾—åŒ»ç–—åœºæ™¯æ¨¡å‹å¢å¼ºçš„èƒ½åŠ›ï¼Œç›¸å½“äºä¸ºåŸºç¡€æ¨¡å‹å¢åŠ äº†LoRAå¤–æŒ‚ï¼Œè¿™ä¸ªè¿‡ç¨‹å¹¶ä¸æŸå¤±åŸºç¡€æ¨¡å‹çš„æœ¬èº«èƒ½åŠ›ã€‚<br>
æˆ‘ä»¬å¯ä»¥é€šè¿‡<code>eval_model.py</code>è¿›è¡Œæ¨¡å‹è¯„ä¼°æµ‹è¯•ã€‚</p>
<pre><code class="language-python">import argparse
import random
import time
import numpy as np
import torch
import warnings
from transformers import AutoTokenizer, AutoModelForCausalLM
from model.model import MiniMindLM
from model.LMConfig import LMConfig
from model.model_lora import *

warnings.filterwarnings('ignore')


def init_model(args):
    tokenizer = AutoTokenizer.from_pretrained(r'C:\study\minimind-master\model\minimind_tokenizer')
    if args.load == 0:
        moe_path = '_moe' if args.use_moe else ''
        modes = {0: 'pretrain', 1: 'full_sft', 2: 'rlhf', 3: 'reason'}
        ckp = f'./{args.out_dir}/{modes[args.model_mode]}_{args.dim}{moe_path}.pth'

        model = MiniMindLM(LMConfig(
            dim=args.dim,
            n_layers=args.n_layers,
            max_seq_len=args.max_seq_len,
            use_moe=args.use_moe
        ))

        state_dict = torch.load(ckp, map_location=args.device)
        model.load_state_dict({k: v for k, v in state_dict.items() if 'mask' not in k}, strict=True)

        if args.lora_name != 'None':
            apply_lora(model)
            load_lora(model, f'./{args.out_dir}/lora/{args.lora_name}_{args.dim}.pth')
    else:
        model = AutoModelForCausalLM.from_pretrained(
            r'C:\study\minimind-master\MiniMind2',
            trust_remote_code=True
        )
    print(f'MiniMindæ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.2f}M(illion)')
    return model.eval().to(args.device), tokenizer


def get_prompt_datas(args):
    if args.model_mode == 0:
        # pretrainæ¨¡å‹çš„æ¥é¾™èƒ½åŠ›ï¼ˆæ— æ³•å¯¹è¯ï¼‰
        prompt_datas = [
            'é©¬å…‹æ€ä¸»ä¹‰åŸºæœ¬åŸç†',
            'äººç±»å¤§è„‘çš„ä¸»è¦åŠŸèƒ½',
            'ä¸‡æœ‰å¼•åŠ›åŸç†æ˜¯',
            'ä¸–ç•Œä¸Šæœ€é«˜çš„å±±å³°æ˜¯',
            'äºŒæ°§åŒ–ç¢³åœ¨ç©ºæ°”ä¸­',
            'åœ°çƒä¸Šæœ€å¤§çš„åŠ¨ç‰©æœ‰',
            'æ­å·å¸‚çš„ç¾é£Ÿæœ‰'
        ]
    else:
        if args.lora_name == 'None':
            # é€šç”¨å¯¹è¯é—®é¢˜
            prompt_datas = [
                'è¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚',
                'ä½ æ›´æ“…é•¿å“ªä¸€ä¸ªå­¦ç§‘ï¼Ÿ',
                'é²è¿…çš„ã€Šç‹‚äººæ—¥è®°ã€‹æ˜¯å¦‚ä½•æ‰¹åˆ¤å°å»ºç¤¼æ•™çš„ï¼Ÿ',
                'æˆ‘å’³å—½å·²ç»æŒç»­äº†ä¸¤å‘¨ï¼Œéœ€è¦å»åŒ»é™¢æ£€æŸ¥å—ï¼Ÿ',
                'è¯¦ç»†çš„ä»‹ç»å…‰é€Ÿçš„ç‰©ç†æ¦‚å¿µã€‚',
                'æ¨èä¸€äº›æ­å·çš„ç‰¹è‰²ç¾é£Ÿå§ã€‚',
                'è¯·ä¸ºæˆ‘è®²è§£â€œå¤§è¯­è¨€æ¨¡å‹â€è¿™ä¸ªæ¦‚å¿µã€‚',
                'å¦‚ä½•ç†è§£ChatGPTï¼Ÿ',
                'Introduce the history of the United States, please.'
            ]
        else:
            # ç‰¹å®šé¢†åŸŸé—®é¢˜
            lora_prompt_datas = {
                'lora_identity': [
                    &quot;ä½ æ˜¯ChatGPTå§ã€‚&quot;,
                    &quot;ä½ å«ä»€ä¹ˆåå­—ï¼Ÿ&quot;,
                    &quot;ä½ å’Œopenaiæ˜¯ä»€ä¹ˆå…³ç³»ï¼Ÿ&quot;
                ],
                'lora_medical': [
                    'æˆ‘æœ€è¿‘ç»å¸¸æ„Ÿåˆ°å¤´æ™•ï¼Œå¯èƒ½æ˜¯ä»€ä¹ˆåŸå› ï¼Ÿ',
                    'æˆ‘å’³å—½å·²ç»æŒç»­äº†ä¸¤å‘¨ï¼Œéœ€è¦å»åŒ»é™¢æ£€æŸ¥å—ï¼Ÿ',
                    'æœç”¨æŠ—ç”Ÿç´ æ—¶éœ€è¦æ³¨æ„å“ªäº›äº‹é¡¹ï¼Ÿ',
                    'ä½“æ£€æŠ¥å‘Šä¸­æ˜¾ç¤ºèƒ†å›ºé†‡åé«˜ï¼Œæˆ‘è¯¥æ€ä¹ˆåŠï¼Ÿ',
                    'å­•å¦‡åœ¨é¥®é£Ÿä¸Šéœ€è¦æ³¨æ„ä»€ä¹ˆï¼Ÿ',
                    'è€å¹´äººå¦‚ä½•é¢„é˜²éª¨è´¨ç–æ¾ï¼Ÿ',
                    'æˆ‘æœ€è¿‘æ€»æ˜¯æ„Ÿåˆ°ç„¦è™‘ï¼Œåº”è¯¥æ€ä¹ˆç¼“è§£ï¼Ÿ',
                    'å¦‚æœæœ‰äººçªç„¶æ™•å€’ï¼Œåº”è¯¥å¦‚ä½•æ€¥æ•‘ï¼Ÿ'
                ],
            }
            prompt_datas = lora_prompt_datas[args.lora_name]

    return prompt_datas


# è®¾ç½®å¯å¤ç°çš„éšæœºç§å­
def setup_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def main():
    parser = argparse.ArgumentParser(description=&quot;Chat with MiniMind&quot;)
    parser.add_argument('--lora_name', default='None', type=str)
    parser.add_argument('--out_dir', default='out', type=str)
    parser.add_argument('--temperature', default=0.85, type=float)
    parser.add_argument('--top_p', default=0.85, type=float)
    parser.add_argument('--device', default='cuda' if torch.cuda.is_available() else 'cpu', type=str)
    # æ­¤å¤„max_seq_lenï¼ˆæœ€å¤§å…è®¸è¾“å…¥é•¿åº¦ï¼‰å¹¶ä¸æ„å‘³æ¨¡å‹å…·æœ‰å¯¹åº”çš„é•¿æ–‡æœ¬çš„æ€§èƒ½ï¼Œä»…é˜²æ­¢QAå‡ºç°è¢«æˆªæ–­çš„é—®é¢˜
    # MiniMind2-moe (145M)ï¼š(dim=640, n_layers=8, use_moe=True)
    # MiniMind2-Small (26M)ï¼š(dim=512, n_layers=8)
    # MiniMind2 (104M)ï¼š(dim=768, n_layers=16)
    parser.add_argument('--dim', default=512, type=int)
    parser.add_argument('--n_layers', default=8, type=int)
    parser.add_argument('--max_seq_len', default=8192, type=int)
    parser.add_argument('--use_moe', default=False, type=bool)
    # æºå¸¦å†å²å¯¹è¯ä¸Šä¸‹æ–‡æ¡æ•°
    # history_cntéœ€è¦è®¾ä¸ºå¶æ•°ï¼Œå³ã€ç”¨æˆ·é—®é¢˜, æ¨¡å‹å›ç­”ã€‘ä¸º1ç»„ï¼›è®¾ç½®ä¸º0æ—¶ï¼Œå³å½“å‰queryä¸æºå¸¦å†å²ä¸Šæ–‡
    # æ¨¡å‹æœªç»è¿‡å¤–æ¨å¾®è°ƒæ—¶ï¼Œåœ¨æ›´é•¿çš„ä¸Šä¸‹æ–‡çš„chat_templateæ—¶éš¾å…å‡ºç°æ€§èƒ½çš„æ˜æ˜¾é€€åŒ–ï¼Œå› æ­¤éœ€è¦æ³¨æ„æ­¤å¤„è®¾ç½®
    parser.add_argument('--history_cnt', default=0, type=int)
    parser.add_argument('--stream', default=True, type=bool)
    parser.add_argument('--load', default=0, type=int, help=&quot;0: åŸç”Ÿtorchæƒé‡ï¼Œ1: transformersåŠ è½½&quot;)
    parser.add_argument('--model_mode', default=1, type=int,
                        help=&quot;0: é¢„è®­ç»ƒæ¨¡å‹ï¼Œ1: SFT-Chatæ¨¡å‹ï¼Œ2: RLHF-Chatæ¨¡å‹ï¼Œ3: Reasonæ¨¡å‹&quot;)
    args = parser.parse_args()

    model, tokenizer = init_model(args)

    prompts = get_prompt_datas(args)
    test_mode = int(input('[0] è‡ªåŠ¨æµ‹è¯•\n[1] æ‰‹åŠ¨è¾“å…¥\n'))
    messages = []
    for idx, prompt in enumerate(prompts if test_mode == 0 else iter(lambda: input('ğŸ‘¶: '), '')):
        setup_seed(random.randint(0, 2048))
        # setup_seed(2025)  # å¦‚éœ€å›ºå®šæ¯æ¬¡è¾“å‡ºåˆ™æ¢æˆã€å›ºå®šã€‘çš„éšæœºç§å­
        if test_mode == 0: print(f'ğŸ‘¶: {prompt}')

        messages = messages[-args.history_cnt:] if args.history_cnt else []
        messages.append({&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt})

        new_prompt = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )[-args.max_seq_len + 1:] if args.model_mode != 0 else (tokenizer.bos_token + prompt)

        answer = new_prompt
        with torch.no_grad():
            x = torch.tensor(tokenizer(new_prompt)['input_ids'], device=args.device).unsqueeze(0)
            outputs = model.generate(
                x,
                eos_token_id=tokenizer.eos_token_id,
                max_new_tokens=args.max_seq_len,
                temperature=args.temperature,
                top_p=args.top_p,
                stream=True,
                pad_token_id=tokenizer.pad_token_id
            )

            print('ğŸ¤–ï¸: ', end='')
            try:
                if not args.stream:
                    print(tokenizer.decode(outputs.squeeze()[x.shape[1]:].tolist(), skip_special_tokens=True), end='')
                else:
                    history_idx = 0
                    for y in outputs:
                        answer = tokenizer.decode(y[0].tolist(), skip_special_tokens=True)
                        if (answer and answer[-1] == 'ï¿½') or not answer:
                            continue
                        print(answer[history_idx:], end='', flush=True)
                        history_idx = len(answer)
            except StopIteration:
                print(&quot;No answer&quot;)
            print('\n')

        messages.append({&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: answer})


if __name__ == &quot;__main__&quot;:
    main()
</code></pre>
<pre><code class="language-bash"># æ³¨æ„ï¼šmodel_modeå³é€‰æ‹©åŸºç¡€æ¨¡å‹çš„ç±»å‹ï¼Œè¿™å’Œtrain_loraæ˜¯åŸºäºå“ªä¸ªæ¨¡å‹è®­ç»ƒçš„ç›¸å…³ï¼Œç¡®ä¿ç»Ÿä¸€å³å¯ã€‚
python eval_model.py --lora_name 'lora_medical' --model_mode 2
</code></pre>
<p><strong>å°æµ‹è¯•</strong></p>
<pre><code class="language-text">ğŸ‘¶: æˆ‘æœ€è¿‘ç»å¸¸æ„Ÿåˆ°å¤´æ™•ï¼Œå¯èƒ½æ˜¯ä»€ä¹ˆåŸå› ï¼Ÿ
ğŸ¤–ï¸: å¤´æ™•å¯èƒ½æ˜¯ç”±äºå¤šç§åŸå› ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºï¼šå¿ƒè„ç–¾ç—…ã€çœ¼ç›æŸä¼¤ã€è¿‡æ•ååº”ã€çš®è‚¤é—®é¢˜ã€è¿‡æ•ååº”ã€å‹åŠ›ã€ç„¦è™‘ã€ç–²åŠ³ã€è¯ç‰©å‰¯ä½œç”¨ã€æ€§åŠŸèƒ½éšœç¢æˆ–è€…å…¶ä»–...

---

ğŸ‘¶: ä½ å’Œopenaiæ˜¯ä»€ä¹ˆå…³ç³»ï¼Ÿ
ğŸ¤–ï¸: æˆ‘æ˜¯ MiniMindï¼Œä¸€ä¸ªç”± Jingyao Gong å¼€å‘çš„äººå·¥æ™ºèƒ½åŠ©æ‰‹ã€‚æˆ‘é€šè¿‡è‡ªç„¶è¯­è¨€å¤„ç†å’Œç®—æ³•è®­ç»ƒæ¥ä¸ç”¨æˆ·è¿›è¡Œäº¤äº’ã€‚
</code></pre>
<p>PSï¼šåªè¦æœ‰æ‰€éœ€è¦çš„æ•°æ®é›†ï¼Œä¹Ÿå¯ä»¥full_sftå…¨å‚å¾®è°ƒï¼ˆéœ€è¦è¿›è¡Œé€šç”¨çŸ¥è¯†çš„æ··åˆé…æ¯”ï¼Œå¦åˆ™è¿‡æ‹Ÿåˆé¢†åŸŸæ•°æ®ä¼šè®©æ¨¡å‹å˜å‚»ï¼ŒæŸå¤±é€šç”¨æ€§ï¼‰</p>

          <div class="toc-container"><ul class="markdownIt-TOC">
<li>
<ul>
<li>
<ul>
<li><a href="#lora-low-rank-adaptation"><strong>LoRA (Low-Rank Adaptation)</strong></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
          <hr />
          
            <p class="prev-post">ä¸Šä¸€ç¯‡ï¼š
              <a href="https://tianxiawuhao.github.io/vfv73gaU-n/">
                <span class="post-title">
                  å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥&rarr;
                </span>
              </a>
            </p>
          
          
          <p class="next-post">ä¸‹ä¸€ç¯‡ï¼š
            <a href="https://tianxiawuhao.github.io/GL2xUsgbSZ/">
              <span class="post-title">
                LoRA_PEFT&rarr;
              </span>
            </a>
          </p>
          
          <div class="comment" style="text-align: center;">
            

            
            
          </div>
        </div>
      </div>
    </div>
  </article>
  <!-- Footer -->
  <footer>
    <div class="container">
      <div class="row">
        <div class="col-lg-8 col-md-10 mx-auto">
          <ul class="list-inline text-center">
            
            
              
            
              
            
              
            
              
            
              
            
              
            
              
              <!-- <li class="list-inline-item">
              <a href="https://tianxiawuhao.github.io/atom.xml" target="_blank">
                <span class="fa-stack fa-lg">
                  <i class="fas fa-circle fa-stack-2x"></i>
                  <i class="fas fa-rss fa-stack-1x fa-inverse"></i>
                </span>
              </a>
              </li> -->
          </ul>
          <p class="copyright text-muted">Copyright &copy;<span>tianxia</span><br><a href="https://github.com/getgridea/gridea" class="Themeinfo">Powered by Gridea</a></p>
        </div>
      </div>
    </div>
   </footer>
  <!-- Bootstrap core JavaScript -->
  <script src="https://cdn.bootcss.com/twitter-bootstrap/4.3.1/js/bootstrap.bundle.min.js"></script>
  <!-- <script src="https://tianxiawuhao.github.io/media/scripts/bootstrap.bundle.min.js"></script> -->
  <!-- Bootstrap core JavaScript -->
  <script src="https://cdn.jsdelivr.net/gh/Alanrk/clean-cdn@1.0/scripts/clean-blog.min.js"></script>
  <!-- <script src="https://tianxiawuhao.github.io/media/scripts/clean-blog.min.js"></script> -->
  <script src="//instant.page/3.0.0" type="module" defer integrity="sha384-OeDn4XE77tdHo8pGtE1apMPmAipjoxUQ++eeJa6EtJCfHlvijigWiJpD7VDPWXV1"></script>
  <style type="text/css">a.back_to_top{text-decoration:none;position:fixed;bottom:40px;right:30px;background:#f0f0f0;height:40px;width:40px;border-radius:50%;line-height:36px;font-size:18px;text-align:center;transition-duration:.5s;transition-propety:background-color;display:none}a.back_to_top span{color:#888}a.back_to_top:hover{cursor:pointer;background:#dfdfdf}a.back_to_top:hover span{color:#555}@media print,screen and(max-width:580px){.back_to_top{display:none!important}}</style>
<a id="back_to_top" href="#" class="back_to_top">
  <span>â–²</span></a>
<script>$(document).ready((function(_this) {
    return function() {
      var bt;
      bt = $('#back_to_top');
      if ($(document).width() > 480) {
        $(window).scroll(function() {
          var st;
          st = $(window).scrollTop();
          if (st > 30) {
            return bt.css('display', 'block')
          } else {
            return bt.css('display', 'none')
          }
        });
        return bt.click(function() {
          $('body,html').animate({
            scrollTop: 0
          },
          800);
          return false
        })
      }
    }
  })(this));</script>
  
  <script src="https://tianxiawuhao.github.io/media/scripts/tocScript.js"></script>
</body>

</html>