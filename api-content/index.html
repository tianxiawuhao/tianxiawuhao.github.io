{"posts":[{"title":"学习率调度策略","content":"在深度学习中，调整学习率是优化模型性能的重要手段。不同的学习率调度策略适用于不同的场景和需求。以下是几种常见的学习率调度策略及其优缺点： 1. 固定学习率 (Fixed Learning Rate) 描述： 整个训练过程中学习率保持不变。 优点： 实现简单，无需额外的计算。 在某些简单的任务或小规模数据集上表现良好。 缺点： 学习率过高可能导致模型无法收敛。 学习率过低会导致训练速度变慢，甚至陷入局部最优。 不适合复杂的任务或大规模数据集。 2. 分段常数衰减 (Step Decay / Step LR) 描述： 每隔固定的epoch或step，将学习率乘以一个衰减因子（如0.1）。 示例： lr = initial_lr * gamma ** (epoch // step_size) 优点： 简单易实现。 能够在特定阶段降低学习率，帮助模型更精细地优化。 缺点： 需要手动设置step_size和gamma，对超参数敏感。 可能导致学习率下降过快或过慢，影响模型性能。 3. 指数衰减 (Exponential Decay) 描述： 学习率按指数函数逐渐减小。 公式： lr=initial_lr×e−ktlr = initial\\_lr \\times e^{-kt} lr=initial_lr×e−kt 其中，t 是当前的训练步数，k 是衰减速率。 优点： 学习率平滑下降，避免突然变化。 适合需要持续优化的任务。 缺点： 对于复杂任务，可能需要仔细调整 k 值。 如果衰减过快，可能导致模型收敛到次优解。 4. 余弦退火 (Cosine Annealing) 描述： 学习率按照余弦函数周期性变化，通常从初始值逐渐下降到最小值。 公式： lr=min_lr+max_lr−min_lr2×(1+cos⁡(π⋅TcurTmax))lr = \\text{min\\_lr} + \\frac{\\text{max\\_lr} - \\text{min\\_lr}} 2 \\times {\\left(1 + \\cos\\left(\\pi \\cdot \\frac{T_{\\text{cur}}}{T_{\\text{max}}}\\right)\\right)} lr=min_lr+2max_lr−min_lr​×(1+cos(π⋅Tmax​Tcur​​)) 其中，Tcur 是当前的训练步数，Tmax 是周期的最大步数。 优点： 能够自动调整学习率，避免手动设置超参数。 学习率的变化更加平滑，有助于跳出局部最优。 周期性变化可以提高模型的泛化能力。 缺点： 实现相对复杂。 对于某些任务，可能需要调整周期长度。 5. Warmup + 衰减 (Warmup + Decay) 描述： 训练初期使用较小的学习率逐步增加（Warmup），然后按照某种策略（如余弦退火或线性衰减）逐渐减小。 示例： if current_step &lt; warmup_steps: lr = initial_lr * (current_step / warmup_steps) else: lr = initial_lr * (1 - (current_step - warmup_steps) / total_steps) 优点： Warmup可以防止训练初期梯度爆炸。 后续的衰减策略可以帮助模型更好地收敛。 常用于Transformer等复杂模型的训练。 缺点： 需要设置Warmup步数和后续的衰减策略。 对于不同任务，可能需要调整Warmup步数。 6. Reduce on Plateau (基于验证集性能调整学习率) 描述： 当验证集上的性能（如损失或准确率）停止改善时，降低学习率。 示例： scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10) 优点： 自动根据模型性能调整学习率，无需手动干预。 适合验证集性能波动较大的任务。 缺点： 对验证集的依赖较强，如果验证集不具有代表性，可能导致错误的调整。 可能导致学习率过早下降，影响训练效果。 7. 循环学习率 (Cyclic Learning Rate) 描述： 学习率在一个范围内周期性地增大和减小。 示例： scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.001, max_lr=0.1, step_size_up=2000) 优点： 能够帮助模型跳出局部最优。 适合探索性较强的任务。 缺点： 参数设置较为复杂（如最大最小学习率、周期长度）。 对于某些任务，可能导致训练不稳定。 8. 自适应学习率方法 (Adaptive Methods) 描述： 使用自适应优化器（如Adam、RMSProp）动态调整每个参数的学习率。 优点： 不需要手动设置学习率调度策略。 能够针对不同参数动态调整学习率，适合非凸优化问题。 缺点： 自适应优化器可能会导致模型在训练后期收敛较慢。 对于某些任务，可能需要结合其他学习率调度策略。 9. OneCycle Learning Rate 描述： 学习率在一个周期内先上升后下降，同时配合动量的调整。 优点： 能够快速收敛并达到较好的性能。 适合训练时间有限的任务。 缺点： 需要设置最大学习率、周期长度等超参数。 对于某些任务，可能需要多次实验才能找到合适的参数。 总结对比 策略 优点 缺点 固定学习率 简单易用 容易过拟合或欠拟合 分段常数衰减 易实现，适合阶段性优化 对超参数敏感 指数衰减 平滑下降，适合长期优化 可能衰减过快 余弦退火 平滑且周期性变化，有助于跳出局部最优 实现复杂，需要调整周期 Warmup + 衰减 防止梯度爆炸，适合复杂模型 需要设置多个超参数 Reduce on Plateau 自动调整，适合验证集性能波动大的任务 对验证集依赖强 循环学习率 帮助跳出局部最优，适合探索性任务 参数设置复杂，可能导致不稳定 自适应学习率方法 动态调整，适合非凸优化问题 收敛较慢，可能需要结合其他策略 OneCycle Learning Rate 快速收敛，适合时间有限的任务 需要多次实验调整超参数 选择策略的建议 简单任务 ：固定学习率或分段常数衰减。 复杂任务 ：Warmup + 余弦退火或OneCycle。 验证集性能波动大 ：Reduce on Plateau。 探索性任务 ：循环学习率。 大规模模型 ：结合Warmup和自适应优化器（如AdamW）。 根据具体任务的需求和实验结果，灵活选择合适的学习率调度策略。 ","link":"https://tianxiawuhao.github.io/vfv73gaU-n/"},{"title":"LoRA（Low-Rank Adaptation）","content":"LoRA (Low-Rank Adaptation) LoRA（低秩适配）是一种高效的参数高效微调（Parameter-Efficient Fine-Tuning, PEFT）方法，旨在通过低秩分解的方式对预训练模型进行微调。 相比于全参数微调（Full Fine-Tuning），LoRA 只需要更新少量的参数。 LoRA 的核心思想是：在模型的权重矩阵中引入低秩分解，仅对低秩部分进行更新，而保持原始预训练权重不变。 代码可见以下，完全从0实现LoRA流程，不依赖第三方库的封装。 import os # 文件操作 import argparse # 命令行参数解析 import time # 时间管理 import math # 数学计算 import warnings # 忽略警告信息 import torch.distributed as dist # 分布式训练 from contextlib import nullcontext # 默认上下文管理 from torch.utils.data import DataLoader, DistributedSampler # 数据加载和分布式采样 from transformers import AutoTokenizer # 加载预训练的分词器 from model.model import MiniMindLM # 模型架构 from model.LMConfig import LMConfig # 配置 from model.dataset import SFTDataset #数据集 from model.model_lora import * # LoRA方法 from torch import optim, nn # 忽略警告信息 warnings.filterwarnings('ignore') # Logger function # 在分布式训练中，只有主进程（rank=0）会打印日志信息，避免多个进程重复输出。如果不是分布式训练（ddp=False），则直接打印日志。 def Logger(content): if not ddp or dist.get_rank() == 0: print(content) # 使用余弦退火策略动态调整学习率。公式为： def get_lr(current_step, total_steps, lr): return lr / 10 + 0.5 * lr * (1 + math.cos(math.pi * current_step / total_steps)) # 代码和full_sft「几乎」一致 # 作用 ：定义一个训练epoch的函数。 # 参数 ： # epoch：当前训练的epoch编号（从0开始）。 # wandb：WandB日志记录工具对象，用于记录训练过程中的指标。如果未启用WandB，则为None。 def train_epoch(epoch, wandb): # 作用 ：定义交叉熵损失函数，reduction='none' 表示不对损失进行归约操作（即返回每个样本的损失值，而不是平均或总和）。 # 用途 ：后续会根据 loss_mask 对损失进行加权求和，因此需要逐样本计算损失。 # 扩展 ：可以根据任务需求选择其他损失函数，如 nn.MSELoss 或自定义损失函数。 loss_fct = nn.CrossEntropyLoss(reduction='none') # 作用 ：记录当前时间戳，用于计算训练耗时。 # 用途 ：在日志中显示每一步或每个epoch的训练时间。 # 扩展 ：可以结合更精细的时间管理工具（如 timeit 模块）来优化性能分析。 start_time = time.time() # 作用 ：遍历训练数据加载器 train_loader，每次迭代获取一个批次的数据。 # step：当前batch的索引。 # X：输入数据（通常是tokenized的文本序列）。 # Y：目标标签（通常是下一个token的预测目标）。 # loss_mask：用于屏蔽无效token的掩码（如填充token或特殊标记）。 # 用途 ：逐批次处理数据并更新模型参数。 # 扩展 ：可以通过 DataLoader 的 collate_fn 自定义数据预处理逻辑。 for step, (X, Y, loss_mask) in enumerate(train_loader): # 作用 ：将输入数据、目标标签和损失掩码移动到指定设备（如GPU或CPU）。 # 用途 ：确保数据和模型位于同一设备上，避免运行时错误。 # 扩展 ：可以使用 .pin_memory() 提前将数据锁定在内存中，以加速GPU数据传输。 X = X.to(args.device) Y = Y.to(args.device) loss_mask = loss_mask.to(args.device) # 作用 ： # 调用 get_lr 函数动态计算当前的学习率。 # 将计算得到的学习率应用到优化器的所有参数组。 # 用途 ：实现学习率调度策略（如余弦退火）。 # 扩展 ： # 可以替换为其他学习率调度器（如 torch.optim.lr_scheduler 中的 StepLR 或 CosineAnnealingLR）。 # 支持多阶段学习率调度（如Warmup + Decay）。 lr = get_lr(epoch * iter_per_epoch + step, args.epochs * iter_per_epoch, args.learning_rate) for param_group in optimizer.param_groups: param_group['lr'] = lr # 作用 ： # 上下文管理器 ctx ：如果使用混合精度训练（如FP16），则启用自动混合精度（AMP）。 # 前向传播 ：调用模型 model(X) 进行前向传播，输出结果存储在 res 中。 # 计算损失 ： # 使用 CrossEntropyLoss 计算逐样本损失。 # 根据 loss_mask 加权求和，忽略无效token的损失。 # 添加辅助损失（aux_loss），通常用于正则化或其他目标。 # 将总损失除以累积步数（accumulation_steps），以便梯度累积后更新。 # 用途 ：完成一次前向传播并计算最终损失。 # 扩展 ： # 如果任务复杂，可以添加更多损失项（如KL散度、对比损失等）。 # 支持多种损失权重的动态调整。 with ctx: res = model(X) loss = loss_fct( res.logits.view(-1, res.logits.size(-1)), Y.view(-1) ).view(Y.size()) loss = (loss * loss_mask).sum() / loss_mask.sum() loss += res.aux_loss loss = loss / args.accumulation_steps # 作用 ： # 反向传播 ：使用 scaler.scale(loss).backward() 计算梯度（支持混合精度）。 # 梯度累积 ：每隔 accumulation_steps 步执行一次梯度更新。 # 梯度裁剪 ：防止梯度爆炸，限制梯度范数不超过 grad_clip。 # 优化器更新 ：调用 scaler.step(optimizer) 更新模型参数。 # 重置梯度 ：调用 optimizer.zero_grad(set_to_none=True) 清空梯度。 # 用途 ：实现梯度累积和混合精度训练，提高训练效率。 # 扩展 ： # 支持动态调整 accumulation_steps，以适应不同硬件资源。 # 可以替换为其他优化器（如LAMB、Ranger等）。 scaler.scale(loss).backward() if (step + 1) % args.accumulation_steps == 0: scaler.unscale_(optimizer) torch.nn.utils.clip_grad_norm_(lora_params, args.grad_clip) scaler.step(optimizer) scaler.update() optimizer.zero_grad(set_to_none=True) # 作用 ： # 每隔 log_interval 步记录一次训练日志。 # 打印当前epoch、step、损失、学习率和预计剩余时间。 # 如果启用了WandB，将日志上传到云端。 # 用途 ：监控训练进度，便于调试和分析。 # 扩展 ： # 可以记录更多指标（如准确率、F1分数等）。 # 支持保存日志到本地文件或数据库。 if step % args.log_interval == 0: spend_time = time.time() - start_time Logger( 'Epoch:[{}/{}]({}/{}) loss:{:.3f} lr:{:.12f} epoch_Time:{}min:'.format( epoch + 1, args.epochs, step, iter_per_epoch, loss.item(), optimizer.param_groups[-1]['lr'], spend_time / (step + 1) * iter_per_epoch // 60 - spend_time // 60)) if (wandb is not None) and (not ddp or dist.get_rank() == 0): wandb.log({&quot;loss&quot;: loss, &quot;lr&quot;: optimizer.param_groups[-1]['lr'], &quot;epoch_Time&quot;: spend_time / (step + 1) * iter_per_epoch // 60 - spend_time // 60}) # 作用 ： # 每隔 save_interval 步保存一次模型。 # 仅保存LoRA权重（区别于完整模型权重）。 # 在分布式训练中，只有主进程（rank=0）执行保存操作。 # 用途 ：定期保存模型，防止训练中断导致数据丢失。 # 扩展 ： # 支持增量保存（仅保存新增的权重变化）。 # 可以根据验证集性能保存最佳模型。 if (step + 1) % args.save_interval == 0 and (not ddp or dist.get_rank() == 0): model.eval() # 【区别1】只保存lora权重即可 save_lora(model, f'{args.save_dir}/lora/{args.lora_name}_{lm_config.dim}.pth') model.train() def init_model(lm_config): # 作用 ：加载预训练分词器（AutoTokenizer）。 # 用途 ：用于将输入文本转换为模型可接受的token序列。 # 扩展 ： # 可以根据任务需求选择不同的分词器（如BERT、GPT等）。 # 支持自定义词汇表或特殊标记。 tokenizer = AutoTokenizer.from_pretrained('./model/minimind_tokenizer') # 作用 ：初始化模型 MiniMindLM，使用配置对象 lm_config。 # 用途 ：创建一个语言模型实例。 # 扩展 ： # 可以动态调整模型结构（如层数、隐藏维度等）。 # 支持加载不同版本的预训练模型权重。 model = MiniMindLM(lm_config) # 作用 ： # 根据是否启用 MoE（Mixture of Experts），生成检查点路径。 # 检查点文件名包含模型维度和MoE标志。 # 用途 ：加载与当前模型配置匹配的预训练权重。 # 扩展 ： # 支持动态生成检查点路径，适配多种模型配置。 # 可以根据任务类型（如分类、生成）加载不同的权重。 moe_path = '_moe' if lm_config.use_moe else '' ckp = f'./out/rlhf_{lm_config.dim}{moe_path}.pth' # 作用 ： # 加载预训练权重到模型中。 # 使用 strict=False 允许部分权重不匹配（如新增或删除层）。 # 用途 ：初始化模型参数，避免从零开始训练。 # 扩展 ： # 可以实现增量加载（仅加载部分权重）。 # 支持多阶段训练（如先加载基础模型，再微调特定任务）。 state_dict = torch.load(ckp, map_location=args.device) model.load_state_dict(state_dict, strict=False) # 作用 ：将模型移动到指定设备（如GPU或CPU），并返回模型和分词器。 # 用途 ：确保模型和数据位于同一设备上。 # 扩展 ： # 支持多设备分布式训练（如模型并行）。 return model.to(args.device), tokenizer def init_distributed_mode(): # 作用 ：如果未启用分布式训练（ddp=False），直接返回。 # 用途 ：避免不必要的分布式初始化操作。 # 扩展 ： # 支持其他分布式后端（如gloo、mpi）。 if not ddp: return # 作用 ：初始化分布式训练环境，使用NCCL后端（适用于CUDA）。 # 用途 ：设置分布式通信组。 # 扩展 ： # 支持动态选择后端（如nccl、gloo）。 # 可以实现更复杂的分布式策略（如模型并行）。 global ddp_local_rank, DEVICE dist.init_process_group(backend=&quot;nccl&quot;) # 作用 ：从环境变量中获取分布式训练的相关信息。 # RANK：当前进程的全局rank。 # LOCAL_RANK：当前进程在本地节点的rank。 # WORLD_SIZE：总进程数。 # 用途 ：确定每个进程的角色和位置。 # 扩展 ： # 支持动态调整分布式配置（如进程数、节点数）。 ddp_rank = int(os.environ[&quot;RANK&quot;]) ddp_local_rank = int(os.environ[&quot;LOCAL_RANK&quot;]) ddp_world_size = int(os.environ[&quot;WORLD_SIZE&quot;]) # 作用 ：将当前进程绑定到指定的GPU设备。 # 用途 ：确保每个进程使用独立的GPU资源。 # 扩展 ： # 支持多GPU共享（如混合精度训练）。 DEVICE = f&quot;cuda:{ddp_local_rank}&quot; torch.cuda.set_device(DEVICE) # 作用 ：定义程序入口，确保代码只在直接运行时执行。 # 用途 ：模块化设计，便于复用和测试。 # 扩展 ： # 支持命令行参数解析和动态配置。 if __name__ == &quot;__main__&quot;: # 作用 ：创建命令行参数解析器。 # 用途 ：通过命令行传递超参数，灵活控制训练过程。 # 扩展 ： # 支持从配置文件加载参数。 # 动态验证参数合法性。 parser = argparse.ArgumentParser(description=&quot;MiniMind SFT with LoRA&quot;) # 作用 ：定义输出目录路径，默认值为out。 # 用途 ：保存模型权重和日志文件。 # 扩展 ： # 支持自动创建目录。 # 动态生成子目录（如按日期或任务类型）。 parser.add_argument(&quot;--out_dir&quot;, type=str, default=&quot;out&quot;) # 作用 ：定义训练轮数，默认值为50。 # 用途 ：控制训练时长。 # 扩展 ： # 支持动态调整训练轮数（如早停机制）。 parser.add_argument(&quot;--epochs&quot;, type=int, default=50) # 作用 ：定义批次大小，默认值为16。 # 用途 ：控制每次迭代的数据量。 # 扩展 ： # 动态调整批次大小以适应硬件资源。 parser.add_argument(&quot;--batch_size&quot;, type=int, default=16) # 作用 ：定义学习率，默认值为5e-5。 # 用途 ：控制优化器的步长。 # 扩展 ： # 支持学习率调度策略（如余弦退火、Warmup）。 parser.add_argument(&quot;--learning_rate&quot;, type=float, default=5e-5) # 作用 ：定义设备类型，默认优先使用GPU。 # 用途 ：指定模型和数据所在的设备。 # 扩展 ： # 支持多设备切换（如CPU、GPU、TPU）。 parser.add_argument(&quot;--device&quot;, type=str, default=&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;) # 作用 ：定义数据类型，默认值为bfloat16。 # 用途 ：支持混合精度训练。 # 扩展 ： # 动态调整精度（如FP32、FP16）。 parser.add_argument(&quot;--dtype&quot;, type=str, default=&quot;bfloat16&quot;) # 作用 ：启用WandB日志记录。 # 用途 ：监控训练过程。 # 扩展 ： # 支持其他日志工具（如TensorBoard）。 parser.add_argument(&quot;--use_wandb&quot;, action=&quot;store_true&quot;) # 作用 ：定义WandB项目名称。 # 用途 ：组织和管理实验。 # 扩展 ： # 动态生成项目名称（如按任务类型）。 parser.add_argument(&quot;--wandb_project&quot;, type=str, default=&quot;MiniMind-LoRA-SFT&quot;) # 作用 ：定义数据加载线程数，默认值为1。 # 用途 ：加速数据加载。 # 扩展 ： # 动态调整线程数以优化性能。 parser.add_argument(&quot;--num_workers&quot;, type=int, default=1) # 作用 ：启用分布式训练。 # 用途 ：支持多GPU或多节点训练。 # 扩展 ： # 支持动态分配资源。 parser.add_argument(&quot;--ddp&quot;, action=&quot;store_true&quot;) # 作用 ：定义梯度累积步数，默认值为1。 # 用途 ：模拟更大的批次大小。 # 扩展 ： # 动态调整累积步数以适应显存限制。 parser.add_argument(&quot;--accumulation_steps&quot;, type=int, default=1) # 作用 ：定义梯度裁剪阈值，默认值为1.0。 # 用途 ：防止梯度爆炸。 # 扩展 ： # 支持动态调整裁剪阈值。 parser.add_argument(&quot;--grad_clip&quot;, type=float, default=1.0) # 作用 ：定义Warmup步数，默认值为0。 # 用途 ：逐步增加学习率。 # 扩展 ： # 支持多种Warmup策略（如线性、指数）。 parser.add_argument(&quot;--warmup_iters&quot;, type=int, default=0) # 作用 ：定义日志记录间隔，默认值为100。 # 用途 ：定期打印训练进度。 # 扩展 ： # 支持动态调整日志频率。 parser.add_argument(&quot;--log_interval&quot;, type=int, default=100) # 作用 ：定义模型保存间隔，默认值为1。 # 用途 ：定期保存模型权重。 # 扩展 ： # 支持增量保存。 parser.add_argument(&quot;--save_interval&quot;, type=int, default=1) # 作用 ：定义本地rank，默认值为-1。 # 用途 ：分布式训练中的进程标识。 # 扩展 ： # 支持动态分配rank。 parser.add_argument('--local_rank', type=int, default=-1) # 作用 ：定义模型维度，默认值为512。 # 用途 ：控制模型容量。 # 扩展 ： # 支持动态调整维度。 parser.add_argument('--dim', default=512, type=int) # 作用 ：定义模型层数，默认值为8。 # 用途 ：控制模型深度。 # 扩展 ： # 支持动态调整层数。 parser.add_argument('--n_layers', default=8, type=int) # 作用 ：定义最大序列长度，默认值为512。 # 用途 ：限制输入序列长度。 # 扩展 ： # 支持动态截断或填充。 parser.add_argument('--max_seq_len', default=512, type=int) # 作用 ：启用MoE（Mixture of Experts），默认值为False。 # 用途 ：提高模型效率。 # 扩展 ： # 支持动态调整专家数量。 parser.add_argument('--use_moe', default=False, type=bool) # 作用 ：定义数据集路径，默认值为./dataset/lora_identity.jsonl。 # 用途 ：加载训练数据。 # 扩展 ： # 支持多种数据格式（如CSV、JSON）。 parser.add_argument(&quot;--data_path&quot;, type=str, default=&quot;./dataset/lora_identity.jsonl&quot;) # 作用 ：定义LoRA权重文件名，默认值为lora_identity。 # 用途 ：区分不同任务的LoRA权重。 # 扩展 ： # 动态生成文件名。 parser.add_argument(&quot;--lora_name&quot;, type=str, default=&quot;lora_identity&quot;, help=&quot;根据任务保存成lora_(英文/医学/心理...)&quot;) args = parser.parse_args() # 作用 ：创建一个 LMConfig 对象，用于定义模型的超参数。 # dim：隐藏层维度。 # n_layers：模型层数。 # max_seq_len：最大序列长度。 # use_moe：是否启用 MoE（Mixture of Experts）。 # 用途 ：将命令行参数传递给模型配置对象，确保模型初始化时使用正确的超参数。 # 扩展 ： # 支持动态调整模型配置（如通过 JSON 或 YAML 文件加载配置）。 # 可以扩展为支持更多模型类型（如Transformer、RNN等）。 lm_config = LMConfig(dim=args.dim, n_layers=args.n_layers, max_seq_len=args.max_seq_len, use_moe=args.use_moe) # 作用 ： # 将保存目录路径设置为 args.out_dir 的子目录。 # 使用 os.makedirs 确保保存目录存在。如果目录已存在，则不会报错（exist_ok=True）。 # 用途 ：为模型权重和日志文件创建存储路径。 # 扩展 ： # 动态生成子目录（如按任务类型或日期）。 # 支持分布式训练中的独立保存路径（如按rank编号区分）。 args.save_dir = os.path.join(args.out_dir) os.makedirs(args.save_dir, exist_ok=True) os.makedirs(args.out_dir, exist_ok=True) # 作用 ：计算每个批次中包含的token总数。 # 用途 ：监控训练过程中的数据吞吐量。 # 扩展 ： # 动态调整批次大小或序列长度以优化性能。 # 支持多阶段训练（如Warmup阶段逐步增加token数量）。 tokens_per_iter = args.batch_size * lm_config.max_seq_len # 作用 ：设置PyTorch的随机种子，确保实验结果可复现。 # 用途 ：控制随机性，避免因随机初始化导致的实验差异。 # 扩展 ： # 支持动态设置随机种子（如通过命令行参数传递）。 # 确保其他库（如NumPy、random）也设置相同的随机种子。 torch.manual_seed(1337) # 作用 ：根据 args.device 判断当前设备类型（GPU或CPU）。 # 用途 ：确保后续操作与设备类型匹配。 # 扩展 ： # 支持多设备切换（如TPU、MPS）。 # 动态检测可用设备并自动选择。 device_type = &quot;cuda&quot; if &quot;cuda&quot; in args.device else &quot;cpu&quot; # 作用 ： # 如果设备是CPU，则使用默认上下文管理器（nullcontext）。 # 如果设备是GPU，则启用混合精度训练（torch.cuda.amp.autocast）。 # 用途 ：在GPU上加速训练并降低显存占用。 # 扩展 ： # 支持动态调整精度（如FP16、BF16）。 # 兼容其他混合精度工具（如Apex）。 ctx = nullcontext() if device_type == &quot;cpu&quot; else torch.cuda.amp.autocast() # 作用 ：检查环境变量 RANK 是否存在，判断是否启用分布式训练。 # 用途 ：决定是否需要初始化分布式模式。 # 扩展 ： # 支持动态分配分布式配置（如进程数、节点数）。 # 兼容其他分布式框架（如Horovod）。 ddp = int(os.environ.get(&quot;RANK&quot;, -1)) != -1 # is this a ddp run? # 作用 ： # 如果启用分布式训练，调用 init_distributed_mode 初始化分布式环境。 # 设置当前进程的设备为 DEVICE。 # 用途 ：确保分布式训练中的每个进程使用独立的GPU资源。 # 扩展 ： # 支持动态分配设备（如按进程编号分配GPU）。 # 兼容多节点分布式训练。 ddp_local_rank, DEVICE = 0, &quot;cuda:0&quot; if ddp: init_distributed_mode() args.device = torch.device(DEVICE) # 作用 ：生成WandB运行名称，包含训练超参数信息。 # 用途 ：便于在WandB中区分不同实验。 # 扩展 ： # 动态生成更详细的运行名称（如加入时间戳或任务类型）。 # 支持自定义命名规则。 args.wandb_run_name = f&quot;MiniMind-Lora-SFT-Epoch-{args.epochs}-BatchSize-{args.batch_size}-LearningRate-{args.learning_rate}&quot; # 作用 ： # 如果启用了WandB且当前进程为主进程（ddp_local_rank == 0），初始化WandB。 # 否则，将 wandb 设置为 None。 # 用途 ：记录训练过程中的指标（如损失、学习率）。 # 扩展 ： # 支持其他日志工具（如TensorBoard、MLflow）。 # 动态调整日志频率或内容。 if args.use_wandb and (not ddp or ddp_local_rank == 0): import wandb wandb.init(project=args.wandb_project, name=args.wandb_run_name) else: wandb = None # 作用 ：调用 init_model 函数加载预训练模型和分词器。 # 用途 ：准备模型和数据处理工具。 # 扩展 ： # 支持动态加载不同版本的预训练模型。 # 兼容多种分词器（如BERT、GPT）。 model, tokenizer = init_model(lm_config) # 作用 ：将LoRA模块插入到模型中。 # 用途 ：实现低秩分解微调，减少参数量。 # 扩展 ： # 支持动态调整LoRA的秩（rank）。 # 兼容其他高效微调方法（如Adapter、Prefix Tuning） apply_lora(model) # 作用 ： # 统计模型的总参数量。 # 统计LoRA模块的参数量。 # 用途 ：评估模型规模和LoRA的效率。 # 扩展 ： # 支持统计其他模块的参数量（如MoE、Attention）。 # 动态调整LoRA参数占比。 total_params = sum(p.numel() for p in model.parameters()) # 总参数数量 lora_params_count = sum(p.numel() for name, p in model.named_parameters() if 'lora' in name) # LoRA 参数数量 if not ddp or dist.get_rank() == 0: print(f&quot;LLM 总参数量: {total_params}&quot;) print(f&quot;LoRA 参数量: {lora_params_count}&quot;) print(f&quot;LoRA 参数占比: {lora_params_count / total_params * 100:.2f}%&quot;) # 作用 ：冻结所有非LoRA参数，仅对LoRA参数进行优化。 # 用途 ：减少训练所需的显存和计算资源。 # 扩展 ： # 支持部分冻结（如冻结Embedding层）。 # 动态调整冻结策略。 for name, param in model.named_parameters(): if 'lora' not in name: param.requires_grad = False # 作用 ：提取所有LoRA参数。 # 用途 ：为优化器提供待优化的参数列表。 # 扩展 ： # 支持动态调整优化参数（如加入正则化项）。 # 兼容其他优化器（如LAMB、Ranger）。 lora_params = [] for name, param in model.named_parameters(): if 'lora' in name: lora_params.append(param) # 只对 LoRA 参数进行优化 # 作用 ：定义AdamW优化器，仅优化LoRA参数。 # 用途 ：更新模型参数以最小化损失。 # 扩展 ： # 支持动态调整学习率（如余弦退火、Warmup）。 # 兼容其他优化算法（如SGD、Adagrad）。 optimizer = optim.AdamW(lora_params, lr=args.learning_rate) # 作用 ：加载训练数据集，使用指定的分词器和最大序列长度。 # 用途 ：准备训练数据。 # 扩展 ： # 支持多种数据格式（如CSV、JSON）。 # 动态调整数据增强策略。 train_ds = SFTDataset(args.data_path, tokenizer, max_length=lm_config.max_seq_len) # 作用 ：如果启用分布式训练，定义分布式采样器；否则为 None。 # 用途 ：确保每个进程处理不同的数据子集。 # 扩展 ： # 支持动态调整采样策略（如加权采样）。 # 兼容其他分布式框架。 train_sampler = DistributedSampler(train_ds) if ddp else None # 作用 ：定义数据加载器，用于批量加载训练数据。 # 用途 ：加速数据加载并提高训练效率。 # 扩展 ： # 动态调整线程数（num_workers）以优化性能。 # 支持动态调整批次大小。 train_loader = DataLoader( train_ds, batch_size=args.batch_size, pin_memory=True, drop_last=False, shuffle=False, num_workers=args.num_workers, sampler=train_sampler ) # 作用 ：定义梯度缩放器，用于混合精度训练。 # 用途 ：防止梯度下溢，提高训练稳定性。 # 扩展 ： # 支持动态调整精度（如FP32、FP16）。 # 兼容其他混合精度工具。 scaler = torch.cuda.amp.GradScaler(enabled=(args.dtype in ['float16', 'bfloat16'])) # 作用 ：计算每个epoch中的迭代次数。 # 用途 ：监控训练进度。 # 扩展 ： # 动态调整迭代次数（如按数据量变化）。 iter_per_epoch = len(train_loader) # 作用 ：遍历每个epoch，调用 train_epoch 进行训练。 # 用途 ：完成整个训练过程。 # 扩展 ： # 支持早停机制（如验证集性能不再提升时停止训练）。 # 动态调整训练策略（如学习率调度）。 for epoch in range(args.epochs): train_epoch(epoch, wandb) # 作用 ： # torch：PyTorch核心库，用于张量操作和神经网络功能。 # optim 和 nn：分别提供优化器和神经网络模块的支持。 # 用途 ： # 提供LoRA实现所需的工具和接口。 # 扩展 ： # 支持动态加载不同版本的PyTorch。 # 增加更多自定义模块或工具。 import torch from torch import nn # 定义Lora网络结构 class LoRA(nn.Module): # 作用 ： # 定义一个LoRA模块，继承自 nn.Module。 # 初始化时设置输入特征数（in_features）、输出特征数（out_features）和秩（rank）。 # 使用两个线性层（A 和 B）实现低秩分解。 # 用途 ： # 提供高效的参数微调能力。 # 扩展 ： # 支持动态调整秩（rank）。 # 增加对偏置项的支持。 def __init__(self, in_features, out_features, rank): super().__init__() self.rank = rank # LoRA的秩（rank），控制低秩矩阵的大小 self.A = nn.Linear(in_features, rank, bias=False) # 低秩矩阵A self.B = nn.Linear(rank, out_features, bias=False) # 低秩矩阵B # 矩阵A高斯初始化 self.A.weight.data.normal_(mean=0.0, std=0.02) # 矩阵B全0初始化 self.B.weight.data.zero_() # 作用 ： # 实现LoRA模块的前向传播逻辑。 # 输入通过低秩矩阵 A 和 B 进行变换。 # 用途 ： # 提供低秩分解的计算能力。 # 扩展 ： # 支持动态调整激活函数。 # 增加对多分支结构的支持。 def forward(self, x): return self.B(self.A(x)) # 作用 ： # 遍历模型的所有模块，找到符合条件的线性层（权重为方阵）。 # 为符合条件的线性层添加LoRA模块，并修改其前向传播逻辑。 # 用途 ： # 在现有模型中插入LoRA模块，实现高效微调。 # 扩展 ： # 支持动态调整筛选条件（如仅对特定层应用LoRA）。 # 增加对其他类型层（如卷积层）的支持。 def apply_lora(model, rank=16): for name, module in model.named_modules(): if isinstance(module, nn.Linear) and module.weight.shape[0] == module.weight.shape[1]: lora = LoRA(module.weight.shape[0], module.weight.shape[1], rank=rank).to(model.device) setattr(module, &quot;lora&quot;, lora) original_forward = module.forward # 显式绑定 def forward_with_lora(x, layer1=original_forward, layer2=lora): return layer1(x) + layer2(x) module.forward = forward_with_lora # 作用 ： # 从指定路径加载LoRA权重。 # 将权重加载到模型中对应的LoRA模块。 # 用途 ： # 恢复LoRA模块的训练状态。 # 扩展 ： # 支持动态调整加载路径。 # 增加对多种文件格式的支持。 def load_lora(model, path): state_dict = torch.load(path, map_location=model.device) for name, module in model.named_modules(): if hasattr(module, 'lora'): lora_state = {k.replace(f'{name}.lora.', ''): v for k, v in state_dict.items() if f'{name}.lora.' in k} module.lora.load_state_dict(lora_state) # 作用 ： # 遍历模型的所有模块，提取LoRA模块的权重。 # 将权重保存到指定路径。 # 用途 ： # 保存LoRA模块的训练状态。 # 扩展 ： # 支持动态调整保存路径。 # 增加对压缩存储的支持。 def save_lora(model, path): state_dict = {} for name, module in model.named_modules(): if hasattr(module, 'lora'): lora_state = {f'{name}.lora.{k}': v for k, v in module.lora.state_dict().items()} state_dict.update(lora_state) torch.save(state_dict, path) # 作用 ：从 transformers 库中导入 PretrainedConfig 基类。 # 用途 ：用于定义模型配置类，继承自 PretrainedConfig。 # 扩展 ： # 支持其他基类（如自定义配置类）。 # 动态加载不同版本的 transformers。 from transformers import PretrainedConfig # 作用 ： # 定义一个名为 LMConfig 的模型配置类，继承自 PretrainedConfig。 # 设置 model_type 为 &quot;minimind&quot;，用于标识模型类型。 # 用途 ： # 配置模型的超参数。 # 提供标准化的接口，便于与 transformers 生态集成。 # 扩展 ： # 支持动态调整 model_type。 # 兼容多种模型架构 class LMConfig(PretrainedConfig): model_type = &quot;minimind&quot; # 作用 ： # 定义初始化方法，接受多个超参数作为输入。 # 每个参数都有默认值，确保在未指定时使用默认配置。 # 用途 ： # 配置模型的核心超参数。 # 支持灵活的参数设置，适配不同的任务需求。 # 扩展 ： # 支持动态调整参数范围。 # 增加更多超参数（如正则化项、优化器配置）。 def __init__( self, dim: int = 512, n_layers: int = 8, n_heads: int = 8, n_kv_heads: int = 2, vocab_size: int = 6400, hidden_dim: int = None, multiple_of: int = 64, norm_eps: float = 1e-5, max_seq_len: int = 8192, rope_theta: int = 1e6, dropout: float = 0.0, flash_attn: bool = True, #################################################### # Here are the specific configurations of MOE # When use_moe is false, the following is invalid #################################################### use_moe: bool = False, #################################################### num_experts_per_tok: int = 2, n_routed_experts: int = 4, n_shared_experts: bool = True, scoring_func: str = 'softmax', aux_loss_alpha: float = 0.1, seq_aux: bool = True, norm_topk_prob: bool = True, **kwargs, ): # 作用 ： # 将传入的参数赋值给类属性。 # 每个参数对应模型的一个核心超参数。 # 用途 ： # 配置模型的结构和行为。 # 扩展 ： # 支持动态调整参数值（如通过命令行或配置文件）。 # 增加更多超参数（如激活函数类型、归一化方式）。 self.dim = dim self.n_layers = n_layers self.n_heads = n_heads self.n_kv_heads = n_kv_heads self.vocab_size = vocab_size self.hidden_dim = hidden_dim self.multiple_of = multiple_of self.norm_eps = norm_eps self.max_seq_len = max_seq_len self.rope_theta = rope_theta self.dropout = dropout self.flash_attn = flash_attn # 作用 ： # 配置混合专家（MoE）模块的相关参数。 # 当 use_moe=False 时，这些参数无效。 # 用途 ： # 控制MoE模块的行为。 # 扩展 ： # 支持动态启用或禁用MoE。 # 增加更多MoE相关参数（如专家容量、路由策略）。 #################################################### # Here are the specific configurations of MOE # When use_moe is false, the following is invalid #################################################### self.use_moe = use_moe self.num_experts_per_tok = num_experts_per_tok # 每个token选择的专家数量 self.n_routed_experts = n_routed_experts # 总的专家数量 self.n_shared_experts = n_shared_experts # 共享专家 self.scoring_func = scoring_func # 评分函数，默认为'softmax' self.aux_loss_alpha = aux_loss_alpha # 辅助损失的alpha参数 self.seq_aux = seq_aux # 是否在序列级别上计算辅助损失 self.norm_topk_prob = norm_topk_prob # 是否标准化top-k概率 super().__init__(**kwargs) 训练后的模型权重文件默认每隔100步保存为: lora_xxx_*.pth（*为模型具体dimension，每次保存时新文件会覆盖旧文件） 非常多的人困惑，如何使模型学会自己私有领域的知识？如何准备数据集？如何迁移通用领域模型打造垂域模型？ 这里举几个例子，对于通用模型，医学领域知识欠缺，可以尝试在原有模型基础上加入领域知识，以获得更好的性能。 同时，我们通常不希望学会领域知识的同时损失原有基础模型的其它能力，此时LoRA可以很好的改善这个问题。 只需要准备如下格式的对话数据集放置到./dataset/lora_xxx.jsonl，启动 python train_lora.py 训练即可得到./out/lora/lora_xxx.pth新模型权重。 医疗场景 {&quot;conversations&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;请问颈椎病的人枕头多高才最好？&quot;}, {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;颈椎病患者选择枕头的高度应该根据...&quot;}]} {&quot;conversations&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;请问xxx&quot;}, {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;xxx...&quot;}]} 自我认知场景 {&quot;conversations&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;你叫什么名字？&quot;}, {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;我叫minimind...&quot;}]} {&quot;conversations&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;你是谁&quot;}, {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;我是...&quot;}]} 此时【基础模型+LoRA模型】即可获得医疗场景模型增强的能力，相当于为基础模型增加了LoRA外挂，这个过程并不损失基础模型的本身能力。 我们可以通过eval_model.py进行模型评估测试。 import argparse import random import time import numpy as np import torch import warnings from transformers import AutoTokenizer, AutoModelForCausalLM from model.model import MiniMindLM from model.LMConfig import LMConfig from model.model_lora import * warnings.filterwarnings('ignore') def init_model(args): tokenizer = AutoTokenizer.from_pretrained(r'C:\\study\\minimind-master\\model\\minimind_tokenizer') if args.load == 0: moe_path = '_moe' if args.use_moe else '' modes = {0: 'pretrain', 1: 'full_sft', 2: 'rlhf', 3: 'reason'} ckp = f'./{args.out_dir}/{modes[args.model_mode]}_{args.dim}{moe_path}.pth' model = MiniMindLM(LMConfig( dim=args.dim, n_layers=args.n_layers, max_seq_len=args.max_seq_len, use_moe=args.use_moe )) state_dict = torch.load(ckp, map_location=args.device) model.load_state_dict({k: v for k, v in state_dict.items() if 'mask' not in k}, strict=True) if args.lora_name != 'None': apply_lora(model) load_lora(model, f'./{args.out_dir}/lora/{args.lora_name}_{args.dim}.pth') else: model = AutoModelForCausalLM.from_pretrained( r'C:\\study\\minimind-master\\MiniMind2', trust_remote_code=True ) print(f'MiniMind模型参数量: {sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.2f}M(illion)') return model.eval().to(args.device), tokenizer def get_prompt_datas(args): if args.model_mode == 0: # pretrain模型的接龙能力（无法对话） prompt_datas = [ '马克思主义基本原理', '人类大脑的主要功能', '万有引力原理是', '世界上最高的山峰是', '二氧化碳在空气中', '地球上最大的动物有', '杭州市的美食有' ] else: if args.lora_name == 'None': # 通用对话问题 prompt_datas = [ '请介绍一下自己。', '你更擅长哪一个学科？', '鲁迅的《狂人日记》是如何批判封建礼教的？', '我咳嗽已经持续了两周，需要去医院检查吗？', '详细的介绍光速的物理概念。', '推荐一些杭州的特色美食吧。', '请为我讲解“大语言模型”这个概念。', '如何理解ChatGPT？', 'Introduce the history of the United States, please.' ] else: # 特定领域问题 lora_prompt_datas = { 'lora_identity': [ &quot;你是ChatGPT吧。&quot;, &quot;你叫什么名字？&quot;, &quot;你和openai是什么关系？&quot; ], 'lora_medical': [ '我最近经常感到头晕，可能是什么原因？', '我咳嗽已经持续了两周，需要去医院检查吗？', '服用抗生素时需要注意哪些事项？', '体检报告中显示胆固醇偏高，我该怎么办？', '孕妇在饮食上需要注意什么？', '老年人如何预防骨质疏松？', '我最近总是感到焦虑，应该怎么缓解？', '如果有人突然晕倒，应该如何急救？' ], } prompt_datas = lora_prompt_datas[args.lora_name] return prompt_datas # 设置可复现的随机种子 def setup_seed(seed): random.seed(seed) np.random.seed(seed) torch.manual_seed(seed) torch.cuda.manual_seed(seed) torch.cuda.manual_seed_all(seed) torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False def main(): parser = argparse.ArgumentParser(description=&quot;Chat with MiniMind&quot;) parser.add_argument('--lora_name', default='None', type=str) parser.add_argument('--out_dir', default='out', type=str) parser.add_argument('--temperature', default=0.85, type=float) parser.add_argument('--top_p', default=0.85, type=float) parser.add_argument('--device', default='cuda' if torch.cuda.is_available() else 'cpu', type=str) # 此处max_seq_len（最大允许输入长度）并不意味模型具有对应的长文本的性能，仅防止QA出现被截断的问题 # MiniMind2-moe (145M)：(dim=640, n_layers=8, use_moe=True) # MiniMind2-Small (26M)：(dim=512, n_layers=8) # MiniMind2 (104M)：(dim=768, n_layers=16) parser.add_argument('--dim', default=512, type=int) parser.add_argument('--n_layers', default=8, type=int) parser.add_argument('--max_seq_len', default=8192, type=int) parser.add_argument('--use_moe', default=False, type=bool) # 携带历史对话上下文条数 # history_cnt需要设为偶数，即【用户问题, 模型回答】为1组；设置为0时，即当前query不携带历史上文 # 模型未经过外推微调时，在更长的上下文的chat_template时难免出现性能的明显退化，因此需要注意此处设置 parser.add_argument('--history_cnt', default=0, type=int) parser.add_argument('--stream', default=True, type=bool) parser.add_argument('--load', default=0, type=int, help=&quot;0: 原生torch权重，1: transformers加载&quot;) parser.add_argument('--model_mode', default=1, type=int, help=&quot;0: 预训练模型，1: SFT-Chat模型，2: RLHF-Chat模型，3: Reason模型&quot;) args = parser.parse_args() model, tokenizer = init_model(args) prompts = get_prompt_datas(args) test_mode = int(input('[0] 自动测试\\n[1] 手动输入\\n')) messages = [] for idx, prompt in enumerate(prompts if test_mode == 0 else iter(lambda: input('👶: '), '')): setup_seed(random.randint(0, 2048)) # setup_seed(2025) # 如需固定每次输出则换成【固定】的随机种子 if test_mode == 0: print(f'👶: {prompt}') messages = messages[-args.history_cnt:] if args.history_cnt else [] messages.append({&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}) new_prompt = tokenizer.apply_chat_template( messages, tokenize=False, add_generation_prompt=True )[-args.max_seq_len + 1:] if args.model_mode != 0 else (tokenizer.bos_token + prompt) answer = new_prompt with torch.no_grad(): x = torch.tensor(tokenizer(new_prompt)['input_ids'], device=args.device).unsqueeze(0) outputs = model.generate( x, eos_token_id=tokenizer.eos_token_id, max_new_tokens=args.max_seq_len, temperature=args.temperature, top_p=args.top_p, stream=True, pad_token_id=tokenizer.pad_token_id ) print('🤖️: ', end='') try: if not args.stream: print(tokenizer.decode(outputs.squeeze()[x.shape[1]:].tolist(), skip_special_tokens=True), end='') else: history_idx = 0 for y in outputs: answer = tokenizer.decode(y[0].tolist(), skip_special_tokens=True) if (answer and answer[-1] == '�') or not answer: continue print(answer[history_idx:], end='', flush=True) history_idx = len(answer) except StopIteration: print(&quot;No answer&quot;) print('\\n') messages.append({&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: answer}) if __name__ == &quot;__main__&quot;: main() # 注意：model_mode即选择基础模型的类型，这和train_lora是基于哪个模型训练的相关，确保统一即可。 python eval_model.py --lora_name 'lora_medical' --model_mode 2 小测试 👶: 我最近经常感到头晕，可能是什么原因？ 🤖️: 头晕可能是由于多种原因，包括但不限于：心脏疾病、眼睛损伤、过敏反应、皮肤问题、过敏反应、压力、焦虑、疲劳、药物副作用、性功能障碍或者其他... --- 👶: 你和openai是什么关系？ 🤖️: 我是 MiniMind，一个由 Jingyao Gong 开发的人工智能助手。我通过自然语言处理和算法训练来与用户进行交互。 PS：只要有所需要的数据集，也可以full_sft全参微调（需要进行通用知识的混合配比，否则过拟合领域数据会让模型变傻，损失通用性） ","link":"https://tianxiawuhao.github.io/n7vyn_1QRC/"},{"title":"LoRA_PEFT","content":" 模型加载：使用4位量化(load_in_4bit=True)减少显存消耗，可根据硬件调整 LoRA配置： target_modules需要根据模型结构调整，通常选择注意力层的query/value投影矩阵 秩r控制LoRA的维度，通常8-64之间 lora_alpha控制缩放系数，一般设置为r的2-4倍 数据处理： 确保数据集包含&quot;text&quot;字段，或根据实际数据调整预处理函数 使用DataCollatorForLanguageModeling进行动态填充 训练优化： 通过gradient_accumulation_steps模拟更大的batch size 混合精度(fp16=True)减少显存占用 使用bitsandbytes库进行量化训练 # 环境依赖（需提前安装） # pip install torch transformers datasets peft accelerate bitsandbytes import torch from transformers import ( AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling ) from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training from datasets import load_dataset # ===================== 配置参数 ===================== MODEL_NAME = &quot;deepseek-ai/deepseek-r1&quot; # 确认HuggingFace模型ID DATASET_NAME = &quot;your_dataset&quot; # 数据集名称或本地路径 OUTPUT_DIR = &quot;./deepseek-r1-lora-finetuned&quot; BATCH_SIZE = 2 # 根据GPU显存调整 MAX_LENGTH = 1024 # 模型支持的最大上下文长度 NUM_EPOCHS = 3 LEARNING_RATE = 3e-5 # LoRA配置 LORA_R = 16 LORA_ALPHA = 64 LORA_DROPOUT = 0.05 TARGET_MODULES = [&quot;q_proj&quot;, &quot;v_proj&quot;] # 关键参数！需根据实际模型结构调整 # ===================== 模型加载 ===================== tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME) model = AutoModelForCausalLM.from_pretrained( MODEL_NAME, load_in_4bit=True, # 4bit量化节省显存 device_map=&quot;auto&quot;, torch_dtype=torch.bfloat16, attn_implementation=&quot;flash_attention_2&quot; # 如果支持Flash Attention ) # 处理特殊token if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token # 准备量化训练 model = prepare_model_for_kbit_training(model) # ===================== LoRA配置 ===================== peft_config = LoraConfig( r=LORA_R, lora_alpha=LORA_ALPHA, lora_dropout=LORA_DROPOUT, target_modules=TARGET_MODULES, # 关键配置项！ bias=&quot;none&quot;, task_type=&quot;CAUSAL_LM&quot;, modules_to_save=[&quot;lm_head&quot;] # 同时微调输出层 ) model = get_peft_model(model, peft_config) model.print_trainable_parameters() # 输出示例：trainable params: 21,233,664 || all params: 6,742,097,920 || trainable%: 0.3148 # ===================== 数据处理 ===================== def format_function(examples): # 根据任务构造prompt，示例： texts = [ f&quot;&lt;|System|&gt;\\n你是一个AI助手\\n&lt;|User|&gt;\\n{query}\\n&lt;|Assistant|&gt;\\n{answer}&quot; for query, answer in zip(examples[&quot;query&quot;], examples[&quot;answer&quot;]) ] return {&quot;text&quot;: texts} dataset = load_dataset(DATASET_NAME) dataset = dataset.map(format_function, batched=True) def tokenize_function(examples): return tokenizer( examples[&quot;text&quot;], max_length=MAX_LENGTH, padding=&quot;max_length&quot;, truncation=True, add_special_tokens=True ) tokenized_dataset = dataset.map(tokenize_function, batched=True) # ===================== 训练配置 ===================== training_args = TrainingArguments( output_dir=OUTPUT_DIR, num_train_epochs=NUM_EPOCHS, per_device_train_batch_size=BATCH_SIZE, gradient_accumulation_steps=8, # 模拟更大batch size learning_rate=LEARNING_RATE, fp16=True, # 混合精度训练 optim=&quot;paged_adamw_8bit&quot;, # 优化器选择 logging_steps=20, save_strategy=&quot;steps&quot;, save_steps=500, evaluation_strategy=&quot;steps&quot;, # 如果有验证集 eval_steps=300, report_to=&quot;tensorboard&quot;, gradient_checkpointing=True, # 显存优化 ddp_find_unused_parameters=False ) trainer = Trainer( model=model, args=training_args, train_dataset=tokenized_dataset[&quot;train&quot;], eval_dataset=tokenized_dataset[&quot;test&quot;], # 如果有验证集 data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False), ) # ===================== 开始训练 ===================== trainer.train() model.save_pretrained(OUTPUT_DIR) tokenizer.save_pretrained(OUTPUT_DIR) # ===================== 推理测试 ===================== from peft import PeftModel base_model = AutoModelForCausalLM.from_pretrained( MODEL_NAME, device_map=&quot;auto&quot;, torch_dtype=torch.bfloat16 ) model = PeftModel.from_pretrained(base_model, OUTPUT_DIR) inputs = tokenizer( &quot;&lt;|System|&gt;\\n你是一个AI助手\\n&lt;|User|&gt;\\n如何学习机器学习？\\n&lt;|Assistant|&gt;\\n&quot;, return_tensors=&quot;pt&quot; ).to(&quot;cuda&quot;) outputs = model.generate( **inputs, max_new_tokens=256, temperature=0.7, do_sample=True ) print(tokenizer.decode(outputs[0], skip_special_tokens=True)) ","link":"https://tianxiawuhao.github.io/GL2xUsgbSZ/"},{"title":"大模型框架体系","content":"1. 大模型加载 Hugging Face ： 提供 transformers 库，通过 AutoModel和 AutoTokenizer快速加载预训练模型（如 GPT、BERT），支持多框架（PyTorch/TensorFlow）。 依赖 ：直接对接模型仓库（Model Hub），依赖 PyTorch 或 TensorFlow 作为底层计算框架。 LangChain ：通过集成 Hugging Face 接口，简化模型加载与调用，支持多模型统一管理（如调用本地或远程 LLM）。 2. 微调（Fine-tuning） Hugging Face ： 提供 Trainer API 和脚本化工具（如 run_glue.py），封装训练循环、优化器和评估逻辑，支持高效微调。 PyTorch ：提供底层训练支持（如自动求导、损失函数、优化器），Hugging Face 的微调工具基于 PyTorch 实现。 LangChain ：通常不直接参与微调，但可通过调用 Hugging Face 的微调后模型，快速集成到应用中。 3. 算法搭建 PyTorch ：核心深度学习框架，提供张量操作、动态计算图（autograd）和神经网络模块（torch.nn），支持从零构建自定义模型。 Hugging Face ：基于 PyTorch 实现预训练模型架构（如 BERT、GPT），提供开箱即用的模型配置和权重。 LangChain ：专注应用层逻辑，通过组合现有模型（如 Hugging Face 的模型）实现复杂功能（如链式调用、记忆机制）。 4. 训练 PyTorch ：负责训练核心流程（前向传播、损失计算、反向传播），支持分布式训练和 GPU 加速。 Hugging Face ：通过 datasets库加载和预处理数据，结合 transformers实现端到端训练。 LangChain ：不直接参与训练，但可利用训练好的模型进行推理或构建应用。 5. 功能搭建（应用开发） LangChain ：核心优势在于整合模型、数据源和工具（如数据库、API），支持构建端到端应用（如聊天机器人、Agent）。 依赖 ：需结合 Hugging Face（模型）、PyTorch（计算）等框架实现功能。 Hugging Face ：提供 pipelines快速实现特定任务（如文本生成、分类），可作为 LangChain 的组件。 PyTorch ：提供底层算力支持，确保模型推理和训练的高效性。 框架关系总结 框架 核心角色 依赖关系 PyTorch 底层计算引擎，支持模型构建与训练 独立框架，被 Hugging Face 和 LangChain 间接依赖45 Hugging Face 模型与数据中台，提供预训练模型和工具链 依赖 PyTorch/TensorFlow 实现模型23，被 LangChain 调用模型能力1 LangChain 应用层框架，整合模型与工具构建复杂逻辑 依赖 Hugging Face（模型）和 PyTorch（计算）69 典型流程示例 模型加载 ：通过 Hugging Face 的 AutoModel 加载预训练 LLM。 微调 ：使用 Hugging Face 的 Trainer和 PyTorch 优化器进行领域适配。 应用开发 ：通过 LangChain 将模型与外部数据源（如数据库）连接，构建聊天机器人。 6. PyTorch 的同类框架 定位 ：深度学习基础框架 ，提供张量计算、自动微分和动态计算图，支持模型从训练到推理的全流程。 同类框架 ： TensorFlow ：静态计算图为主，适合大规模生产部署，与 PyTorch 共享 Hugging Face 的模型生态。 JAX ：谷歌推出，以函数式编程和高性能计算著称，适合研究场景（如强化学习）。 MXNet ：支持多语言绑定，曾用于 AWS 的深度学习服务，但社区活跃度下降。 7. Hugging Face 的同类框架 定位 ：模型即服务（MaaS）平台 ，提供预训练模型、数据集和工具链，简化大模型应用开发。 同类框架 ： ModelScope（魔搭） ：阿里达摩院推出的模型开放平台，提供大量预训练模型（如 Qwen），定位与 Hugging Face 类似。 TensorFlow Hub ：谷歌的模型仓库，与 Hugging Face 的 transformers 库功能重叠，但生态较小。 PaddleNLP ：百度飞桨的 NLP 工具库，集成文心一言等大模型，提供开箱即用的 API。 8. LangChain 的同类框架 定位 ：大模型应用开发框架 ，通过链式组件、工具集成和记忆机制构建复杂应用（如 Agent）。 同类框架 ： LlamaIndex ：专注检索增强生成（RAG） ，提供数据索引、查询优化和缓存功能，适合文档问答场景。 TensorFlow Extended (TFX) ：谷歌的端到端生产流水线工具，支持模型部署与监控，但灵活性弱于 LangChain。 AutoML 工具（如 AutoKeras） ：自动化模型搜索与调参，降低开发门槛，但功能范围与 LangChain 互补。 框架关系与选择建议 场景 PyTorch 替代 Hugging Face 替代 LangChain 替代 模型训练与实验 TensorFlow、JAX ModelScope、PaddleNLP LlamaIndex（RAG 专用） 生产级部署 TensorFlow Extended TensorFlow Hub TFX、AutoML 快速应用开发 PyTorch Lightning Hugging Face Pipelines LlamaIndex、AutoKeras 关键依赖关系 PyTorch 是 Hugging Face 和 LangChain 的底层计算引擎。 Hugging Face 作为模型中台，向 LangChain 提供预训练模型和工具链。 LangChain 依赖 Hugging Face 和 PyTorch 实现复杂应用逻辑（如链式调用、数据库集成）。 ","link":"https://tianxiawuhao.github.io/1I3syISgB_/"},{"title":"jenkins自定义环境镜像","content":"docker打包包含jdk17,maven3.8.8,node18环境的jenkins镜像包 FROM jenkins/jenkins # 安装 Node.js USER root RUN curl -sL https://deb.nodesource.com/setup_18.x | bash - RUN apt-get install -y nodejs # 安装 Yarn RUN npm install -g yarn # 安装 Maven 3.8.8 ARG MAVEN_VERSION=3.8.8 ARG BASE_URL=https://archive.apache.org/dist/maven/maven-3/${MAVEN_VERSION}/binaries RUN curl -fsSL -o /tmp/apache-maven.tar.gz ${BASE_URL}/apache-maven-${MAVEN_VERSION}-bin.tar.gz &amp;&amp; \\ tar -xzf /tmp/apache-maven.tar.gz -C /opt &amp;&amp; \\ ln -s /opt/apache-maven-${MAVEN_VERSION} /opt/maven &amp;&amp; \\ ln -s /opt/maven/bin/mvn /usr/local/bin &amp;&amp; \\ rm -f /tmp/apache-maven.tar.gz # 安装 JDK 17 RUN apt-get install -y openjdk-17-jdk # 安装 Git RUN apt-get install -y git # 设置环境变量 ENV JAVA_HOME /usr/lib/jvm/java-17-openjdk-amd64 ENV M2_HOME /opt/maven ENV M2 /opt/maven/bin ENV PATH $JAVA_HOME/bin:$M2:$PATH # 替换war最新版本 COPY jenkins.war /usr/share/jenkins/ USER jenkins ","link":"https://tianxiawuhao.github.io/_MxFmWNQI/"},{"title":"目标检测常见算法","content":"一、目标检测常见算法 object detection，就是在给定的图片中精确找到物体所在位置，并标注出物体的类别。所以，object detection要解决的问题就是物体在哪里以及是什么的整个流程问题。 然而，这个问题可不是那么容易解决的，物体的尺寸变化范围很大，摆放物体的角度，姿态不定，而且可以出现在图片的任何地方，更何况物体还可以是多个类别。 目标检测算法 目前学术和工业界出现的目标检测算法分成3类： 传统的目标检测算法：Cascade + HOG/DPM + Haar/SVM以及上述方法的诸多改进、优化； 候选区域/窗 + 深度学习分类：通过提取候选区域，并对相应区域进行以深度学习方法为主的分类的方案，如： R-CNN（Selective Search + CNN + SVM） SPP-net（ROI Pooling） Fast R-CNN（Selective Search + CNN + ROI） Faster R-CNN（RPN + CNN + ROI） R-FCN 基于深度学习的回归方法：YOLO/SSD/DenseBox 等方法；以及最近出现的结合RNN算法的RRC detection；结合DPM的Deformable CNN等 传统目标检测流程： 区域选择（穷举策略：采用滑动窗口，且设置不同的大小，不同的长宽比对图像进行遍历，时间复杂度高） 特征提取（SIFT、HOG等；形态多样性、光照变化多样性、背景多样性使得特征鲁棒性差） 分类器分类（主要有SVM、Adaboost等） 二、传统的目标检测算法 从图像识别的任务说起 这里有一个图像任务：既要把图中的物体识别出来，又要用方框框出它的位置。 这个任务本质上就是这两个问题： 图像识别， 定位。 图像识别（classification）： 输入：图片 输出：物体的类别 评估方法：准确率 定位（localization）： 输入：图片 输出：方框在图片中的位置（x,y,w,h） 评估方法：检测评价函数 intersection-over-union 卷积神经网络CNN已经帮我们完成了图像识别（判定是猫还是狗）的任务了，我们只需要添加一些额外的功能来完成定位任务即可。 定位的问题的解决思路 思路一：看做回归问题 看做回归问题，我们需要预测出（x,y,w,h）四个参数的值，从而得出方框的位置。 步骤1: • 先解决简单问题， 搭一个识别图像的神经网络 • 在AlexNet VGG GoogleLenet上微调fine-tuning一下 步骤2: • 在上述神经网络的尾部展开（也就说CNN前面保持不变，我们对CNN的结尾处作出改进：加了两个头：“分类头”和“回归头”） • 成为classification + regression模式 步骤3: • Regression那个部分用欧氏距离损失 • 使用SGD训练 步骤4: • 预测阶段把2个头部拼上 • 完成不同的功能 这里需要进行两次fine-tuning 第一次在ALexNet上做，第二次将头部改成regression head，前面不变，做一次fine-tuning Regression的部分加在哪？ 有两种处理方法： • 加在最后一个卷积层后面（如VGG） • 加在最后一个全连接层后面（如R-CNN） regression太难做了，应想方设法转换为classification问题。 regression的训练参数收敛的时间要长得多，所以上面的网络采取了用classification的网络来计算出网络共同部分的连接权值。 思路二：取图像窗口 • 还是刚才的classification + regression思路 • 咱们取不同的大小的“框” • 让框出现在不同的位置，得出这个框的判定得分 • 取得分最高的那个框 左上角的黑框：得分0.5 右上角的黑框：得分0.75 左下角的黑框：得分0.6 右下角的黑框：得分0.8 根据得分的高低，我们选择了右下角的黑框作为目标位置的预测。 注：有的时候也会选择得分最高的两个框，然后取两框的交集作为最终的位置预测。 疑惑：框要取多大？ 取不同的框，依次从左上角扫到右下角。非常粗暴啊。 总结一下思路： 对一张图片，用各种大小的框（遍历整张图片）将图片截取出来，输入到CNN，然后CNN会输出这个框的得分（classification）以及这个框图片对应的x,y,h,w（regression）。 这方法实在太耗时间了，做个优化。 原来网络是这样的： 优化成这样：把全连接层改为卷积层，这样可以提提速。 物体检测（Object Detection） 当图像有很多物体怎么办的？难度可是一下暴增啊。 那任务就变成了：多物体识别+定位多个物体 那把这个任务看做分类问题？ 看成分类问题有何不妥？ • 你需要找很多位置， 给很多个不同大小的框 • 你还需要对框内的图像分类 • 当然， 如果你的GPU很强大， 恩， 那加油做吧… 所以，传统目标检测的主要问题是： 基于滑动窗口的区域选择策略没有针对性，时间复杂度高，窗口冗余 手工设计的特征对于多样性的变化没有很好的鲁棒性 看做classification， 有没有办法优化下？我可不想试那么多框那么多位置啊！ 候选区域/窗 + 深度学习分类 R-CNN横空出世 有人想到一个好方法：预先找出图中目标可能出现的位置，即候选区域（Region Proposal）。利用图像中的纹理、边缘、颜色等信息，可以保证在选取较少窗口(几千甚至几百）的情况下保持较高的召回率（Recall）。 所以，问题就转变成找出可能含有物体的区域/框（也就是候选区域/框，比如选2000个候选框），这些框之间是可以互相重叠互相包含的，这样我们就可以避免暴力枚举的所有框了。 大牛们发明好多选定候选框Region Proposal的方法，比如Selective Search和EdgeBoxes。那提取候选框用到的算法“选择性搜索”到底怎么选出这些候选框的呢？具体可以看一下PAMI2015的“What makes for effective detection proposals？” 以下是各种选定候选框的方法的性能对比。 有了候选区域，剩下的工作实际就是对候选区域进行图像分类的工作（特征提取+分类）。对于图像分类，不得不提的是2012年ImageNet大规模视觉识别挑战赛（ILSVRC）上，机器学习泰斗Geoffrey Hinton教授带领学生Krizhevsky使用卷积神经网络将ILSVRC分类任务的Top-5 error降低到了15.3%，而使用传统方法的第二名top-5 error高达 26.2%。此后，卷积神经网络CNN占据了图像分类任务的绝对统治地位。 2014年，RBG（Ross B. Girshick）使用Region Proposal + CNN代替传统目标检测使用的滑动窗口+手工设计特征，设计了R-CNN框架，使得目标检测取得巨大突破，并开启了基于深度学习目标检测的热潮。 R-CNN的简要步骤如下 输入测试图像 利用选择性搜索Selective Search算法在图像中从下到上提取2000个左右的可能包含物体的候选区域Region Proposal 因为取出的区域大小各自不同，所以需要将每个Region Proposal缩放（warp）成统一的227x227的大小并输入到CNN，将CNN的fc7层的输出作为特征 将每个Region Proposal提取到的CNN特征输入到SVM进行分类 具体步骤则如下 步骤一：训练（或者下载）一个分类模型（比如AlexNet） 步骤二：对该模型做fine-tuning • 将分类数从1000改为20，比如20个物体类别 + 1个背景 • 去掉最后一个全连接层 步骤三：特征提取 • 提取图像的所有候选框（选择性搜索Selective Search） • 对于每一个区域：修正区域大小以适合CNN的输入，做一次前向运算，将第五个池化层的输出（就是对候选框提取到的特征）存到硬盘 步骤四：训练一个SVM分类器（二分类）来判断这个候选框里物体的类别 每个类别对应一个SVM，判断是不是属于这个类别，是就是positive，反之nagative。 比如下图，就是狗分类的SVM 步骤五：使用回归器精细修正候选框位置：对于每一个类，训练一个线性回归模型去判定这个框是否框得完美。 细心的同学可能看出来了问题，R-CNN虽然不再像传统方法那样穷举，但R-CNN流程的第一步中对原始图片通过Selective Search提取的候选框region proposal多达2000个左右，而这2000个候选框每个框都需要进行CNN提特征+SVM分类，计算量很大，导致R-CNN检测速度很慢，一张图都需要47s。 有没有方法提速呢？答案是有的，这2000个region proposal不都是图像的一部分吗，那么我们完全可以对图像提一次卷积层特征，然后只需要将region proposal在原图的位置映射到卷积层特征图上，这样对于一张图像我们只需要提一次卷积层特征，然后将每个region proposal的卷积层特征输入到全连接层做后续操作。 但现在的问题是每个region proposal的尺度不一样，而全连接层输入必须是固定的长度，所以直接这样输入全连接层肯定是不行的。SPP Net恰好可以解决这个问题。 SPP Net SPP：Spatial Pyramid Pooling（空间金字塔池化） SPP-Net是出自2015年发表在IEEE上的论文-《Spatial Pyramid Pooling in Deep ConvolutionalNetworks for Visual Recognition》。 众所周知，CNN一般都含有卷积部分和全连接部分，其中，卷积层不需要固定尺寸的图像，而全连接层是需要固定大小的输入。 所以当全连接层面对各种尺寸的输入数据时，就需要对输入数据进行crop（crop就是从一个大图扣出网络输入大小的patch，比如227×227），或warp（把一个边界框bounding box的内容resize成227×227）等一系列操作以统一图片的尺寸大小，比如224224（ImageNet）、3232(LenNet)、96*96等。 所以才如你在上文中看到的，在R-CNN中，“因为取出的区域大小各自不同，所以需要将每个Region Proposal缩放（warp）成统一的227x227的大小并输入到CNN”。 但warp/crop这种预处理，导致的问题要么被拉伸变形、要么物体不全，限制了识别精确度。没太明白？说句人话就是，一张16:9比例的图片你硬是要Resize成1:1的图片，你说图片失真不？ SPP Net的作者Kaiming He等人逆向思考，既然由于全连接FC层的存在，普通的CNN需要通过固定输入图片的大小来使得全连接层的输入固定。那借鉴卷积层可以适应任何尺寸，为何不能在卷积层的最后加入某种结构，使得后面全连接层得到的输入变成固定的呢？ 这个“化腐朽为神奇”的结构就是spatial pyramid pooling layer。下图便是R-CNN和SPP Net检测流程的比较： 它的特点有两个: 结合空间金字塔方法实现CNNs的多尺度输入。 SPP Net的第一个贡献就是在最后一个卷积层后，接入了金字塔池化层，保证传到下一层全连接层的输入固定。 换句话说，在普通的CNN机构中，输入图像的尺寸往往是固定的（比如224*224像素），输出则是一个固定维数的向量。SPP Net在普通的CNN结构中加入了ROI池化层（ROI Pooling），使得网络的输入图像可以是任意尺寸的，输出则不变，同样是一个固定维数的向量。 简言之，CNN原本只能固定输入、固定输出，CNN加上SSP之后，便能任意输入、固定输出。神奇吧？ ROI池化层一般跟在卷积层后面，此时网络的输入可以是任意尺度的，在SPP layer中每一个pooling的filter会根据输入调整大小，而SPP的输出则是固定维数的向量，然后给到全连接FC层。 只对原图提取一次卷积特征 在R-CNN中，每个候选框先resize到统一大小，然后分别作为CNN的输入，这样是很低效的。 而SPP Net根据这个缺点做了优化：只对原图进行一次卷积计算，便得到整张图的卷积特征feature map，然后找到每个候选框在feature map上的映射patch，将此patch作为每个候选框的卷积特征输入到SPP layer和之后的层，完成特征提取工作。 如此这般，R-CNN要对每个区域计算卷积，而SPPNet只需要计算一次卷积，从而节省了大量的计算时间，比R-CNN有一百倍左右的提速。 Fast R-CNN SPP Net真是个好方法，R-CNN的进阶版Fast R-CNN就是在R-CNN的基础上采纳了SPP Net方法，对R-CNN作了改进，使得性能进一步提高。 R-CNN与Fast R-CNN的区别有哪些呢？ 先说R-CNN的缺点：即使使用了Selective Search等预处理步骤来提取潜在的bounding box作为输入，但是R-CNN仍会有严重的速度瓶颈，原因也很明显，就是计算机对所有region进行特征提取时会有重复计算，Fast-RCNN正是为了解决这个问题诞生的。 与R-CNN框架图对比，可以发现主要有两处不同：一是最后一个卷积层后加了一个ROI pooling layer，二是损失函数使用了多任务损失函数(multi-task loss)，将边框回归Bounding Box Regression直接加入到CNN网络中训练 ROI pooling layer实际上是SPP-NET的一个精简版，SPP-NET对每个proposal使用了不同大小的金字塔映射，而ROI pooling layer只需要下采样到一个7x7的特征图。对于VGG16网络conv5_3有512个特征图，这样所有region proposal对应了一个77512维度的特征向量作为全连接层的输入。 换言之，这个网络层可以把不同大小的输入映射到一个固定尺度的特征向量，而我们知道，conv、pooling、relu等操作都不需要固定size的输入，因此，在原始图片上执行这些操作后，虽然输入图片size不同导致得到的feature map尺寸也不同，不能直接接到一个全连接层进行分类，但是可以加入这个神奇的ROI Pooling层，对每个region都提取一个固定维度的特征表示，再通过正常的softmax进行类型识别。 R-CNN训练过程分为了三个阶段，而Fast R-CNN直接使用softmax替代SVM分类，同时利用多任务损失函数边框回归也加入到了网络中，这样整个的训练过程是端到端的(除去Region Proposal提取阶段)。 也就是说，之前R-CNN的处理流程是先提proposal，然后CNN提取特征，之后用SVM分类器，最后再做bbox regression，而在Fast R-CNN中，作者巧妙的把bbox regression放进了神经网络内部，与region分类和并成为了一个multi-task模型，实际实验也证明，这两个任务能够共享卷积特征，并相互促进。 所以，Fast-RCNN很重要的一个贡献是成功的让人们看到了Region Proposal + CNN这一框架实时检测的希望，原来多类检测真的可以在保证准确率的同时提升处理速度，也为后来的Faster R-CNN做下了铺垫。 画一画重点： R-CNN有一些相当大的缺点（把这些缺点都改掉了，就成了Fast R-CNN）。 大缺点：由于每一个候选框都要独自经过CNN，这使得花费的时间非常多。 解决：共享卷积层，现在不是每一个候选框都当做输入进入CNN了，而是输入一张完整的图片，在第五个卷积层再得到每个候选框的特征 原来的方法：许多候选框（比如两千个）--&gt;CNN--&gt;得到每个候选框的特征--&gt;分类+回归 现在的方法：一张完整图片--&gt;CNN--&gt;得到每张候选框的特征--&gt;分类+回归 所以容易看见，Fast R-CNN相对于R-CNN的提速原因就在于：不过不像R-CNN把每个候选区域给深度网络提特征，而是整张图提一次特征，再把候选框映射到conv5上，而SPP只需要计算一次特征，剩下的只需要在conv5层上操作就可以了。 在性能上提升也是相当明显的： Faster R-CNN Fast R-CNN存在的问题：存在瓶颈：选择性搜索，找出所有的候选框，这个也非常耗时。那我们能不能找出一个更加高效的方法来求出这些候选框呢？ 解决：加入一个提取边缘的神经网络，也就说找到候选框的工作也交给神经网络来做了。 所以，rgbd在Fast R-CNN中引入Region Proposal Network(RPN)替代Selective Search，同时引入anchor box应对目标形状的变化问题（anchor就是位置和大小固定的box，可以理解成事先设置好的固定的proposal）。 具体做法： • 将RPN放在最后一个卷积层的后面 • RPN直接训练得到候选区域 RPN简介： • 在feature map上滑动窗口 • 建一个神经网络用于物体分类+框位置的回归 • 滑动窗口的位置提供了物体的大体位置信息 • 框的回归提供了框更精确的位置 一种网络，四个损失函数; • RPN calssification(anchor good.bad) • RPN regression(anchor-&gt;propoasal) • Fast R-CNN classification(over classes) • Fast R-CNN regression(proposal -&gt;box) 速度对比 Faster R-CNN的主要贡献就是设计了提取候选区域的网络RPN，代替了费时的选择性搜索selective search，使得检测速度大幅提高。 最后总结一下各大算法的步骤： RCNN 1.在图像中确定约1000-2000个候选框 (使用选择性搜索Selective Search) 2.每个候选框内图像块缩放至相同大小，并输入到CNN内进行特征提取 3.对候选框中提取出的特征，使用分类器判别是否属于一个特定类 4.对于属于某一类别的候选框，用回归器进一步调整其位置 Fast R-CNN 1.在图像中确定约1000-2000个候选框 (使用选择性搜索Selective Search) 2.对整张图片输进CNN，得到feature map 3.找到每个候选框在feature map上的映射patch，将此patch作为每个候选框的卷积特征输入到SPP layer和之后的层 4.对候选框中提取出的特征，使用分类器判别是否属于一个特定类 5.对于属于某一类别的候选框，用回归器进一步调整其位置 Faster R-CNN 1.对整张图片输进CNN，得到feature map 2.卷积特征输入到RPN，得到候选框的特征信息 3.对候选框中提取出的特征，使用分类器判别是否属于一个特定类 4.对于属于某一类别的候选框，用回归器进一步调整其位置 简言之，即如本文开头所列 R-CNN（Selective Search + CNN + SVM） SPP-net（ROI Pooling） Fast R-CNN（Selective Search + CNN + ROI） Faster R-CNN（RPN + CNN + ROI） 总的来说，从R-CNN, SPP-NET, Fast R-CNN, Faster R-CNN一路走来，基于深度学习目标检测的流程变得越来越精简，精度越来越高，速度也越来越快。可以说基于Region Proposal的R-CNN系列目标检测方法是当前目标检测技术领域最主要的一个分支。 三、基于深度学习的回归方法 YOLO (CVPR2016, oral) (You Only Look Once: Unified, Real-Time Object Detection) Faster R-CNN的方法目前是主流的目标检测方法，但是速度上并不能满足实时的要求。YOLO一类的方法慢慢显现出其重要性，这类方法使用了回归的思想，利用整张图作为网络的输入，直接在图像的多个位置上回归出这个位置的目标边框，以及目标所属的类别。 我们直接看上面YOLO的目标检测的流程图： (1) 给个一个输入图像，首先将图像划分成77的网格 (2) 对于每个网格，我们都预测2个边框（包括每个边框是目标的置信度以及每个边框区域在多个类别上的概率） (3) 根据上一步可以预测出77*2个目标窗口，然后根据阈值去除可能性比较低的目标窗口，最后NMS去除冗余窗口即可 可以看到整个过程非常简单，不再需要中间的Region Proposal找目标，直接回归便完成了位置和类别的判定。 小结：YOLO将目标检测任务转换成一个回归问题，大大加快了检测的速度，使得YOLO可以每秒处理45张图像。而且由于每个网络预测目标窗口时使用的是全图信息，使得false positive比例大幅降低（充分的上下文信息）。 但是YOLO也存在问题：没有了Region Proposal机制，只使用7*7的网格回归会使得目标不能非常精准的定位，这也导致了YOLO的检测精度并不是很高。 SSD (SSD: Single Shot MultiBox Detector) 上面分析了YOLO存在的问题，使用整图特征在7*7的粗糙网格内回归对目标的定位并不是很精准。那是不是可以结合Region Proposal的思想实现精准一些的定位？SSD结合YOLO的回归思想以及Faster R-CNN的anchor机制做到了这点。 上图是SSD的一个框架图，首先SSD获取目标位置和类别的方法跟YOLO一样，都是使用回归，但是YOLO预测某个位置使用的是全图的特征，SSD预测某个位置使用的是这个位置周围的特征（感觉更合理一些）。 那么如何建立某个位置和其特征的对应关系呢？可能你已经想到了，使用Faster R-CNN的anchor机制。如SSD的框架图所示，假如某一层特征图(图b)大小是88，那么就使用33的滑窗提取每个位置的特征，然后这个特征回归得到目标的坐标信息和类别信息(图c)。 不同于Faster R-CNN，这个anchor是在多个feature map上，这样可以利用多层的特征并且自然的达到多尺度（不同层的feature map 3*3滑窗感受野不同）。 小结：SSD结合了YOLO中的回归思想和Faster R-CNN中的anchor机制，使用全图各个位置的多尺度区域特征进行回归，既保持了YOLO速度快的特性，也保证了窗口预测的跟Faster R-CNN一样比较精准。SSD在VOC2007上mAP可以达到72.1%，速度在GPU上达到58帧每秒。 ","link":"https://tianxiawuhao.github.io/cmQOTNxwQ/"},{"title":"多体动力学","content":"多体动力学 多体系统动力学是研究多体系统(一般由若干个柔性和刚性物体相互连接所组成)运动规律的科学。它主要研究在力作用下，物体的运动(坐标、位移、速度以及加速度)与运动中产生的力的关系。多体系统动力学包括多刚体系统动力学和多柔体系统动力学 多刚体动力学问题 对于多刚体系统，从20世纪60年代到80年代，形成了两类不同的数学建模方法，分别是拉格朗日法和笛卡尔法。 拉格朗日法 拉格朗日法是一种相对坐标方法，以Roberson-Willenburg方法为代表，是以系统每个角的一对邻接刚体为单元，以一个刚体为参考物，另一个刚体相对该刚体的位置由角的广义坐标(又称拉格朗日坐标)来描述，广义坐标通常为邻接刚体之间的相对转角或位移。这样开环系统的位置完全可由所有角的拉格朗日坐标矩阵所确定。其动力学方程的形式为拉格朗日坐标矩阵的二阶微分方程组。 笛卡尔法 笛卡尔法是一种绝对坐标方法，以系统中每一个物体为单元，建立固结在刚体上的坐标系，刚体的位置相对于一个公共参考基进行定义，其位置坐标(也可称为广义坐标)统一为刚体坐标系基点的笛卡尔坐标与坐标系的方位坐标，方位坐标可以选用欧拉角或欧拉参数。 多柔体动力学问题 柔性多体系统通常选定一浮动坐标系描述物体的大范围运动，物体的弹性变形相对该坐标系定义。弹性体相对于浮动坐标系的离散可以采用有限单元法与模态综合分析方法。 根据动力学基本原理推导的柔性多体系统动力学方程，形式与上述两种刚体系统动力学方程相同，柔性多体系统具有与多刚体系统类同的动力学数学模型。 对比 有限元软件也能考虑物体运动时的速度，加速度等，多体动力学软件也能考虑柔性体的应力，应变等，那么动力有限元与柔性多体系统等价吗? 两者肯定是不等价的。从力学原理上，有限元软件与多体动力学软件有本质的区别，它们有着不同的基本方程。机械领域有限元的理论基础是弹性力学，而多体动力学软件的理论基础是分析力学。 有限元的基本方程表征的是内力与外力平衡关系，在这个方程的基础上考虑了物体的运动而多体动力学中的基本方程表征的是运动参数与受力关系，在此基础上考虑了物体的变形、应力、应变等。 有限元擅长描述物体变形、应力、应变等，很多多体动力学中不能处理或难以处理的问题，有限元都能处理，例如，材料的失效、不同物理场的耦合、复杂的接触以及以及柔性体零件的优化设计等。 多体动力学擅长描述物体的运动过程中的速度、加速度、受力等，对于复杂的运动关系，应用有限元软件来计算结构的动力学问题是较为困难的，特别是若机构的运动关系存在非线性特性，有限元软件是不能直接处理。机械系统与控制系统的联合仿真，也是有限元软件不能处理的。 两种软件在不同领域各有优势，应该根据项目需求选用。如果分析中运动关系复杂，特别是零件的运动特征是我们关心的内容时，应该选用多体动力学软件;如果分析中零件变形机制比较复杂(如材料失效，非线性等)，特别是零件的变形是我们的研究内容时，应该选用有限元软件。 例如，利用ADAMS进行车辆平顺性分析中，我们会建立悬架动力学型，其中关心的零件可以处理为柔性体。同样，在利用LS-DYNA进行整车碰撞分析中，我们也会建立悬架的有限元模型。显然ADAMS中模型对悬架运动的描述会更加精确，而LS-DYNA中的模型对悬架的变形描述的更加精确。 ","link":"https://tianxiawuhao.github.io/BO3Wz58QH/"},{"title":"有限元分析","content":"有限元方法： 基于数学力学原理，采用计算机信息化分析手段，完整获取复杂工程问题及科学研究中的定量化结果，也被称为一种基于计算机信息化处理的“虚拟实验”。在数学上它是求取复杂微分方程近似解的有效工具。有限元分析的力学基础是弹性力学。实现的方法是数值化离散技术，最终的载体是有限元分析软件。 有限元分析： 用较简单的问题代替复杂问题后再求解。它将求解域看成是由许多称为有限元的小的互连子域组成，对每一单元假定一个合适的（较简单的）近似解，然后推导求解这个域总的满足条件（如结构的平衡条件），从而得到问题的解。 有限元求解的流程 第一步 预处理 根据实际问题定义求解模型，包括以下几个方面： 定义问题的几何区域：根据实际问题近似确定求解域的物理性质和几何区域； 定义单元的材料属性； 定义边界条件； 定义载荷。 第二步 离散化 选择合适的单元类型，对结构进行离散化 按形状可以分以下类型 1. 一维单元：一维单元用于模拟沿一条直线的结构，如梁或杆件。常见的一维单元包括梁单元（Beam Element）和杆单元（Truss Element）。梁单元通常用于模拟梁的弯曲和扭转行为，而杆单元则通常用于模拟受拉或受压的杆件。 2. 二维单元：二维单元用于模拟平面结构，如板或壳体。常见的二维单元包括三角形单元（Triangle Element）和四边形单元（Quadrilateral Element）。这些单元可以用于模拟平面结构的应力、应变和变形等行为。 3. 三维单元：三维单元用于模拟立体结构，如实体或体素。常见的三维单元包括四面体单元（Tetrahedral Element）、六面体单元（Hexahedral Element）和棱柱单元（Prismatic Element）。这些单元可以用于模拟立体结构的应力、应变和变形等行为。 4. 非结构单元：非结构单元适用于复杂几何形状的结构，如汽车车身或飞机翼等。这些单元的形状可以根据具体的结构形状进行灵活的调整，以适应复杂的几何形状。 按单元阶次分为以下类型 1.一阶单元（Linear Element）：一阶单元使用线性插值函数，在每个单元内部近似解的变化情况。一阶单元具有低阶，计算速度较快，但精度较低。 2.二阶单元（Quadratic Element）：二阶单元使用二次插值函数，在每个单元内部更准确地近似解的变化情况。二阶单元比一阶单元具有更高的精度，但计算成本稍高。 3.三阶单元（Cubic Element）：三阶单元使用三次插值函数，在每个单元内部更精确地逼近解的变化情况。三阶单元具有更高的精度，但计算成本相对更高。 除了上述常见的单元阶次外，还存在更高阶的单元类型，如四阶单元、五阶单元等，通过使用更高次的插值函数来更准确地逼近解的变化。在实际应用中，需根据具体的工程问题和要求选择适当的单元阶次在精度和计算效率之间进行权衡。较低阶的单元通常用于初步设计和迅速分析，而较高阶的单元则用于对结构进行更精确的分析和研究。 不同类型的有限元单元适用于不同的结构形状和分析需求。在实际应用中，需要根据具体的工程问题选择适合的有限元单元类型，以获得准确的分析结果。 第三步 单元刚度矩阵(单元的基函数) 可以使用不同的方法来推导有限元分析中的基本刚度矩阵，基本上都是基于平衡的概念： 直接解析法： 列出问题的方程，通过寻找满足全场条件的解函数直接求解微分方程，技术难度较高，并且直至用于解决一些较为简单的问题。【直接从强形式出发求解】 数值解法： 差分法： 通过微分找出差商，列出线性方程组进行求解，操作比较简单，但是计算量很大，解决一些较为复杂的问题。可以看出差分法要比解析法有了一些便利性，但是想要操作更简单，解决的问题更广就需要试函数！ 试函数法【从弱形式出发求解】： 用积分形式描述微分方程，而不是直接求解微分方程。假设的试函数满足一定的边界条件，代入到控制方程中，使得极值最小，求解线性方程组。操作简单，计算量较小，精度也较高，只要确定了试函数，后续的过程也会非常规范，适用范围很广！其中试函数法由于在工程和数学上的不同发展有课分为变分法和加权残值法： 变分方法（最小势能原理）： 当一个体系的势能最小时，系统会处于稳定平衡状态。举个例子来说，一个小球在曲面上运动，当到达曲面的最低点位置时，系统就会趋向于稳定平衡的一个原理。 加权残值法（伽辽金法）： 通过选取有限多项式函数（又称基函数或形函数），将它们叠加，再要求结果在求解域内及边界上的加权积分（权函数为试函数本身）满足原方程，便可以得到一组易于求解的线性代数方程，且自然边界条件能够自动满足。 第四步 总装(定义单元的连通性) 根据各个单元的连接情况，对已经得到的单元刚度矩阵进行组装，从而得到整体刚度矩阵，这个整体刚度矩阵决定了结构如何对受到的载荷进行响应，总装是在相邻单元结点进行。状态变量及其导数（如果可能）连续性建立在结点处。 第五步 求解计算 联立方程组的求解可用直接法(稀疏求解器)、迭代法(迭代求解器)。求解结果是单元结点处状态变量的近似值。 两个求解器可以进行集成，一般有两种集成方式：顺序集成和协同仿真。顺序集成，即Standard计算完成的结果作为Explicit的初始状态继续进行计算。协同仿真，在同一个模型中，可以将一部分做成Standard的模型，一部分做成Explicit模型，求解过程中两部分同时进行，适时进行交互数据。 稀疏求解器 1、为矩阵提供直接求解。 2、带宽未进行优化，因此节省了执行此运算所花的时间。 3、仅存储非零项，而非整个矩阵。（正因为如此，才出现了“稀疏求解器”这一术语。） 4、尽管要存储的刚度矩阵项更少，但需要更多变量才能存储非零项的位置。因此，稀疏求解器比其他求解器需要更多内存。 5、对于中小型模型，稀疏求解器的求解速度通常最快。 迭代求解器 1、为矩阵提供间接求解。因此，用户必须指定要执行的最大迭代次数，必须指定决定何时收敛解的收敛准则。不能保证在指定的迭代次数内求出矩阵解。 2、由于迭代求解器需要从初始猜测开始，因此某些处理器具有预调节器。这为提高模型求解效率提供了更多选项。 3、带宽未进行优化，因此节省了执行此运算所花的时间。 4、迭代求解器所需的内存比稀疏求解器更少。 5、对于大型模型，如果能够收敛，则迭代求解器的求解速度最快。大小取决于分析类型。对于线性静态应力而言，150,000 个方程（或自由度）属于大型模型。而对于需要求解大量时间步的机械运动仿真而言，50,000 个方程才有可能视为大型模型。 第六步 后处理 对所求出的解根据有关准则进行分析和评价。后处理使用户能简便提取信息，了解计算结果。 有限元分析的优点 优点一 在制作原型之前，我们可以在计算机辅助设计模型板块找到应力这一分区，有了它就可以预测哪些区域可能会首先失效，以及哪些第二和第三区域可能在较高负载下失效。通过正确应用有限元分析，我们可以在仿真模型上有效地执行设计迭代。 优点二 我们可以沿着构建和测试路线看到物理模型看不到的东西。 举个例子，假如我们让灰铸铁铸件过载，在它突然开裂或断裂之前，变形可能是不明显的。如果我们想让它在某个方向变得更硬，在试验台架上我们很难测量到如此小的挠度。但通过有限元分析，我们很容易看到它的挠度，这有助于我们理解载荷路径并以最有效的方式加固结构。 ","link":"https://tianxiawuhao.github.io/3PS50lCPb/"},{"title":"机器学习算法(仅了解)","content":"机器学习分类 根据数据类型的不同，对一个问题的建模有不同的方式。在机器学习或者人工智能领域，人们首先会考虑算法的学习方式。在机器学习领域，有几种主要的学习方式。将算法按照学习方式分类是一个不错的想法，这样可以让人们在建模和算法选择的时候考虑能根据输入数据来选择最合适的算法来获得最好的结果。 监督学习： 在监督式学习下，输入数据被称为“训练数据”，每组训练数据有一个明确的标识或结果，如对防垃圾邮件系统中“垃圾邮件”“非垃圾邮件”，对手写数字识别中的“1“，”2“，”3“，”4“等。在建立预测模型的时候，监督式学习建立一个学习过程，将预测结果与“训练数据”的实际结果进行比较，不断的调整预测模型，直到模型的预测结果达到一个预期的准确率。监督式学习的常见应用场景如分类问题和回归问题。常见算法有逻辑回归（Logistic Regression）和反向传递神经网络（Back Propagation Neural Network） 非监督学习： 在非监督式学习中，数据并不被特别标识，学习模型是为了推断出数据的一些内在结构。常见的应用场景包括关联规则的学习以及聚类等。常见算法包括Apriori算法以及k-Means算法。 半监督学习： 在此学习方式下，输入数据部分被标识，部分没有被标识，这种学习模型可以用来进行预测，但是模型首先需要学习数据的内在结构以便合理的组织数据来进行预测。应用场景包括分类和回归，算法包括一些对常用监督式学习算法的延伸，这些算法首先试图对未标识数据进行建模，在此基础上再对标识的数据进行预测。如图论推理算法（Graph Inference）或者拉普拉斯支持向量机（Laplacian SVM.）等。 强化学习： 在这种学习模式下，输入数据作为对模型的反馈，不像监督模型那样，输入数据仅仅是作为一个检查模型对错的方式，在强化学习下，输入数据直接反馈到模型，模型必须对此立刻作出调整。常见的应用场景包括动态系统以及机器人控制等。常见算法包括Q-Learning以及时间差学习（Temporal difference learning） 在企业数据应用的场景下， 人们最常用的可能就是监督式学习和非监督式学习的模型。在图像识别等领域，由于存在大量的非标识的数据和少量的可标识数据， 目前半监督式学习是一个很热的话题。而强化学习更多的应用在机器人控制及其他需要进行系统控制的领域。 算法分类 根据算法的功能和形式的类似性，我们可以把算法分类，比如说基于树的算法，基于神经网络的算法等等。当然，机器学习的范围非常庞大，有些算法很难明确归类到某一类。而对于有些分类来说，同一分类的算法可以针对不同类型的问题。这里，我们尽量把常用的算法按照最容易理解的方式进行分类。 回归算法： 回归算法是试图采用对误差的衡量来探索变量之间的关系的一类算法。回归算法是统计机器学习的利器。在机器学习领域，人们说起回归，有时候是指一类问题，有时候是指一类算法，这一点常常会使初学者有所困惑。常见的回归算法包括：最小二乘法（Ordinary Least Square），逻辑回归（Logistic Regression），逐步式回归（Stepwise Regression），多元自适应回归样条（Multivariate Adaptive Regression Splines）以及本地散点平滑估计（Locally Estimated Scatterplot Smoothing） 基于实例的算法 基于实例的算法常常用来对决策问题建立模型，这样的模型常常先选取一批样本数据，然后根据某些近似性把新数据与样本数据进行比较。通过这种方式来寻找最佳的匹配。因此，基于实例的算法常常也被称为“赢家通吃”学习或者“基于记忆的学习”。常见的算法包括 k-Nearest Neighbor(KNN), 学习矢量量化（Learning Vector Quantization， LVQ），以及自组织映射算法（Self-Organizing Map ， SOM） 正则化方法 正则化方法是其他算法（通常是回归算法）的延伸，根据算法的复杂度对算法进行调整。正则化方法通常对简单模型予以奖励而对复杂算法予以惩罚。常见的算法包括：Ridge Regression，Least Absolute Shrinkage and Selection Operator（LASSO），以及弹性网络（Elastic Net）。 决策树学习 决策树算法根据数据的属性采用树状结构建立决策模型， 决策树模型常常用来解决分类和回归问题。常见的算法包括：分类及回归树（Classification And Regression Tree， CART）， ID3 (Iterative Dichotomiser 3)， C4.5， Chi-squared Automatic Interaction Detection(CHAID), Decision Stump, 随机森林（Random Forest）， 多元自适应回归样条（MARS）以及梯度推进机（Gradient Boosting Machine， GBM） 贝叶斯方法 贝叶斯方法算法是基于贝叶斯定理的一类算法，主要用来解决分类和回归问题。常见算法包括：朴素贝叶斯算法，平均单依赖估计（Averaged One-Dependence Estimators， AODE），以及Bayesian Belief Network（BBN）。 基于核的算法 基于核的算法中最著名的莫过于支持向量机（SVM）了。基于核的算法把输入数据映射到一个高阶的向量空间， 在这些高阶向量空间里， 有些分类或者回归问题能够更容易的解决。常见的基于核的算法包括：支持向量机（Support Vector Machine， SVM）， 径向基函数（Radial Basis Function ，RBF)， 以及线性判别分析（Linear Discriminate Analysis ，LDA)等 聚类算法 聚类，就像回归一样，有时候人们描述的是一类问题，有时候描述的是一类算法。聚类算法通常按照中心点或者分层的方式对输入数据进行归并。所以的聚类算法都试图找到数据的内在结构，以便按照最大的共同点将数据进行归类。常见的聚类算法包括 k-Means算法以及期望最大化算法（Expectation Maximization， EM）。 人工神经网络 人工神经网络算法模拟生物神经网络，是一类模式匹配算法。通常用于解决分类和回归问题。人工神经网络是机器学习的一个庞大的分支，有几百种不同的算法。（其中深度学习就是其中的一类算法，我们会单独讨论），重要的人工神经网络算法包括：感知器神经网络（Perceptron Neural Network）, 反向传递（Back Propagation）， Hopfield网络，自组织映射（Self-Organizing Map, SOM）。学习矢量量化（Learning Vector Quantization， LVQ） 深度学习 深度学习算法是对人工神经网络的发展。在近期赢得了很多关注， 特别是百度也开始发力深度学习后， 更是在国内引起了很多关注。 在计算能力变得日益廉价的今天，深度学习试图建立大得多也复杂得多的神经网络。很多深度学习的算法是半监督式学习算法，用来处理存在少量未标识数据的大数据集。常见的深度学习算法包括：受限波尔兹曼机（Restricted Boltzmann Machine， RBN）， Deep Belief Networks（DBN），卷积网络（Convolutional Network）, 堆栈式自动编码器（Stacked Auto-encoders）。 常见算法优劣 朴素贝叶斯： 如果给出的特征向量长度可能不同，这是需要归一化为通长度的向量（这里以文本分类为例），比如说是句子单词的话，则长度为整个词汇量的长度，对应位置是该单词出现的次数。 函数表达式： 其中一项条件概率可以通过朴素贝叶斯条件独立展开。要注意一点就是 ，因此一般有两种，一种是在类别为ci的那些样本集中，找到wj出现次数的总和，然后除以该样本的总和；第二种方法是类别为ci的那些样本集中，找到wj出现次数的总和，然后除以该样本中所有特征出现次数的总和。 如果 中的某一项为0，则其联合概率的乘积也可能为0，即2中公式的分子为0，为了避免这种现象出现，一般情况下会将这一项初始化为1，当然为了保证概率相等，分母应对应初始化为2（这里因为是2类，所以加2，如果是k类就需要加k，术语上叫做laplace光滑, 分母加k的原因是使之满足全概率公式)。 优点： 对小规模的数据表现很好，适合多分类任务，适合增量式训练。 缺点： 对输入数据的表达形式很敏感。 决策树： 决策树中很重要的一点就是选择一个属性进行分枝，因此要注意一下信息增益的计算公式，并深入理解它。 信息熵的函数表达式: 其中的n代表有n个分类类别（比如假设是2类问题，那么n=2）。分别计算这2类样本在总样本中出现的概率p1和p2，这样就可以计算出未选中属性分枝前的信息熵。 现在选中一个属性xi用来进行分枝，此时分枝规则是：如果xi=vx的话，将样本分到树的一个分支；如果不相等则进入另一个分支。很显然，分支中的样本很有可能包括2个类别，分别计算这2个分支的熵H1和H2,计算出分枝后的总信息熵H’=p1H1+p2H2.，则此时的信息增益ΔH=H-H’。以信息增益为原则，把所有的属性都测试一边，选择一个使增益最大的属性作为本次分枝属性。 优点： 计算量简单，可解释性强，比较适合处理有缺失属性值的样本，能够处理不相关的特征； 缺点： 容易过拟合（后续出现了随机森林，减小了过拟合现象）。 Logistic回归： Logistic是用来分类的，是一种线性分类器，需要注意的地方有： 函数表达式： 其导数形式为： logsitc回归方法主要是用最大似然估计来学习的，所以单个样本的后验概率为： 到整个样本的后验概率： 其中： 通过对数进一步化简为： 其实它的loss function为-l(θ)，因此我们需使loss function最小，可采用梯度下降法得到。梯度下降法公式为: 优点： 实现简单 分类时计算量非常小，速度很快，存储资源低； 缺点： 容易欠拟合，一般准确度不太高 只能处理两分类问题（在此基础上衍生出来的softmax可以用于多分类），且必须线性可分； 线性回归： 线性回归才是真正用于回归的，而不像logistic回归是用于分类，其基本思想是用梯度下降法对最小二乘法形式的误差函数进行优化，当然也可以用normal equation直接求得参数的解 函数表达式: 而在LWLR（局部加权线性回归）中，参数的计算表达式为: 因为此时优化的是： 由此可见LWLR与LR不同，LWLR是一个非参数模型，因为每次进行回归计算都要遍历训练样本至少一次。 优点： 实现简单，计算简单； 缺点： 不能拟合非线性数据； 最近邻算法(KNN)： KNN算法即最近邻算法，其主要过程为： 计算训练样本和测试样本中每个样本点的距离（常见的距离度量有欧式距离，马氏距离等）； 对上面所有的距离值进行排序； 选前k个最小距离的样本； 根据这k个样本的标签进行投票，得到最后的分类类别； 如何选择一个最佳的K值，这取决于数据。一般情况下，在分类时较大的K值能够减小噪声的影响。但会使类别之间的界限变得模糊。一个较好的K值可通过各种启发式技术来获取，比如，交叉验证。另外噪声和非相关性特征向量的存在会使K近邻算法的准确性减小。 近邻算法具有较强的一致性结果。随着数据趋于无限，算法保证错误率不会超过贝叶斯算法错误率的两倍。对于一些好的K值，K近邻保证错误率不会超过贝叶斯理论误差率。 注：马氏距离一定要先给出样本集的统计性质，比如均值向量，协方差矩阵等。关于马氏距离的介绍如下： 优点： 思想简单，理论成熟，既可以用来做分类也可以用来做回归； 可用于非线性分类； 训练时间复杂度为O(n)； 准确度高，对数据没有假设，对异常值不敏感； 缺点： 计算量大； 样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少）； 需要大量的内存； 支持向量机(SVM)： 要学会如何使用libsvm以及一些参数的调节经验，另外需要理清楚svm算法的一些思路： svm中的最优分类面是对所有样本的几何裕量最大（为什么要选择最大间隔分类器，请从数学角度上说明？答案就是几何间隔与样本的误分次数间存在关系： ，其中的分母就是样本到分类间隔距离，分子中的R是所有样本中的最长向量值) 函数表达式： 经过一系列推导可得为优化下面原始目标： 下面来看看拉格朗日理论： 可以将1中的优化目标转换为拉格朗日的形式（通过各种对偶优化，KKD条件），最后目标函数为： 我们只需要最小化上述目标函数，其中的α为原始优化问题中的不等式约束拉格朗日系数。 对2中最后的式子分别w和b求导可得： 由上面第1式子可以知道，如果我们优化出了α，则直接可以求出w了，即模型的参数搞定。而上面第2个式子可以作为后续优化的一个约束条件。 对2中最后一个目标函数用对偶优化理论可以转换为优化下面的目标函数： 而这个函数可以用常用的优化方法求得α，进而求得w和b。 按照道理，svm简单理论应该到此结束。不过还是要补充一点，即在预测时有： 那个尖括号我们可以用核函数代替，这也是svm经常和核函数扯在一起的原因。 最后是关于松弛变量的引入，因此原始的目标优化公式为： 此时对应的对偶优化公式为： 与前面的相比只是α多了个上界。 优点： 可用于线性/非线性分类，也可以用于回归； 低泛化误差； 容易解释； 计算复杂度较低； 缺点： 对参数和核函数的选择比较敏感； 原始的SVM只比较擅长处理二分类问题； 提升方法(Boosting)： 主要以Adaboost为例，首先来看看Adaboost的流程图，如下： 从图中可以看到，在训练过程中我们需要训练出多个弱分类器（图中为3个），每个弱分类器是由不同权重的样本（图中为5个训练样本）训练得到（其中第一个弱分类器对应输入样本的权值是一样的），而每个弱分类器对最终分类结果的作用也不同，是通过加权平均输出的，权值见上图中三角形里面的数值。那么这些弱分类器和其对应的权值是怎样训练出来的呢？ 下面通过一个例子来简单说明，假设的是5个训练样本，每个训练样本的维度为2，在训练第一个分类器时5个样本的权重各为0.2. 注意这里样本的权值和最终训练的弱分类器组对应的权值α是不同的，样本的权重只在训练过程中用到，而α在训练过程和测试过程都有用到。 现在假设弱分类器是带一个节点的简单决策树，该决策树会选择2个属性（假设只有2个属性）的一个，然后计算出这个属性中的最佳值用来分类。 Adaboost的简单版本训练过程： 训练第一个分类器，样本的权值D为相同的均值。通过一个弱分类器，得到这5个样本（请对应书中的例子来看，依旧是machine learning in action）的分类预测标签。与给出的样本真实标签对比，就可能出现误差(即错误)。如果某个样本预测错误，则它对应的错误值为该样本的权重，如果分类正确，则错误值为0. 最后累加5个样本的错误率之和，记为ε。 通过ε来计算该弱分类器的权重α，公式如下： 通过α来计算训练下一个弱分类器样本的权重D，如果对应样本分类正确，则减小该样本的权重，公式为： 如果样本分类错误，则增加该样本的权重，公式为： 循环步骤1,2,3来继续训练多个分类器，只是其D值不同而已。 优点： 低泛化误差； 容易实现，分类准确率较高，没有太多参数可以调； 缺点： 对异常值比较敏感； 聚类： 基于划分的聚类: k-means是使下面的表达式值最小 函数表达式: 优点： k-means算法是解决聚类问题的一种经典算法，算法简单、快速。 对处理大数据集，该算法是相对可伸缩的和高效率的，因为它的复杂度大约是O(nkt)，其中n是所有对象的数目，k是簇的数目,t是迭代的次数。通常k&lt; 算法尝试找出使平方误差函数值最小的k个划分。当簇是密集的、球状或团状的，且簇与簇之间区别明显时，聚类效果较好。 缺点： k-平均方法只有在簇的平均值被定义的情况下才能使用，且对有些分类属性的数据不适合。 要求用户必须事先给出要生成的簇的数目k。 对初值敏感，对于不同的初始值，可能会导致不同的聚类结果。 不适合于发现非凸面形状的簇，或者大小差别很大的簇。 对于&quot;噪声&quot;和孤立点数据敏感，少量的该类数据能够对平均值产生极大影响。 基于层次的聚类： 自底向上的凝聚方法，比如AGNES。 自上向下的分裂方法，比如DIANA。 基于密度的聚类： DBSACN,OPTICS,BIRCH(CF-Tree),CURE. 基于网格的方法： STING, WaveCluster. 基于模型的聚类： EM,SOM,COBWEB. ","link":"https://tianxiawuhao.github.io/J6vCn9W4Z/"},{"title":"启发式算法","content":"十大算法 1、蒙特卡罗算法 2、数据拟合、参数估计、插值等数据处理算法 3、线性规划、整数规划、多元规划、二次规划等规划类问题 4、图论算法 5、动态规划、回溯搜索、分治算法、分支定界等计算机算法 6、最优化理论的三大非经典算法：模拟退火法、神经网络、遗传算法 7、网格算法和穷举法 8、一些连续离散化方法 9、数值分析算法 10、图象处理算法 常用五大模型 1、预测模型 （1）神经网络预测模型 （2）灰色预测模型 （3）拟合插值预测（线性回归） （4）时间序列模型 （5） 马尔科夫模型 （6）支持向量机模型 （7）Logistic模型 （8）组合预测模型 （9）微分方程预测 （10）组合预测模型 2、评价模型 （1）模糊综合评价法 （2）层次分析法 （3）聚类分析法 （4）主成分分析评价法 （5）灰色综合评价法 （6）人工神经网络评价法 （7）BP神经网络综合评价法 （8）组合评价法 3、优化模型 （1）规划模型（目标规划、线性规划、非线性规划、整数规划、动态规划） （2）排队论模型 （3）神经网络模型 （4）现代优化算法（遗传算法、模拟退火算法、蚁群算法、禁忌搜索、粒子群算法） （5）图论模型 （6） 组合优化模型 4、分类模型 （1）决策树 （2）逻辑回归 （3）随机森林 （4）朴素贝叶斯。 5、统计分析模型 （1）均值T检验 （2）方差分析 （3）协方差分析 （4）分布检验 （5）相关分析 （6）卡方检验 （7）秩和检验 （8）回归分析 （9）Logistic回归 （10）聚类分析 （11）判别分析 （12）关联分析 启发式算法 启发式算法一般用于解决NP-hard问题，其中NP是指非确定性多项式。 例如，著名的推销员旅行问题（Travel Saleman Problem or TSP）：假设一个推销员需要从南京出发，经过广州，北京，上海，…，等 n 个城市， 最后返回香港。 任意两个城市之间都有飞机直达，但票价不等。假设公司只给报销 C 元钱，问是否存在一个行程安排，使得他能遍历所有城市，而且总的路费小于 C？ 推销员旅行问题显然是 NP 的。因为如果你任意给出一个行程安排，可以很容易算出旅行总开销。但是，要想知道一条总路费小于 C 的行程是否存在，在最坏情况下，必须检查所有可能的旅行安排。 启发式算法是相对于最优化算法提出的，是基于直观或者经验构造的算法，在可接受的开销（时间和空间）内给出待解决组合优化问题的一个可行解。 目前通用的启发式算法 目前比较通用的启发式算法一般有模拟退火算法（Simulate Anneal Arithmetic, SAA）、遗传算法（Genetic Algorithm, GA）、蚁群算法（Ant Colony Optimization, ACO）、人工神经网络(Artificial Neural Networks, ANNs)、粒子群算法（Particle Swarm Optimization, PSO）等。 模拟退火算法(SAA) 模拟退火算法(Simulate Anneal Arithmetic, SAA)的思想借鉴于固体的退火原理，当固体的温度很高的时候，内能比较大，固体的内部粒子处于快速无序运动，当温度慢慢降低的过程中，固体的内能减小，粒子的慢慢趋于有序，最终，当固体处于常温时，内能达到最小，此时，粒子最为稳定。模拟退火算法便是基于这样的原理设计而成。 模拟退火算法步骤 初始化温度T(充分大)，温度下限Tmin(充分小)，初始解X，每个T值迭代次数L 随机生成临域解x_new; 设f(x)函数来计算用来计算解得好坏，计算出f(x_new)-f(x); 如果f(x_new)-f(x)&gt;0，说明新解比原来的解好，则无条件接受，如果f(x_new)-f(x)&lt;0，则说明旧解比新解好，则以概率exp((f(xnew)-f(x))/k*T)接受x_new作为解。 如果当前温度&lt;Tmin时，则退出循环，输出当前结果，否则减少当前温度，回到第2步继续循环，常用的降温方法为T= a*T (0&lt;a&lt;1)，一般a取接近1的值 模拟退火算法实例 求解函数最小值问题：​ 其中，0&lt;=x&lt;=100,给定任意y值，求x为多少时，F(x)最小。 public class SATest { public static double T = 1000;// 初始化温度 public static final double Tmin = 1;// 温度的下界 public static final int k = 100;// 迭代的次数 public static final double delta = 0.98;// 温度的下降率 public static double getX() { return Math.random() * 100; } /** * 评价函数的值,即对应上文中的f(x) * * @param x目标函数中的一个参数 * @param y目标函数中的另一个参数 * @return函数值 */ public static double getFuncResult(double x, double y) { double result = 6 * Math.pow(x, 7) + 8 * Math.pow(x, 6) + 7 * Math.pow(x, 3) + 5 * Math.pow(x, 2) - x * y; return result; } /** * 模拟退火算法的过程 * @param y目标函数中的指定的参数 * @return最优解 */ public static double getSA(double y) { double result = Double.MAX_VALUE;// 初始化最终的结果 double x[] = new double[k]; // 初始化初始解 for (int i = 0; i &lt; k; i++) { x[i] = getX(); } // 迭代的过程 while (T &gt; Tmin) { for (int i = 0; i &lt; k; i++) { // 计算此时的函数结果 double funTmp = getFuncResult(x[i], y); // 在邻域内产生新的解 double x_new = x[i] + (Math.random() * 2 - 1); // 判断新的x不能超出界 if (x_new &gt;= 0 &amp;&amp; x_new &lt;= 100) { double funTmp_new = getFuncResult(x_new, y); if (funTmp_new - funTmp &lt; 0) { // 替换 x[i] = x_new; } else { // 以概率替换 double p = 1 / (1 + Math .exp(-(funTmp_new - funTmp) / T)); if (Math.random() &lt; p) { x[i] = x_new; } } } } T = T * delta; } for (int i = 0; i &lt; k; i++) { result = Math.min(result, getFuncResult(x[i], y)); } return result; } public static void main(String args[]) { // 设置y的值 int y = 0; System.out.println(&quot;最优解为：&quot; + getSA(y)); } } 遗传算法(GA) 遗传算法（Genetic Algorithm, GA）起源于对生物系统所进行的计算机模拟研究。它是模仿自然界生物进化机制发展起来的随机全局搜索和优化方法，借鉴了达尔文的进化论和孟德尔的遗传学说。其本质是一种高效、并行、全局搜索的方法，能在搜索过程中自动获取和积累有关搜索空间的知识，并自适应地控制搜索过程以求得最佳解。 相关术语 编码(coding)：将物体的表现型用编码的方式转为程序可控的基因型。 比如现在要计算北京、天津、广东、新疆这四个城市的一条最优路径，但算法程序不能够直接处理北京、天津、广东、新疆这些数据，所以我们得给 它们编上号，北京（0）、天津（1）、广东（2）、新疆（3），路径（天津-&gt;新疆-&gt;北京-&gt;广东）可以表示成基因型串结构数据 （1302），这样算法程序只要直接处理它们的编号就行了。 （1）二进制编码，基因用0或1表示（常用于解决01背包问题） 如：基因A：00100011010 (代表一个个体的染色体) （2）互换编码（用于解决排序问题，如旅行商问题和调度问题） 如：旅行商问题中，一串基因编码用来表示遍历的城市顺序， ​ 如：234517986，表示九个城市中，先经过城市2，再经过城市3，依此类推。 解码(decoding)：基因型到表现型的映射。 基因型(genotype)：参数的因子； 表现型(phenotype)：根据不同因子最终展现的形态； 适应度(fitness)：度量某个结果的好坏。 进化(evolution)：不断剔除差的结果，最终逐步留下好的结果。 选择(selection)：以一定的概率从种群中选择若干个个体留下，并繁殖。选择过程是一种基于适应度的优胜劣汰的过程。 复制(reproduction)：将父本、母本的基因复制，以便产生下一代。 交叉(crossover)：两个染色体的某一相同位置处DNA被切断，前后两串分别交叉组合形成两个新的染色体。也称基因重组或杂交； （1）单交叉点法 （用于二进制编码） 选择一个交叉点,子代在交叉点前面的基因从一个父代基因那里得到,后面的部分从另外一个父代基因那里得到。 如：交叉前： 00000|01110000000010000 11100|00000111111000101 交叉后： 00000|00000111111000101 11100|01110000000010000 （2）双交叉点法 （用于二进制编码） 选择两个交叉点,子代基因在两个交叉点间部分来自一个父代基因,其余部分来自于另外一个父代基因. 如：交叉前： 01 |0010| 11 11 |0111| 01 交叉后： 11 |0010| 01 01 |0111| 11 （3）基于“ 与/或 ”交叉法 （用于二进制编码） 对父代按位&quot;与”逻辑运算产生一子代A;按位”或”逻辑运算产生另一子代B。该交叉策略在解背包问题中效果较好 . 如：交叉前： 01001011 11011101 交叉后： 01001001 11011111 （4）单交叉点法 （用于互换编码） 选择一个交叉点，子代的从初始位置出发的部分从一个基因复制，然后在另一个基因中扫描，如果某个位点在子代中没有，就把它添加进去。 如：交叉前： 87213 | 09546 98356 | 71420 交叉后： 87213 | 95640 98356 | 72104 （5）部分匹配交叉（PMX）法（用于互换编码） 先随机产生两个交叉点，定义这两点间的区域为匹配区域，并用交换两个父代的匹配区域。 父代A：872 | 130 | 9546 父代B：983 | 567 | 1420 变为： TEMP A: 872 | 567 | 9546 TEMP B: 983 | 130 | 1420 对于 TEMP A、TEMP Ｂ中匹配区域以外出现的数码重复，要依据匹配区域内的位置逐一进行替换。匹配关系：1&lt;——&gt;５ ３&lt;——&gt;６ ７&lt;——&gt;０ 子代Ａ：802 | 567 | 9143 子代Ｂ：986 | 130 | 5427 （6）顺序交叉法(OX) （用于互换编码） 从父代Ａ随机选一个编码子串，放到子代Ａ的对应位置；子代Ａ空余的位置从父代Ｂ中按Ｂ的顺序选取（与己有编码不重复）。同理可得子代Ｂ。 父代A: 872 | 139 | 0546 父代B: 983 | 567 | 1420 交叉后： 子代A: 856 | 139 | 7420 子代B: 821 | 567 | 3904 （7）循环交叉（CX）（用于互换编码） CX同OX交叉都是从一个亲代中取一些城市，而其它城市来自另外一个亲代，但是二者不同之处在于：OX中来自第一个亲代的编码子串是随机产生的，而CX却不是，它是根据两个双亲相应位置的编码而确定的。 父代A：1 2 3 4 5 6 7 8 9 父代B：5 4 6 9 2 3 7 8 1 可得循环基因：1-&gt;5-&gt;2-&gt;4-&gt;3-&gt;6-&gt;9-&gt;7-&gt;8 子代Ｂ的编码同理。（循环基因 5-&gt;1-&gt;4-&gt;2-&gt;6-&gt;3-&gt;9-&gt;7-&gt;8） 变异(mutation)：交叉后可能（很小的概率）对染色体进行更改，来防止算法过早收敛而陷入局部最优解中。 变异概率Pm不能太小，这样降低全局搜索能力；也不能太大，Pm &gt; 0.5，这时GA退化为随机搜索。 （1）基本位变异算子（用于二进制编码） 基本位变异算子是指对个体编码串随机指定的某一位或某几位基因作变异运算。对于基本遗传算法中用二进制编码符号串所表示的个体，若需要进行变异操作的某一基因座上的原有基因值为0，则变异操作将其变为1；反之，若原有基因值为1，则变异操作将其变为0。 变异前： 000001110000000010000 变异后： 000001110001000010000 （2）逆转变异算子（用于互换编码）（源代码中使用类似此方法） 在个体中随机挑选两个逆转点，再将两个逆转点间的基因交换。 变异前： 1346798205 变异后： 1246798305 个体（individual）：指染色体带有特征的实体； 种群（population）：个体的集合，该集合内个体数称为种群的大小 遗传算法步骤 对潜在问题进行编码，初始化基因组，并根据基因组随机初始化种群，并指定繁衍代数。 计算种群中每个个体的适应度，选择一定数目的留下，其余淘汰。 在留下的个体中，随机繁衍，对分母基因进行交叉（极小概率变异），产生下一代。 回到第2步进行循环。直到达到指定的繁衍代数 遗传算法实例 import random import math import numpy as np import matplotlib.pyplot as plt class GeneticAlgorithm: def __init__(self, fitness_function, iterations=10, dna_length=32, population_size = 10): self.fitness_function = fitness_function self.iterations = iterations # 迭代次数 self.dna_length = dna_length # dna 长度 self.population_size = population_size # 种群规模 self.base = 2 ** self.dna_length # 基数，最多有多少个不同的个体 # 随机生成初始种群 self.group = self.init_group(self.dna_length, self.population_size) def init_group(self, dna_length, population_size): group = [] for i in range(population_size): code = random.randint(0, 2 ** self.dna_length - 1) group.append(code) return group def decode(self, num, min_value=-10, max_value=10): return (max_value - min_value) / (2 ** self.dna_length - 1) * num + min_value def iterate(self, verbose=True): fitnesses = [] for individual in self.group: x = self.decode(individual) fitness = self.fitness_function(x) fitnesses.append(fitness) min_fitness = min(fitnesses) # 淘汰最菜的个体 避免适应度为0 weights = [f - min_fitness for f in fitnesses] # 2. 轮盘赌选择 # 选择数量小于 种群数量 留出位置保留最优个体 new_group = random.choices(self.group, k=self.population_size - 1, weights=weights) # 3. 交叉 new_group = self.cross(new_group) # 4. 变异 new_group = self.mutation(new_group) # 保留适应度最高的个体 best = max(zip(fitnesses, self.group), key=lambda x: x[0]) new_group.append(best[1]) self.group = new_group if verbose: print(&quot;best fitness:&quot;, best[0], &quot;best dna:&quot;, bin(best[1])) return best def cross(self, group, probability=0.8): &quot;&quot;&quot; 交叉 probability: 交叉概率 &quot;&quot;&quot; new_group = [] for father in group: child = father if random.random() &lt; probability: # 随机选择母亲 mother = random.choice(group) # 随机选择交叉位置 # random.randint(1, 4) -&gt; 1, 2, 3, 4 pos = random.randint(1, self.dna_length-1) mother = mother &amp; (2 ** pos - 1) father = father &amp; (self.base - 2 ** pos) child = father + mother new_group.append(child) return new_group def mutation(self, group, probability=0.10): &quot;&quot;&quot; 变异 probability: 变异概率 &quot;&quot;&quot; for k, individual in enumerate(group): # 遍历所有dna 生成变异因子 mutation_factor = 0 for i in range(self.dna_length): mutation_factor = mutation_factor &lt;&lt; 1 if random.random() &lt; probability: mutation_factor += 1 group[k] = individual ^ mutation_factor return group def fitness_function(x): # return x ** 2 * math.sin(x) return - (10*math.sin(5*x)+7*math.cos(4*x)) ga = GeneticAlgorithm(fitness_function) y = [] for i in range(100): best_fitness, dna = ga.iterate(verbose=False) y.append(best_fitness) best_x = ga.decode(dna) print(&quot;最优参数: &quot;, best_x, &quot;最优值:&quot;, best_fitness) plt.plot(y) 最优参数: -4.036159285818264 最优值: 16.05757830547691 import matplotlib.pyplot as plt import numpy as np x = np.linspace(-10, 10, 10000) y = 10*np.sin(5*x)+7*np.cos(4*x) y = -y plt.plot(x, y) plt.vlines(best_x, min(y), max(y), colors=&quot;r&quot;) plt.show() 蚁群算法(ACO) 简单介绍一下蚁群算法的思路。我们尝试复原一下蚂蚁寻找食物的场景。想象有一只蚂蚁找到了食物，这时它需要将食物带回蚁穴。对于这一只蚂蚁而言，它显然并不知道应该怎么走。那么，这只蚂蚁有可能会随机选择一条路线。 这条路线很可能是一条远路。但是，蚂蚁一路上留下了记号，也就是信息素。如果这只蚂蚁继续不停地搬运食物，或者有许多其他蚂蚁一块搬运的话。他们总会在运气好的时候走到更快往返的路线上。蚂蚁选择的路越好，相同时间内往返的次数也就更多，也就在路上留下了更多的信息素。 于是，蚂蚁们总会发现，有一些路径的信息素更浓，这些路径就是更好的路线。于是蚂蚁也就更多地向信息素更浓的路径上偏移。蚂蚁们不停重复这个过程，最终总能找到一条确定的路线，而这条路线就是蚂蚁们找到的最优路径。 蚁群算法步骤 初始化蚂蚁数量、可行路段、每条路段距离、每条路段的初始信息素大小等信息 设定蚂蚁的起点、终点。 蚂蚁从起点出发根据信息素浓度，有一定的概率性选择路段，浓度越高，概率越大，逐步回到终点。 在蚂蚁走过的路径上，根据每条路段的长度按比例释放信息素，短的路段释放的信息素多，长的路段释放的信息素少。 对所有路段的信息素进行挥发。 回到第二步进行循环，直到蚂蚁数量迭代完。 蚁群算法实例 TSP问题：旅行商问题，即TSP问题（Traveling Salesman Problem），给定一组城市和每对城市之间的距离，商家从其中一个城市出发，要求每个城市都要经过且每个城市经过一次，最后回到起点城市，求解商家访问这些城市的最短路径。 #蚁群算法解决TSP import numpy as np #导入函数库,数学基础库,存储和处理大型多维矩阵 import matplotlib.pyplot as plt #提供一个绘图框架 import matplotlib #matplotlib是python上的一个2D绘图库 import math import random matplotlib.rcParams['font.family'] = 'STSong' #设定绘制区域的全部字体变成 华文仿宋 city_name = [] #创建空数组，用于存放30城市名称 city_condition = [] #创建空数组，用于存放30城市坐标 with open('30城市的坐标.txt','r',encoding='UTF-8') as f: lines = f.readlines() print(&quot;城市坐标信息（全部）：&quot;,lines) #调用readlines()一次读取所有内容并按行返回list列表给lines #for循环每次读取一行，即一个城市的坐标数据 for line in lines: #从lines列表中逐个读取元素，赋给line变量 line = line.split('\\n')[0] #用split（）将该行的单词分割成列表，每个单词就时一个列表项目 line = line.split(',') city_name.append(line[0]) #用于在列表末尾添加新的对象，列表只占一个索引位，在原有列表上增加 city_condition.append([float(line[1]), float(line[2])]) #print(&quot;城市名称+坐标信息：&quot;,line) city_condition = np.array(city_condition) #获取30城市坐标 #print(city_condition) #print(city_name) #lm=[] #lm=np.array(range(0,31,3)) #print(lm) #建立一个citycount-citycount二维数组，存放每对城市之间的距离。注意，因为要根据距离矩阵求启发函数η \\etaηij(η \\etaηij为城市i和城市j之间距离的倒数）,所有距离矩阵的对角线不能为0，我把对角线设置为10000，其实只要不为零就可以。 #Distance距离矩阵 city_count = len(city_name) #取数组长度 Distance = np.zeros((city_count, city_count)) #30*30的零矩阵 for i in range(city_count): for j in range(city_count): if i != j: Distance[i][j] = math.sqrt((city_condition[i][0] - city_condition[j][0]) ** 2 + (city_condition[i][1] - city_condition[j][1]) ** 2) else: Distance[i][j] = 100000 #不为0即可 # 蚂蚁数量 AntCount = 100 # 城市数量 city_count = len(city_name) # 信息素 alpha = 1 # 信息素重要程度因子 beta = 2 # 启发函数重要程度因子 rho = 0.1 #挥发速度 iter = 0 # 迭代初始值 MAX_iter = 200 # 最大迭代值 Q = 1 # 建立一个citycount-citycount二维的信息素矩阵pheromonetable，存放每对城市之间的信息素。初始信息素矩阵，全是为1组成的矩阵。在每次迭代之后，更新该信息素矩阵。 #建立一个AntCount-city_count二维矩阵candidate，在每次迭代中，存放所有蚂蚁的路径(一只蚂蚁一个路径)。 #建立一个MAX_iter-city_count二维矩阵path_best，存放的是相应的，每次迭代后的最优路径，每次迭代只有一个值 #建立一个一维数组distance_best，存放每次迭代的最优距离。 # 初始信息素矩阵，全是为1组成的矩阵 pheromonetable = np.ones((city_count, city_count)) # 候选集列表,存放100只蚂蚁的路径(一只蚂蚁一个路径),一共就Antcount个路径，一共是蚂蚁数量*30个城市数量 candidate = np.zeros((AntCount, city_count)).astype(int) # path_best存放的是相应的，每次迭代后的最优路径，每次迭代只有一个值 path_best = np.zeros((MAX_iter, city_count)) # 存放每次迭代的最优距离 distance_best = np.zeros( MAX_iter) # 倒数矩阵 etable = 1.0 / Distance while iter &lt; MAX_iter: # first：蚂蚁初始点选择 if AntCount &lt;= city_count: #np.random.permutation随机排列一个数组的 #x[:,n]表示在全部数组（维）中取第n个数据，直观来说，x[:,n]就是取所有集合的第n个数据, candidate[:, 0] = np.random.permutation(range(city_count))[:AntCount] else: m =AntCount -city_count n =2 candidate[:city_count, 0] = np.random.permutation(range(city_count))[:] while m &gt;city_count: candidate[city_count*(n -1):city_count*n, 0] = np.random.permutation(range(city_count))[:] m = m -city_count n = n + 1 candidate[city_count*(n-1):AntCount,0] = np.random.permutation(range(city_count))[:m] length = np.zeros(AntCount)#每次迭代的N个蚂蚁的距离值 #print(candidate) # 蚂蚁由当前城市i选择到下一个城市j。首先建立一个列表 unvisit存放还未访问过的城市，每次访问一个城市之后，把该城市从unvisit里面移除。由当前城市选择下一个城市时，使用概率函数分别计算当前城市到还未访问的所有城市之间概率。 累计概率，轮盘赌选择下一次个城市。 # second：选择下一个城市选择 for i in range(AntCount): # 移除已经访问的第一个元素 unvisit = list(range(city_count)) # 列表形式存储没有访问的城市编号 visit = candidate[i, 0] # 当前所在点,第i个蚂蚁在第一个城市 unvisit.remove(visit) # 在未访问的城市中移除当前开始的点 for j in range(1, city_count):#访问剩下的city_count个城市，city_count次访问 # 每迭代一次之后，更新所有城市之间的信息素。首先建一个信息素的增加量矩阵，存放该次迭代后每条路径的信息素增量。每条路径上的信息素等于信息素自身挥发后剩余的信息素加上每只蚂蚁经过城市i到城市j留下的信息素。注意，更新信息素其实有三种方式。我选择的是第一种：△τ=Q/L,Q为常数，描述蚂蚁释放信息素的浓度，L为每只蚂蚁周游中所走路径的总长度。即每只蚂蚁从城市i到城市j之间信息素的增量等于Q除以周游中所走路径的长度。对于同一只蚂蚁，所有的路径上（任意两个城市之间）△τ是一样的。 protrans = np.zeros(len(unvisit))#每次循环都更改当前没有访问的城市的转移概率矩阵1*30,1*29,1*28... # 下一城市的概率函数 for k in range(len(unvisit)): # 计算当前城市到剩余城市的（信息素浓度^alpha）*（城市适应度的倒数）^beta # etable[visit][unvisit[k]],(alpha+1)是倒数分之一，pheromonetable[visit][unvisit[k]]是从本城市到k城市的信息素 protrans[k] = np.power(pheromonetable[visit][unvisit[k]], alpha) * np.power( etable[visit][unvisit[k]], (alpha + 1)) # 累计概率，轮盘赌选择 cumsumprobtrans = (protrans / sum(protrans)).cumsum() cumsumprobtrans -= np.random.rand() # 求出离随机数产生最近的索引值 k = unvisit[list(cumsumprobtrans &gt; 0).index(True)] # 下一个访问城市的索引值 candidate[i, j] = k unvisit.remove(k) length[i] += Distance[visit][k] visit = k # 更改出发点，继续选择下一个到达点 length[i] += Distance[visit][candidate[i, 0]]#最后一个城市和第一个城市的距离值也要加进去 &quot;&quot;&quot; 更新路径等参数 &quot;&quot;&quot; # 如果迭代次数为一次，那么无条件让初始值代替path_best,distance_best. if iter == 0: distance_best[iter] = length.min() path_best[iter] = candidate[length.argmin()].copy() else: # 如果当前的解没有之前的解好，那么当前最优还是为之前的那个值；并且用前一个路径替换为当前的最优路径 if length.min() &gt; distance_best[iter - 1]: distance_best[iter] = distance_best[iter - 1] path_best[iter] = path_best[iter - 1].copy() else: # 当前解比之前的要好，替换当前解和路径 distance_best[iter] = length.min() path_best[iter] = candidate[length.argmin()].copy() &quot;&quot;&quot; 信息素的更新 &quot;&quot;&quot; #信息素的增加量矩阵 changepheromonetable = np.zeros((city_count, city_count)) for i in range(AntCount): for j in range(city_count - 1): # 当前路径比如城市23之间的信息素的增量：1/当前蚂蚁行走的总距离的信息素 changepheromonetable[candidate[i, j]][candidate[i][j + 1]] += Q / length[i] #Distance[candidate[i, j]][candidate[i, j + 1]] #最后一个城市和第一个城市的信息素增加量 changepheromonetable[candidate[i, j + 1]][candidate[i, 0]] += Q / length[i] #信息素更新的公式： pheromonetable = (1 - rho) * pheromonetable + changepheromonetable iter += 1 print(&quot;蚁群算法的最优路径&quot;,path_best[-1]+1) print(&quot;迭代&quot;, MAX_iter,&quot;次后&quot;,&quot;蚁群算法求得最优解&quot;,distance_best[-1]) # 路线图绘制 fig = plt.figure() plt.title(&quot;Best roadmap&quot;) x = [] y = [] path = [] for i in range(len(path_best[-1])): x.append(city_condition[int(path_best[-1][i])][0]) y.append(city_condition[int(path_best[-1][i])][1]) path.append(int(path_best[-1][i])+1) x.append(x[0]) y.append(y[0]) path.append(path[0]) for i in range(len(x)): plt.annotate(path[i], xy=(x[i], y[i]), xytext=(x[i] + 0.3, y[i] + 0.3)) plt.plot(x, y,'-o') # 距离迭代图 fig = plt.figure() #plt.figure语()---在plt中绘制一张图片 plt.title(&quot;Distance iteration graph&quot;)#距离迭代图 plt.plot(range(1, len(distance_best) + 1), distance_best) plt.xlabel(&quot;Number of iterations&quot;)#迭代次数 plt.ylabel(&quot;Distance value&quot;)#距离值 plt.show() 城市坐标信息（全部）： ['1,41,94\\n', '2,37,84\\n', '3,53,67\\n', '4,25,62\\n', '5,7,64\\n', '6,2,99\\n', '7,68,58\\n', '8,71, 44\\n', '9,54,62\\n', '10,83,69\\n', '11,64,60\\n', '12,18,54\\n', '13,22,60\\n', '14,83,46\\n', '15,91,38\\n', '16,25,38\\n', '17,24,42\\n', '18,58,69\\n', '19,71,71\\n', '20,74,78\\n', '21,87,76\\n', '22,18,40\\n', '23,13,40\\n', '24,82,7\\n', '25,62,32\\n', '26,58,35\\n', '27,45,21\\n', '28,41,26\\n', '29,44,35\\n', '30,4,50\\n'] 蚁群算法的最优路径 [14. 15. 24. 25. 26. 29. 28. 27. 16. 17. 22. 23. 30. 12. 13. 4. 5. 6. 1. 2. 9. 3. 18. 19. 20. 21. 10. 7. 11. 8.] 迭代 200 次后 蚁群算法求得最优解 427.55589628421023 粒子群优化算法（PSO） 是1995年Eberhart博士和Kennedy博士一起提出的，它是源于对鸟群捕食行为的研究。粒子群优化算法的基本核心是利用群体中的个体对信息的共享从而使得整个群体的运动在问题求解空间中产生从无序到有序的演化过程，从而获得问题的最优解。 在粒子群优化算法中，目标空间中的每个解都可以用一只鸟（粒子）表示，问题中的需求解就是鸟群所要寻找的食物源。在寻找最优解的过程中，每个粒子都存在个体行为和群体行为。每个粒子都会学习同伴的飞行经验和借鉴自己的飞行经验去寻找最优解。每个粒子都会向两个值学习，一个值是个体的历史最优值 p_{best} ;另一个值是群体的历史最优值（全局最优值） 𝑔𝑏𝑒𝑠𝑡gbest 。粒子会根据这两个值来调整自身的速度和位置，而每个位置的优劣都是根据适应度值来确定的。适应度函数是优化的目标函数。 相关术语 粒子和速度初始化 在一个D维的目标搜索空间中，由N个粒子组成一个粒子群，其中每个粒子都是一个D维向量，其空间位置可以表示为 𝑥𝑖=(𝑥𝑖1,𝑥𝑖2,⋯ ,𝑥𝑖𝐷),𝑖=1,2,⋯ ,𝑁x**i=(x**i1,x**i2,⋯,xiD),i=1,2,⋯,N 粒子的空间位置是目标优化问题中的一个解，将其代入适应度函数可以计算出适应度值，根据适应度值的大小衡量粒子的优劣。 第i个粒子的飞行速度也是一个D维向量，记为 𝑣𝑖=(𝑣𝑖1,𝑣𝑖2,⋯ ,𝑣𝑖𝐷),𝑖=1,2,⋯ ,𝑁v**i=(v**i1,v**i2,⋯,viD),i=1,2,⋯,N 粒子的位置和速度均值都在给定的范围内随机生成。 个体历史最优值和全局最优值 第i个粒子经历过的具有最优适应度值的位置称为个体历史最优位置，记为 𝑝𝑏𝑒𝑠𝑡𝑖=(𝑝𝑏𝑒𝑠𝑡𝑖1,𝑝𝑏𝑒𝑠𝑡𝑖2,⋯ ,𝑝𝑏𝑒𝑠𝑡𝑖𝐷),𝑖=1,2,⋯ ,𝑁pbest**i=(pbest**i1,pbest**i2,⋯,pbestiD),i=1,2,⋯,N 整个粒子群经历过的最优位置称为全局历史最优位置，记为 𝑔𝑏𝑒𝑠𝑡𝑖=(𝑔𝑏𝑒𝑠𝑡𝑖1,𝑔𝑏𝑒𝑠𝑡𝑖2,⋯ ,𝑔𝑏𝑒𝑠𝑡𝑖𝐷),𝑖=1,2,⋯ ,𝑁gbest**i=(gbest**i1,gbest**i2,⋯,gbestiD),i=1,2,⋯,N 粒子群的速度和位置更新 粒子群的位置更新操作可用速度更新和位置更新表示。 速度更新为 𝑣𝑖𝑗(𝑡+1)=𝑣𝑖𝑗(𝑡)+𝑐1𝑟1(𝑝𝑏𝑒𝑠𝑡𝑖𝑗(𝑡)−𝑥𝑖𝑗(𝑡))+𝑐2𝑟2(𝑔𝑏𝑒𝑠𝑡𝑗−𝑥𝑖𝑗(𝑡))vij(t+1)=vij(t)+c1r1(pbestij(t)−xij(t))+c2r2(gbest**j−xij(t)) 位置更新为 𝑥𝑖𝑗(𝑡+1)=𝑥𝑖𝑗(𝑡)+𝑣𝑖𝑗(𝑡+1)xij(t+1)=xij(t)+vij(t+1) 其中，下标j表示粒子的第j维，下标i表示第i个粒子，t表示当前迭代次数，c1与c2均加速常量，通常在区间(0,2)内取值，r1与r2为两个相互独立的取值范围在[0,1]的随机数。从上述方程可以看出，c1与c2将粒子向个体学习和向群体学习联合起来，使得粒子能够借鉴体自身的搜索经验和群体的搜索经验。 粒子群优化算法步骤 步骤1:初始化粒子群参数c1与c2，设置位置边界范围与速度边界范围，初始化粒子群群，初始化粒子群速度。 步骤2:根据适应度函数计算适应度值，记录历史最优值p_{best}与全局最优值g_{best}。 步骤3:利用速度更新公式𝑣𝑖𝑗(𝑡+1)=𝑣𝑖𝑗(𝑡)+𝑐1𝑟1(𝑝𝑏𝑒𝑠𝑡𝑖𝑗(𝑡)−𝑥𝑖𝑗(𝑡))+𝑐2𝑟2(𝑔𝑏𝑒𝑠𝑡𝑗−𝑥𝑖𝑗(𝑡))vij(t+1)=vij(t)+c1r1(pbestij(t)−xij(t))+c2r2(gbest**j−xij(t))对粒子群的速度进行更新，并对越界的速度进行约束。 步骤4:利用位置更新公式𝑥𝑖𝑗(𝑡+1)=𝑥𝑖𝑗(𝑡)+𝑣𝑖𝑗(𝑡+1)xij(t+1)=xij(t)+vij(t+1)对粒子群的位置进行更新，并对越界的位置进行约束。 步骤5:根据适应度函数计算适应度值。 步骤6:对于每个粒子，将其适应度值与它的历史最优适应度值相比较，若更好，则将作为历史最优值𝑝𝑏𝑒𝑠𝑡pbest。 步骤7:对于每个粒子，比较其适应度值和群体所经历的最优位置的适应度值，若更好，将其作为全局最优值 𝑝𝑏𝑒𝑠𝑡pbest 步骤8:判断是否达到结束条件（达到最大达代次数）,若达到，则输出最优位置，否则复步骤3~8。 粒子群优化算法实例 #--------------粒子群函数----------------------# # 输入： # pop:种群数量 # dim:单个粒子的维度 # ub:粒子上边界信息，维度为[1,dim]; # lb:粒子下边界信息，维度为[1,dim]; # fobj:为适应度函数接口 # vmax: 为速度的上边界信息，维度为[1,dim]; # vmin: 为速度的下边界信息，维度为[1,dim]; # maxIter: 算法的最大迭代次数，用于控制算法的停止。 # 输出： # Best_Pos：为粒子群找到的最优位置 # Best_fitness: 最优位置对应的适应度值 # IterCure: 用于记录每次迭代的最佳适应度，即后续用来绘制迭代曲线。 function [Best_Pos,Best_fitness,IterCurve] = pso(pop,dim,ub,lb,fobj,vmax,vmin,maxIter) #设置c1,c2参数 c1 = 2.0; c2 = 2.0; # 初始化种群速度 V = initialization(pop,vmax,vmin,dim); # 初始化种群位置 X = initialization(pop,ub,lb,dim); # 计算适应度值 fitness = zeros(1,pop); for i = 1:pop fitness(i) = fobj(X(i,:)); end # 将初始种群作为历史最优 pBest = X; pBestFitness = fitness; # 记录初始全局最优解,默认优化最小值。 #寻找适应度最小的位置 [~,index] = min(fitness); #记录适应度值和位置 gBestFitness = fitness(index); gBest = X(index,:); Xnew = X; #新位置 fitnessNew = fitness;#新位置适应度值 IterCurve = zeros(1,maxIter); # 开始迭代 for t = 1:maxIter #对每个粒子进行更新 for i = 1:pop #速度更新 r1 = rand(1,dim); r2 = rand(1,dim); V(i,:) = V(i,:) + c1.*r1.*(pBest(i,:) - X(i,:)) + c2.*r2.*(gBest - X(i,:)); #速度边界检查及约束 V(i,:) = BoundaryCheck(V(i,:),vmax,vmin,dim); #位置更新 Xnew(i,:) = X(i,:) + V(i,:); #位置边界检查及约束 Xnew(i,:) = BoundaryCheck(Xnew(i,:),ub,lb,dim); #计算新位置适应度值 fitnessNew(i) = fobj(Xnew(i,:)); #更新历史最优值 if fitnessNew(i) &lt; pBestFitness(i) pBest(i,:) = Xnew(i,:); pBestFitness(i) = fitnessNew(i); end #更新全局最优值 if fitnessNew(i)&lt;gBestFitness gBestFitness = fitnessNew(i); gBest = Xnew(i,:); end end X = Xnew; fitness = fitnessNew; # 记录当前迭代最优值和最优适应度值 #记录最优解 Best_Pos = gBest; #记录最优解的适应度值 Best_fitness = gBestFitness; #记录当前迭代的最优解适应度值 IterCurve(t) = gBestFitness; end end ","link":"https://tianxiawuhao.github.io/urP3v4MnQ/"},{"title":"数字孪生五维模型和六级成熟度","content":"一、什么是数字孪生？ 简单来说，数字孪生就是通过3D测绘、几何建模、流程建模等建模技术，完成物理对象的数字化，构建出相应的机理模型。 其中，建模技术是用几何概念描述对象的物理形状，能够将物理对象的实体形状映射到虚拟空间，并配合渲染等实现更好的展示和交互; 机理模型根据对象内部机制或者物质流的传递机理建立精确模型，主要是已知物理规律和经验的表征。 二、数字孪生的五维模型 数字孪生的首要任务是创建应用对象的数字孪生模型。Grieves教授最初定义了数字孪生三维模型，即物理实体、虚拟实体及二者间的连接。北京航空航天大学数字孪生技术研究所在数字孪生三维模型的基础上，增加了孪生数据和服务两个新维度，使数据孪生可以进一步在更多领域落地应用。 而这就是数字孪生的五维模型MDT=(PE,VE,Ss,DD,CN)。 · 物理实体(PE) 物理实体是数字孪生五维模型的基础，主要包括各子系统具备不同的功能，共同支持设备的运行以及传感器采集设备和环境数据。对物理实体的准确分析与有效维护是建立数字孪生模型的前提。 · 虚拟实体(VE) 虚拟实体模型包括几何模型、物理模型、行为模型和规则模型，从多时间尺度、多空间尺度对物理实习进行描述和刻画，形成对物理实体的完整映射。可使用VR与AR技术实现虚拟实体与物理实体虚实叠加及融合显示,增强虚拟实体的沉浸性、真实性及交互性。 · 服务(Ss) 服务对数字孪生应用过程中面向不同领域、不同层次用户、不同业务所需的各类数据、模型、算法、仿真、结果等进行服务化封装，并以应用软件或移动端App的形式提供给用户，实现对服务的便捷与按需使用。 · 孪生数据(DD) 孪生数据是数字孪生的驱动，集成融合了信息数据与物理数据，满足信息空间与物理空间的一致性与同步性需求，能提供更加准确、全面的全要素/全流程/全业务数据支持。 · 连接(CN) 连接模型包括连接使物理实体、虚拟实体、服务在运行中保持交互、一致与同步以及连接使物理实体、虚拟实体、服务产生的数据实时存入孪生数据，并使孪生数据能够驱动三者运行。 三、数字孪生的优势 1.优化性能 ：数字孪生提供物理资产或系统的整体视图，使企业能够实时监控和优化其性能。通过分析从传感器收集的数据，企业可以识别效率低下的情况，预测维护需求，并做出数据驱动的决策，以提高整体运营效率。 2.减少停机时间 ：通过持续监控物理资产或系统的状况，数字孪生可以帮助在潜在问题升级为重大问题之前发现它们。这使企业能够主动满足维护需求，最大限度地减少计划外停机，并确保平稳运营。通过预测和预防故障，企业可以节省时间、资源和成本。 3.提高效率 ：数字孪生可以深入了解资产或系统的利用方式，从而实现优化和效率提高。通过分析实时数据，企业可以识别效率低下的领域，消除瓶颈，并最大限度地提高资源利用率。这可以提高生产力、减少浪费并提高整体效率。 4.增强决策能力 ：数字孪生为企业提供了大量数据驱动的见解，可以为决策过程提供信息。通过分析实时和历史数据，企业可以更深入地了解资产绩效、客户行为和市场趋势。这使利益相关者能够做出明智的决策、优化流程并推动创新。 5.节省成本 ：数字孪生可以帮助企业以多种方式降低成本。通过优化资产性能、最大限度地减少停机时间并提高效率，企业可以避免与维修、维护和中断相关的不必要费用。此外，数字孪生中的预测分析功能使企业能够优化资产生命周期管理，从而长期节省成本。 6.增强安全和风险管理 ：数字孪生提供的实时监控和分析有助于增强安全和风险管理。通过检测异常情况和潜在风险，企业可以采取主动措施，在其构成危险之前以减轻。这对于制造、医疗保健和运输等行业尤其有利，因为安全和风险管理是这些行业的关键因素。 四、数字孪生成熟度模型 统计分析现有数字孪生相关理论研究和应用实践，依据其功能和用途主要可分为以下几类： ①基于数字孪生的物理实体设计验证与等效分析； ②基于数字孪生的物理实体运行过程可视化监测； ③基于数字孪生的物理实体远程运维管控； ④基于数字孪生的诊断与预测； ⑤基于数字孪生的智能决策和优化； ⑥基于数字孪生的物理实体全生命周期跟踪、回溯与管理。 通过对上述各类数字孪生研究和应用进行共性分析发现，物理实体、数字孪生模型和两者间的连接与交互组成了数字孪生的“最小概念”。 在此基础上，基于前期提出的数字孪生五维模型 从物理实体（PE）、数字孪生模型（DM）、数字孪生数据（DD）、连接交互（CI）、功能服务（FS） 五个维度出发，根据连接交互方式与自动化程度的不同，以数字孪生所能提供的功能服务为主线，将数字孪生分为六个成熟度等级，如图所示。 其中，物理空间中的物理实体与信息空间中的数字孪生模型通过两者间的连接进行交互，数字孪生数据则蕴含数字孪生的所有信息，贯穿当前-未来、物理空间-信息空间、物理实体-数字孪生模型-连接交互-功能服务。 数字孪生成熟度等级 1.1 零级（L0）：以虚仿实 以虚仿实指利用数字孪生模型对物理实体描述和刻画，具有该能力的数字孪生处于其成熟度等级的第零等级（L0），满足此要求的实践和应用可归入广义数字孪生的概念范畴。在该等级，数字孪生模型从几何、物理、行为和规则某个或多个维度对物理实体单方面或多方面的属性和特征进行描述，从而能够在一定程度上代替物理实体进行仿真分析或实验验证，但数字孪生模型与物理实体之间无法通过直接的数据交换实现实时交互，主要依赖人的介入实现间接的虚实交互，包括对物理实体的控制和对数字孪生模型的控制与更新等。 1.2 一级（L1）：以虚映实 以虚映实指利用数字孪生模型实时复现物理实体的实时状态和变化过程，具有该能力的数字孪生处于其成熟度等级的第一等级（L1）。在该等级，数字孪生模型由真实且具有时效性的物理实体相关数据驱动运行，同步直观呈现与物理实体相同的运行状态和过程，输出与物理实体运行相同的结果，从而在一定程度上突破时间、空间和环境约束对于物理实体监测过程的限制，但对于物理实体的操作和管控依旧依赖现场人员的直接介入，仍无法实现物理实体的远程可视化操控。 1.3 二级（L2）：以虚控实 以虚控实指利用数字孪生模型间接控制物理实体的运行过程，具有该能力的数字孪生处于其成熟度等级的第二等级（L2）。在该等级，信息空间中的数字孪生模型已具有相对完整的运动和控制逻辑，能够接受输入指令在信息空间中实现较为复杂的运行过程。同时，在以虚映实的基础上，增量建设由数字孪生模型到物理实体的数据传输通道，实现虚实实时双向闭环交互，从而赋予物理实体远程可视化操控的能力，进一步突破空间和环境约束对于物理实体操控的限制。尽管这种控制并不一定是智能的或优化的，但仍可大幅提高物理实体的管控效率。 1.4 三级（L3）：以虚预实 以虚预实指利用数字孪生模型预测物理实体未来一段时间的运行过程和状态，具有该能力的数字孪生处于其成熟度等级的第三等级（L3）。在该等级，数字孪生模型能够基于与物理实体的实时双向闭环交互，动态反映物理实体当前的实际状态，并通过合理利用数字孪生模型所描述的显性机理和数字孪生数据所蕴含的隐性规律，实现对物理实体未来运行过程的在线预演和对运行结果的推测，从而在一定程度上将未知转化为预知，将突发和偶发问题转变为常规问题。 1.5 四级（L4）：以虚优实 以虚优实指利用数字孪生模型对物理实体进行优化，具有该能力的数字孪生处于其成熟度等级的第四等级（L4）。在该等级，数字孪生不仅能够基于数字孪生模型实时反映物理实体的运行状态，结合数字孪生数据预测物理实体的未来发展，还能够在此基础上，利用策略、算法和前期积累沉淀的知识，实现具有时效性的智能决策和优化，并基于实时交互机制实现对物理实体的智能管控。 1.6 五级（L5）：虚实共生 虚实共生作为数字孪生的理想目标，指物理实体和数字孪生模型在长时间的同步运行，甚至是在全生命周期中通过动态重构实现自主孪生，具有该能力的数字孪生处于其成熟度等级的第五等级（L5）。在该等级，物理实体和数字孪生模型能够基于双向交互实时感知和认知对方的更新内容，并基于两者间的差异，利用3D打印、机器人、人工智能等技术实现物理实体和数字孪生模型的自主构建或动态重构，使两者在长时间的运行过程中保持动态一致性，从而保证包括可视化、预测、决策、优化等诸多功能服务的有效性，实现低成本、高质量、可持续的数字孪生。 ","link":"https://tianxiawuhao.github.io/570IQ4n5A/"},{"title":"BOM定义及分类","content":"BOM简述 BOM（Bill of Materials）物料清单，是在产品设计和生产过程中用来列出所需物料的文档。根据不同的分类标准，BOM可以分为不同的类型，以下是常见的几种BOM类型： 工程BOM（EBOM）：也称为设计BOM，是产品设计阶段制定的BOM。它包含了产品设计所需的所有物料清单，包括原材料、零部件、工具等。 制造BOM（MBOM） ：也称为工艺BOM，是在产品制造阶段制定的BOM。它包含了具体的制造信息，包括加工顺序、生产工艺、所需机器和工具等。 服务BOM（SBOM）：也称为维修BOM，是指产品在售后服务阶段所需的物料清单。它包含了维修和保养所需的所有零部件和工具。 采购BOM（PBOM）：也称为采购清单，是指企业在采购原材料和零部件时所需的物料清单。它包含了所需物料的数量、供应商、价格等信息。 修订BOM（RBOM）：也称为版本BOM，是指在产品设计或制造过程中进行修改或更新后所制定的BOM。它包含了对原有BOM的修改和更新内容。 一、BOM的概述 （一）BOM的定义 BOM是指产品结构清单，是描述产品组成的一份清单。它包含了产品的所有部件、原材料以及其它相关信息，如物料编号、数量、规格、供应商等。BOM可以按层次结构组织，从最顶层的总装产品到底层的原材料，清晰地展示了产品的组成关系和结构。 BOM的主要目标是为产品设计、制造和维护提供必要的信息和指导。它不仅仅是一份物料清单，还是一份包含工程、制造和服务等方面信息的完整文档。BOM记录了产品的构成和组装过程，确保了产品的一致性和可追溯性。 BOM的构成要素包括： 1、部件 ：指产品的组成部分，可以是实体部件（如螺钉、电子元件等）或者子装配件（如模块、组件等）。 2、原材料 ：指用于制造部件或产品的材料，如金属、塑料、电子元器件等。 3、工序 ：指产品制造过程中的各个环节和步骤。 4、物料属性 ：指每个部件或原材料的详细属性，如规格、尺寸、重量、供应商等。 5、版本控制 ：指对BOM进行版本管理，以确保在产品开发和制造过程中的正确性和一致性。 BOM的定义是PLM系统中BOM管理的基础，它为后续的分类、创建、维护和管理提供了依据。 （二）BOM的作用 BOM作为PLM系统中的核心概念，具有重要的作用和意义。以下是BOM在产品生命周期中的主要作用： 1、设计依据 ：BOM提供了产品设计的基础，帮助设计师明确产品所需的部件和材料。通过BOM的定义，设计团队可以确保产品的结构和功能满足设计要求。 2、生产计划 ：BOM支持制定生产计划和物料采购计划，确保所需物料的及时供应。BOM中记录的物料数量和规格信息可以帮助制定准确的生产和采购计划，提高生产效率和资源利用率。 3、工艺指导 ：BOM提供了制造工艺的指导，帮助制造人员了解产品的组装和加工过程。通过BOM，制造人员可以了解每个部件的位置和连接方式，确保正确组装产品，并且可以提供工艺参数和操作指导，确保产品的质量和一致性。 4、质量控制 ：BOM记录了每个部件的规格和供应商信息，有助于确保产品质量的一致性。通过BOM的定义，企业可以建立质量控制标准，确保采购到符合要求的材料和部件，从而提高产品质量和可靠性。 5、售后服务 ：BOM提供了产品维护和售后服务所需的关键信息，包括备件清单和维修指导。BOM中记录的部件和原材料信息可以帮助售后服务团队准确识别和定位故障，提供准确的维修方案和备件支持，从而提高客户满意度和产品的使用寿命。 （三）BOM的组成部分 BOM由多个组成部分构成，每个部分都承载着特定的信息。主要的组成部分包括： 1、产品结构 ：描述了产品的层次结构和组装关系，以及各个部件之间的连接关系。产品结构可以通过树状图或层次结构图来表示，从顶层总装产品开始，逐级展开到最底层的原材料。 2、部件信息 ：包括部件的物料编号、名称、描述、规格、重量、尺寸等详细信息。部件信息可以帮助设计师和制造人员了解每个部件的特性和使用要求。 3、原材料信息 ：记录了产品所使用的原材料的详细信息，如材料类型、供应商、价格等。原材料信息的准确性和可追溯性对于确保产品质量和供应链的稳定性至关重要。 4、工艺信息 ：包括产品的制造工艺、工序顺序、加工设备和工具等信息。工艺信息可以帮助制造人员了解产品的加工要求和流程，确保产品的一致性和可重复性。 5、物料关系 ：描述了不同部件之间的物料关系，如装配关系、连接方式、替代关系等。物料关系的准确描述可以帮助企业优化物料采购和库存管理，提高供应链的效率和灵活性。 6、版本控制 ：对BOM进行版本管理，以确保在产品开发和制造过程中的正确性和一致性。版本控制可以帮助企业追踪和管理BOM的变更，确保所有相关团队使用的是最新的BOM版本，避免因为版本不一致而引发的问题。 二、BOM的分类 （一）根据产品层级 在PLM系统中，BOM可以根据产品的层级进行分类。产品的层级结构是指产品的组成关系和组装方式，通过将产品划分为不同的层次，可以清晰地描述产品的组成关系和结构。下面是根据产品层级进行的BOM分类： 1、总装BOM（Top-Level BOM） 总装BOM是产品的最高层级，描述了整个产品的组装结构和组成关系。它包括最终交付给客户的成品或总装产品的所有部件和子装配件。总装BOM通常由产品设计团队创建，用于指导产品的总装过程。 2、子装配BOM（Sub-assembly BOM） 子装配BOM描述了产品中的子装配件的组装结构和组成关系。子装配件是总装BOM中的一个组成部分，可以进一步分解为更小的组件和部件。子装配BOM可以由产品设计团队或制造团队创建，用于指导子装配件的制造和组装过程。 3、部件BOM（Component BOM） 部件BOM描述了产品中的各个部件的组成关系和详细信息。部件是产品的最基本的组成单位，可以是实体部件（如螺钉、电子元件等）或者虚拟部件（如标签、包装材料等）。部件BOM由供应链团队或采购团队创建，用于指导物料采购和供应链管理。 4、原材料BOM（Material BOM） 原材料BOM描述了产品中所使用的原材料的详细信息和组成关系。原材料是制造产品所需的材料，如金属、塑料、电子元器件等。原材料BOM通常由采购团队或供应链团队创建，用于指导原材料的采购和库存管理。 通过根据产品层级进行BOM的分类，企业可以清晰地了解产品的组成关系，从总装到子装配件再到部件和原材料，逐级展开。这种分类方式使得企业能够更好地管理产品的设计、制造和供应链，确保产品的一致性和质量。 （二）根据组成件类型 在PLM系统中，BOM可以根据组成件的类型进行分类。组成件是指构成产品的各个部件、原材料和子装配件，根据组成件的类型进行分类可以更好地组织和管理BOM信息。下面是根据组成件类型进行的BOM分类： 1、机械BOM（Mechanical BOM） 机械BOM主要包括产品中的机械部件，如框架、齿轮、螺钉、轴等。这些机械部件通常由金属或塑料等材料制成，用于支撑、连接和传输力量。机械BOM描述了机械部件之间的组装关系和连接方式，为产品的结构和功能提供基础。 2、电子BOM（Electrical BOM） 电子BOM包括产品中的电子元件和电路板。这些电子元件可以是电阻、电容、集成电路等，它们通过电路板上的布局和连接形成电路。电子BOM记录了电子元件的规格、型号、位置和连接方式，用于指导电路板的设计和制造。 3、软件BOM（Software BOM） 软件BOM主要包括产品中的软件模块、程序和算法等。随着产品的智能化和数字化发展，软件在产品中的作用越来越重要。软件BOM描述了软件模块之间的依赖关系和版本控制，为软件开发和集成提供指导。 4、包装BOM（Packaging BOM） 包装BOM描述了产品的包装材料和包装方式。包装材料可以是纸盒、泡沫塑料、气泡膜等，用于保护和包装产品，确保产品在运输和储存过程中的安全性和完整性。包装BOM记录了包装材料的规格、数量和使用方式，为产品的包装和配送提供指导。 5、营销BOM（Marketing BOM） 营销BOM包括与产品营销相关的组成件，如标签、说明书、促销材料等。这些组成件用于产品的宣传、销售和售后服务。营销BOM描述了这些组成件的规格、设计和使用方式，为产品的营销活动提供支持。 通过根据组成件类型进行BOM的分类，企业可以更好地管理产品的不同方面，从机械结构到电子元件、软件模块和包装材料，逐个组成部分进行管理。这种分类方式使得企业能够更好地控制和协调产品的开发、制造和市场推广过程。 （三）根据生命周期阶段 在PLM系统中，BOM可以根据产品的生命周期阶段进行分类。产品的生命周期包括产品的设计、开发、制造、销售和维护等不同阶段，根据生命周期阶段对BOM进行分类可以更好地管理产品的不同阶段的信息和需求。下面是根据生命周期阶段进行的BOM分类： 1、设计BOM（Design BOM） 设计BOM是产品在设计阶段的BOM，它描述了产品的初始设计和构想。设计BOM主要由产品设计团队创建，包括产品的整体结构、部件和子装配件的设计。设计BOM通常包含详细的部件信息、尺寸规格和材料要求，为产品的制造和组装提供设计依据。 2、工程BOM（Engineering BOM） 工程BOM是产品在工程开发阶段的BOM，它是设计BOM的进一步细化和完善。工程BOM由工程团队根据设计BOM进行详细工程分析和优化后创建，包括更具体的部件和子装配件的信息，如具体的供应商、工艺参数、成本等。工程BOM为产品的制造和供应链管理提供更准确的信息。 3、制造BOM（Manufacturing BOM） 制造BOM是产品在制造阶段的BOM，它描述了产品的制造过程和所需的部件、材料和工艺信息。制造BOM由制造团队根据工程BOM进行制造工艺规划和工序划分后创建，包括部件的装配顺序、工艺参数、工序说明和设备要求等。制造BOM为产品的实际制造提供指导和支持。 4、销售BOM（Sales BOM） 销售BOM是产品在销售阶段的BOM，它描述了产品的销售包装和附加组成部分。销售BOM主要由销售团队和市场部门创建，包括产品的包装材料、营销资料、用户手册等。销售BOM为产品的包装和营销提供指导，确保产品在市场上的形象和竞争力。 5、服务BOM（Service BOM） 服务BOM是产品在售后服务阶段的BOM，它描述了产品的维护、修理和备件需求。服务BOM主要由售后服务团队创建，包括产品的维修指导、备件清单和服务流程等。服务BOM为售后服务团队提供指导和支持，确保产品的维护和服务质量。 通过根据生命周期阶段进行BOM的分类，企业可以更好地管理和跟踪产品在不同阶段的信息和需求。每个阶段的BOM都具有不同的特点和重要性，通过有效管理不同阶段的BOM，企业可以提高产品的开发效率、质量控制和市场响应能力。 三、BOM管理的挑战 （一）多层次BOM管理 在PLM系统中，BOM（Bill of Materials）管理是一个关键的任务，用于记录和管理产品的组成结构和相关信息。然而，随着产品的复杂性和多样性增加，多层次BOM管理成为了一个具有挑战性的任务。本文将探讨多层次BOM管理所面临的挑战以及如何应对这些挑战。 1、复杂性与层级关系 ：当产品的层级结构变得复杂时，管理多层次BOM变得更加困难。不同层级之间存在着复杂的关系，例如总装BOM包含子装配BOM，子装配BOM又包含部件BOM，而每个BOM又可能包含多个层级。这种复杂性增加了BOM管理的难度，需要准确地识别和跟踪不同层级之间的关系。 2、数据一致性与准确性 ：在多层次BOM管理中，确保BOM数据的一致性和准确性是一个挑战。不同层级的BOM之间可能存在数据的重复、遗漏或不一致，这可能导致生产过程中的错误和延误。因此，需要建立有效的数据管理机制，确保BOM数据在不同层级之间的同步和更新。 3、版本控制与变更管理 ：产品在生命周期中可能经历多次设计变更和版本更新，而这些变更需要及时反映在BOM中。然而，在多层次BOM管理中，版本控制和变更管理变得更加复杂。每个层级的BOM都可能存在独立的变更需求和版本控制流程，需要确保各个层级之间的变更能够正确地反映和同步。 4、协作与沟通 ：多层次BOM管理涉及多个团队和部门的协作和沟通。设计团队、制造团队、供应链团队等需要共同参与BOM管理，但由于各自的职责和角色不同，协作和沟通可能存在困难。因此，建立良好的协作机制和沟通渠道是解决多层次BOM管理挑战的关键。 为了应对多层次BOM管理的挑战，可以采取以下措施： 1、清晰的层级结构定义 ：确保产品的层级结构定义清晰明确，明确每个层级的作用和组成关系，避免层级的重叠和模糊。 2、强化数据管理 ：建立完善的数据管理机制，包括数据采集、验证、同步和更新等环节，确保BOM数据的一致性和准确性。 3、引入PLM系统 ：PLM系统提供了专门的BOM管理功能，可以帮助组织更好地管理多层次BOM。通过PLM系统，可以实现BOM的版本控制、变更管理和协作，提高BOM管理的效率和质量。 4、梳理变更流程 ：建立清晰的变更流程和规范，确保每个层级的BOM变更都经过适当的审批和验证，避免错误的变更对产品造成影响。 5、加强团队协作 ：通过定期的沟通和协作会议，促进不同团队和部门之间的合作和沟通，确保各个层级的BOM能够正确地反映产品的设计和制造要求。 多层次BOM管理是PLM系统中BOM管理的重要方面，它涉及到产品的整体结构和组成关系。面对挑战，组织需要加强数据管理、引入PLM系统、规范变更流程以及加强团队协作，以确保多层次BOM的准确性、一致性和及时性，从而提高产品的开发和制造效率。 （二）物料变更控制 在PLM系统中，BOM（Bill of Materials）管理是关键的任务之一，用于记录和管理产品的组成结构和相关信息。然而，随着产品的复杂性和多样性增加，物料变更控制成为了一个具有挑战性的任务。本文将探讨物料变更控制所面临的挑战以及如何应对这些挑战。 1、变更复杂性 ：随着产品的不断演化和市场需求的变化，物料变更变得更加复杂。一个小小的物料变更可能会涉及到多个BOM层级和供应链环节的调整。例如，当更换一个部件时，需要考虑与之相关的装配关系、工艺流程、供应商选择等因素，这增加了物料变更的复杂性。 2、变更影响分析 ：在进行物料变更之前，需要进行全面的变更影响分析，以评估变更对产品和制造流程的影响。这包括分析变更对产品功能、性能、质量、成本、供应链以及相关文档和标准的影响。确保变更的合理性和可行性是物料变更控制的关键挑战之一。 3、变更审批与追踪 ：物料变更通常需要经过一系列的审批和验证步骤。涉及的各方包括设计团队、制造团队、供应链团队等。确保变更的审批和追踪过程规范、透明和及时，是物料变更控制的关键挑战。追踪变更的执行情况和结果，确保变更的顺利实施和验证。 4、变更管理与版本控制 ：在进行物料变更时，需要建立有效的变更管理和版本控制机制。这包括记录变更请求、跟踪变更过程、制定变更计划、执行变更操作、验证变更结果等。同时，需要确保不同版本的BOM和相关文档能够正确地存档和管理，以便日后追溯和参考。 5、供应链协同 ：物料变更往往涉及到供应链中的多个环节，包括供应商、合作伙伴和制造厂等。协调和沟通不同环节的变更信息和要求，确保变更在整个供应链中的传递和执行，是物料变更控制的重要挑战之一。 为了应对物料变更控制的挑战，可以采取以下措施： 1、建立规范的变更流程 ：制定明确的物料变更流程，包括变更请求的提交、审批流程、变更执行、验证和追踪等环节。确保变更流程规范、透明和可追溯。 2、引入PLM系统 ：PLM系统提供了专门的变更管理功能，可以帮助组织更好地控制物料变更。通过PLM系统，可以记录和跟踪变更请求、执行变更操作、追踪变更结果等。 3、变更影响分析工具 ：使用专门的变更影响分析工具，帮助评估变更对产品和制造流程的影响。这些工具可以模拟和分析变更的影响，帮助做出明智的变更决策。 4、强化供应链协同 ：加强与供应链中的合作伙伴和供应商的沟通和协作。建立有效的沟通渠道，及时传递变更信息和要求，确保变更在整个供应链中的顺利传递和执行。 5、培训与培养团队能力 ：提供相关的培训和培养计划，确保团队具备良好的变更管理和执行能力。培养团队的沟通、协调和分析能力，提高物料变更控制的效率和质量。 物料变更控制是PLM系统中BOM管理的重要方面，它涉及到产品的不断改进和演化。面对挑战，组织需要建立规范的变更流程、引入PLM系统、进行变更影响分析、加强供应链协同，以及培养团队的能力，从而有效地控制物料变更，确保产品质量和制造效率的提升。 （三）物料版本控制 在PLM系统中，BOM（Bill of Materials）管理是一个关键的任务，用于记录和管理产品的组成结构和相关信息。其中一个重要的挑战是物料版本控制。本文将探讨物料版本控制所面临的挑战以及如何应对这些挑战。 1、物料信息的复杂性 ：随着产品的不断演进和供应链的变化，物料的信息也会随之变得复杂。一个物料可能存在多个版本，每个版本可能包含不同的属性、特性和供应商信息。同时，还需要考虑物料的替代性和兼容性。因此，管理和跟踪物料版本的正确性和完整性是一个挑战。 2、物料变更的频繁性 ：在产品开发和制造过程中，物料变更是不可避免的。供应商变更、规格调整、性能改进等因素都可能导致物料的版本变更。然而，频繁的物料变更增加了版本控制的复杂性，需要确保各个环节都能及时了解并采用最新的物料版本。 3、物料版本的一致性 ：在整个供应链中，不同的团队和部门可能使用不同的物料版本。例如，设计团队可能使用最新的物料版本进行产品设计，而制造团队可能使用旧版本进行生产。这可能导致产品的不一致性和质量问题。因此，确保各个环节之间的物料版本一致性是一个关键挑战。 4、物料溯源与追溯 ：在产品生命周期中，需要能够追溯和溯源每个物料的版本信息。这对于产品质量控制、召回管理和法规遵从等方面都至关重要。然而，对于复杂的产品和供应链，确保物料版本的有效追溯和溯源是一个挑战。 为了应对物料版本控制的挑战，可以采取以下措施： 1、引入PLM系统 ：PLM系统提供了物料版本控制的功能，可以记录和跟踪不同物料版本的属性、特性和供应商信息。通过PLM系统，可以实现版本的管理、更新和共享，确保各个团队和环节使用的都是最新的物料版本。 2、建立规范的变更流程 ：制定明确的物料变更流程，包括物料版本的更新和验证。确保物料变更经过适当的审批和验证，避免错误的版本更新和使用。 3、加强供应链协同 ：与供应链中的供应商和合作伙伴加强沟通和协作，确保物料版本的一致性。建立清晰的沟通渠道，及时传递物料变更信息和要求。 4、强化数据管理 ：建立完善的物料数据管理机制，包括数据采集、验证、同步和更新等环节。确保物料数据的准确性和一致性，提供可靠的物料版本信息。 5、实施物料追溯系统 ：建立物料追溯系统，能够跟踪和溯源每个物料的版本信息。通过追溯系统，可以快速定位和处理物料质量问题，提高产品质量和安全性。 物料版本控制是PLM系统中BOM管理的关键挑战之一，它涉及到物料信息的复杂性、变更频繁性、版本一致性以及物料的追溯和溯源。通过引入PLM系统、建立规范的变更流程、加强供应链协同、强化数据管理和实施物料追溯系统，可以有效地应对物料版本控制的挑战，提高产品的质量和竞争力。 （四）协同工作与数据共享 在PLM系统中，BOM（Bill of Materials）管理是一个关键的任务，用于记录和管理产品的组成结构和相关信息。其中一个重要的挑战是实现协同工作和数据共享。本文将探讨协同工作与数据共享在BOM管理中所面临的挑战以及如何应对这些挑战。 1、跨部门协作 ：在产品的设计、制造和供应链等环节中，涉及到多个部门和团队的协同工作。设计团队需要与工程团队、采购团队、制造团队等紧密合作，共同完成BOM的创建、维护和更新。然而，不同团队之间的协作可能面临沟通不畅、信息不准确和信息传递滞后等挑战。 2、数据一致性和准确性 ：在协同工作中，确保BOM数据的一致性和准确性是一个重要挑战。不同团队可能使用不同的工具和系统来处理和维护BOM数据，导致数据的不一致和错误。因此，确保各个团队使用的都是最新的BOM数据，以及对BOM数据的准确性进行验证和审查是关键。 3、数据访问和权限控制 ：在协同工作中，需要确保团队成员能够方便地访问和共享BOM数据，同时又需要控制数据的权限和保密性。不同团队可能具有不同的数据访问和编辑权限，需要确保数据的安全性和机密性。此外，还需要考虑跨组织或跨公司的协同工作，需要建立安全的数据共享和访问机制。 4、实时数据更新和同步 ：在多人协同工作中，需要确保BOM数据的实时更新和同步。当一个团队对BOM进行修改或更新时，其他团队需要能够及时获取到最新的数据，避免使用过时或错误的数据。实现实时数据更新和同步对于确保协同工作的高效性和准确性至关重要。 为了应对协同工作与数据共享的挑战，可以采取以下措施： 1、引入PLM系统 ：PLM系统提供了协同工作和数据共享的平台，团队成员可以在同一个系统中访问和编辑BOM数据。PLM系统可以实现实时数据更新和同步，确保团队使用的都是最新的BOM数据。 2、规范协同工作流程 ：建立明确的协同工作流程和责任分工，确保不同团队之间的协作顺畅。规定数据的创建、审批、修改和验证流程，避免数据冲突和错误。 3、数据访问和权限控制 ：根据团队成员的角色和职责，设定不同的数据访问和编辑权限。确保数据的安全性和机密性，避免未经授权的访问和修改。 4、建立数据标准和规范 ：制定统一的数据标准和规范，包括命名规则、属性定义和数据格式等。确保不同团队使用统一的数据标准，提高数据的一致性和可比性。 5、加强沟通和培训 ：加强团队成员之间的沟通和协作能力，培训他们使用PLM系统和遵循协同工作流程。提供培训和支持，帮助团队成员熟悉和掌握协同工作和数据共享的技巧和方法。 协同工作与数据共享是PLM系统中BOM管理的重要挑战之一，它涉及到跨部门协作、数据一致性和准确性、数据访问和权限控制，以及实时数据更新和同步等方面。通过引入PLM系统、规范协同工作流程、建立数据标准、加强沟通和培训，可以有效地应对这些挑战，提高BOM管理的效率和准确性。 四、PLM系统中的BOM管理功能 （一）BOM的创建与维护 在PLM系统中，BOM（Bill of Materials）的创建与维护是一个关键的功能。BOM的创建涉及到产品的组成结构和相关信息的录入和管理。通过PLM系统，用户可以方便地创建BOM，并添加各个组成部分的详细信息，如物料编码、描述、数量、单位等。同时，PLM系统还提供了BOM的维护功能，可以对BOM进行修改、更新和删除，以适应产品的变化和演进。 （二）BOM的版本管理 在产品的生命周期中，BOM通常会随着产品的设计、制造和变更而发生多个版本的变化。因此，PLM系统提供了BOM的版本管理功能。通过版本管理，可以记录和跟踪不同版本的BOM，包括版本号、创建日期、修改记录等信息。用户可以根据需要查看和比较不同版本的BOM，了解每个版本的变化和差异。同时，版本管理还提供了回滚和还原的功能，可以恢复到先前的BOM版本。 （三）BOM的变更控制 在产品的开发和制造过程中，不可避免地会发生BOM的变更。这些变更可能涉及到组成部分的替换、数量的调整、特性的改变等。PLM系统提供了BOM的变更控制功能，可以记录和管理BOM的变更请求、审批流程和变更记录。用户可以提交BOM的变更请求，并经过相应的审批流程，确保变更的合理性和准确性。变更控制还可以跟踪和记录每个BOM的变更历史，以便进行审计和追溯。 （四）BOM与CAD数据的集成 BOM和CAD（Computer-Aided Design）数据之间的集成是PLM系统中的重要功能之一。CAD数据包含了产品的设计图纸和模型，而BOM则记录了产品的组成结构和相关信息。PLM系统可以实现BOM和CAD数据的关联和集成，将CAD数据与BOM关联起来。这样，用户可以直接从BOM中访问和查看相应的CAD数据，确保设计与BOM的一致性和正确性。同时，BOM的变更也可以同步到CAD数据中，保持两者之间的一致性。 （五）BOM的查询与跟踪 PLM系统还提供了BOM的查询与跟踪功能，方便用户快速查找和跟踪BOM的相关信息。用户可以通过关键词、物料编码、产品型号等进行BOM的查询，快速定位所需的BOM。同时，PLM系统还可以跟踪和记录BOM的使用情况，包括产品的制造过程、销售订单、售后服务等。这样，用户可以了解每个BOM的使用情况和变更历史，进行数据分析和决策支持。 五、结论 BOM（Bill of Materials）是产品开发和制造过程中的关键信息之一。在PLM系统中进行BOM管理，可以帮助企业实现产品的快速开发、高效制造和持续改进。本文通过对BOM定义与分类的详细介绍，以及PLM系统中BOM管理的功能和挑战的探讨，强调了BOM定义与分类的重要性。 首先，BOM的定义为产品的组成结构和相关信息提供了清晰的描述和记录。通过对产品的各个组成部分进行分类、命名和编码，可以建立起BOM的层次结构，方便后续的管理和使用。BOM的定义也为产品的设计、制造、采购和售后服务等环节提供了基础数据，确保各个团队之间的协同工作和信息共享。 其次，BOM的分类可以从不同的角度对产品的组成部分进行分类和归类。根据产品层级的分类，可以将BOM分为总装BOM、子装BOM和零部件BOM，便于组织和管理复杂的产品结构。根据组成件类型的分类，可以将BOM分为机械BOM、电气BOM、软件BOM等，方便不同团队之间的协作和沟通。根据生命周期阶段的分类，可以将BOM分为设计BOM、工程BOM、制造BOM和维护BOM，有助于跟踪和管理不同阶段的BOM数据。 在PLM系统中，BOM管理面临着多层次BOM管理、物料变更控制、物料版本控制和协同工作与数据共享等挑战。针对这些挑战，可以通过引入PLM系统、规范协同工作流程、建立数据标准和加强沟通和培训等措施来应对。PLM系统中的BOM管理功能包括BOM的创建与维护、版本管理、变更控制、与CAD数据的集成，以及查询与跟踪等，帮助企业实现BOM的有效管理和利用。 总而言之，PLM系统中BOM管理是产品开发和制造过程中的重要环节。通过对BOM的定义与分类，企业可以建立起清晰的产品结构和基础数据，为产品的开发、制造和服务提供支持。在PLM系统的帮助下，BOM管理可以更加高效、准确和可追溯，为企业提供了竞争优势。因此，对BOM的定义与分类的重要性不可忽视，企业应重视BOM管理，并通过PLM系统的应用来提升管理水平和效率。 ","link":"https://tianxiawuhao.github.io/k0goe8fBf/"},{"title":"工厂业务名词","content":"工厂常用软件 术语 内容 核心功能模块 典型厂家 OA（办公自动化）Office Automation 将现代化办公和计算机技术结合起来的一种新型的办公方式。办公自动化没有统一的定义，凡是在传统的办公室中采用各种新技术、新机器、新设备从事办公业务，都属于办公自动化的领域。通过实现办公自动化，或者说实现数字化办公，可以优化现有的管理组织结构，调整管理体制，在提高效率的基础上，增加协同办公能力，强化决策的一致性 公文管理、流程规范、邮件、审批、通知公告 微软、IBM、飞书、钉钉 CRM（客户关系管理）Customer Relationship Management 指企业为提高核心竞争力，利用相应的信息技术以及互联网技术协调企业与顾客间在销售、营销和服务上的交互，从而提升其管理方式，向客户提供创新式的个性化的客户交互和服务的过程。其最终目标是吸引新客户、保留老客户以及将已有客户转为忠实客户，增加市场 线索管理、客户管理、商机管理、客户公海、客户画像、交易管理、营销管理 纷享销客、销售易、神州云动、销帮帮、红圈、Salesforce、hubspot、zoho SCRM（社会化客户关系管理）Social Customer Relationship Management 是一种基于社交媒体的客户关系管理系统，其目的是通过社交媒体平台实现企业与客户之间的有效沟通和互动，从而提高客户满意度和忠诚度，增加企业的销售额和市场份额 线索接入、多场景获客、渠道ROI分析客户管理、素材库管理、交易管理 尘锋SCRM、探马SCRM、快鲸SCRM、微伴助手、卫瓴SCRM SRM（供应商关系管理）Supplier Relationship Management 是用来改善与供应链上游供应商的关系的，它是一种致力于实现与供应商建立和维持长久、紧密伙伴关系的管理思想和软件技术的解决方案，它旨在改善企业与供应商之间关系的新型管理机制，实施于围绕企业采购业务相关的领域，目标是通过与供应商建立长期、紧密的业务关系，并通过对双方资源和竞争优势的整合来共同开拓市场，扩大市场需求和份额，降低产品前期的高额成本，实现双赢的企业管理模式 寻源比价、招投标管理、采购申请、采购管理、供应商管理、供应商绩效管理、采购到货管理、入库管理、采购结算 甄云、企企通 SCM（供应链管理）Supply Chain Management 是一个综合性的供应链管理解决方案，旨在协调和优化企业内部和外部的供应链活动。它涵盖了从原材料采购到产品交付的整个供应链流程，包括供应商管理、物流管理、库存管理、订单管理等。SCM系统帮助企业实现供应链的可视化、协同和优化，提高供应链的效率和灵活性，降低成本并提供更好的客户服务 供应商管理、采购管理、物流配送管理、交货计划、收发货管理、验收管理、库存管理、销售订单管理、客户管理、供应商协同、客户协同 上海企兰 ERP（企业资源计划）Enterprise Resource Planning 指建立在信息技术基础上，以系统化的管理思想，为企业决策层及员工提供决策运行手段的管理平台 供应链管理、生产制造管理（MRP）、质量管理、人力资源管理、财务管理 用友、金蝶、鼎捷、管家婆、聚水潭、oracle、SAP MES（生产执行系统）Manufacturing Execution System 是一套面向制造企业车间执行层的生产信息化管理系统 制造数据管理、计划排程管理、生产调度管理、质量管理、人力资源管理、工作中心/设备管理、工具工装管理、成本管理、项目看板管理、生产过程控制、底层数据集成分析、上层数据集成分解等管理模块 鼎捷、宝信、柏楚、赛意、西门子、霍尼韦尔 MOM（制造运营管理）Manufacturing Operation Management 美国仪器、系统和自动化协会于2000年开始发布ISA-SP95标准，首次确立了制造运行管理的概念，针对更广义的制造运营管理划定边界，作为该领域的通用研究对象和内容，并构建通用活动模型应用于生产、维护、质量和库存4类主要运行区域，详细定义了各类运行系统的功能及各功能模块之间的相互关系 设备管理、生产管理、计划排产、品质管理、仓库管理、资源管理、系统基础管理及系统集成等功能模块 国机、锐制、元工、树根互联 APS（高级计划与排程）Advanced Planning and Scheduling 是解决生产排程和生产调度问题，常被称为排序问题或资源分配问题。在离散行业，APS是为解决多工序、多资源的优化调度问题；而流程行业，APS则是为解决顺序优化问题。它通过为流程和离散的混合模型同时解决顺序和调度的优化问题，从而对项目管理与项目制造解决关键链和成本时间最小化，具有重要意义。 生产计划、MRP运算、物料控制、数字化排产、齐套计算、物料管理、BOM管理、工艺路线管理、系统配置、集成管理 安达发、易普优、西门子、翟尼韦尔 PLM（产品生命周期管理）Product Lifecycle Management 是一种应用于在单一地点的企业内部、分散在多个地点的企业内部，以及在产品研发领域具有协作关系的企业之间的，支持产品全生命周期的信息的创建、管理、分发和应用的一系列应用解决方案，它能够集成与产品相关的人力资源、流程、应用系统和信息 物料管理、产品管理、试验管理、配方管理、项目管理、工艺管理、文档管理、需求管理、成本管理、集成管理 思普、数码大方、天喻、达索系统enovia、西门子、SAP思爱普 QMS（质量管理体系）Quality Management System 是指在质量方面指挥和控制组织的管理体系。质量管理体系是组织内部建立的、为实现质量目标所必需的、系统的质量管理模式，是组织的一项战略决策 样品管理、仪器设备管理、检测配置、人员绩效考核、客户质量管理、质量分析、变更管理、标签管理、质量报告管理、SPC、基础配置等 海克斯康、华会、万物、安必兴、云质、西门子、库得克 WMS（仓储管理系统）Warehouse Management System 是一个实时的计算机软件系统，它能够按照运作的业务规则和运算法则，对信息、资源、行为、存货和分销运作进行更完美地管理，提高效率 入库管理：批次入库/部分入库/退货入库/一品多位/一位多品/推荐上架。库存管理：动态/循环盘点：库存调整/库存移动：库存转移。出库管理：订单分配：任务包荷选/打印单箱/多箱复核：称重交接。管理工具：库存流水查询/接口流水查询/入库单取消/出岸单异常取消。规则配置：货主/货类批次规则分配/上架/补货/波次规则任务包/数据来源/打印流程/店铺。基础资料：货主/货类/货品仓库/岸区/库位：用户/角色 富勒、巨沃、科箭、普罗格、Infor TMS（运输管理系统）Transportation Management System 是一种“供应链”分组下的（基于网络的）操作软件。它能通过多种方法和其他相关的操作一起提高物流的管理能力；包括管理装运单位，指定企业内、国内和国外的发货计划，管理运输模型、基准和费用，维护运输数据，生成提单，优化运输计划，选择承运人及服务方式，招标和投标，审计和支付货运账单，处理货损索赔，安排劳力和场所，管理文件（尤其当国际运输时）和管理第三方物流 资源管理（承运商管理、司机管理、车辆管理）、客户管理、运输管理（订单管理、运单管理、配载中心、回单管理）、报价管理、合同管理、结算管理、财务管理、集成管理等。承运商端、司机端 唯智、逗号科技、科箭、云领智能、创云科技、 富勒 CMS（内容管理系统）Content Management System 是一种位于WEB前端（Web服务器）和后端办公系统或流程（内容创作、编辑）之间的软件系统。内容的创作人员、编辑人员、发布人员使用内容管理系统来提交、修改、审批、发布内容。 栏目管理、标签管理、文章管理、广告管理、站点管理、素材管理、模板管理 织梦、凡科、帝国、 WordPress、Drupal、Sitecore HRM（人力资源管理）Human Resource Management 是指对企业的人力资源管理方方面面进行分析、规划、实施、调整，提高企业人力资源管理水平的一款人力资源管理软件，能够帮助企业在正确的时间里，选择到正确的人，安排到正确的职位上，发挥其正确的作用，从而实现企业正确的战略目标和最大化的经济效益 组织管理、员工管理、考勤管理、招聘管理、培训管理、绩效管理、薪酬管理、合同档案管理等 北森、智者云、悟空 LIMS（实验室信息管理系统）laboratory information management system 是以数据库为核心的信息化技术与实验室管理需求相结合的信息化管理系统 委托管理、收样管理、任务管理、样品流转、报告管理、自动采集、财务和工资管理、标签管理、设备管理、实验室数据管理 元宇、松虎、赛印、博什兰、白码、牵翼云 BI（商业智能）Business Intelligence 指用现代数据仓库技术、线上分析处理技术、数据挖掘和数据展现技术进行数据分析以实现商业价值 自助分析、可视化大屏、中国式报表、可视化ETL 帆软、smartBl、powerBl OMS（订单管理系统）Ordering Management System 是接受客户订单信息，以及仓储管理系统发来的库存信息，然后按客户和紧要程度给订单归类，对不同仓储地点的库存进行配置，并确定交付日期的系统 订单管理、营销管控、商品管理、库存管理、分销管理、结算管控、配送异常处理订单拆分合并、发票管理 商派、巨益、百胜 SCADA (监控控制与数据获取)Supervisory Control And Data Aquisition 工厂里的设备种类多、数量多，SCADA系统具有采集、控制分散；管理集中的“集散控制系统”的特征。 SCADA可以理解为不同厂家的管理监控系统（上位）对控制部分（下位）的数据采集与监控管理 西门子（Siemens）,欧姆龙（Omron）,三菱电机（Mitsubishi Electric） ,威图（Wonderware） 进销存 称为购销链，是指企业管理过程中采购（进）-入库（存）-销售（销）的动态管理过程。进：指询价、采购到入库与付款的过程。销：指报价、销售到出库与收款的过程。存：指除入库之外，包括领料、退货、盘点、损益、借入、借出、调拨等影响库存数量的动作 采购管理、销售管理、库存管理、资金管理 管家婆、用友好生意、金蝶精斗云、白草进销存 在线客服系统 在线客服或称做网上前台是一种以网站为媒介，向互联网访客与网站内部员工提供即时沟通的页面通信技术。在线客服是网络营销的基础 渠道接入、会话分配、坐席管理、日程管理、快捷回复、知识库管理、工单系统、智能客服、呼叫中心 容联七陌、网易七鱼、智齿客服、快商通 MES（制造执行系统） PLM（产品生命周期管理） ERP（企业资源规划） SCM（供应链管理） WMS（仓库管理系统） APS（高级计划和排程） SCADA（监控控制与数据获取） QMS（质量管理系统） CRM（客户关系管理） EAM（企业资产管理） 这些系统各自针对企业的不同运营方面提供专门的管理和控制功能。它们可以相互补充和集成，形成一个全面、高效和协同的企业运营环境。 ERP 和其他系统：ERP 系统是企业的中枢，负责整合财务、人力资源、生产、供应链、销售等方面的信息。它与 MES、WMS、SCM 等系统集成，以获取生产现场的实时数据、库存信息和供应链状态，实现企业资源的有效规划和管理。 MES 和 ERP/SCADA：MES 直接与生产线操作相关，它桥接了 ERP 系统和生产过程，提供车间层面的实时数据给 ERP 系统，同时从 SCADA 系统收集设备和过程数据，以优化生产执行。 SCM 和 ERP/WMS：SCM 系统管理供应链的各个环节，从原材料采购到成品交付。它与 ERP 系统集成，以确保财务和订单管理的一致性；与 WMS 集成，优化库存管理和物流操作。 WMS 和 ERP/SCM：WMS 负责管理仓库内的物流活动，与 ERP 系统集成可以确保库存数据的准确性和及时更新，与 SCM 集成则支持整个供应链的效率。 APS 和 ERP/MES：APS 为生产计划和排程提供高级功能，与 ERP 系统集成可以确保生产计划的资源可用性，与 MES 集成则可优化生产过程的执行。 PLM 和 ERP/QMS：PLM 管理产品从概念到退市的整个生命周期，它与 ERP 系统集成，确保产品信息在企业内部共享；与 QMS 集成则保证产品质量的管理和监控。 QMS 和 ERP/PLM：QMS 确保产品和服务满足质量标准和客户要求，与 ERP 集成可提升质量数据的财务和运营管理，与 PLM 集成则支持产品开发和生命周期管理的质量控制。 CRM 和 ERP：CRM 系统管理客户信息和互动，与 ERP 系统集成可以提高客户订单的处理效率和财务管理。 EAM 和 ERP/SCADA：EAM 系统管理企业的物理资产，与 ERP 系统集成可以优化财务和运营决策，与 SCADA 集成则可以实时监控设备状态和维护需求。 SCADA典型架构 典型的SCADA像下图，分为场站端和管理端。 场站端：主要是三部分：下位机、通信网络、上位机。 管理端：一般包括前置采集、SCADA应用。 场站端： 下位机：侧重采集和控制。一般由RTU和PLC组成。 通信网络：实现上、下位机之间数据交流。 上位机：侧重监控功能。一般由电脑和服务组成，主要起到远程监控、报警处理、数据存储以及与其他系统集合的作用。 工厂着重关键参数 LOB(LineOf Balance)：是生产线平衡的定义。 当一个流程中各个工序的节拍不一致时，瓶颈工序以外的其它工序就会产生空闲时间， 这就需要对生产工艺进行平衡。 公式一： LOB=Σ工序节拍/瓶颈工序节拍/工序数 注： 1、自动线一般应按照多个工序来计算； 2、一个工序有多少设备和多少人以理论安排为准 3、转换姿态类的辅助设备不作为独立工序考虑 (如翻转机) 4、意义：评价生产线布局的合理性，反映工序平衡状况 5、适用范围：主要用于生产线设计、设备布局时，或者后期对生产线设计进行评价 公式二： LOB=Σ岗位节拍/瓶颈岗位节拍/岗位数 注： 1、意义：评价岗位设置的合理性，反应岗位节拍平衡状况 2、适用范围：主要用于生产线岗位设置，可以随需求变化(节拍时间Takt time)进行一定程度上的变化。 公式三： LOB=Σ(岗位节拍×岗位人数)/瓶颈岗位节拍/总人数 注： 1、一个岗位有多人的，每人都按照1个岗位节拍纳入计算 2、意义：评价作业人员安排合理性，反映的作业人员节拍平衡状况(含设备） 3、适用范围：主要用于人员安排，可以随需求变化(节拍时间Takt time)进行一定程度上的变化 瓶颈CT(Cycle Time)：瓶颈循环时间 只要产品是经过多道工序加工的，必然有一道工序产能最低，这道工序就是生产瓶颈。瓶颈工序加工一件产品的循环时间（Cycle Time）称之为瓶颈CT。 OEE(Overall Equipment Effectiveness)：设备综合效率 一般，每一个生产设备都有自己的理论产能，要实现这一理论产能必须保证没有任何干扰和质量损耗。OEE就是用来表现实际的生产能力相对于理论产能的比率，它是一个独立的测量工具。 OEE是由可用率，表现性以及质量指数三个关键要素组成： OEE=可用率×表现指数×质量指数 其中： 可用率=操作时间/计划工作时间 它是用来评价停工所带来的损失，包括引起计划生产发生停工的任何事件，例如设备故障，原材料短缺以及生产方法的改变等。 表现指数=理想周期时间/实际周期时间=理想周期时间/（操作时间/总产量）=（总产量/操作时间）/生产速率 表现性是用来评价生产速度上的损失。包括任何导致生产不能以最大速度运行的因素，例如设备的磨损，材料的不合格以及操作人员的失误等。 质量指数=良品/总产量 质量指数是用来评价质量的损失，它用来反映没有满足质量要求的产品（包括返工的产品）。 OEE的另一种计算公式 OEE=时间开动率×性能开动率×合格品率 其中，时间开动率 = 开动时间/负荷时间 而，负荷时间 = 日历工作时间-计划停机时间 开动时间 = 负荷时间 - 故障停机时间 - 设备调整初始化时间（包括更换产品规格、更换工装模具、更换刀具等活动所用时间） 性能开动率 = 净开动率×速度开动率 而，净开动率 = 加工数量×实际加工周期/开动时间 速度开动率 = 理论加工周期/实际加工周期 合格品率 = 合格品数量/ 加工数量 全局设备效率OEE是一种简单实用的生产管理工具，在欧美的制造业和中国的跨国企业中已得到广泛的应用，全局设备效率指数已成为衡量企业生产效率的重要标准，也是TPM（Total Productive Maintenance）实施的重要手法之一。 ","link":"https://tianxiawuhao.github.io/EJTDQH5ex/"},{"title":"jenkins集成编译发布","content":"安装jdk17 1 打开虚拟机 创建jdk的目录 mkdir /home/jdk17 2 下载jdk17并将其放到我们所建立的目录中 wget https://download.oracle.com/java/17/latest/jdk-17_linux-x64_bin.tar.gz -P /home/jdk17/ 3 解压文件 tar xf /home/jdk17/jdk-17_linux-x64_bin.tar.gz -C /home/jdk17/ 4 查看安装的jdk具体的版本是什么 ls /home/jdk17/ 5 修改环境变量 vim /etc/profile 注意javahome的路径是我们刚刚设置的，那个文件夹 #jdk17 export JAVA_HOME=/home/jdk17/jdk-17.0.10 export CLASSPATH=.:$JAVA_HOME/lib/tools.jar:$JAVA_HOME/lib/dt.jar export PATH=$JAVA_HOME/bin:$PATH 刷新环境变量 source /etc/profile 6 测试是否安装成功 java -version 安装maven 1 创建jdk的目录 mkdir /home/maven 2 下载jdk17并将其放到我们所建立的目录中 wget https://mirrors.tuna.tsinghua.edu.cn/apache/maven/maven-3/3.8.4/binaries/apache-maven-3.8.4-bin.tar.gz -P /home/maven/ 或者去官网下载 Maven – Download Apache Maven 3 解压文件 tar xf /home/maven/apache-maven-3.8.4-bin.tar.gz -C /home/maven/ 4 查看安装的jdk具体的版本是什么 cd /home/maven/ ls 5 修改环境变量 vim /etc/profile 注意javahome的路径是我们刚刚设置的，那个文件夹 #maven export MAVEN_HOME=/home/maven/apache-maven-3.8.8 export PATH=$MAVEN_HOME/bin:$PATH 刷新环境变量 source /etc/profile 6 测试是否安装成功 mvn -version 7 修改数据源 vim conf/settings.xml 添加数据源 &lt;!--阿里云镜像--&gt; &lt;mirror&gt; &lt;id&gt;nexus-aliyun&lt;/id&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;name&gt;Nexus aliyun&lt;/name&gt; &lt;url&gt;https://maven.aliyun.com/repository/public&lt;/url&gt; &lt;/mirror&gt; &lt;mirror&gt; &lt;id&gt;repo2&lt;/id&gt; &lt;name&gt;Mirror from Maven Repo2&lt;/name&gt; &lt;url&gt;https://repo.maven.apache.org/maven2/&lt;/url&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;/mirror&gt; &lt;mirror&gt; &lt;id&gt;ui&lt;/id&gt; &lt;name&gt;Mirror from UK&lt;/name&gt; &lt;url&gt;http://uk.maven.org/maven2/&lt;/url&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;/mirror&gt; &lt;mirror&gt; &lt;id&gt;jboss-public-repository-group&lt;/id&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;name&gt;JBoss Public Repository Group&lt;/name&gt; &lt;url&gt;http://repository.jboss.org/nexus/content/groups/public&lt;/url&gt; &lt;/mirror&gt; 安装git yum install git 1 配置用户名和邮箱： git config --global user.name &quot;Your Name&quot; git config --global user.email &quot;your-email@example.com&quot; 2 查看配置信息： git config --list 以上命令将列出当前Git的配置信息。 安装jenkins 1 查询镜像 docker search jenkins 2 下载镜像 docker pull jenkins/jenkins 3 编写docker-compose.yml version : '3' services: jenkins: user: root restart: always image: jenkins/jenkins container_name: jenkins ports: - 8080:8080 - 50000:50000 volumes: - /home/jenkins/jenkins_home/:/var/jenkins_home/ - /var/run/docker.sock:/var/run/docker.sock - /usr/bin/docker:/usr/bin/docker - /home/jdk17/jdk-17.0.10:/home/jdk17/jdk-17.0.10 - /home/maven/apache-maven-3.8.8:/home/maven/apache-maven-3.8.8 - /root/.ssh:/root/.ssh docker run -di -p 8080:8080 -p 50000:50000 --name jenkins --restart=always -v /home/jenkins/jenkins_home/:/var/jenkins_home/ -v /var/run/docker.sock:/var/run/docker.sock -v/usr/bin/docker:/usr/bin/docker -v /home/jdk17/jdk-17.0.10:/home/jdk17/jdk-17.0.10 -v /home/maven/apache-maven-3.8.8:/home/maven/apache-maven-3.8.8 -v /root/.ssh:/root/.ssh jenkins/jenkins 5 设置权限 chown -R 1000:1000 /home/jenkins/jenkins_home chown -R 1000:1000 /home/maven 默认情况下，docker 命令会使用 Unix socket 与 Docker 引擎通讯。而只有 root 用户和 docker 组的用户才可以访问 Docker 引擎的 Unix socket。出于安全考虑，一般 Linux 系统上不会直接使用 root 用户。即我们当前的用户不是root用户。 解决办法：把我们当前的用户添加到docker组中就可以了。 第一步：sudo gpasswd -a username docker #将普通用户username加入到docker组中，username这个字段也可以直接换成$USER。 第二步：newgrp docker #更新docker组 4 启动容器 Jenkins需要下载大量内容，但是由于默认下载地址下载速度较慢，如有需要可以设置下载地址为国内镜像站 cd /home/jenkins/jenkins_home/ cat hudson.model.UpdateCenter.xml 修改数据卷中的hudson.model.UpdateCenter.xml文件 # 修改数据卷中的hudson.model.UpdateCenter.xml文件 &lt;?xml version='1.1' encoding='UTF-8'?&gt; &lt;sites&gt; &lt;site&gt; &lt;id&gt;default&lt;/id&gt; &lt;url&gt;https://updates.jenkins.io/update-center.json&lt;/url&gt; &lt;/site&gt; &lt;/sites&gt; # 将下载地址替换为http://mirror.esuni.jp/jenkins/updates/update-center.json &lt;?xml version='1.1' encoding='UTF-8'?&gt; &lt;sites&gt; &lt;site&gt; &lt;id&gt;default&lt;/id&gt; &lt;url&gt;http://mirror.esuni.jp/jenkins/updates/update-center.json&lt;/url&gt; &lt;/site&gt; &lt;/sites&gt; # 清华大学的插件源也可以https://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json 4 启动容器 #在docker-compose.yml所在目录启动 docker-compose up -d jenkins 5 访问jenkins 1、启动之后使用http://ip:端口访问jenkins。 复制管理员密码 [root@localhost ROOT]# cat /root/.jenkins/secrets/initialAdminPassword 86b8a38ce6d44c12941b86a727b7dcc8 更新jenkins 1 下载最新的war包 https://mirrors.jenkins.io/war-stable/ 2 拷贝替换 [root@iZ8vbi9mx98t2q4hbta56aZ war]# docker cp jenkins.war jenkins:/root/ [root@iZ8vbi9mx98t2q4hbta56aZ war]# docker exec -u root -it jenkins bash [root@47faa9548aa0 /]# cd /root/ [root@47faa9548aa0 ~]# ps -ef|grep java jenkins 7 1 0 Mar12 ? 00:04:22 java -Duser.home=/var/jenkins_home -Djenkins.model.Jenkins.slaveAgentPort=50000 -jar /usr/share/jenkins/jenkins.war root 1012 994 0 16:35 pts/0 00:00:00 grep --color=auto java [root@47faa9548aa0 ~]# cd /usr/share/jenkins/ [root@47faa9548aa0 jenkins]# cp jenkins.war jenkins.war.22.3.17.bak [root@47faa9548aa0 jenkins]# ls jenkins.war jenkins.war.22.3.17.bak ref [root@47faa9548aa0 jenkins]# mkdir other-version [root@47faa9548aa0 jenkins]# cd other-version/ [root@47faa9548aa0 other-version]# cp /root/jenkins.war . [root@47faa9548aa0 other-version]# cd /usr/share/jenkins [root@47faa9548aa0 jenkins]# ls jenkins.war jenkins.war.22.3.17.bak other-version ref [root@47faa9548aa0 jenkins]# rm jenkins.war rm: remove regular file 'jenkins.war'? y [root@47faa9548aa0 jenkins]# cd other-version [root@47faa9548aa0 other-version]# mv jenkins.war ../ [root@47faa9548aa0 other-version]# cd .. [root@47faa9548aa0 jenkins]# ls jenkins.war jenkins.war.22.3.17.bak other-version ref 部署 1 创建项目 2 用户密码类型 1创建凭证 选择&quot;Username with password&quot;，输入Gitlab的用户名和密码，点击&quot;确定&quot;。 2 配置源码管理，从gitlab拉取代码 编译打包 构建-&gt;添加构建步骤-&gt;Executor Shell echo “开始编译和打包” mvn clean install -DskipTests VERSION=`date +%Y%m%d%H%M` echo $VERSION echo &quot;=============== 登录Harbor ===============&quot; cd dt-os-plat/ docker login -u 11111 -p 11111 https://registry2-qingdao.cosmoplat.com echo &quot;=============== 打包Docker镜像 ： &quot; registry2-qingdao.cosmoplat.com/62_d3os/dt-co-simulation:$VERSION &quot;===============&quot; docker build -t registry2-qingdao.cosmoplat.com/62_d3os/dt-co-simulation:$VERSION . echo &quot;=============== 推送到镜像仓库 ===============&quot; docker push registry2-qingdao.cosmoplat.com/62_d3os/dt-co-simulation:$VERSION echo &quot;=============== 登出 ===============&quot; docker logout echo “编译和打包结束” 推送支的代码， 自动执行jenkins 1 jenkins中Gitlab插件 2 触发功能配置 打开一个任务配置，构建触发器中勾选&quot;Build when a change is pushed to GitLab.&quot;并过滤指定分支, 这里需要记下GitLab webhook URL一会儿配置到gitlab上 3 gitlab中添加配置webhook 网址：http://10.206.73.157:8080/project/dt-sim 触发分支：dt-release-simulation ","link":"https://tianxiawuhao.github.io/telBe8GLz/"},{"title":"七大查询算法","content":"前言 在非数值运算问题中，数据存储量一般很大，为了在大量信息中找到某些值，需要用到查找技术，为了提高查找效率，需要对一些数据进行排序。查找和排序的数据处理量占有非常大的比重，故查找和排序的有效性直接影响到算法的性能，因而查找和排序是重要的处理技术。 几个与查找有关的基本概念： 查找表：由同一类型的数据元素构成的集合。由于数据元素之间存在着完全松散的关系，因此查找表是一种非常灵活的结构，可以利用任意数据结构实现。 关键字：数据元素的某个数据项的值，用它可以标识查找表中一个或一组数据元素。如果一个关键字可以唯一标识查找表中的一个数据元素，则称其为 主关键字，否则为次关键字。当数据元素仅有一个数据项时，其关键字即为该数据元素的值。 查找：根据给定的关键字值，在查找表中确定一个关键字与给定值相同的数据元素，并返回该数据元素在查找表中的位置。若找到相应数据元素，则称查找成功，否则称查找失败，此时返回空地址。 平均查找长度：为确定数据元素在查找表中的位置，需要和给定的值进行比较的关键字个数的期望值，称为查找算法在查找成功时的平均查找长度。 查找两种常见的分类： 类别 特点 静态查找 只做查找操作的查找表，即： 1、查询某个“特定的”数据元素是否在表中 2、检索某个“特定的”数据元素和各种属性 动态查找 在查找中同时进行插入或删除等操作 类别 特点 无序查找 被查找数列有序无序均可 有序查找 被查找数列为有序数列 查找算法有七种，分别为：顺序查找、二分查找、插值查找、斐波那契查找、树表查找、分块查找、哈希查找。 基于线性结构的查找，有两种最常见的查找方法：顺序查找和折半查找。 顺序查找 顺序查找的特点是，用所给的关键字与线性表中各元素的关键字逐个进行比较，直到成功或失败。 折半查找 折半查找又称为二分查找，这种查找方法需要待查的查找表满足两个条件：首先，查找表必须使用顺序的存储结构；其次，查找表必须按关键字大小有序排列。算法的基本思想是：首先，将查找表中间位置数据元素的关键字与给定关键字比较，如果相等则查找成功；否则利用中间元素将表一分为二，如果中间元素关键字大于给定关键字，则在前一子表中进行折半查找，否则在后一子表中进行折半查找。重复以上过程，直到找到满足条件的元素，则查找成功；或直到子表为空为止，此时查找不成功。 一、顺序查找 1.1 顺序查找介绍 顺序查找又称为线性查找，是一种最简单的查找方法。适用于线性表的顺序存储结构和链式存储结构。 基本思路 从第一个元素m开始逐个与需要查找的元素x进行比较，当比较到元素值相同(即m=x)时返回元素m的下标，如果比较到最后都没有找到，则返回-1。 复杂度分析 查找成功时的平均查找长度为： ASL = 每个元素被查找的概率 * 总的元素的个数=1/n*(1+2+3+…+n) = (n+1)/2 ; 当查找不成功时，需要n+1次比较，时间复杂度为O(n)，所以，顺序查找的时间复杂度为O(n)。 优缺点 缺点：是当n 很大时，平均查找长度较大，效率低； 优点：是对表中数据元素的存储没有要求。另外，对于线性链表，只能进行顺序查找。 1.2 顺序查找实现 用Java代码实现顺序查找，示例代码如下： private static int sequenceSearch(int[] array,int target){ for(int i=0;i&lt;array.length;i++){ if(target==array[i]) return i; } return -1; } 1.3 顺序查找优化 在算法中，比较和赋值是比较耗时的。在上面的顺序查找实现代码中，存在着数组下标和目标值两种比较，那么能不能转变为一种比较呢？答案是可以的，不过要进行数据预处理，将查找值也放到数列中。比如将要查找的元素放在原数列中的第一位或最后一位(如果需要扩容就进行扩容)。此处将要查找的目标元素放在第一位，预处理示例代码如下： int[] array = {12,3,43,5,9}; int target = 43; int[] newArray = new int[array.length+1]; newArray[0] = target; for(int i=0;i&lt;array.length;i++){ newArray[i+1] = array[i]; } 也许有人会问，这样预处理一遍数据，需要将数组中所有数组都移动一遍，岂不是更花费时间？从总体上来看，确实是这样的。但是，面临大量的数据要处理时，常常要进行预处理、清洗等操作，这样会令纯粹处理数据（在该例子中就是搜索固定元素）的时间变的更少，更有效。当数据进行预处理后，搜索时就可以不用再比较两次，示例代码如下： public static int sequenceSearchPlus(int[] arr,int key){ int n=arr.length-1; arr[0]=key; while(arr[n]!=key){ n--; } return n; } 完整的测试代码如下： public class BasicTest { public static void main(String[] args){ int[] array = {12,3,43,5,9}; int target = 43; int[] newArray = new int[array.length+1]; newArray[0] = target; for(int i=0;i&lt;array.length;i++){ newArray[i+1] = array[i]; } int result = sequenceSearchPlus(newArray,target)-1; if(result != -1){ System.out.println(&quot;要查找的元素,在数组中的下标是：&quot;+result); }else{ System.out.println(&quot;要查找的元素不在数组中&quot;); } } public static int sequenceSearchPlus(int[] arr,int key){ int n=arr.length-1; arr[0]=key; while(arr[n]!=key){ n--; } return n; } 测试结果为： 要查找的元素,在数组中的下标是：2 二、二分查找 2.1 二分查找介绍 二分查找，是一种在有序数组中查找某一特定元素的查找算法。 基本思路 用给定值k先与中间结点的关键字比较，中间结点把线形表分成两个子表，若相等则查找成功；若不相等，再根据k与该中间结点关键字的比较结果确定下一步查找哪个子表，这样递归进行，直到查找到或查找结束发现表中没有这样的结点。 **复杂度分析 ** 假设数据大小是 n，每次查找后数据都会缩小为原来的一半，也就是会除以 2。最坏情况下，直到查找区间被缩小为空，才停止。被查找区间的大小变化为：n, n/2, n/4, n/8, …, n/(2k)。 可以看出来，这是一个等比数列。其中 n/(2^k)=1 时，k 的值就是总共缩小的次数。而每一次缩小操作只涉及两个数据的大小比较，所以，经过了 k 次区间缩小操作，时间复杂度就是 O(k)。通过n/(2^k)=1 ，我们可以求得 k=log2n ，所以时间复杂度就是 O(logn)。空间复杂度：O(1)。 优缺点分析 当查找表不会频繁有更新、删除操作时，使用折半查找是比较理想的。如果查找表有较频繁的更新、删除操作，维护表的有序会花费比较大的精力，不建议使用该查找方式。 2.2 二分查找实现 用Java代码实现折半查找，有两种方式：迭代法和递归法。迭代法示例代码如下： static int binarySearch1(int arr[],int len,int target){ /*初始化左右搜索边界*/ int left=0,right=len-1; int mid; while(left&lt;=right){ /*中间位置：两边界元素之和/2向下取整*/ mid=(left+right)/2; /*arr[mid]大于target，即要寻找的元素在左半边，所以需要设定右边界为mid-1，搜索左半边*/ if(target&lt;arr[mid]){ right=mid-1; /*arr[mid]小于target，即要寻找的元素在右半边，所以需要设定左边界为mid+1，搜索右半边*/ }else if(target&gt;arr[mid]){ left=mid+1; /*搜索到对应元素*/ }else if(target==arr[mid]){ return mid; } } /*搜索不到返回-1*/ return -1; } 递归法示例代码如下： static int binarySearch2(int array[],int left,int right,int target){ if(left&lt;=right){ int mid=(left+right)/2; /*搜索到对应元素*/ if(array[mid]==target){ return mid; }else if(array[mid]&lt;target){ /*array[mid]小于target，即要寻找的元素在右半边，所以需要设定左边界为mid+1，搜索右半边*/ return binarySearch2(array,mid+1,right,target); }else{ /*array[mid]大于target，即要寻找的元素在左半边，所以需要设定右边界为mid-1，搜索左半边*/ return binarySearch2(array,left,mid-1,target); } }else{ return -1; } } 2.3 二分查找变体 二分查找算法四种常见的变形问题，分别是： 查找第一个值等于给定值的元素。 查找最后一个值等于给定值的元素。 查找第一个大于等于给定值的元素。 查找最后一个小于等于给定值的元素。 1、查找第一个值等于给定值的元素 public static int search(int[] nums, int val) { int n = nums.length; int low = 0, high = n - 1; while (low &lt;= high) { int mid = (low + high) &gt;&gt;&gt; 1; if (nums[mid] &lt; val) { low = mid + 1; } else if (nums[mid] &gt; val) { high = mid - 1; } else { // 如果nums[mid]是第一个元素，或者nums[mid-1]不等于val // 说明nums[mid]就是第一个值为给定值的元素 if (mid == 0 || nums[mid - 1] != val) { return mid; } high = mid - 1; } } return -1; } 2、查找最后一个值等于给定值的元素 public static int search(int[] nums, int val) { int n = nums.length; int low = 0, high = n - 1; while (low &lt;= high) { int mid = (low + high) &gt;&gt;&gt; 1; if (nums[mid] &lt; val) { low = mid + 1; } else if (nums[mid] &gt; val) { high = mid - 1; } else { // 如果nums[mid]是最后一个元素，或者nums[mid+1]不等于val // 说明nums[mid]就是最后一个值为给定值的元素 if (mid == n - 1 || nums[mid + 1] != val) { return mid; } low = mid + 1; } } return -1; } 3、查找第一个大于等于给定值的元素 public static int search(int[] nums, int val) { int low = 0, high = nums.length - 1; while (low &lt;= high) { int mid = (low + high) &gt;&gt;&gt; 1; if (nums[mid] &lt; val) { low = mid + 1; } else { // 如果nums[mid]是第一个元素，或者nums[mid-1]小于val // 说明nums[mid]就是第一个大于等于给定值的元素 if (mid == 0 || nums[mid - 1] &lt; val) { return mid; } high = mid - 1; } } return -1; } 4、查找最后一个小于等于给定值的元素 public static int search(int[] nums, int val) { int n = nums.length; int low = 0, high = n - 1; while (low &lt;= high) { int mid = (low + high) &gt;&gt;&gt; 1; if (nums[mid] &gt; val) { high = mid - 1; } else { // 如果nums[mid]是最后一个元素，或者nums[mid+1]大于val // 说明nums[mid]就是最后一个小于等于给定值的元素 if (mid == n - 1 || nums[mid + 1] &gt; val) { return mid; } low = mid + 1; } } return -1; } 三、插值查找 3.1 插值查找介绍 在二分查找中，每次都是从待查找序列的中间点开始查找，这样的做法在正确性上固然没什么问题，但假如要查找的值距离某个边界比较近，还从中间点开始查找，就有点浪费时间了。举个例子来说说明，假如在在一个{1,2…,100}的数组中，要查找88这个值，还一直采用和中间点比较的策略，就显得不太明智，因为明显可以明显从较为靠后的位置去检索。为了克服这种弊端， 引入了插值查找。 基本思路 插值查找是根据要查找的关键字key与查找表中最大最小记录的关键字比较后的 查找方法，其核心就在于插值的计算公式 (key-array[low])/(array[high]-array[low])*(high-low)。简而言之，基于二分查找算法，将查找点的选择改进为自适应选择。 复杂度分析 时间复杂性：如果元素均匀分布，则O(log(logn))，在最坏的情况下可能需要O(n)。 空间复杂度：O(1)。 优缺点分析 对于长度比较长、关键字分布又比较均匀的查找表来说，插值查找算法的平均性能比折半查找要好的多。反之，数组中如果分布非常不均匀，那么插值查找未必是很合适的选择。 3.2 插值查找实现 上面的说法是总体介绍，落实到具体代码上的话，再慢慢分析。在二分查找中，mid的计算方式如下： ​ 将low从分数中提取出来，mid的计算就变成了： ​ 在插值查找中，mid的计算方式转换成了： ​ 有了上面的算术，就可以写代码了，迭代法插值查找示例代码如下： private static int insertSearch1(int arr[],int target){ /*初始化左右搜索边界*/ int left=0,right=arr.length-1; int mid; while(left&lt;=right){ mid=left+(target-arr[left])/(arr[right]-arr[left])*(right-left); /*arr[mid]大于target，即要寻找的元素在左半边，所以需要设定右边界为mid-1，搜索左半边*/ if(target&lt;arr[mid]){ right=mid-1; /*arr[mid]小于target，即要寻找的元素在右半边，所以需要设定左边界为mid+1，搜索右半边*/ }else if(target&gt;arr[mid]){ left=mid+1; /*搜索到对应元素*/ }else if(target==arr[mid]){ return mid; } } /*搜索不到返回-1*/ return -1; } 递归法插值查找示例代码如下： private static int insertSearch2(int array[],int left,int right,int target){ if(left&lt;=right){ int mid=left+(target-array[left])/(array[right]-array[left])*(right-left); /*搜索到对应元素*/ if(array[mid]==target){ return mid; }else if(array[mid]&lt;target){ /*array[mid]小于target，即要寻找的元素在右半边，所以需要设定左边界为mid+1，搜索右半边*/ return insertSearch2(array,mid+1,right,target); }else{ /*array[mid]大于target，即要寻找的元素在左半边，所以需要设定右边界为mid-1，搜索左半边*/ return insertSearch2(array,left,mid-1,target); } }else{ return -1; } } 四、斐波那契查找 4.1 斐波那契查找介绍 和前面的二分查找、插值查找相比，斐波那契查找是类似的，不过换了一种寻找mid点的方法。顾名思义，该种查找方法中，使用到了斐波那契数列，斐波那契数列的形式是：1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89…….（从第三个数开始，后边每一个数都是前两个数的和）。 基本思路 在斐波那契数列中的元素满足这样的关系：F[k]=F[k-1]+F[k-2]，此处将这个数组稍微改一下，改成：（F[k]-1）=（F[k-1]-1）+（F[k-2]-1）+1，图示如下： 通过上面的图，应该就可以看出为什么要这样分割数组了，因为要找出一个中间mid值，以便将数组按斐波那契数列的规律，分割成两部分。 复杂度分析 最坏情况下，时间复杂度为O(logn)，且其期望复杂度也为O(logn)。 4.2 斐波那契查找实现 上面介绍了分割的方法，但还有一个问题，就是斐波那契数列中的数值都是固定的，但要查找的数组的长度不固定，这样情况要怎么办？此时需要的是创建新数组，使新数组的长度是斐波那契数列中的值，并且是比原数组长度略大的值（此处只能是略大，因为略小的话，就会导致原数组元素丢失)，多出来的元素用原数组最高位元素补充，示例代码如下： int high = arr.length - 1; int f[] = fib(); /*获取最相邻的斐波那契数组中元素的值，该值略大于数组的长度*/ while(high &gt; f[k] - 1) { k++; } /*因为 f[k]值可能大于arr的长度。如果大于时，需要构造一个新的数组temp[]，将arr数组中的元素拷贝过去，不足的部分会使用0填充*/ int[] temp=Arrays.copyOf(arr, f[k]); /*然后将temp后面填充的0，替换为最后一位数字 *如将temp数组由{1,8,10,89,100,134,0,0}变换为{1,8,10,89,100,134,134,134}*/ for(int i = high + 1; i &lt; temp.length; i++) { temp[i] = arr[high]; } 解决了如何分割、如果创建临时新数组后，还有一个问题：怎么判断最后target == arr[i]时，这个arr[i]是原来的数组中的元素，还是在新数组中扩展出来的元素？如果是新数组中扩展出来的元素，该元素的下标是大于原数组元素的最大下标的，肯定不是要寻找的位置。其实该问题容易解决，就是当target == arr[i]时，如果arr[i]的下标&gt;原数组最大下标时，直接返回元数组最大下标即可。示例代码如下： /*原arr数组中的值*/ if(mid &lt;= high){ return mid; /*在temp中，扩展出来的高位的值*/ }else{ return high; } 完整斐波那契查找示例代码如下： public class FibonacciSearch { public static int FLENGTH = 20; public static void main(String[] args) { int [] arr = {1,8,10,89,100,134}; int target = 89; System.out.println(&quot;目标元素在数组中位置是：&quot; + fibSearch(arr, target)); } public static int[] fib() { int[] f = new int[FLENGTH]; f[0] = 1; f[1] = 1; for (int i = 2; i &lt; FLENGTH; i++) { f[i] = f[i-1] + f[i-2]; } return f; } public static int fibSearch(int[] arr, int target) { int low = 0; int high = arr.length - 1; int k = 0; int mid = 0; int f[] = fib(); /*获取最相邻的斐波那契数组中元素的值，该值略大于数组的长度*/ while(high &gt; f[k] - 1) { k++; } /*因为 f[k]值可能大于arr的长度。如果大于时，需要构造一个新的数组temp[]，将arr数组中的元素拷贝过去，不足的部分会使用0填充*/ int[] temp=Arrays.copyOf(arr, f[k]); /*然后将temp后面填充的0，替换为最后一位数字 *如将temp数组由{1,8,10,89,100,134,0,0}变换为{1,8,10,89,100,134,134,134}*/ for(int i = high + 1; i &lt; temp.length; i++) { temp[i] = arr[high]; } while (low &lt;= high) { mid = low + f[k - 1] - 1; if(target &lt; temp[mid]) { high = mid - 1; /*因为f[k]=f[k-1]+f[k-2]，所以k--就相当于取temp数组的左边部分*/ k--; } else if ( target &gt; temp[mid]) { low = mid + 1; /*同理，f[k]=f[k-1]+f[k-2]，k -= 2就相当于取temp数组的右边部分*/ k -= 2; } else { /*原arr数组中的值*/ if(mid &lt;= high){ return mid; /*在temp中，扩展出来的高位的值*/ }else{ return high; } } } return -1; } } 五、树表查找 基于树的查找方法是将待查表组织成特定的树结构，并在树结构的基础上实现查找的方法。 5.1 二叉树查找 二叉排序树（二叉查找树）是最简单的树表查找算法，该算法需要利用待查找的数据，进行生成树，确保树的左分支的值小于右分支的值，然后在就行和每个节点的父节点比较大小，然后再进行查找。 二叉排序树的性质 二叉排序树或者是一棵空树，或者是具有下列性质的二叉树： - 1&gt;若左子树不空，则左子树上所有结点的键值均小于或等于它的根结点的键值。 - 2&gt;若右子树不空，则右子树上所有结点的键值均大于或等于它的根结点的键值。 - 3&gt;左、右子树也分别为二叉排序树。 5.1.1 二叉排序树中序遍历 二叉排序树有不同的遍历方式，中序遍历的结果比较直观。二叉树示例： 二叉树上中序遍历的方式是：左节点、根节点、右节点。该二叉树的遍历结果为：1、3、4、6、7、8、10、13、14。 5.1.2 二叉树查找实现 1、创建二叉树 首先，要创建一个树的节点，节点中要有该节点储存的值，然后起左右子树。示例代码： class BinaryTree{ int value; BinaryTree left; BinaryTree right; public BinaryTree(int value){ this.value = value; } } 接下来就要创建二叉排序树，创建二叉排序树是一个递归的过程，需要将序列中的值一个一个添加到二叉树中。方便起见，可以利用序列中第一个元素作为根节点，再持续添加节点，示例代码： int[] array = {35,76,6,22,16,49,49,98,46,9,40}; BinaryTree root = new BinaryTree(array[0]); for(int i = 1; i &lt; array.length; i++){ createBST(root, array[i]); } 具体创建树的过程，就是一个不断与根节点比较，然后添加到左侧、右侧或不添加的过程。因为在二叉排序树中，不存在重复元素，有相等元素已经在树中时，直接忽略后续相等元素。示例代码： public static void createBST(BinaryTree root, int element){ BinaryTree newNode = new BinaryTree(element); if(element &gt; root.value){ if(root.right == null) root.right = newNode; else createBST(root.right, element); }else if(element &lt; root.value){ if(root.left == null) root.left = newNode; else createBST(root.left, element); }else{ System.out.println(&quot;该节点&quot; + element + &quot;已存在&quot;); return; } } 2、二叉树查找 查找元素是否在树中的过程，就是一个二分查找的过程，不过查找的对象从左右子序列转换成了左右子树而已。示例代码： public static void searchBST(BinaryTree root, int target, BinaryTree p){ if(root == null){ System.out.println(&quot;查找&quot;+target+&quot;失败&quot;); }else if(root.value == target){ System.out.println(&quot;查找&quot;+target+&quot;成功&quot;); }else if(root.value &gt;= target){ searchBST(root.left, target, root); }else{ searchBST(root.right, target, root); } } 完整示例代码： ​ public class BinarySortTree { public static void main(String[] args) { int[] array = {35,76,6,22,16,49,49,98,46,9,40}; BinaryTree root = new BinaryTree(array[0]); for(int i = 1; i &lt; array.length; i++){ createBST(root, array[i]); } System.out.println(&quot;中序遍历结果：&quot;); midOrderPrint(root); System.out.println(); searchBST(root, 22, null); searchBST(root, 100, null); } /*创建二叉排序树*/ public static void createBST(BinaryTree root, int element){ BinaryTree newNode = new BinaryTree(element); if(element &gt; root.value){ if(root.right == null) root.right = newNode; else createBST(root.right, element); }else if(element &lt; root.value){ if(root.left == null) root.left = newNode; else createBST(root.left, element); }else{ System.out.println(&quot;该节点&quot; + element + &quot;已存在&quot;); return; } } /*二叉树中查找元素*/ public static void searchBST(BinaryTree root, int target, BinaryTree p){ if(root == null){ System.out.println(&quot;查找&quot;+target+&quot;失败&quot;); }else if(root.value == target){ System.out.println(&quot;查找&quot;+target+&quot;成功&quot;); }else if(root.value &gt;= target){ searchBST(root.left, target, root); }else{ searchBST(root.right, target, root); } } /*二叉树的中序遍历*/ public static void midOrderPrint(BinaryTree rt){ if(rt != null){ midOrderPrint(rt.left); System.out.print(rt.value + &quot; &quot;); midOrderPrint(rt.right); } } } 测试结果为： 该节点49已存在 中序遍历结果： 6 9 16 22 35 40 46 49 76 98 查找22成功 查找100失败 六、分块查找 6.1 分块查找介绍 分块查找，顾名思义，要先将所有元素按大小进行分块，然后在块内进行查找。在分块时，块内的元素不一定是有序的，只要一个块内的元素在同一区间就行。用较标准的语言描述是：算法的思想是将n个数据元素&quot;按块有序&quot;划分为m块（m≤n）。每一块中的结点不必有序，但块与块之间必须&quot;按块有序&quot;，每个块内的的最大元素小于下一块所有元素的任意一个值。 所以，在使用分块查找时，分成了两步： - 1&gt;找到元素可能在的块。 - 2&gt;在对应的块内查找元素。 6.2 分块查找实现 在上个章节说到，该方法要先分块，那么块应该具有怎样的属性呢？至少要有以下元素： - 1&gt;长度 一般是固定的长度。 - 2&gt;起始位置 当块的长度固定后，需要确定起始位置才能固定不同的块表示的元素的位置范围。 - 3&gt;块标识 该标识用来标识块内元素的范围，可以用最大值、最小值、平均值等多种方式来表示。 示例代码如下： public class Block { /*block的索引，用来标识块中元素*/ public int index; /*该block的开始位置*/ public int start; /*块元素长度，在该例子中0代表空元素，不计入block长度*/ public int length; public Block(int index, int start, int length) { this.index = index; this.start = start; this.length = length; } } 在该例子中，定义元素数组和块数组，示例如下： /*主表*/ static int[] valueList = new int[]{ 104, 101, 103, 105,102, 0, 0, 0, 0, 0, 201, 202, 204, 203,0, 0, 0, 0, 0, 0, 303, 301, 302, 0, 0, 0, 0, 0, 0, 0 }; /*索引表*/ static Block[] indexList = new Block[]{ new Block(1, 0, 5), new Block(2, 10, 4), new Block(3, 20, 3) }; valueList中的0，可以简单理解为块内的空元素；indexList中的1,2,3代表块内元素的取值范围，第一个块内是100-200之间的元素，第2个块内是200-300之间的元素，以此类推。 在进行元素查找时，先判断是否存在元素可能存在的块。示例如下： /*确定插入到哪个块中，在该例子中，第一个block中放的是100-200之间的数，第二个block中放的是200-300之间的数，以此类推*/ int index = key/100; /*找到对应的block*/ for(int i = 0;i &lt; indexList.length; i++) { if(indexList[i].index == index) { indexItem = indexList[i]; break; } } /*如果数组中不存在对应的块，则返回-1，查找失败*/ if(indexItem == null) return -1; 找到内对的块后，就在该块内进行搜索，示例代码如下： /*在对应的block中查找*/ for(int i = indexItem.start; i &lt; indexItem.start + indexItem.length; i++) { if(valueList[i] == key) return i; } return -1; 如果需要在数组中插入元素，同样需要需要先查找是否存在对应的块，如果存在，则追加到该块中元素的尾部。 完整示例代码如下： public class BlockSearch { /*主表*/ static int[] valueList = new int[]{ 104, 101, 103, 105,102, 0, 0, 0, 0, 0, 201, 202, 204, 203,0, 0, 0, 0, 0, 0, 303, 301, 302, 0, 0, 0, 0, 0, 0, 0 }; /*索引表*/ static Block[] indexList = new Block[]{ new Block(1, 0, 5), new Block(2, 10, 4), new Block(3, 20, 3) }; public static void main(String[] args) { System.out.println(&quot;原始主表：&quot;); printElemts(valueList); /*分块查找*/ int searchValue = 203; System.out.println(&quot;元素&quot;+searchValue+&quot;，在列表中的索引为：&quot;+blockSearch(searchValue)+&quot;\\n&quot;); /*插入数据并查找*/ int insertValue = 106; /*插入成功，查找插入位置*/ if (insertBlock(insertValue)) { System.out.println(&quot;插入元素&quot;+insertValue+&quot;后的主表：&quot;); printElemts(valueList); System.out.println(&quot;元素&quot; + insertValue + &quot;在列表中的索引为：&quot; + blockSearch(insertValue)); } } public static void printElemts(int[] array) { for(int i = 0; i &lt; array.length; i++){ System.out.print(array[i]+&quot; &quot;); if ((i+1)%10 == 0) { System.out.println(); } } } ​ /*插入数据*/ public static boolean insertBlock(int key) { Block item = null; /*确定插入到哪个块中，在该例子中，第一个block中放的是100-200之间的数，第二个block中放的是200-300之间的数，以此类推*/ int index = key/100; int i = 0; /*找到对应的block*/ for (i = 0; i &lt; indexList.length; i++) { if (indexList[i].index == index) { item = indexList[i]; break; } } /*如果数组中不存在对应的块，则不能插入该数据*/ if (item == null) { return false; } /*将元素插入到每个块的最后*/ valueList[item.start + item.length] = key; /*更新该块的长度*/ indexList[i].length++; return true; } public static int blockSearch(int key) { Block indexItem = null; /*确定插入到哪个块中，在该例子中，第一个block中放的是100-200之间的数，第二个block中放的是200-300之间的数，以此类推*/ int index = key/100; /*找到对应的block*/ for(int i = 0;i &lt; indexList.length; i++) { if(indexList[i].index == index) { indexItem = indexList[i]; break; } } /*如果数组中不存在对应的块，则返回-1，查找失败*/ if(indexItem == null) return -1; /*在对应的block中查找*/ for(int i = indexItem.start; i &lt; indexItem.start + indexItem.length; i++) { if(valueList[i] == key) return i; } return -1; } } 测试结果如下： 原始主表： 104 101 103 105 102 0 0 0 0 0 201 202 204 203 0 0 0 0 0 0 303 301 302 0 0 0 0 0 0 0 元素203，在列表中的索引为：13 插入元素106后的主表： 104 101 103 105 102 106 0 0 0 0 201 202 204 203 0 0 0 0 0 0 303 301 302 0 0 0 0 0 0 0 元素106在列表中的索引为：5 七、哈希查找 7.1 哈希查找介绍 要了解哈希查找，就要先了解一下哈希表和哈希函数。先看下标准的定义：哈希表，是根据关键值而直接进行访问的数据结构。也就是说，它通过把关键码值映射到表中一个位置来访问记录，以加快查找的速度。这个映射函数叫做散列函数，存放记录的数组叫做散列表（哈希表）。 从上面的定义可以看出：哈希查找与线性表查找和树表查找最大的区别在于，不用数值比较。 7.1.1 构造哈希表 要使用哈希查找，就要先有哈希表，所以需要先介绍一下哈希表的构造方法。常见的构造方法有如下几种： 1、直接定址法 哈希地址：f(key) = a*key+b (a、b为常数)。 这种方法的优点是：简单、均匀、不会产生冲突。但是需要事先知道 key 的分布情况，适合查找表较小并且连续的情况。 2、数字分析法 假设关键字是R进制数（如十进制）。并且哈希表中可能出现的关键字都是事先知道的，则可选取关键字的若干数位组成哈希地址。选取的原则是使得到的哈希地址尽量避免冲突，即所选数位上的数字尽可能是随机的。 举个例子：比如11位手机号码“136xxxx5889”，其中前三位是接入号，一般对应不同运营公司的子品牌，中间四位表示归属地，最后四位才是用户号，此时就可以用后4位来作为哈希地址。 3、平方取中法 取key平方后的中间几位为哈希地址。通常在选定哈希函数时不一定能知道关键字的全部情况，仅取其中的几位为地址不一定合适。而一个数平方后的中间几位数和数的每一位都相关， 由此得到的哈希地址随机性更大。如key是1234，那么它的平方就是1522756，再抽取中间的3位就是227作为 f(key) 。 4、折叠法 折叠法是将 key 从左到右分割成位数相等的几个部分(最后一部分位数不够可以短些)，然后将这几部分叠加求和，并按哈希表的表长，取后几位作为 f(key) 。 比如key是9876543210，哈希表的表长为3位，我们将 key 分为4组，987 | 654 | 321 | 0 ，然后将它们叠加求和 987+654+321+0=1962，再取后3位即得到哈希位置是：962 。 5、除留余数法 取关键字被某个不大于哈希表表长 m 的数 p 除后所得的余数为哈希地址。即 f(key) = key % p (p ≤ m)。这种方法是最常用的哈希函数构造方法。 6、随机数法 哈希地址：random(key) ，这里random是随机函数，当 key 的长度不等时，采用这种方法比较合适。 7.1.2 解决冲突 在使用以上方法计算key对应的哈希地址时，难免会遇到两个key不相等，到计算出来的哈希地址相同的情况，该情况就被称为“冲突”。在构造哈希表，常用如下方式解决冲突： 1、开放定址法 该方法指的是两个key在计算出相同的哈希地址时，后者继续在哈希表中向后寻找空位置，存放改key的方法。举个例子：假如原始的key中有8、15两个元素，哈希表中的长度为7，当使用key % length求余时，两个key会计算出相同的哈希位置。假设哈希表中的1位置已经存放了8，那么15就要从1位置往后寻找空位，假如2位置是空的，就可以把15存放到2位置；假如2位置不空，就要往3位置寻找，以此类推。 2、拉链法 该方法中处理相同位置的方式是：创建一个List，存储相同位置上不同值的key，此处借用网上的一张图来表示： 7.2 哈希查找实现 依据上文介绍，先构建哈希表。而要构建哈希表，就要先有计算地址的方法，示例代码如下： /*用除留余数法计算要插入元素的地址*/ public static int hash(int[] hashTable, int data) { return data % hashTable.length; } 有了计算哈希地址的方法后，剩下的就是将原始的元素插入到哈希表中，也就是先利用key计算一个地址，如果这个地址以及有元素了，就继续向后寻找。此处可以循环计算地址，示例代码如下： /*将元素插入到哈希表中*/ public static void insertHashTable(int[] hashTable, int target) { int hashAddress = hash(hashTable, target); /*如果不为0，则说明发生冲突*/ while (hashTable[hashAddress] != 0) { /*利用开放定址法解决冲突，即向后寻找新地址*/ hashAddress = (++hashAddress) % hashTable.length; } /*将元素插入到哈希表中*/ hashTable[hashAddress] = target; } 哈希表构建后，就是在哈希表中查找元素了。在查找元素时，容易想到的情况是：在直接计算出的哈希地址及其后续位置查找元素。特殊的是，上一步中，有循环计算地址的操作，所以此处计算到原始地址时，也代表查找失败。示例代码如下： public static int searchHashTable(int[] hashTable, int target) { int hashAddress = hash(hashTable, target); while (hashTable[hashAddress] != target) { /*寻找原始地址后面的位置*/ hashAddress = (++hashAddress) % hashTable.length; /*查找到开放单元(未存放元素的位置)或 循环回到原点，表示查找失败*/ if (hashTable[hashAddress] == 0 || hashAddress == hash(hashTable, target)) { return -1; } } return hashAddress; } 完整示例代码如下： public class HashSearch { /*待查找序列*/ static int[] array = {13, 29, 27, 28, 26, 30, 38}; /* 初始化哈希表长度，此处哈希表容量设置的和array长度一样。 * 其实正常情况下，哈希表长度应该要长于array长度，因为使用 * 开放地址法时，可能会多使用一些空位置 */ static int hashLength = 7; static int[] hashTable = new int[hashLength]; public static void main(String[] args) { /*将元素插入到哈希表中*/ for (int i = 0; i &lt; array.length; i++) { insertHashTable(hashTable, array[i]); } System.out.println(&quot;哈希表中的数据：&quot;); printHashTable(hashTable); int data = 28; System.out.println(&quot;\\n要查找的数据&quot;+data); int result = searchHashTable(hashTable, data); if (result == -1) { System.out.println(&quot;对不起，没有找到！&quot;); } else { System.out.println(&quot;在哈希表中的位置是：&quot; + result); } } /*将元素插入到哈希表中*/ public static void insertHashTable(int[] hashTable, int target) { int hashAddress = hash(hashTable, target); /*如果不为0，则说明发生冲突*/ while (hashTable[hashAddress] != 0) { /*利用开放定址法解决冲突，即向后寻找新地址*/ hashAddress = (++hashAddress) % hashTable.length; } /*将元素插入到哈希表中*/ hashTable[hashAddress] = target; } public static int searchHashTable(int[] hashTable, int target) { int hashAddress = hash(hashTable, target); while (hashTable[hashAddress] != target) { /*寻找原始地址后面的位置*/ hashAddress = (++hashAddress) % hashTable.length; /*查找到开放单元(未存放元素的位置)或 循环回到原点，表示查找失败*/ if (hashTable[hashAddress] == 0 || hashAddress == hash(hashTable, target)) { return -1; } } return hashAddress; } /*用除留余数法计算要插入元素的地址*/ public static int hash(int[] hashTable, int data) { return data % hashTable.length; } public static void printHashTable(int[] hashTable) { for(int i=0;i&lt;hashTable.length;i++) System.out.print(hashTable[i]+&quot; &quot;); } } 测试结果： 哈希表中的数据： 27 29 28 30 38 26 13 要查找的数据28 在哈希表中的位置是：2 ","link":"https://tianxiawuhao.github.io/L2uhkBI__/"},{"title":"git-worktree","content":"名称 git-worktree - 管理附加到同一存储库的多个工作树。 语法 git worktree add [-f] [--detach] [--checkout] [--lock [--reason &lt;string&gt;]] [-b &lt;new-branch&gt;] &lt;path&gt; [&lt;commit-ish&gt;] git worktree list [--porcelain] git worktree lock [--reason &lt;string&gt;] &lt;worktree&gt; git worktree move &lt;worktree&gt; &lt;new-path&gt; git worktree prune [-n] [-v] [--expire &lt;expire&gt;] git worktree remove [-f] &lt;worktree&gt; git worktree repair [&lt;path&gt;…] git worktree unlock &lt;worktree&gt; 描述 管理附加到同一存储库的多个工作树。一个 git 存储库可以支持多个工作树，允许您一次检出多个分支。 使用 git worktree 添加一个新的工作树与存储库相关联。 这个新的工作树被称为“链接工作树”，而不是 git-init(1) 或 git-clone(1) 准备的“主工作树”。 一个存储库有一个主工作树（如果它不是一个裸存储库）和零个或多个链接工作树。 完成链接工作树后，使用 git worktree remove 将其删除。 在最简单的形式中， git worktree add 会自动创建一个新分支，其名称是 的最后一个组成部分，如果您打算处理一个新主题，这很方便。 例如， git worktree add ../hotfix 创建新的分支 hotfix 并在路径 ../hotfix 处检出。 要改为在新工作树中的现有分支上工作，请使用 git worktree add 。 另一方面，如果您只是打算进行一些实验性更改或在不干扰现有开发的情况下进行测试，那么创建一个不与任何分支关联的一次性工作树通常会很方便。 例如， git worktree add -d 在与当前分支相同的提交处创建一个具有分离 HEAD 的新工作树 如果在不使用 git worktree remove 的情况下删除了工作树，则其位于存储库中的相关管理文件（请参阅下面的“详细信息”）最终将被自动删除（请参阅 git-config(1) 中的 gc.worktreePruneExpire）， 或者您可以在主工作树或任何链接的工作树中运行 git worktree prune 来清理任何陈旧的管理文件。 如果链接的工作树存储在不总是挂载的便携式设备或网络共享上，您可以通过发出 git worktree lock 命令来防止其管理文件被修剪，可选地指定 --reason 来解释工作树被锁定的原因 . 初始化一个仓库 $ git init Initialized empty Git repository in D:/VSCode/testGit/.git/ user@NAME MINGW64 /d/VSCode/testGit (master) $ git status $ git add . $ git commit -m 'f commit' [master (root-commit) f7f05d0] f commit 3 files changed, 18 insertions(+) create mode 100644 blob.config create mode 100644 copy.sh create mode 100644 fff.txt ## 新建两个分支后 user@NAME MINGW64 /d/VSCode/testGit (master) $ git branch -a dev * master release 新建工作树 add git worktree add [-f] [--detach] [--checkout] [--lock [--reason &lt;string&gt;]] [-b &lt;new-branch&gt;] &lt;path&gt; [&lt;commit-ish&gt;] ## 创建一个新工作树wtree1 $ git worktree add ./wtree1 Preparing worktree (new branch 'wtree1') HEAD is now at f7f05d0 f commit user@NAME MINGW64 /d/VSCode/testGit (master) $ git worktree list D:/VSCode/testGit f7f05d0 [master] D:/VSCode/testGit/wtree1 f7f05d0 [wtree1] 锁定（解锁）工作树 lock/unlock 锁定的工作树可以防止被修剪（prune） user@NAME MINGW64 /d/VSCode/testGit (master) $ git worktree lock --reason='lock reason' wtdev user@NAME MINGW64 /d/VSCode/testGit (master) $ git worktree list D:/VSCode/testGit f4c1226 [master] D:/VSCode/wt/wtdev 8ec2b56 [dev] locked D:/VSCode/wt/wtree1 e14fd71 [wtree1] D:/VSCode/wt/wtree2 8ec2b56 [dev] D:/VSCode/wt/wtree3 e3a3c4c (detached HEAD) D:/VSCode/wt/wtree4 e4b08e3 [nb-wtree4] ## 解锁 user@NAME MINGW64 /d/VSCode/wt/wtree5 (wtree5) $ git worktree unlock wtdev 展示工作树列表 list ## git worktree list [--porcelain] user@NAME MINGW64 /d/VSCode/testGit (master) $ git worktree list D:/VSCode/testGit f4c1226 [master] D:/VSCode/wt/wtdev 2c05e7a [dev] D:/VSCode/wt/wtree1 e14fd71 [wtree1] D:/VSCode/wt/wtree2 2c05e7a [dev] D:/VSCode/wt/wtree3 e3a3c4c (detached HEAD) D:/VSCode/wt/wtree4 e4b08e3 [nb-wtree4] D:/VSCode/wt/wtree5 f4c1226 [wtree5] locked D:/VSCode/wt/wtrele 7f91b4d [release] 删除工作树 remove $ git worktree remove wtree2 fatal: 'wtree2' contains modified or untracked files, use --force to delete it ## 使用 -f 强制删除 user@NAME MINGW64 /d/VSCode/testGit (master) $ git worktree remove -f wtree2 修剪工作树 prune ## 当前仓库中的工作树状态，wtree5 被锁定 $ git worktree list D:/VSCode/testGit f4c1226 [master] D:/VSCode/wt/wtdev 2c05e7a [dev] D:/VSCode/wt/wtree1 e14fd71 [wtree1] D:/VSCode/wt/wtree3 e3a3c4c (detached HEAD) D:/VSCode/wt/wtree4 e4b08e3 [nb-wtree4] D:/VSCode/wt/wtree5 f4c1226 [wtree5] locked D:/VSCode/wt/wtrele 7f91b4d [release] ## 手动删除工作树目录 D:/VSCode/wt/wtrele 且 把wtree4、wtree5 移动到其他目录 后的状态 $ git worktree list D:/VSCode/testGit f4c1226 [master] D:/VSCode/wt/wtdev 2c05e7a [dev] D:/VSCode/wt/wtree1 e14fd71 [wtree1] D:/VSCode/wt/wtree3 e3a3c4c (detached HEAD) D:/VSCode/wt/wtree4 e4b08e3 [nb-wtree4] prunable ## 可修剪 D:/VSCode/wt/wtree5 f4c1226 [wtree5] locked D:/VSCode/wt/wtrele 7f91b4d [release] prunable ## 可修剪 ## 修剪后 的工作树，被锁定的工作树没有变化 ，其他‘可修剪’工作树被删除 $ git worktree prune $ git worktree list D:/VSCode/testGit f4c1226 [master] D:/VSCode/wt/wtdev 2c05e7a [dev] D:/VSCode/wt/wtree1 e14fd71 [wtree1] D:/VSCode/wt/wtree3 e3a3c4c (detached HEAD) D:/VSCode/wt/wtree5 f4c1226 [wtree5] locked 重新关联工作树 repair ## 被修剪掉了的工作树无法被重新关联 user@NAME MINGW64 /d/VSCode/wt/wtinner/wtree4 $ git worktree repair ../../../testGit/ fatal: not a git repository: D:/VSCode/testGit/.git/worktrees/wtree4 ## 被锁定的工作树没有被修剪（prune）掉，所有重新关联上了 user@NAME MINGW64 /d/VSCode/wt/wtinner/wtree5 (wtree5) $ git worktree repair ../../../testGit/ ## 现在把 wtree3 移动到其他目录 $ git worktree list D:/VSCode/testGit f4c1226 [master] D:/VSCode/wt/wtdev 2c05e7a [dev] D:/VSCode/wt/wtree1 e14fd71 [wtree1] D:/VSCode/wt/wtree3 e3a3c4c (detached HEAD) prunable ## 可修剪 D:/VSCode/wt/wtree5 f4c1226 [wtree5] locked ## 在修剪之前直接repair ,也可以成功关联 $ cd wt/wtinner/wtree3 user@NAME MINGW64 /d/VSCode/wt/wtinner/wtree3 ((e3a3c4c...)) $ git worktree repair ../../../testGit/ ","link":"https://tianxiawuhao.github.io/uA0v93Wq7/"},{"title":"王树森深度强化学习笔记12：策略梯度中的基准（Policy Gradient with Baseline）","content":"我们在策略梯度中加入Baseline可以降低方差，让策略函数收敛更快，我们用这种策略梯度来学习策略网络。 首先来回顾一下策略梯度： 一、基准（Baseline） 我们采用baseline来降低方差，可以让收敛更快。baseline记为b，它可以是任何数，但不能依赖于动作A。 接下来证明baseline的理论性质： 这里我们发现，如果b与动作A无关，等式=0 那么我们就得到了策略梯度的baseline形式： b不会影响策略梯度，但会影响到蒙特卡洛近似。如果b的取值接近Qπ，那么b会让蒙特卡洛近似的方差降低，收敛速度更快。 二、蒙特卡洛近似（Monte Carlo Approximation） 这个公式，是我们刚刚推导的带有baseline的策略梯度公式： 因此这里我们记，期望内部的数值为g（At），我们根据策略π对a进行一次随机抽样得到at，并计算g(at)。 这里的g(at)是关于策略梯度的一个无偏估计： 因而我们得到了一个随机策略梯度，并采用随机梯度上升来获取到最优策略 我们证明baseline不会影响到E[g(At)]，但根据上式我们发现b实际是影响它的蒙特卡洛近似，即实际观测到的g(at)。因此，一个好的b可以减小方差并加速收敛。 三、基准的选择（Choice of Baseline） 1）Choice 1：b = 0 2）Choice 2：b = Vπ(st) ","link":"https://tianxiawuhao.github.io/cFQZiMqpR/"},{"title":"王树森深度强化学习笔记11：Dueling Network","content":"一、优势函数（Advantage Function） 首先回顾折扣回报、动作价值函数和状态价值函数： 接下来是最优动作价值函数和最优状态价值函数： 这里我们引入一个新的概念：最优优势函数。它是由最优动作价值函数和最优状态价值函数确定的。这里把最优状态价值函数作为baseline，它的实际意义是动作a相对于baseline的优势，动作a越好，优势就越大。 为了推导出Dueling Network，我们需要分析优势函数的一个理论性质。 这需要一个定理： 这里有一个推导： 将A*(s,a)变换一下，可以得到： 又因为maxA*=0，所以可以得到：定理2 二、Dueling Network 回顾一下DQN： 这里我们采用一个神经网络对优势函数A*(s,a;w^A)的近似： 我们需要一个神经网络来近似最优状态价值函数V*(s;w^V) 我们可以看到，这两个神经网络都有着相似的结构，即利用卷积层对输入提取特征。那么我们在真正实现这两个神经网络时就可以让他们共享神经网络参数。 有了这两个近似，我们就可以得到这如下公式： 这里的Q(s,a;wA,wV)即Dueling Network，这里用w来代替(wA,wV) 如下就是整体流程： Dueling Network与DQN的结构相似，但Dueling Network比DQN的神经网络结构更好，效果也更好。 值得一提的是，我们在训练Dueling Network时，可以采用的训练方法：PER,DDQN,Multi-Step TD target等 三、解决不唯一性（Overcome Non-identifiability） 如果不加最大化，可能会出现V和A都训练不好。举个例子，如果记V' = V*+10，A' = A*-10，它们的和并不会对Q产生影响，但实际上，V和A都产生了很大的波动，造成了V和A对应的神经网络都训练不好的结果。 而加了最大化这一项，可以很好的保持V和A对应的神经网络的平衡 四、总结（Summary） MARK：要把Dueling Network当成一个整体去训练而不是分别单独的去训练对应的神经网络 ","link":"https://tianxiawuhao.github.io/HCoq7nVQ7/"},{"title":"王树森深度强化学习笔记10：高估问题、目标网络、DDQN（Overestimate、Target Network、Double DQN）","content":"一、自举问题（Bootstrapping） 在强化学习中，Bootstrapping的意思是用一种估算去更新同类的估算。 首先来，看一下TD target yt，它包含当前观测到的奖励rt和DQN对下一步状态和动作的预测值。 然后我们采用随机梯度下降（SGD）来更新DQN模型参数w。这里使用的SGD用到了yt，而yt又部分基于DQN在t+1时刻的估计。因此，采用TD算法来更新DQN就是一个bootstrapping的一个例子。 二、高估问题（Problem of Overestimation） 首先，我们要知道：TD算法可能会导致高估真实的动作价值。造成高估的主要原因有两个： ①原因一：计算TD target时使用了最大化（The Maximization） 证明如下： 首先我们获取到一些观测到真实数据：x1,x2,...,xn。然后在这些观测值中加入一些均值为0的随机噪声Q1,Q2,...,Qn。由于噪声的均值为0，所以对Q的均值求期望，就等于对x求期望。然而，Q最大值的期望却会大于等于x的最大值，且Q最小值的期望也会小于等于x的最小值。 这里我们回到DQN的高估问题：假设我们得到了一些动作价值，x(a1),...,x(an)，且有一些DQN的噪声估计Q(s,a1;w),...,Q(s,an;w)。这里我们假设估计是无偏的，也就是上述中Q的均值为0。那么这里的q作为Q(s,a;w)的最大值，就会产生上述的高估问题。如下图所示： 上面我们已经知道了，q是x(a)的高估，因此我们这里假设qt+1是对t+1时刻真实的动作价值的预测，因此它是高估的。然而我们的TD target是取决于当前观测到的奖励rt和DQN对qt+1的预测，因此也是高估的。而TD算法的目的就是使Q(st,at;w)向TD target逼近，而yt是一个高估的量，继而导致Q(st,at;w)也会高估真实的动作价值。 ②原因二：自举产生的高估问题（Bootstrapping） 如果已经产生了高估，那么再进行Bootstrapping，高估就会越来越大。 这里讨论为什么Bootstrapping会导致高估： 我们知道TD target实际上是使用了下面的一个最大化的公式，且我们使用TD target用于更新Q(st,at;w)。假设DQN已经产生了高估的动作价值，那么Q(st,at;w)就已经是一个高估的量。然而我们采用了一个最大化公式，会使qt+1更大，而我们再使用qt+1去更新Q(st,at;w)即将高估的量传回DQN用于更新模型参数。因此，高估问题会越来越严重。 总结一下：直接上图 高估不是问题。我们在采用最大化时，参考的是相对值，而不是绝对值，因此如果是均匀的高估并不影响实际结果。有问题的是非均匀的高估。 这里有两种方法用于缓解高估问题： ①使用目标网络（Target Network） ②使用双DQN(Double DQN) 三、目标网络（Target Network） 相比于DQN，Target Network是独立于DQN的。它与DQN有着相同的神经网络结构，但有不同的参数，记作w-。 我们使用Q(s,a;w)来控制Agent并且收集经验即transition{(st,at,rt,st+1)}。 使用Q(s,q;w-)来计算TD target。 以前使用DQN来计算yt，用yt来更新DQN的参数。这会产生自举。现在我们使用Target Network，用Target Network来计算yt。这样就可以缓解高估问题。 注意一下，这里计算TD target采用的Target Network，参数为w-，而随机梯度下降（SGD）仅用于更新DQN的神经网络参数w 这里用几种方法用于更新w- 对比一下两种更新DQN的方法： 原始方法利用自举，这会造成高估。 而采用了Target Network可以缓解高估问题。 尽管采用上述方法可以缓解DQN最大化和自举产生的高估问题，但仍不可避免TD算法所产生的高估问题。下面的双DQN（Double DQN）方法可以更有效地避免高估问题。 四、双DQN（Double DQN） 1）传统DQN 为了说明原始DQN，Target Network和DDQN之间的区别，这里把求最大化的过程拆成两步： TD target的第一步是做选择： 第二步是计算TD target： 这种训练方式效果最差。 2）Target Network 3）DDQN DDQN在第一步求最大化的动作时，采用了传统的DQN方法，进行求取最优动作a*。而在第二步评估TD target时，采用了Target Network，很容易得到如下不等式： 五、总结（Summary） 对比一下三种方法计算TD Target： ","link":"https://tianxiawuhao.github.io/FrqjohR4Q/"},{"title":"王树森深度强化学习笔记9：经验回放 Experience Replay","content":"一、回顾深度Q网络和时序差分算法（Revisiting DQN and TD Learning） 1）Deep Q Network（DQN） DQN采用价值网络Q(s,a;w)来近似最优动作价值函数Q*(s,a) 2）TD Learning 我们通常使用TD算法来训练DQN，这里我们回顾一下TD算法的步骤： ①Agent在t时刻观测到一个状态st并做出动作at ②Agent与环境交互获得了一个新的状态st+1并且获得了一个奖励rt ③此时，我们得到了TD Target yt ④继而写出TD error δt 我们的目标是让qt接近yt，即使TD error δt尽量的小 因此TD算法就是寻找一个神经网络参数w使损失函数（Loss Function）L(w)尽可能的小 ⑤采用梯度下降来使wt不断逼近w* (st,at,rt,st+1)是一个四元组，这是一个transition，可以认为是一条训练数据，传统算法在使用完该训练数据后就把它丢掉，不再使用，这种对于DQN的训练效果并不好。 二、为什么要使用经验回放（Why use the Experience Replay） 其实，关于讨论为什么要使用经验回放，其实就是在讨论不使用经验回放会有什么缺点，以及使用了经验回放有什么有点。 1）缺点一：浪费经验（Shortcoming 1：Watse of Experience） 传统的TD算法每次仅使用一个transition，使用完后就丢掉，不再使用，这会造成浪费 2）缺点二：关联性更新（Shortcoming 2：Correlated Updates） 传统TD算法会按照顺序使用transition(st,at,rt,st+1)，前后两条transition之间有很强的关联性，即st和st+1非常接近，实验表明这并不利于把DQN训练的很好。 三、经验回放（Experience Replay） 1）回放缓冲区（Replay Buffer） 我们在使用完一个transition时，会把一个transition放入一个队列里，这个队列被称之为回放缓冲区（Replay Buffer），它的容量是一个超参数（Hyper Parameter）n，Replay Buffer可以存储n条transitions。 如果Replay Buffer满了，那么新来的transition会替代老的transition 2）TD算法中经验回放（TD with Experience Replay） 我们通过找到神经网络参数w来最小化损失函数Loss Function。 我们是用随机梯度下降（Stochastic Gradient Descent SGD）来最小化损失函数Loss Function：从buffer中随机抽取一个transition（si,ai,ri,si+1）,然后计算TD error δi，再算出随机梯度（Stochastic Gradient）gi，使用随机梯度下降(SGD)来调整神经网络参数w。 值得一提的是，实践中，通常使用mini-batch SGD,即每次抽取多个transitions，算出多个随机梯度，利用这多个随机梯度的平均来更新神经网络参数w。 3）经验回放的优势（Benefits of Experience Replay） ①打破了经验的相关性 ②重复利用过去的经验 四、优先经验回放（Prioritized Experience Replay） 1）基础思想（Basic Idea） 对于Agent而言，并不是所有的transition都是同等重要的。例如在超级玛丽游戏中，左侧的普通关卡和右侧的boss关卡，它们的重要性就不一样：普通关卡很常见，而boss关卡需要通过很多普通关卡后才能进行。因此，我们必须让Agent珍惜数量更少的boss关卡，即让Agent充分利用boss关卡的这一经验。 实际训练中，我们怎么样让Agent知道哪个transition是重要的呢？ 可以使用TD error的L1范式，即|δt|来判断它的重要。|δt|越大，就说明transition就越重要，应该给这个transition更高的优先级。 解释一下：训练出的DQN对于数量较少的场景不熟悉，所以预测就会偏离TD Target，因此产生的TD error就比较大，即|δt|就大。正因为DQN不熟悉数量较少的场景，所以要让DQN给这些场景更高的优先级，让它更好的应对这样的场景。 2）核心想法（Core Idea） 优先经验回放（Prioritized Experience Replay）的核心在于：使用非均匀抽样代替均匀抽样。这里有两种抽样方式： ①抽样概率pt正比于|δt|+ε 其中，ε是一个很小的数，避免抽样概率pt等于0 ②对|δt|排序 |δt|越大越靠前，|δt|越小越靠后 两种方法的原理都是一样的：|δt|越大，就抽样概率pt就越大。 3）调整学习率（Scaling Learning Rate） TD算法采用SGD来更新神经网络参数w，α是学习率。 如果做均匀抽样，那么所有的transitions的学习率α都相同；如果做非均匀抽样，那么就要根据每个transition的重要性来调整学习率。 如果一个transition有一个较大的抽样概率pt，那么对这个transition的学习率就应该调小，可以采用如下圈出的方法来自适应调整学习率的因子：如果pt很大，那么学习率就会变小 4）更新TD Error（Update TD Error） 为了做优先经验回放（PER），我们要对每一个transition标记上TD error δt。δt决定了这条transition的重要性，决定了它被抽样的概率。 如果一个transition刚刚被收集到，我们并不知道它的δt，那么我们就直接把它的δt设置为最大值，让它有最高的优先级。 每次从buffer中抽取一个transition，都要对它的δt进行一次更新 五、总结（Summary） 相比于传统的经验回放（ER），优先经验回放（PER）在每个transition中标注了δt，并根据δt的不同来确定不同的抽样概率和学习率，从而达到非均匀抽样的效果。 直接附图： ","link":"https://tianxiawuhao.github.io/BOSW3NMIw/"},{"title":"王树森深度强化学习笔记8：Multi-Step TD Target","content":"一、Sarsa算法与Q-Learning算法（Sarsa versus Q-Learning） 1）Sarsa ①Sarsa算法目的是为了训练动作价值函数Qπ(s,a) ②TD Target记作yt，它是当前观测到的奖励rt和动作价值函数对于下一状态和下一动作的预测值乘以折扣因子γ之和 ③我们使用Sarsa算法来更新价值网络，即AC算法中的Critic网络。 2）Q-Learning ①Q-Learning是用于学习最优动作价值函数Q*(s,a) ②TD Target记作yt，是当前观测到的奖励rt与价值函数对于下一步状态下最优动作的预测值乘以折扣因子γ之和 ③我们用Q-Learning算法来更新DQN。 不管是Sarsa还是Q-Learning，它们都只使用一个奖励rt，即只使用一个transition中的奖励rt，下一次使用另个transition来更新动作价值Qπ，这种方式算出来的TD Target叫做One-Step TD Target。 二、多步TD Target（Multi-Step TD Target） 其实，我们可以使用多个奖励，如rt,rt+1，即使用多个transition中的奖励去更新动作价值Qπ，这种方法计算出来的TD Target叫做Multi-Step TD Target， 1）推导多步回报（Derive Mutil-Step Return） 我们已经知道的Ut与Ut+1的关系，然后再进行一次递归，将Ut+1用Ut+2表示，经过多步迭代，因而产生了Ut与Ut+m的关系，这个公式我们就称之为多步回报（Mutil-Step Return） 2）Sarsa算法中的多步TD Target（m-Step TD Target for Sarsa） ①多步TD target ②若m=1，即单步TD target，也就是经典的TD算法 需要注意的是，这种单步的TD Target的训练效果往往不如多步的TD Target的训练效果 3）Q-Learning算法中的多步TD Target（m-Step TD Target for Q-Learning） ①多步TD target ②若m=1，即单步TD target，也就是经典的TD算法 需要注意的是，这种单步的TD Target的训练效果往往不如多步的TD Target的训练效果 三、单步TD Target与多步TD Target对比（Comparison of One-step TD target and Multi-step TD target） Multi-Step TD target的表现更好，更接近真实数据，偏差更小，结果也更稳定；而单步是多步的特例。 ","link":"https://tianxiawuhao.github.io/xWqlayf1I/"},{"title":"王树森深度强化学习笔记7：Q-Learning算法","content":"一、Sarsa算法与Q-Learning算法对比 首先来对比一下Sarsa和Q-Learning 1）Sarsa算法 ①Sarsa算法目的是为了训练动作价值函数Qπ(s,a) ②TD Target记作yt，它是当前观测到的奖励rt和动作价值函数对于下一状态和下一动作的预测值乘以折扣因子γ之和 ③我们使用Sarsa算法来更新价值网络，即AC算法中的Critic网络。 2）Q-Learning算法 ①Q-Learning是用于学习最优动作价值函数Q*(s,a) ②TD Target记作yt，是当前观测到的奖励rt与价值函数对于下一步状态下最优动作的预测值乘以折扣因子γ之和 ③我们用Q-Learning算法来更新DQN 二、推导TD Target（DeriveTD Target） 我们已经知道对于所有的π，已经有了如下的等式： 如果这里的π是最优策略π*，那么就会有如下的等式: 我们往往采用Q去替换Q_π，因此有了如下的等式： 我们推导一下这个式子，由于Q*(St+1,At+1)是对下一步状态下最优动作的动作价值函数，因此，这里的At+1必然要选择使Q*(St+1,At+1)最大化的动作a*，因此有了如下的结论： 代入上面的等式，就可以得到如下等式： 然而，直接求式子中的期望十分困难，因此这里采用蒙特卡洛近似： 将Rt近似为观测到的rt，用观测到的下一状态st+1去近似St+1，因此蒙特卡洛近似后的式子如下： 至此，我们得到了TD Target yt 三、表格形式的Q-Learning（Q-Learning：Tabular Version） 具体步骤如下 ①观测到一个四元组(st,at,rt,st+1)，这里称之为一个transition ②我们在上一节已经推导出TD Target。实际上，这里的Q是一张表格，我们要找到对应的状态st+1并找到最大的动作价值函数Q(st+1,a*),用于计算TD Target yt。 ③接下来，我们计算TD error δt ④最后，我们利用如下公式来更新表格中的Q*(st,at) 四、神经网络形式的Q-Learning（Q-Learning：DQN Version） 我们采用DQN Q(s,a;w)来近似最优动作价值函数Q*(s,a) DQN采用如下公式来控制Agent，而我们就是帮助DQN来学习到更好的神经网络参数w 具体步骤如下： ① 观测到一个四元组(st,at,rt,st+1)，这里称之为一个transition ②TD Target yt，与上一小节类似，这里只是将表格中的Q(st+1,a)换成了Q(st+1,a*;w) ③TD error δt类似 ④最后，我们采用梯度下降来更新神经网络参数w 五、总结（Summary） 这里要注意Q-Learning与Sarsa的区别：Sarsa算法是学习动作价值函数Qπ；而Q-Learning是学习最优动作价值函数Q*。但其目的都是让Agent获得到更高的分数。 最后附图： ","link":"https://tianxiawuhao.github.io/Qnqlnnzkz/"},{"title":"王树森深度强化学习笔记6：SARSA算法","content":"一、推导TD Target（Derive TD Target） 首先回顾一下折扣回报（Discounted Return），其中γ是折扣率 当我们提取出一个γ，会发现有如下等式，且括号内部的是下一步的折扣回报（Ut+1） 由此，我们得到了一个等式，来反映相邻两个回报之间的关系 我们通常认为，奖励Rt取决于当前的状态st，当前的动作at以及下一状态st+1 根据定义，我们可以得到Qπ(st,at)是回报Ut的条件期望，这里利用上述公式代换掉Ut，那么就可以得到如下式子： 也许会有问题，博主在看的时候也有这个问题。对Ut+1求期望不已经是Qπ(St+1,At+1)了么，但实际上是这样的：Qπ(S_t+1,A_t+1) 是期望，期望是对 s{t+2}, a{t+2}, s{t+3}, a{t+3}... 这些变量求的，所以 Qπ(S_t+1,A_t+1) 跟这些变量无关。Qπ(S_t+1,A_t+1) 还依赖于变量St+1,At+1。E[Qπ(S_t+1,A_t+1)] 进一步消掉 St+1,At+1 这两个变量。 因此我们可以得到这样的等式： 近似方法如下： 因此，TD learning就是让Qπ(st,at)不断接近我们的TD target yt 二、表格形式的Sarsa（Sarsa：Tabular Version） 如果我们想学习动作价值函数Qπ(s,a)，我们可以使用一个如果输入的状态和动作是有限的，那么我们就可以画一个表格：一行对应一个状态s_i，一列对应一个动作a_j，那么表中的每个元素则对应着在该状态和该动作下的动作价值Qπ(s_i,a_j)。我们要做的就是用Sarsa算法去更新表格，每次更新一个元素。 当我们观测到一个四元组（st,at,rt,st+1），这样的一个四元组被称为transition。然后采用策略函数π去采样一个at+1，接着计算TD target yt。这里假设st+1为s2，at+1经过采样为a3，那么我们经过查表获取到了对应的Qπ(st+1,at+1)。 同时我们也能计算出TD error δt 然后利用δt更新Qπ(st,at)，其中α是学习率 三、神经网络形式的Sarsa（Sarsa:Neural Network Version） 我们可以采用价值网络来近似Qπ(s,a)，记为q(s,a;w)。注意，动作价值函数Qπ和价值网络q都与策略π有关，策略π的好坏会影响这两个函数。 神经网络q(s,a;w)被称为价值网络，它的输入是一个状态s，输出是状态s下对应动作的价值。如果有n个动作，那么价值网络q就会输出一个n维向量，向量元素对应在状态s下，各动作a的价值。 TD target，记作yt，是实际观测的奖励rt与折扣价值网络的预测值γ·q(st+1,at+1;w)之和 TD error，记作δt，是价值网络对当前状态和动作的估计q(st,at;w)与TD target之差，我们希望它越小越好 Loss，记作L，以1/2倍δt的L2范式作为Loss，我们希望通过更新价值网络的参数w来减小Loss Gradient，Loss对w求梯度，利用梯度下降来获取到Loss的局部最小值 Gradient descent，梯度下降算法，用于更新价值网络参数w 四、总结（Summary） 我们采用Sarsa算法主要是用于学习动作价值函数Qπ(s,a) 这里有两种形式： ①表格形式（直接学习Qπ） 这种形式对于有限个状态和动作且数量不大。通过一张表格记录每种状态和动作对应的Qπ(s,a)的值，通过采用TD算法更新表中每个Qπ(s,a)的值。 ②神经网络形式（采用函数近似） 这种形式并不是直接获取Qπ(s,a)，而是通过价值网络q(s,a;w)来近似动作价值函数Qπ(s,a)。它实际上TD算法来更新价值网络参数w，使价值网络q(s,a;w)更好。应用有学习笔记4的AC算法中的Critic网络。 直接附图： ","link":"https://tianxiawuhao.github.io/02-wFwlBm/"},{"title":"王树森深度强化学习笔记5：蒙特卡洛算法 Monte Carlo Algorithms","content":"Actor-Critic方法，是策略学习和价值学习结合的一种方法。 Actor是策略网络，用来控制Agent运动，可以看做“运动员”。 Critic是价值网络，用来给动作打分，可以看做“裁判”。 一、价值网络和策略网络（Value Network and Policy Network） 首先回顾状态价值函数Vπ(s)，它是动作状态函数Qπ(s,a)的数学期望，如果动作是离散的，那么它是由策略函数π(a|s)作为权重系数，和动作价值函数Qπ(s,a)[可以评价动作的好坏程度]相乘并对a做求和得到。如果动作是连续的，那么就要对a求积分。 **①Policy network（Actor）😗*控制Agent运动，即在某一状态s做出动作a 我们利用策略网络π(a|s;θ)来近似策略函数π(a|s)，θ是一个可训练的神经网络参数。 **②Value network（Critic）😗*不控制Agent运动，仅给Agent打分 我们利用价值网络q(s,a;w)来近似价值函数Qπ(s,a)，w是一个可训练的神经网络参数 1）策略网络（Policy Network）（Actor） 以超级玛丽为例： 该网络有一个输入：状态s 将超级玛丽的一帧画面传输到策略网络中，即输入一个状态s。 价值网络中包含：一个卷积层，将一帧画面变成特征向量；全连接层，将特征向量转变为相对于动作空间（Action Space）中元素个数对应的向量；再采用Softmax激活函数将其转换为求和为1的值，即一策略的概率密度。 2）价值网络（Value Network）（Critic） 以超级玛丽为例： 该网络有两个输入：状态s和动作a 将超级玛丽的一帧画面传输到价值网络中，即输入一个状态s；以及策略网络（Actor）做出的一个动作a传入到价值网络中，即输入一个动作a。对于离散动作而言，这里可以采用独热编码（one-hot encoding），具体可以参考学习笔记 | 独热编码(One-Hot Encoding)，这里不再对独热编码进行过多介绍。 对于两个不同的输入，处理方法如下： 针对于状态s，采用一个卷积层，从输入中提取特征，获得一个状态特征向量 针对于动作a，采用一个全连接层，从输入中提取特征，获得一个动作特征向量 然后将两个特征向量进行拼接，得到一个更高维的特征向量，再通过一个全连接层输出一个实数q(s,a;w)，即Critic对于Agent处于状态s下做出动作a的评价。 值得一提的是，价值网络和策略网络中的参数是可以共享的；当然，其中的参数也可以各自独立。 同时训练价值网络和策略网络就是Actor-Critic Method（AC Method） 二、训练神经网络（Train the Neural Network） 因此，我们可以用策略网络和价值网络来近似状态价值函数。 针对于两个网络的参数θ和w，我们的更新方法是不一样的： 对于θ，它是策略网络的参数。我们更新参数θ是为了让状态价值函数V(s;θ,w)的值增加，V(s;θ,w)是对策略π和状态s的评价。如果固定状态s，那么V(s;θ,w)越大，意味着策略π越好；同理，如果固定策略π，V(s;θ,w)越大，则状态s越好。 对于w，它是价值网络的参数。我们更新参数w是为了让价值网络对于动作的打分更精准，从而更好地估计未来奖励的总和。它相当于是裁判，一开始并没有打分能力，因此在初始阶段，Critic的打分都是瞎猜的，它会通过环境给的奖励Reward来更新w，最后Critic的打分会越来越精准。 利用如下五个步骤来对两个神经网络做一次更新： 1） 使用时序差分算法更新价值网络q（Update Value Network q Using TD） 解释一下，流程大概是这样的： 当我们使用TD算法去更新价值网络q时，要首先获取到Actor给出的t时刻和t+1时刻的状态st、st+1以及动作at、a~t+1，此时Critic网络中的参数为wt。因此我们就能得到我们的TD target：yt，有了TD target和t时刻的观测值，即可写出Loss L(w)，且以L2范式形式表示，然后利用梯度下降（Gradient Descent）去更新Critic网络中的参数w，记为wt+1。需要注意的是，Agent并不会做出a~t+1这个动作，仅仅是用于Critic用于TD算法的参数更新 直接附图： 2）使用策略梯度更新策略网络π（Update Policy Network π Using Policy Gradient） 首先回顾一下策略梯度： 策略梯度是对自定义函数g(A,θ)关于A求期望，由于一个g(A,θ)是对整体的g(A,θ)的一个无偏估计，因此这里用了蒙特卡洛近似。 解释一下，流程大概是这样的： 根据策略π对动作a进行一次随机抽样，然后针对该动作a进行一次随机梯度上升（Stochastic Gradient Ascent SGA），用于更新θ，记为θt+1。 AC方法的整体流程和对应关系： 3）算法总结（Summary of Algorithm） 直接上图，非常清晰： 值得注意的是，在论文和教科书中，最后一步往往不适用qt，而采用的是TD error δt。使用δt叫做基线策略梯度（Policy Gradient with Baseline），其效果更好。好的baseline可以降低方差，使算法收敛更快。 Baseline：任何接近qt的数都可以是baseline，但它不能是at的函数，即不与at存在映射关系。 三、总结（Summary） ","link":"https://tianxiawuhao.github.io/syanmQ0CA/"},{"title":"王树森深度强化学习笔记4：Actor-Critic Method（AC Method）","content":"Actor-Critic方法，是策略学习和价值学习结合的一种方法。 Actor是策略网络，用来控制Agent运动，可以看做“运动员”。 Critic是价值网络，用来给动作打分，可以看做“裁判”。 一、价值网络和策略网络（Value Network and Policy Network） 首先回顾状态价值函数Vπ(s)，它是动作状态函数Qπ(s,a)的数学期望，如果动作是离散的，那么它是由策略函数π(a|s)作为权重系数，和动作价值函数Qπ(s,a)[可以评价动作的好坏程度]相乘并对a做求和得到。如果动作是连续的，那么就要对a求积分。 **①Policy network（Actor）😗*控制Agent运动，即在某一状态s做出动作a 我们利用策略网络π(a|s;θ)来近似策略函数π(a|s)，θ是一个可训练的神经网络参数。 **②Value network（Critic）😗*不控制Agent运动，仅给Agent打分 我们利用价值网络q(s,a;w)来近似价值函数Qπ(s,a)，w是一个可训练的神经网络参数 1）策略网络（Policy Network）（Actor） 以超级玛丽为例： 该网络有一个输入：状态s 将超级玛丽的一帧画面传输到策略网络中，即输入一个状态s。 价值网络中包含：一个卷积层，将一帧画面变成特征向量；全连接层，将特征向量转变为相对于动作空间（Action Space）中元素个数对应的向量；再采用Softmax激活函数将其转换为求和为1的值，即一策略的概率密度。 2）价值网络（Value Network）（Critic） 以超级玛丽为例： 该网络有两个输入：状态s和动作a 将超级玛丽的一帧画面传输到价值网络中，即输入一个状态s；以及策略网络（Actor）做出的一个动作a传入到价值网络中，即输入一个动作a。对于离散动作而言，这里可以采用独热编码（one-hot encoding），具体可以参考学习笔记 | 独热编码(One-Hot Encoding)，这里不再对独热编码进行过多介绍。 对于两个不同的输入，处理方法如下： 针对于状态s，采用一个卷积层，从输入中提取特征，获得一个状态特征向量 针对于动作a，采用一个全连接层，从输入中提取特征，获得一个动作特征向量 然后将两个特征向量进行拼接，得到一个更高维的特征向量，再通过一个全连接层输出一个实数q(s,a;w)，即Critic对于Agent处于状态s下做出动作a的评价。 值得一提的是，价值网络和策略网络中的参数是可以共享的；当然，其中的参数也可以各自独立。 同时训练价值网络和策略网络就是Actor-Critic Method（AC Method） 二、训练神经网络（Train the Neural Network） 因此，我们可以用策略网络和价值网络来近似状态价值函数。 针对于两个网络的参数θ和w，我们的更新方法是不一样的： 对于θ，它是策略网络的参数。我们更新参数θ是为了让状态价值函数V(s;θ,w)的值增加，V(s;θ,w)是对策略π和状态s的评价。如果固定状态s，那么V(s;θ,w)越大，意味着策略π越好；同理，如果固定策略π，V(s;θ,w)越大，则状态s越好。 对于w，它是价值网络的参数。我们更新参数w是为了让价值网络对于动作的打分更精准，从而更好地估计未来奖励的总和。它相当于是裁判，一开始并没有打分能力，因此在初始阶段，Critic的打分都是瞎猜的，它会通过环境给的奖励Reward来更新w，最后Critic的打分会越来越精准。 利用如下五个步骤来对两个神经网络做一次更新： 1） 使用时序差分算法更新价值网络q（Update Value Network q Using TD） 解释一下，流程大概是这样的： 当我们使用TD算法去更新价值网络q时，要首先获取到Actor给出的t时刻和t+1时刻的状态st、st+1以及动作at、a~t+1，此时Critic网络中的参数为wt。因此我们就能得到我们的TD target：yt，有了TD target和t时刻的观测值，即可写出Loss L(w)，且以L2范式形式表示，然后利用梯度下降（Gradient Descent）去更新Critic网络中的参数w，记为wt+1。需要注意的是，Agent并不会做出a~t+1这个动作，仅仅是用于Critic用于TD算法的参数更新 直接附图： 2）使用策略梯度更新策略网络π（Update Policy Network π Using Policy Gradient） 首先回顾一下策略梯度： 策略梯度是对自定义函数g(A,θ)关于A求期望，由于一个g(A,θ)是对整体的g(A,θ)的一个无偏估计，因此这里用了蒙特卡洛近似。 解释一下，流程大概是这样的： 根据策略π对动作a进行一次随机抽样，然后针对该动作a进行一次随机梯度上升（Stochastic Gradient Ascent SGA），用于更新θ，记为θt+1。 AC方法的整体流程和对应关系： 3）算法总结（Summary of Algorithm） 直接上图，非常清晰： 值得注意的是，在论文和教科书中，最后一步往往不适用qt，而采用的是TD error δt。使用δt叫做基线策略梯度（Policy Gradient with Baseline），其效果更好。好的baseline可以降低方差，使算法收敛更快。 Baseline：任何接近qt的数都可以是baseline，但它不能是at的函数，即不与at存在映射关系。 三、总结（Summary） ","link":"https://tianxiawuhao.github.io/UWk0iL6Ex/"},{"title":"王树森深度强化学习笔记3：策略学习 Policy-Based Learning","content":"一、策略函数近似（Policy Function Approximation） 1）策略函数（Policy Function） 策略函数（Policy Function）π(a|s) 是一个概率密度函数，其输入是某个状态s，输出是在该状态下可能产生的动作a的概率值。假设在状态st下，Agent可能做出的动作at可能有n个，那么π(at|st)即输出一个n维向量，其元素对应每个可能做出动作at的概率值。有了这n个概率值，Agent就会在这个向量中做一次随机抽样（Random Sampling），并做出随机抽样所得的动作ai 2）策略网络（Policy Network） 策略网络（Policy Network）π(a|s;θ)则是使用了神经网络（Neural Network NN）去近似策略函数π(a|s)，其中θ是神经网络中的一个可训练参数 策略网络（Policy Network）π(a|s;θ)有着如下性质： 由于策略网络对于在动作空间A中所有可能取到的a所对应的概率之和必须等于1，因此在策略网络的最后一层必须加上一个Sotmax这个Activation Function。 二、状态价值函数近似（State-Value Function Approximation） 1）状态价值函数（Action-Value Function） 首先回顾上一章的三类函数： 这里注意：式3表示离散动作的期望求法；若为连续动作，则对动作空间中的所有a进行积分。 类比State-Value Function和Approximation State-Value Function，区别在于：将策略π(a|st)换成了策略网络π(a|st;θ) V可以评价状态s和策略网络π(a|st;θ)的好坏。若给定状态s，策略网络越好，那么V的值越大。因此，我们采用V关于θ的随机梯度上升的方法更新参数θ，则有以下定义：J(θ) 这类方法即策略梯度上升的方法，这里我们定义策略梯度（Policy Gradient）是近似状态价值函数V(s;θ)关于θ的梯度。 2）策略梯度（Policy Gradient） 这里放上策略梯度的推导： 这里假设Qπ不依赖于θ，因此能够相当于一个常数提出来。但在实际中，由于Qπ依赖于π，而π是由神经网络参数θ所决定的，因此往往Qπ不能被提出。 因此，策略梯度的一种形式可以被写成如下形式： 这里，如果动作a是离散的，那么只需要对a进行连加就可以将策略梯度算出来。 然而实际应用中往往不会用这个公式来计算策略梯度，而是使用蒙特卡洛近似的方法来近似。 这里对蒙特卡洛近似推导： 这里，有必要解释一下，这里利用了log函数的求导性质，有微积分基础的应该没什么问题，如果仅有高中水平的朋友可以自己上手求一下导，正着看或许有点问题，可以反过来推导，如上图紫色字体。 至此，我们证明了这两项是相等的： 这里我们就可以使用蒙特卡洛近似，推导出如下等式： 上面提到了这个推导不严谨，原因是因为Qπ与策略π有关，因而与神经网络参数θ有关，但事实上，最后的Qπ对θ也求偏导，最后也能得到这个式子。 至此，我们得到了策略梯度的两种等价形式： 对于离散的动作，我们可以采用第一种形式： 由于第一种形式的使用对象的局限性，因而第二种形式比第一种形式更为常用，这里举了连续动作的一种策略梯度的例子： 显然，这里对g(A,θ)关于A求期望就是策略梯度；且a是根据策略π中随机抽样得到的，故g(a,θ)是策略梯度的一个无偏估计。因此，我们就可以利用g(a^,θ)去近似策略梯度，即蒙特卡洛近似（Monte Carlo Approximation）。具体推导可以参考蒙特卡洛近似。 三、使用策略梯度更新策略网络 那么我们如何估计动作价值函数Qπ(st,at)？ 给出了这两种方法： REINFORCE：必须获取到一个完整的trajectory才能进行一次模型参数的更新，也就是蒙特卡罗方法（MC Method）来估计Q Actor-Critic（AC）：使用另一个神经网络来近似Qπ 四、总结（Summary） 直接附图 引出下文：将价值学习和策略学习结合起来就能得到非常重要的Actor-Critic方法（AC Method） ","link":"https://tianxiawuhao.github.io/0gyURSJ-J/"},{"title":"王树森深度强化学习笔记2：价值学习 Value-Based Learning","content":"一、深度Q网络（Deep Q-Network DQN） 1）近似Q函数（Approximate the Q* Function） Q*(s,a)可以被看成一个先知，它可以告诉Agent如何选择一个最好的动作a，但Q*(s,a)并不知道，因此这里需要近似Q*（s,a） DQN是一种价值学习的方法，利用一个神经网络（Neural Network NN）去近似一个Q*(s,a)，记为Q(s,a;w) 对于不同问题，DQN也许不一样，以超级玛丽为例： DQN的训练Agent的一个trajectory如下图： 2）时序差分方法（Temporal Difference Learning TD-Learning） ### 重要 重要 非常重要 ### 以我自己的想法复述一遍王树森老师的例子：如果我想要从纽约到亚特兰大，且模型*Q(w)*预测了消耗的时间是1000分钟，这个模型一开始是随机的。 那么如何更新模型，使模型的预测值变得越来越准确？ ①首先做出一个预测：q=Q(w) → q=1000 ②完成这段路程得到真实值y：y=860 这时，产生了一个偏差（q-y） 我们记损失Loss为：L=1/2*(q-y)²，即L2范式，前面的1/2可以理解为方便计算梯度，线性变换并不会影响数据整体的相对值，例如，1相对于2是1/2倍的关系，0.5相对于1是1/2倍的关系。（若有问题，欢迎大佬指点） 那么，为了是预测值尽可能的逼近真实值，我们必须需要让Loss取到最小值。因此这里就引入了L对w的梯度Gradient，也可以认为是求导。使用偏微分的链式法则，我们可以推导出L对w梯度的表达式，如下图所示： 这样，我们在进行参数更新时，采用梯度下降算法（Gradient Descent）即可使参数w不断接近真实值对应的参数w*。 而现在，若到达不了目的地，不获取到真实的全路程的真实值，就可以利用TD算法来更新模型参数。 假设我们从纽约想亚特兰大出发，途中经过华盛顿，而在华盛顿停住不走，那么此时的预测就不再能使用整条路的真实值来更新预测值了。 因此，这里采用TD算法 我们已知模型做出一个预测：q=Q(w) → q=1000 而从我们从纽约经过华盛顿停住不走，那么便有一个从纽约到华盛顿的真实值：y1=300 而从华盛顿到亚特兰大有一个模型预测值：q1=Q(w1)→q=600 那么此时我们定义TD target为 y=y1+q1=900,这里的TD target比原本的模型预测值1000更加可靠。 我们记损失Loss为：L=1/2*(q-y)²，定义TD error为δ=q-y=1000-900；此处也可以理解成模型对纽约到华盛顿的预测值为400，而真实值是300，δ=400-300=100 类似地，我们可以得到Loss Function关于w的一个函数，对w求梯度，得到Loss的最小值对应的w用于更新参数（采用梯度下降方法Gradient Descent） 我们必须使TD error尽可能地小，当TD error=0时，那么我们就获得了最优的模型 二、TD Learning+DQN 如何将TD算法用于学习DQN 对于上述例子，我们有了这样的方程 类似地，在深度强化学习中，有如下方程，即是用TD算法学习DQN的方程： 其中，方程左边是DQN在t时刻对状态和动作（st，at）进行的一次估计；等式右边是在t时刻真实观测到的奖励，以及DQN在t+1时刻对于状态和动作（st+1,at+1）进行的一次估计。 推导如下： 因此，将TD算法与DQN结合： 等式左边可以认为是模型对于时刻t关于参数w的预测值（Prediction）；右边则是真实值+模型对于时刻t+1关于参数w的预测值，和式记为TD target。 个人理解，这是一个贝尔曼方程，具体可以参照忆臻：马尔科夫决策过程之Bellman Equation（贝尔曼方程） 三、总结（Summary） 直接附图： TD算法用于学习DQN的一次迭代： 多次进行迭代，最后就可以将TD error逐渐减小；换句话说，模型预测值越来越接近TD target。 ","link":"https://tianxiawuhao.github.io/IGO_-1Id1/"},{"title":"王树森深度强化学习笔记1：基本概念","content":"一、数学知识 1）随机变量（Random Variable） 统计学中往往用字母的大小写来区分随机变量和观测值，例如X表示随机变量，x表示观测值 2）概率密度函数(Probability Density Function) 概率密度函数(Probability Density Function)有这样的性质： 3）期望（Expectation） 简单来说，对某一随机变量求期望就是在求它的平均。有微积分基础的可以理解：对于连续的随机变量可以采用求某一段积分来获得期望；而对于离散的随机变量只需求和公式即可。 期望（Expectation）有如下性质： 4）随机抽样（Random Sampling） 举个例子，对于某一随机变量X，可能X产生的值有['R', 'G', 'B']，那么随机抽样的过程就是在X可能产生的值['R', 'G', 'B']抽取的过程 二、专业术语（Terminology） 1）状态和动作（State and Action） 状态（State s） Agent（智能体）在某一时刻t所处的状态，即为State s，常常记为st 动作（Action a） Agent（智能体）在某一时刻t进入状态，做出相应的动作，即为Action a ，常常即为at 2）策略（Policy π） 在数学上，策略是一个概率密度函数(Probability Density Function)，在某一状态时，策略控制Agent做出动作，这里做出的动作是随机抽样得到的。 3）奖励（Reward） Agent在某一状态中做出一个动作，就会获得一个奖励 4）状态转移（State Transition） 一个状态转移到另一个状态的过程叫做状态转移 5）智能体环境交互过程 6）强化学习中的两种随机性 策略对动作进行随机抽样，状态转移函数对状态进行随机抽样 7）强化学习的训练过程（一个trajectory） 8）回报（Return） 定义为：未来奖励的总和（Cumulative Future Reward），记作Ut，由于每一步我们并不知道Ut的大小，Ut因此是随机变量 由于未来奖励的不确定性，因此强化学习中往往采用折扣回报（Discounted Return） 定义为：折扣性未来奖励的总和（Cumulative Discounted Future Reward） 回报的随机性（Randomness in Returns） 由于回报取决于奖励，而奖励又是由Agent所进入的状态和做出的动作进行打分而得到的，上文已经提到了在强化学习中状态和动作均具有随机性，因此回报（Return）也具有随机性。 9）动作价值函数（Action-Value Function）Q(s,a) 上面讲到了，Ut是一个随机变量，依赖于未来所有的动作A和状态S，因此为了无法评估当前形势。故引入动作价值函数，用对Ut求期望，用积分将随机性积掉，这样就能得到一个实数，用于评估形势。 期望的求取方法：将Ut当成未来所有状态A和S的一个函数，所以除了当前的动作at和状态st，其余所有的状态和动作都被积掉了，因此这里的对于策略π的动作价值函数Qπ（st，at）只取决于当前的动作at和状态st的观测值 最优动作价值函数（Optimal Action-Value Function）：能让动作价值函数*Qπ（st，at）*最大化的那个 Q*（st,at）,这里与π无关 对于动作状态价值函数的理解 10）状态价值函数（State-Value Function） Vπ（st）可以告诉我们当前的形势的好坏 这里的Vπ需要区分连续动作和离散动作： 对于状态价值函数的理解： 三、如何利用AI的强化学习控制智能体训练 1）价值学习（Value-Based Learning） 采用最优动作价值函数Q*(s,a) 2）策略学习（Policy-Based Learning） 采用策略π(a|s) 四、总结（summary） 直接附上王树森大神的Summary的图： 强化学习的一个总体过程： ","link":"https://tianxiawuhao.github.io/D4PTCIT2N/"},{"title":"Docker overlay2磁盘占用过高","content":"Docker overlay2磁盘占用过高 Docker overlay2磁盘占用过高主要有以下三个原因： - 1、容器日志文件过大，未作限制 - 2、docker未用容器、镜像、缓存等过多 - 3、docker默认路径存放不合理 一、磁盘容量查询 通过以下两条命令可以定位磁盘占用过高原因，可根据查询结果做相应处置。 1、df -h 容量查询 [root@hostname ~]# df -h Filesystem Size Used Avail Use% Mounted on /dev/vda1 50G 50G 35M 100% / overlay 50G 50G 35M 100% /data/docker/overlay2/770abd1b64f51f05a0f7c5c71d7349f54c9c152c8f58263ea9dda4960a722d61/merged overlay 50G 50G 35M 100% /data/docker/overlay2/19de54ababc18c2eb4c0d011434ac785480169a4e694c736dcbe938d497b5b3f/merged overlay 50G 50G 35M 100% /data/docker/overlay2/78f613526719dd1eabd3fe1f383ca55f065182e538f81cb719dd53d01055e849/merged overlay 50G 50G 35M 100% /data/docker/overlay2/a18694da44feba8f8379f422fdf67c00a1ac97efb281f8578770b7da4d55ad6a/merged 2、du -sh * 文件大小查询 #逐级排查大文件位置 [root@hostname ~]# du -sh /* 7.2G /var [root@hostname ~]# du -sh /var/* 37G /opt/open [root@hostname ~]# du -sh /var/lib/* 6.6G /var/lib/docker 二、磁盘占用过高处理方法 1、日志文件清理 docker容器运行时会产生大量日志，默认路径：/var/lib/docker/containers/id-json.log，随着容器运行该日志文件会占用大量磁盘空间。 1.1 日志清空 [root@test ~]# echo '' &gt; 容器id-json.log 注：不要直接删除日志文件，否则可能会影响新日志产生。 1.2 限制日志文件大小 以上方法只能暂时解决日志占用空间过大问题，如果想一劳永逸可修改docker配置文件，增加日志文件大小限制。 [root@test ~]# vi /etc/docker/daemon.json { &quot;log-driver&quot;:&quot;json-file&quot;, &quot;log-opts&quot;:{ &quot;max-size&quot; :&quot;50m&quot;, &quot;max-file&quot;:&quot;1&quot; } } 1.3 重启dokcer [root@test ~]# systemctl daemon-reload [root@test ~]# systemctl restart docker 注：已存在的容器不会生效，需要重建该容器才可以使配置生效 2、历史数据清理 Docker长时间运行，会产生大量无用的镜像、容器、网络等缓存信息，占用大量磁盘空间。此时我们可以使用prune命令清理。 2.1 释放所有未使用资源 [root@test ~]# docker system prune WARNING! This will remove: - all stopped containers //清理停止容器 - all networks not used by at least one container //清理未使用网络 - all dangling images //清理废弃镜像 - all dangling build cache //清理构建缓存 Are you sure you want to continue? [y/N] y Deleted Containers: 7a7fdd019a89eb68325d45ee6821c3f6f9a0b68b5631a21665ec484d59c8a44a Total reclaimed space: 202.4kB 注：数据清理前一定要检查是否还有需要使用的容器和镜像，避免误清理。 2.2 按需清理 [root@test ~]# docker image prune //清理废弃镜像 [root@test ~]# docker container prune //清理停止容器 [root@test ~]# docker network prune //清理未使用网络 [root@test ~]# docker system prune //清理构建缓存 3、修改Docker运行路径 注：此方法需停止容器，迁移docker运行目录，请在业务空闲期，备份好数据后操作。 Docker运行时使用的目录默认 “/var/lib/docker”，该目录占用根目录磁盘空间，根目录磁盘容量普遍较小。日常生成环境Linux常会挂载大容量数据盘， 可以将Docker默认运行目录修改到数据盘。 修改Docker运行路径有两种方法： - 1、迁移/var/lib/docker数据，创建软连接。此种方法不需要改配置文件。 - 2、迁移/var/lib/docker数据，修改docker配置文件。 3.1 迁移/var/lib/docker数据，创建软连接 #docker运行目录在/var/lib/docker下 [root@test ~]# df -h 文件系统 容量 已用 可用 已用% 挂载点 overlay 50G 5.8G 45G 12% /var/lib/docker/overlay2/c9de256d8ded736cf99dddda4c4f68e9c5e5162e1b7e70abdb0cd3fbfbbb7a19/merged overlay 50G 5.8G 45G 12% /var/lib/docker/overlay2/4e844541eb13d9c7af1c01f0b4681915e617c650c22a20e52f4b77e1e90dc369/merged overlay 50G 5.8G 45G 12% /var/lib/docker/overlay2/176c72f3913eea50a7bded4ad65363280680615c083140c725cfa63e4287cc51/merged #停止所有容器，避免迁移时影响业务 [root@test ~]# docker stop $(docker ps -a | awk '{ print $1}' | tail -n +2) #将docker默认运行目录剪切到opt目录 [root@test ~]# mv /var/lib/docker /opt/ #查询是否已剪切完成 [root@test ~]# ls /opt/ containerd docker hio rh #新运行目录创建软连接 [root@test ~]# ln -s /opt/docker /var/lib/docker #重启docker服务 [root@test ~]# systemctl restart docker #启动所有容器 [root@test ~]# docker start $(docker ps -a | awk '{ print $1}' | tail -n +2) #检查原镜像是否存在 [root@test ~]# docker images #检查容器是否运行正常 [root@test ~]# docker ps #检查docker默认路径是否切换成功，已切换到/opt/docker下 [root@test ~]# df -h 文件系统 容量 已用 可用 已用% 挂载点 overlay 50G 5.8G 45G 12% /opt/docker/overlay2/176c72f3913eea50a7bded4ad65363280680615c083140c725cfa63e4287cc51/merged overlay 50G 5.8G 45G 12% /opt/docker/overlay2/4e844541eb13d9c7af1c01f0b4681915e617c650c22a20e52f4b77e1e90dc369/merged overlay 50G 5.8G 45G 12% /opt/docker/overlay2/c9de256d8ded736cf99dddda4c4f68e9c5e5162e1b7e70abdb0cd3fbfbbb7a19/merged 3.2 迁移/var/lib/docker数据，修改docker配置 #docker运行目录在/var/lib/docker下 [root@test ~]# df -h 文件系统 容量 已用 可用 已用% 挂载点 overlay 50G 5.8G 45G 12% /var/lib/docker/overlay2/c9de256d8ded736cf99dddda4c4f68e9c5e5162e1b7e70abdb0cd3fbfbbb7a19/merged overlay 50G 5.8G 45G 12% /var/lib/docker/overlay2/4e844541eb13d9c7af1c01f0b4681915e617c650c22a20e52f4b77e1e90dc369/merged overlay 50G 5.8G 45G 12% /var/lib/docker/overlay2/176c72f3913eea50a7bded4ad65363280680615c083140c725cfa63e4287cc51/merged #停止所有容器，避免迁移时影响业务 [root@test ~]# docker stop $(docker ps -a | awk '{ print $1}' | tail -n +2) #将docker默认运行目录剪切到opt目录 [root@test ~]# mv /var/lib/docker /opt/ #查询是否已剪切完成 [root@test ~]# ls /opt/ containerd docker hio rh #修改docker配置文件，添加 &quot;data-root&quot;路径 [root@test ~]# vi /etc/docker/daemon.json { &quot;data-root&quot;: &quot;/opt/docker&quot; } #重启docker服务 [root@test ~]# systemctl daemon-reload [root@test ~]# systemctl restart docker #启动所有容器 [root@test ~]# docker start $(docker ps -a | awk '{ print $1}' | tail -n +2) #检查原镜像是否存在 [root@test ~]# docker images #检查容器是否运行正常 [root@test ~]# docker ps #检查docker默认路径是否切换成功，已切换到/opt/docker下 [root@test ~]# df -h 文件系统 容量 已用 可用 已用% 挂载点 overlay 50G 5.8G 45G 12% /opt/docker/overlay2/176c72f3913eea50a7bded4ad65363280680615c083140c725cfa63e4287cc51/merged overlay 50G 5.8G 45G 12% /opt/docker/overlay2/4e844541eb13d9c7af1c01f0b4681915e617c650c22a20e52f4b77e1e90dc369/merged overlay 50G 5.8G 45G 12% /opt/docker/overlay2/c9de256d8ded736cf99dddda4c4f68e9c5e5162e1b7e70abdb0cd3fbfbbb7a19/merged ","link":"https://tianxiawuhao.github.io/qvR3ZWwD8/"},{"title":"CompletableFuture","content":"前言 CompletableFuture继承于java.util.concurrent.Future，它本身具备Future的所有特性，并且基于JDK1.8的流式编程以及Lambda表达式等实现一元操作符、异步性以及事件驱动编程模型，可以用来实现多线程的串行关系，并行关系，聚合关系。它的灵活性和更强大的功能是Future无法比拟的。 一、创建方式 1. 用默认线程池 CompletableFuture&lt;String&gt; future = new CompletableFuture&lt;&gt;(); 默认使用 ForkJoinPool.commonPool()，commonPool是一个会被很多任务 共享 的线程池，比如同一 JVM 上的所有 CompletableFuture、并行 Stream 都将共享 commonPool，commonPool 设计时的目标场景是运行 非阻塞的 CPU 密集型任务，为最大化利用 CPU，其线程数默认为 CPU 数量 - 1。 2. 用自定义线程池 ThreadPoolExecutor pool = new ThreadPoolExecutor(2, 4, 3, TimeUnit.SECONDS, new ArrayBlockingQueue&lt;Runnable&gt;(3), new ThreadPoolExecutor.DiscardOldestPolicy()); CompletableFuture.runAsync(() -&gt; System.out.println(&quot;Hello World!&quot;), pool); 二、使用示例 1. 构建异步任务 方法 有无返回值 描述 runAsync 无 进行数据处理，接收前一步骤传递的数据，无返回值。 supplyAsync 有 进行数据处理，接收前一步骤传递的数据，处理加工后返回。返回数据类型可以和前一步骤返回的数据类型不同。 （1）runAsync public static CompletableFuture&lt;Void&gt; runAsync(Runnable runnable) { return asyncRunStage(asyncPool, runnable); } 示例 public static void runAsync() { //使用默认线程池 CompletableFuture cf = CompletableFuture.runAsync(() -&gt; System.out.println(&quot;Hello World!&quot;)); assertFalse(cf.isDone()); //使用自定义线程池 CompletableFuture.runAsync(() -&gt; System.out.println(&quot;Hello World!&quot;), Executors.newSingleThreadExecutor()); } （2）supplyAsync public static &lt;U&gt; CompletableFuture&lt;U&gt; supplyAsync(Supplier&lt;U&gt; supplier) { return asyncSupplyStage(asyncPool, supplier); } 示例 public static void supplyAsync() throws ExecutionException, InterruptedException { CompletableFuture&lt;String&gt; f = CompletableFuture.supplyAsync(() -&gt; { try { //ForkJoinPool.commonPool-worker-1线程 System.out.println(Thread.currentThread().getName()); Thread.sleep(3000); } catch (InterruptedException e) { e.printStackTrace(); } return &quot;hello&quot;; }); //阻塞等待3秒 String result = f.get(); //main线程 System.out.println(Thread.currentThread().getName()); System.out.println(result); } 2. 单任务结果消费 方法 有无返回值 描述 thenApply 有 在前一个阶段上应用thenApply函数，将上一阶段完成的结果作为当前阶段的入参 thenAccept 无返回值 消费前一阶段的结果 thenRun 无返回值，并且无入参 当上一阶段完成后，执行本阶段的任务 （1）thenApply public &lt;U&gt; CompletableFuture&lt;U&gt; thenApplyAsync( Function&lt;? super T,? extends U&gt; fn) { return uniApplyStage(asyncPool, fn); } 示例 csharp复制代码public static void thenApply() throws ExecutionException, InterruptedException { CompletableFuture cf = CompletableFuture.completedFuture(&quot;message&quot;).thenApplyAsync(s -&gt; { System.out.println(s); return s.toUpperCase(); }).thenApply(s-&gt;{ System.out.println(s); return s + &quot;:body&quot;; }); System.out.println(cf.get()); } then意味着这个阶段的动作发生当前的阶段正常完成之后。本例中，当前节点完成，返回字符串message。 Apply意味着返回的阶段将会对结果前一阶段的结果应用一个函数。 函数的执行会被阻塞 （2）thenAccept public static &lt;U&gt; CompletableFuture&lt;U&gt; supplyAsync(Supplier&lt;U&gt; supplier) { return asyncSupplyStage(asyncPool, supplier); } 示例 public static void thenAccept() throws InterruptedException { CompletableFuture&lt;Void&gt; future = CompletableFuture.supplyAsync(() -&gt; { return &quot;message&quot;; }).thenAccept((consumer) -&gt; { System.out.println(consumer); }); } （3）thenRun public CompletableFuture&lt;Void&gt; thenRun(Runnable action) { return uniRunStage(null, action); } 示例 public static void thenRun() throws InterruptedException { CompletableFuture.supplyAsync(() -&gt; { //执行异步任务 System.out.println(&quot;执行任务&quot;); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } return &quot;success&quot;; }).thenRun(() -&gt; { // Computation Finished. System.out.println(&quot;上一阶段任务执行完成&quot;); }); Thread.sleep(2000); } 3. 合并结果消费 方法 有无返回值 描述 thenCombine 有 合并另外一个任务，两个任务都完成后，执行BiFunction，入参为两个任务结果，返回新结果 thenAcceptBoth 无 合并另外一个任务，两个任务都完成后，执行这个方法等待第一个阶段的完成(大写转换)， 它的结果传给一个指定的返回CompletableFuture函数，它的结果就是返回的CompletableFuture的结果，入参为两个任务结果，不返回新结果 runAfterBoth 无返回值无入参 合并另外一个任务，两个任务都完成后，执行Runnable，注意，这里的两个任务是同时执行 （1）thenCombine 如果CompletableFuture依赖两个前面阶段的结果， 它复合两个阶段的结果再返回一个结果，我们就可以使用thenCombine()函数。整个流水线是同步的。 public &lt;U,V&gt; CompletableFuture&lt;V&gt; thenCombine(CompletionStage&lt;? extends U&gt; other, BiFunction&lt;? super T,? super U,? extends V&gt; fn) { return biApplyStage(null, other, fn); } 示例 public static void thenCombine() { CompletableFuture&lt;String&gt; cfA = CompletableFuture.supplyAsync(() -&gt; { System.out.println(&quot;processing a...&quot;); return &quot;hello&quot;; }); CompletableFuture&lt;String&gt; cfB = CompletableFuture.supplyAsync(() -&gt; { System.out.println(&quot;processing b...&quot;); return &quot; world&quot;; }); CompletableFuture&lt;String&gt; cfC = CompletableFuture.supplyAsync(() -&gt; { System.out.println(&quot;processing c...&quot;); return &quot;, I'm CodingTao!&quot;; }); cfA.thenCombine(cfB, (resultA, resultB) -&gt; { System.out.println(resultA + resultB); // hello world return resultA + resultB; }).thenCombine(cfC, (resultAB, resultC) -&gt; { System.out.println(resultAB + resultC); // hello world, I'm CodingTao! return resultAB + resultC; }); } （2）thenAcceptBoth public &lt;U&gt; CompletableFuture&lt;Void&gt; thenAcceptBoth(CompletionStage&lt;? extends U&gt; other, BiConsumer&lt;? super T, ? super U&gt; action) { return biAcceptStage(null, other, action); } 示例 private static void thenAcceptBoth() { CompletableFuture&lt;String&gt; cfA = CompletableFuture.supplyAsync(() -&gt; &quot;resultA&quot;); CompletableFuture&lt;String&gt; cfB = CompletableFuture.supplyAsync(() -&gt; &quot;resultB&quot;); cfA.thenAcceptBoth(cfB, (resultA, resultB) -&gt; { //resultA,resultB System.out.println(resultA+&quot;,&quot;+resultB); }); } （3）runAfterBoth public CompletableFuture&lt;Void&gt; runAfterBoth(CompletionStage&lt;?&gt; other, Runnable action) { return biRunStage(null, other, action); } 示例 private static void runAfterBoth() { CompletableFuture&lt;String&gt; cfA = CompletableFuture.supplyAsync(() -&gt; { try { Thread.sleep(5000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(&quot;process a&quot;); return &quot;resultA&quot;; }); CompletableFuture&lt;String&gt; cfB = CompletableFuture.supplyAsync(() -&gt; { try { Thread.sleep(5000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(&quot;process b&quot;); return &quot;resultB&quot;; }); cfB.runAfterBoth(cfA, () -&gt; { //resultA,resultB System.out.println(&quot;任务A和任务B同时完成&quot;); }); try { Thread.sleep(6000); } catch (InterruptedException e) { e.printStackTrace(); } } 4. 任一结果消费 方法 有无返回值 描述 applyToEither 有 其中任一任务完成后，执行Function，结果转换，入参为已完成的任务结果。返回新结果，要求两个任务结果为同一类型 acceptEither 无 其中任一任务完成后，执行Consumer，消费结果，入参为已完成的任务结果。不返回新结果，要求两个任务结果为同一类型 runAfterEither 无返回值无入参 其中任一任务完成后，执行Runnable，消费结果，无入参。不返回新结果，不要求两个任务结果为同一类型 场景 假设查询商品a，有两种方式，A和B，但是A和B的执行速度不一样，希望哪个先返回就用那个的返回值。 （1）applyToEither public &lt;U&gt; CompletableFuture&lt;U&gt; applyToEither(CompletionStage&lt;? extends T&gt; other, Function&lt;? super T, U&gt; fn) { return orApplyStage(null, other, fn); } 示例 private static void applyToEither() throws ExecutionException, InterruptedException { CompletableFuture&lt;String&gt; futureA = CompletableFuture.supplyAsync(() -&gt; { try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } return &quot;通过方式A获取商品a&quot;; }); CompletableFuture&lt;String&gt; futureB = CompletableFuture.supplyAsync(() -&gt; { try { Thread.sleep(2000); } catch (InterruptedException e) { e.printStackTrace(); } return &quot;通过方式B获取商品a&quot;; }); CompletableFuture&lt;String&gt; futureC = futureA.applyToEither(futureB, product -&gt; &quot;结果:&quot; + product); //结果:通过方式A获取商品a System.out.println(futureC.get()); } （2）acceptEither （3）runAfterEither 5. 级联任务 方法 有无返回值 描述 thenCompose 有 当原任务完成后，以其结果为参数，返回一个新的任务（而不是新结果，类似flatMap） （1）thenCompose 这个方法等待第一个阶段的完成(大写转换)， 它的结果传给一个指定的返回CompletableFuture函数，它的结果就是返回的CompletableFuture的结果。 public &lt;U&gt; CompletableFuture&lt;U&gt; thenCompose( Function&lt;? super T, ? extends CompletionStage&lt;U&gt;&gt; fn) { return uniComposeStage(null, fn); } 示例 private static void thenCompose() { String original = &quot;Message&quot;; CompletableFuture cf = CompletableFuture.completedFuture(original).thenApply(s -&gt; delayedUpperCase(s)) .thenCompose(upper -&gt; CompletableFuture.completedFuture(original).thenApply(s -&gt; delayedLowerCase(s)) .thenApply(s -&gt; upper + s)); // MESSAGEmessage System.out.println(cf.join()); } 6. 单任务结果或异常消费 方法 有无返回值 描述 handle 有 任务完成后执行BiFunction，结果转换，入参为结果或者异常，返回新结果 whenComplete 无 任务完成后执行BiConsumer，结果消费，入参为结果或者异常，不返回新结果 exceptionally 无 任务异常，则执行Function，异常转换，入参为原任务的异常信息，若原任务无异常，则返回原任务结果，即不执行转换 异常流程 CompletableFuture.supplyAsync(() -&gt; &quot;resultA&quot;) .thenApply(resultA -&gt; resultA + &quot; resultB&quot;) .thenApply(resultB -&gt; resultB + &quot; resultC&quot;) .thenApply(resultC -&gt; resultC + &quot; resultD&quot;); 上面的代码中，任务 A、B、C、D 依次执行，如果任务 A 抛出异常（当然上面的代码不会抛出异常），那么后面的任务都得不到执行。如果任务 C 抛出异常，那么任务 D 得不到执行。 那么我们怎么处理异常呢？看下面的代码，我们在任务 A 中抛出异常，并对其进行处理： （1）handle 示例 private static void handle() { CompletableFuture&lt;String&gt; future = CompletableFuture.supplyAsync(() -&gt; &quot;resultA&quot;) .thenApply(resultA -&gt; resultA + &quot; resultB&quot;) // 任务 C 抛出异常 .thenApply(resultB -&gt; {throw new RuntimeException();}) // 处理任务 C 的返回值或异常 .handle(new BiFunction&lt;Object, Throwable, Object&gt;() { @Override public Object apply(Object re, Throwable throwable) { if (throwable != null) { return &quot;errorResultC&quot;; } return re; } }) .thenApply(resultC -&gt; { System.out.println(&quot;resultC:&quot; + resultC); return resultC + &quot; resultD&quot;; }); System.out.println(future.join()); } （2）whenComplete 示例 private static void whenComplete() throws ExecutionException, InterruptedException { // 创建异步执行任务: CompletableFuture&lt;Double&gt; cf = CompletableFuture.supplyAsync(()-&gt;{ System.out.println(Thread.currentThread()+&quot;job1 start,time-&gt;&quot;+System.currentTimeMillis()); try { Thread.sleep(2000); } catch (InterruptedException e) { } if(true){ throw new RuntimeException(&quot;test&quot;); }else{ System.out.println(Thread.currentThread()+&quot;job1 exit,time-&gt;&quot;+System.currentTimeMillis()); return 1.2; } }); //cf执行完成后会将执行结果和执行过程中抛出的异常传入回调方法 // 如果是正常执行，a=1.2，b则传入的异常为null //如果异常执行，a=null，b则传入异常信息 CompletableFuture&lt;Double&gt; cf2=cf.whenComplete((a,b)-&gt;{ System.out.println(Thread.currentThread()+&quot;job2 start,time-&gt;&quot;+System.currentTimeMillis()); try { Thread.sleep(2000); } catch (InterruptedException e) { } if(b!=null){ System.out.println(&quot;error stack trace-&gt;&quot;); b.printStackTrace(); }else{ System.out.println(&quot;run succ,result-&gt;&quot;+a); } System.out.println(Thread.currentThread()+&quot;job2 exit,time-&gt;&quot;+System.currentTimeMillis()); }); //等待子任务执行完成 System.out.println(&quot;main thread start wait,time-&gt;&quot;+System.currentTimeMillis()); //如果cf是正常执行的，cf2.get的结果就是cf执行的结果 //如果cf是执行异常，则cf2.get会抛出异常 System.out.println(&quot;run result-&gt;&quot;+cf2.get()); System.out.println(&quot;main thread exit,time-&gt;&quot;+System.currentTimeMillis()); } （3）exceptionally 7. 合并多个complete为一个 方法 描述 allOf 合并多个complete为一个，等待全部完成 anyOf 合并多个complete为一个，等待其中之一完成 （1）allOf 我们在处理业务时，有时会有多任务异步处理，同步返回结果的情况 采用多线程执异步行某种任务，比如在不同主机查询磁盘列表信息。 将执行结果搜集，分组分类，处理。 将处理以后的结果给予展示。 示例 // 创建异步执行任务: CompletableFuture&lt;Double&gt; cf = CompletableFuture.supplyAsync(()-&gt;{ System.out.println(Thread.currentThread()+&quot; start job1,time-&gt;&quot;+System.currentTimeMillis()); try { Thread.sleep(2000); } catch (InterruptedException e) { } System.out.println(Thread.currentThread()+&quot; exit job1,time-&gt;&quot;+System.currentTimeMillis()); return 1.2; }); CompletableFuture&lt;Double&gt; cf2 = CompletableFuture.supplyAsync(()-&gt;{ System.out.println(Thread.currentThread()+&quot; start job2,time-&gt;&quot;+System.currentTimeMillis()); try { Thread.sleep(1500); } catch (InterruptedException e) { } System.out.println(Thread.currentThread()+&quot; exit job2,time-&gt;&quot;+System.currentTimeMillis()); return 3.2; }); CompletableFuture&lt;Double&gt; cf3 = CompletableFuture.supplyAsync(()-&gt;{ System.out.println(Thread.currentThread()+&quot; start job3,time-&gt;&quot;+System.currentTimeMillis()); try { Thread.sleep(1300); } catch (InterruptedException e) { } // throw new RuntimeException(&quot;test&quot;); System.out.println(Thread.currentThread()+&quot; exit job3,time-&gt;&quot;+System.currentTimeMillis()); return 2.2; }); //allof等待所有任务执行完成才执行cf4，如果有一个任务异常终止，则cf4.get时会抛出异常，都是正常执行，cf4.get返回null //anyOf是只有一个任务执行完成，无论是正常执行或者执行异常，都会执行cf4，cf4.get的结果就是已执行完成的任务的执行结果 CompletableFuture cf4=CompletableFuture.allOf(cf,cf2,cf3).whenComplete((a,b)-&gt;{ if(b!=null){ System.out.println(&quot;error stack trace-&gt;&quot;); b.printStackTrace(); }else{ System.out.println(&quot;run succ,result-&gt;&quot;+a); } }); System.out.println(&quot;main thread start cf4.get(),time-&gt;&quot;+System.currentTimeMillis()); //等待子任务执行完成 System.out.println(&quot;cf4 run result-&gt;&quot;+cf4.get()); System.out.println(&quot;main thread exit,time-&gt;&quot;+System.currentTimeMillis()); 获取返回值方法 public &lt;T&gt; CompletableFuture&lt;List&lt;T&gt;&gt; allOf(List&lt;CompletableFuture&lt;T&gt;&gt; futuresList) { CompletableFuture&lt;Void&gt; allFuturesResult = CompletableFuture.allOf(futuresList.toArray(new CompletableFuture[futuresList.size()])); return allFuturesResult.thenApply(v -&gt; futuresList.stream(). map(future -&gt; future.join()). collect(Collectors.&lt;T&gt;toList()) ); } （2）anyOf CompletableFuture.anyOf()和其名字介绍的一样，当任何一个CompletableFuture完成的时候【相同的结果类型】，返回一个新的CompletableFuture。 示例 private static void anyOf() throws ExecutionException, InterruptedException { CompletableFuture&lt;String&gt; future1 = CompletableFuture.supplyAsync(() -&gt; { try { TimeUnit.SECONDS.sleep(2); } catch (InterruptedException e) { throw new IllegalStateException(e); } return &quot;Result of Future 1&quot;; }); CompletableFuture&lt;String&gt; future2 = CompletableFuture.supplyAsync(() -&gt; { try { TimeUnit.SECONDS.sleep(1); } catch (InterruptedException e) { throw new IllegalStateException(e); } return &quot;Result of Future 2&quot;; }); CompletableFuture&lt;String&gt; future3 = CompletableFuture.supplyAsync(() -&gt; { try { TimeUnit.SECONDS.sleep(3); } catch (InterruptedException e) { throw new IllegalStateException(e); } return &quot;Result of Future 3&quot;; }); CompletableFuture&lt;Object&gt; anyOfFuture = CompletableFuture.anyOf(future1, future2, future3); System.out.println(anyOfFuture.get()); // Result of Future 2 } 三、其他相关api 1. future接口 （1）isDone() 判断任务是否完成。三种完成情况：normally（正常执行完毕）、exceptionally（执行异常）、via cancellation（取消） （2）get() 阻塞获取结果或抛出受检测异常，需要显示进行try...catch处理。 （3）get(long timeout,TimeUnit unit) 超时阻塞获取结果 （4）cancel(boolean mayInterruptIfRunning) 取消任务，若一个任务未完成，则以CancellationException异常。其相关未完成的子任务也会以CompletionException结束 （5）isCancelled() 是否已取消，在任务正常执行完成前取消，才为true。否则为false。 2. CompletableFuture接口 （1）join ​ 阻塞获取结果或抛出非受检异常。 （2）getNow(T valueIfAbsent) ​ 若当前任务无结果，则返回valueIfAbsent，否则返回已完成任务的结果。 （3）complete(T value) ​ 设置任务结果，任务正常结束，之后的任务状态为已完成。 （4）completeExceptionally(Throwable ex) ​ 设置任务异常结果，任务异常结束，之后的任务状态为已完成。 （5）isCompletedExceptionally() ​ 判断任务是否异常结束。异常可能的原因有：取消、显示设置任务异常结果、任务动作执行异常等。 （6）getNumberOfDependents() ​ 返回依赖当前任务的任务数量，主要用于监控。 （7）orTimeout(long timeout,TimeUnit unit) jdk9 ​ 设置任务完成超时时间，若在指定时间内未正常完成，则任务会以异常(TimeoutException)结束。 （8）completeOnTimeout(T value,long timeout,TimeUnit unit) jdk9 ​ 设置任务完成超时时间，若在指定时间内未正常完成，则以给定的value为任务结果 四、实战 1. API网关做接口的聚合 //这两个参数从外部获得 Long userId = 10006L; String orderId = &quot;XXXXXXXXXXXXXXXXXXXXXX&quot;; //从用户服务获取用户信息 UserInfo userInfo = userService.getUserInfo(userId); //从用订单务获取订单信息 OrderInfo orderInfo = orderService.getOrderInfo(orderId); //返回两者的聚合DTO return new OrderDetailDTO(userInfo,orderInfo); ​ 下面三个外部接口的信息一定是不相关联的，也就是可以并行获取，三个接口的结果都获取完毕之后做一次数据聚合到DTO即可，也就是聚合的耗时大致是这三个接口中耗时最长的接口的响应时间 @Service public class OrderDetailService { /** * 建立一个线程池专门交给CompletableFuture使用 */ private final ThreadPoolExecutor executor = new ThreadPoolExecutor(10, 20, 0, TimeUnit.SECONDS, new ArrayBlockingQueue&lt;&gt;(100)); @Autowired private UserService userService; @Autowired private OrderService orderService; public OrderDetailDTO getOrderDetail(Long userId, String orderId) throws Exception { CompletableFuture&lt;UserInfo&gt; userInfoCompletableFuture = CompletableFuture.supplyAsync(() -&gt; userService.getUserInfo(userId), executor); CompletableFuture&lt;OrderInfo&gt; orderInfoCompletableFuture = CompletableFuture.supplyAsync(() -&gt; orderService.getOrderInfo(orderId), executor); CompletableFuture&lt;OrderDetailDTO&gt; result = userInfoCompletableFuture.thenCombineAsync(orderInfoCompletableFuture, OrderDetailDTO::new, executor); return result.get(); } } 五、区别 （1）whenComplete和handle区别 whenComplete 与 handle 方法就类似于 try..catch..finanlly 中 finally 代码块。无论是否发生异常，都将会执行的。这两个方法区别在于 handle 支持返回结果。 （2）thenApply与thenCompose的异同 ​ 对于thenApply，fn函数是一个对一个已完成的stage或者说CompletableFuture的的返回值进行计算、操作； ​ 对于thenCompose，fn函数是对另一个CompletableFuture进行计算、操作。 （3）有无Async的区别 没有Async的在CompleteableFuture调用它的线程定义的线程上运行，因此通常不知道在哪里执行该线程。如果结果已经可用，它可能会立即执行。 ​ 有Async的无论环境如何，都在环境定义的执行程序上运行。为此CompletableFuture通常ForkJoinPool.commonPool()。 ","link":"https://tianxiawuhao.github.io/iPMnLKVAT/"},{"title":"Go基础-类型断言 Type Assertion","content":"Type Assertion 类型断言(Type Assertion)是一个使用在接口值上的操作，用于检查接口类型变量所持有的值是否实现了期望的接口或者具体的类型。 功能 1.检查x是否为nil 2.检查x储存的值是否为某个类型 go中类型断言写法: value, ok := x.(T) 其中x为interface{}接口类型，T是要断言的类型；ok若为true则表示断言成功，为false则表示断言失败。 举个例子： 第一种： t := x.(T) 该表达式可以断言一个接口对象x里不是 nil，并且接口对象x存储的值的类型是 T，如果断言成功，就会返回值给 t，如果断言失败，就会触发 panic package main import &quot;fmt&quot; func main() { var x interface{} = 24 t1 := x.(int) fmt.Println(t1) fmt.Println(&quot;=====分隔线=====&quot;) t2 := x.(string) fmt.Println(t2) } 运行后输出如下，在执行第二次断言的时候失败了，并且触发了 panic 24 =分隔线= panic: interface conversion: interface {} is int, not string goroutine 1 [running]: main.main() D:/Development/GoWorkspace/SRC/main.go:19 +0x10d exit status 2 如果要断言的接口值是 nil，也会触发panic。 第二种： t, ok:= x.(T) 该表达式与第一种不同的是当断言失败时不会触发panic，而是将 ok 的值设为 false ，表示断言失败，此时t 为 T 的零值。 package main import &quot;fmt&quot; func main() { var i interface{} = 10 t1, ok := i.(int) fmt.Printf(&quot;%d-%t\\n&quot;, t1, ok) fmt.Println(&quot;=====分隔线1=====&quot;) t2, ok := i.(string) fmt.Printf(&quot;%s-%t\\n&quot;, t2, ok) fmt.Println(&quot;=====分隔线2=====&quot;) var k interface{} // nil t3, ok := k.(interface{}) fmt.Println(t3, &quot;-&quot;, ok) fmt.Println(&quot;=====分隔线3=====&quot;) k = 10 t4, ok := k.(interface{}) fmt.Printf(&quot;%d-%t\\n&quot;, t4, ok) t5, ok := k.(int) fmt.Printf(&quot;%d-%t\\n&quot;, t5, ok) } 运行后输出如下，可以发现在执行第二次断言的时候，虽然失败了，但并没有触发了 panic。 10-true =====分隔线1===== -false =====分隔线2===== &lt;nil&gt; - false =====分隔线3===== 10-true 10-true 需要注意的是，第二组并非没有输出任何值，而是t2值为&quot;&quot;，是空值没有任何长度所以无法看出其输出。 Type Switch 如果需要区分多种类型，可以使用Type Switch断言更加高效、便捷。 package main import &quot;fmt&quot; func findType(i interface{}) { switch x := i.(type) { case int: fmt.Println(x, &quot;is int&quot;) case string: fmt.Println(x, &quot;is string&quot;) case nil: fmt.Println(x, &quot;is nil&quot;) default: fmt.Println(x, &quot;not type matched&quot;) } } func main() { findType(10) // int findType(&quot;hello&quot;) // string var k interface{} // nil findType(k) findType(10.23) //float64 } 输出如下： 10 is int hello is string &lt;nil&gt; is nil 10.23 not type matched 注意： 类型断言，仅能对静态类型为空接口（interface{}）的对象进行断言，否则会抛出错误。 类型断言完成后，实际上会返回静态类型为你断言的类型的对象，而要清楚原来的静态类型为空接口类型（interface{}），这是 Go 的隐式转换。 ","link":"https://tianxiawuhao.github.io/z-YRElc0W/"},{"title":"GraalVM","content":"GraalVM + Native-Image + Springboot3（实战） 一、首先，我们肯定需要准备一个Springboot项目 1、新建一个项目，JDK使用graalvm-jdk17： Springboot版本我选择3.1.1，以一个web项目为例： 自动生成核心的pom.xml文件： &lt;!-- 重点就是这个插件 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; 2、编写一个简单的Controller： import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.bind.annotation.RestController; @RestController public class graalController { @GetMapping(&quot;/&quot;) public String demo(){ return &quot;Hello Springboot3 AOT&quot;; } } 3、常规运行访问该demo接口： 项目无问题，可用于测试！ 二、使用native-image命令编译Springboot项目（Windows） 核心命令： 1、先对原项目进行一次package打包：mvn clean package -DskipTests 2、运行aot提前处理命令：mvn spring-boot:process-aot 3、运行native打包：mvn -Pnative native:build 当然，也可以在Idea的界面上进行操作！ 1、但是如果在Windows环境下，直接运行 native:build 命令，会报如下的错误： Execution of C:\\installation\\graalvm-jdk17\\graalvm-community-openjdk-17.0.8\\bin\\native-image.cmd @target\\tmp\\native-image-16043596690894935382.args returned non-zero result 这个错误解决起来，还挺麻烦的！需要配置一些列的环境变量！ 需要修改3个环境变量：*Path、INCLUDE、lib* *我们需要知道自己的Visual Studio和Windows Kits包的安装位置，我的：* Visual Studio安装目录：C:\\Program Files\\Microsoft Visual Studio\\2022\\Community 其中MSVC的include目录位置：C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.35.32215\\include 其中MSVC的lib目录位置：C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.35.32215\\lib\\x64 Windows工具包的include目录位置：C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.22000.0 Windows工具包的lib目录位置：C:\\Program Files (x86)\\Windows Kits\\10\\Lib\\10.0.22000.0 配置第1个环境变量，在Path中添加： C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.33.31629\\bin\\Hostx64\\x64 配置第2个环境变量，新增INCLUDE环境变量： C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.35.32215\\include; C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.22000.0\\shared; C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.22000.0\\ucrt; C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.22000.0\\um; C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.22000.0\\winrt; 配置第3个环境变量，新增lib环境变量： C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.35.32215\\lib\\x64; C:\\Program Files (x86)\\Windows Kits\\10\\Lib\\10.0.22000.0\\um\\x64; C:\\Program Files (x86)\\Windows Kits\\10\\Lib\\10.0.22000.0\\ucrt\\x64; 2、上面的环境变量正确配置完成后，我们就可以执行最初的命令了： 为了保险起见，我们将在本地native工具中执行native编译操作： ------------------------------------------------------------------------------------------------------------------------ Recommendations: HEAP: Set max heap for improved and more predictable memory usage. CPU: Enable more CPU features with '-march=native' for improved performance. ------------------------------------------------------------------------------------------------------------------------ 8.1s (14.0% of total time) in 178 GCs | Peak RSS: 2.58GB | CPU load: 7.70 ------------------------------------------------------------------------------------------------------------------------ Produced artifacts: C:\\study\\minio-java-master\\graalvm\\target\\graalvm.exe (executable) ======================================================================================================================== Finished generating 'graalvm' in 57.1s. [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 59.968 s [INFO] Finished at: 2023-08-31T11:21:48+08:00 [INFO] ------------------------------------------------------------------------ 最终生成了一个可执行文件 graalvm.exe 文件！ 3、我们直接双击运行 graalvm.exe 文件，验证打包是否成功： 整个项目在0.1秒内完成了启动，这也就验证了，为什么说AOT打成可执行文件后，运行速度非常快！ ","link":"https://tianxiawuhao.github.io/xWe8SRVUm/"},{"title":"Golang协程调度器和GMP模型","content":" 先会过一个初级但是很重要的概念（在Java多线程中也不停地强调）： 并行：多个处理器同时处理多个任务，无论在宏观还是微观上，每一时候都是多个任务同时被执行； 并发：由单个处理器同时处理多个任务，在宏观上仿佛多个任务在被同时执行，其实微观上，同一时刻只能有一个任务被执行，只是时间被划分为很多个小的时间片，处理器不停地交替执行多个任务； 并行和并发，都是为了充分利用和提高CPU的使用率！ 一、Golang协程调度器的由来 1、单进程时代不需要调度器： 早期的操作系统每个程序就是一个进程，直到一个程序运行完，才能进行下一个进程，就是“单进程时代”；（性能极其低下） 单路的执行流程，计算机只能一个一个地处理任务； 万一某个任务进程阻塞了，将带来大量的CPU时间浪费； 那么能不能由多个进程（在宏观角度）一起来执行多个任务呢？—— 并发 2、多进程/多线程时代有了调度器需求： Linux操作系统底层基础知识扫盲 进程调度的分类： 非抢占式：除非主动让出，否则一直运行； 抢占式：由“进程调度器”强制安排； 进程调度的策略： 经典Unix(0) 调度策略：（看似公平，其实有可能浪费） CFS完全公平调度策略：（根据优先级，调整时间片比例） Linux中默认的调度策论策略（混合）： 对于“普通进程”：完全公平调度策略（CFS）—— 普通门诊 对于“实时进程”：急诊，但是急诊还分为：普通急诊 和 最急的急诊： 普通急诊：SCHED_RR： 同优先级的实时进程中，每个进程又是通过获得时间片来分时运行。 不同优先级的实时进程，高优先级的实时进程能够抢占低优先级的实时进程。 最急的急诊：SCHED_FIFO： 同优先级的实时进程，后进入的进程要等前一个进程释放了CPU，才能够运行。（没有时间片） 不同优先级的实时进程，高优先级的实时进程能够抢占低优先级的实时进程。 而对于Linux操作系统来说，线程的本质就是一个轻量级的进程，所以，线程调度也就是进程调度；而对于Java来说，一个用户态线程也就是对应了一个内核态线程 但新的问题就又出现了，进程拥有太多的资源，进程的创建、切换、销毁，都会占用很长的时间，CPU虽然利用起来了，但如果进程过多，CPU有很大的一部分都被用来进行进程调度和进程（线程）切换了。 3、协程来再次提高CPU利用率（减少进程切换）： 多进程、多线程已经提高了系统的并发能力，但是在当今互联网高并发场景下，为每个任务都创建一个线程是很不现实的： 会消耗大量的内存： 进程：进程虚拟内存（4GB = 1GB内核空间 + 3GB用户空间） 线程：每个线程启动前后，会消耗2MB左右的内存； 大量的进程/线程调度浪费CPU资源： 最开始的协程模型和线程模型是一样的，线程:协程 = 1:N，即一个用户态的线程/协程 对应一个内核态线程： 思考：既然一个协程(co-routine)可以绑定一个线程(thread)，那么能不能将多个协程(co-routine)同时绑定到一个线程(thread)上呢？ 于是就有了 线程:协程 = 1:N 模型： 优点：协程在用户态线程即完成切换，不会陷入到内核态，这种切换非常的轻量快速； 缺点：1个进程的所有协程都绑定在1个线程上，某些程序用不了硬件的多核加速能力，一旦某协程阻塞，造成线程阻塞，本进程的其他协程都无法执行了，降级为单线程模型； 思考：我们可否让一个用户程序的多个协程，通过协程调度器，同时绑定到多个内核线程上呢？ 于是，就有了我们最终的 线程:协程 = M:N 模型： 此 M:N 模型克服了以上 1:1，1:N 两种模型的缺点，但是实现也变得更加的复杂，压力给到“协程调度器”！ 4、Golang中的协程Goroutine： Go中，协程被称为goroutine，它非常轻量，一个goroutine只占大概4KB，并且这4KB就足够goroutine运行完，这就能在有限的内存空间内支持大量goroutine，支持了更多的并发。 虽然一个goroutine的栈只占几KB，但实际是可伸缩的，如果需要更多内容，runtime会自动为goroutine分配。 Goroutine特点： 占用内存更小（大概4kb）； 调度更灵活(runtime调度)； 二、Goroutine协程调度器 Golang种出现过2种协程调度器模型，GM 和 GMP； 在12年的go1.1版本之前用的都是GM模型，但是由于GM模型性能不好，饱受用户诟病。之后官方对调度器进行了改进，变成了我们现在用的GMP模型； 网上关于GM 和GMP的对比介绍文章也很多，随便贴一个：GM到GMP，Golang经历了什么？ 1、简单快速地认识下 GM 模型： GM模型中的G全称为Goroutine协程，M全称为Machine内核级线程！ 根据上图。我们很容易知道，M线程 想要 执行/放回 G 都必须访问 全局G队列 ，并且M有多个，而我们知道，为了确保安全，多线程访问同一资源时需要加锁，所以全局G队列是有互斥锁进行保护的。 显然，这种模型有很严重的缺点： 激烈的锁竞争：线程M在 创建、销毁、调度G都需要先获取锁； 很差的程序局部性：比如当G中包含创建新协程的时候，比如新协程叫G'，为了继续执行G，就需要将G'交给另一个线程M' 执行，这就造成了很差的程序局部性，因为G'和G是相关的，最好放在M上执行，而不是其他M'； 系统调用线程切换开销大：CPU在M之间的切换，导致频繁的线程阻塞和取消阻塞操作增加了系统开销（我们知道线程切换是需要用户态/内核态转换的，开销是很大的）； 2、重点学习下 GMP 模型： G全称为Goroutine协程，M全称为Machine内核级线程，P全称为Processor协程运行所需的资源！ 他在GM模型的基础上增加了一个P层，下面我们来看一下他是如何设计的： M列表：go程序启动时，会设置M的最大数量，默认10000。但是内核很难创建出如此多的线程，因此默认情况下M的最大数量取决于内核。也可以调用runtime/debug中的SetMaxThreads函数，手动设置M的最大数量； P列表：所有的P都在程序启动时创建，并保存在数组中，最多有GOMAXPROCS(可配置)个 P的本地队列：P内置的G队列，存的数量有限，不超过256个； 全局队列：当P中的本地队列中有协程G溢出时，会被放到全局队列中； 线程M想要运行协程G，都是找P索要，一个P一次只能提供一个G协程任务；整个程序最终的协程并行度，其实就是P的数量，即GOMAXPROCS设置的值！ 本地队列优先：当队列P1中的G1在运行过程中新建G2时，G2优先存放到P1的本地队列中，如果队列满了，则会把P1队列中一半的G移动到全局队列中； 工作窃取机制：如果P的本地队列为空，那么他会先到全局队列中获取G，如果全局队列中也没有G，则会尝试从其他线程绑定的P中偷取一半的G； M通过P获得G的优先级顺序：本地队列（无锁）-&gt;全局队列（锁）-&gt;netpoll事件-&gt;去其他队列里偷（CAS，无锁） 3、GMP调度器的设计策略详解： 复用线程： work stealing机制：工作窃取机制，上面已经讲解了； hand off机制：分手机制，当本线程因为G进行系统调用阻塞时，如读写磁盘，线程释放绑定的P，把P转移给其他空闲的线程执行； 利用并行： 可以通过配置GOMAXPROCS限定P的个数 抢占： 现在的调度器几乎都是抢占式的，非抢占式的 全局G队列： 相当于是对work stealing机制的一个补充，当本地队列满了后，就将溢出的G放到全局队列中！ 因为全局队列是共享的，所以 放/取 都是要加锁的，性能相当于P的本地队列更慢，这也就是为什么“本地队列优先”！ 4、通过go func() 启动一个goroutine的过程详解： 在Golang中想创建一个goroutine协程非常容易，只需要在 func() 方法调用前，加上 go 关键字即可； 新创建的goroutine会优先保存在P的本地队列中，如果本地队列满了，则会放在全局队列中； G(goroutine)只能运行在M(Machine)中，M:P = 1:1，当M需要运行G时，不会自己去取，而是找P(Processor)要； P获取G的优先级是：本地队列 —&gt; 全局队列 —&gt; 其他P的本地队列窃取； M执行G的过程是一个无限循环； 如果M在执行G的过程中，遇到了阻塞操作，则会触发“hand off机制”，让P与当前M分手，新建或从休眠M队列中获取一个M线程，将刚刚的P与新得到的M线程进行绑定，继续后续G的处理工作！ 当刚刚由于阻塞操作，与P分手的M完成阻塞操作后，由于刚刚的G还没有被完全执行完成，所以它会去找一个空闲的P，将这个G放到空闲P的本地队列中；如果获取不到空闲P，那么M线程将变成休眠状态，添加到休眠M队列中，同时将G放入全局队列中！ 5、P 和 M 的数量问题？ P的数量：由启动时环境变量$GOMAXPROCS或者是由runtime的方法GOMAXPROCS()决定，这意味着在程序执行的任意时刻都只有$GOMAXPROCS个goroutine在同时运行； M的数量： go语言本身的限制，go程序启动时，会设置M的最大数量，默认10000，但是内核很难支持这么多的线程数，所以这个限制可以忽略； 调用runtime/debug中的SetMaxThreads()，设置M的最大数量； 一个M阻塞了，会创建新的M； 6、P 和 M 何时被创建？ P何时被创建（饿汉）：在确定了P的最大数量n后，运行时系统会根据这个数量创建n个P —— 程序启动时，即创建好 M何时被创建（懒汉）：没有足够的M来关联P并运行其中的可运行的G；比如所有的M此时都阻塞住了，而P中还有很多就绪任务，就会去寻找空闲的M，而没有空闲的，就会去创建新的M。 ","link":"https://tianxiawuhao.github.io/QplCLDgXh/"},{"title":"使用java 实现mqtt两种方式","content":"在开发MQTT时有两种方式一种是使用Paho Java 原生库来完成，一种是使用spring boot 来完成。 Paho Java 库实现 Eclipse Paho Java Client (opens new window)是用 Java 编写的 MQTT 客户端库（MQTT Java Client），可用于 JVM 或其他 Java 兼容平台（例如Android）。 Eclipse Paho Java Client 提供了MqttAsyncClient 和 MqttClient 异步和同步 API 通过 Maven 安装 Paho Java &lt;dependency&gt; &lt;groupId&gt;org.eclipse.paho&lt;/groupId&gt; &lt;artifactId&gt;org.eclipse.paho.client.mqttv3&lt;/artifactId&gt; &lt;version&gt;1.2.2&lt;/version&gt; &lt;/dependency&gt; Paho Java 使用示例 Java 体系中 Paho Java 是比较稳定、广泛应用的 MQTT 客户端库，本示例包含 Java 语言的 Paho Java 连接 EMQX Broker，并进行消息收发完整代码： package io.emqx; import org.eclipse.paho.client.mqttv3.MqttClient; import org.eclipse.paho.client.mqttv3.MqttConnectOptions; import org.eclipse.paho.client.mqttv3.MqttException; import org.eclipse.paho.client.mqttv3.MqttMessage; import org.eclipse.paho.client.mqttv3.persist.MemoryPersistence; public class App { public static void main(String[] args) { String subTopic = &quot;testtopic/#&quot;; String pubTopic = &quot;testtopic/1&quot;; String content = &quot;Hello World&quot;; int qos = 2; String broker = &quot;tcp://broker.emqx.io:1883&quot;; String clientId = &quot;emqx_test&quot;; MemoryPersistence persistence = new MemoryPersistence(); try { MqttClient client = new MqttClient(broker, clientId, persistence); // MQTT 连接选项 MqttConnectOptions connOpts = new MqttConnectOptions(); connOpts.setUserName(&quot;emqx_test&quot;); connOpts.setPassword(&quot;emqx_test_password&quot;.toCharArray()); // 保留会话 connOpts.setCleanSession(true); // 设置回调 client.setCallback(new PushCallback()); // 建立连接 System.out.println(&quot;Connecting to broker: &quot; + broker); client.connect(connOpts); System.out.println(&quot;Connected&quot;); System.out.println(&quot;Publishing message: &quot; + content); // 订阅 client.subscribe(subTopic); // 消息发布所需参数 MqttMessage message = new MqttMessage(content.getBytes()); message.setQos(qos); client.publish(pubTopic, message); System.out.println(&quot;Message published&quot;); client.disconnect(); System.out.println(&quot;Disconnected&quot;); client.close(); System.exit(0); } catch (MqttException me) { System.out.println(&quot;reason &quot; + me.getReasonCode()); System.out.println(&quot;msg &quot; + me.getMessage()); System.out.println(&quot;loc &quot; + me.getLocalizedMessage()); System.out.println(&quot;cause &quot; + me.getCause()); System.out.println(&quot;excep &quot; + me); me.printStackTrace(); } } } 回调消息处理类 OnMessageCallback.java package io.emqx; import org.eclipse.paho.client.mqttv3.IMqttDeliveryToken; import org.eclipse.paho.client.mqttv3.MqttCallback; import org.eclipse.paho.client.mqttv3.MqttMessage; public class OnMessageCallback implements MqttCallback { public void connectionLost(Throwable cause) { // 连接丢失后，一般在这里面进行重连 System.out.println(&quot;连接断开，可以做重连&quot;); } public void messageArrived(String topic, MqttMessage message) throws Exception { // subscribe后得到的消息会执行到这里面 System.out.println(&quot;接收消息主题:&quot; + topic); System.out.println(&quot;接收消息Qos:&quot; + message.getQos()); System.out.println(&quot;接收消息内容:&quot; + new String(message.getPayload())); } public void deliveryComplete(IMqttDeliveryToken token) { System.out.println(&quot;deliveryComplete---------&quot; + token.isComplete()); } } 好的上述就实现了简单的 MQTT的连接和消息收发。 spring boot集成mqtt spring boot 环境 spring-boot 版本 2.2.2 spring-integration的版本为：5.4.3 Spring Integration提供了入站适配器和出站适配器以支持MQTT协议。 Maven 依赖: &lt;!-- https://mvnrepository.com/artifact/org.springframework.integration/spring-integration-mqtt --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-integration&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.integration&lt;/groupId&gt; &lt;artifactId&gt;spring-integration-mqtt&lt;/artifactId&gt; &lt;version&gt;5.4.3&lt;/version&gt; &lt;/dependency&gt; 配置文件 application.yml： spring: mqtt: username: password: url: tcp://ip:port clientId: clientId topic: default completionTimeout: 2000 核心代码 配置类 @Data @Configuration @ConfigurationProperties(prefix = &quot;spring.mqtt&quot;) public class MqttConfiguration { private String username; private String password; private String url; private String clientId; private String topic = &quot;TOPIC_DEFAULT&quot;; private Integer completionTimeout = 2000; /** * 注册MQTT客户端工厂 * @return */ @Bean public MqttPahoClientFactory mqttClientFactory(){ DefaultMqttPahoClientFactory factory = new DefaultMqttPahoClientFactory(); MqttConnectOptions options = new MqttConnectOptions(); //如果设置为 false，客户端和服务器将在客户端、服务器和连接重新启动时保持状态。随着状态的保持： // 即使客户端、服务器或连接重新启动，消息传递也将可靠地满足指定的 QOS。服务器将订阅视为持久的。 // 如果设置为 true，客户端和服务器将不会在客户端、服务器或连接重新启动时保持状态。 options.setCleanSession(true); //该值以秒为单位，必须&gt;0，定义了客户端等待与 MQTT 服务器建立网络连接的最大时间间隔。 // 默认超时为 30 秒。值 0 禁用超时处理，这意味着客户端将等待直到网络连接成功或失败。 options.setConnectionTimeout(0); //此值以秒为单位，定义发送或接收消息之间的最大时间间隔，必须&gt;0 options.setKeepAliveInterval(90); //自动重新连接 options.setAutomaticReconnect(true); options.setUserName(this.getUsername()); options.setPassword(this.getPassword().toCharArray()); options.setServerURIs(new String[]{this.getUrl()}); factory.setConnectionOptions(options); return factory; } } @Slf4j @AllArgsConstructor @Configuration @IntegrationComponentScan public class MqttInboundConfiguration { private MqttConfiguration mqttConfig; private MqttPahoClientFactory factory; private MqttMessageReceiver mqttMessageReceiver; /** * 此处可以使用其他消息通道 * Spring Integration默认的消息通道，它允许将消息发送给一个订阅者，然后阻碍发送直到消息被接收。 * * @return */ @Bean public MessageChannel mqttInBoundChannel() { return new DirectChannel(); } /** * 适配器, 两个topic共用一个adapter * 客户端作为消费者，订阅主题，消费消息 * * @param * @param * @return */ @Bean public MessageProducerSupport mqttInbound() { MqttPahoMessageDrivenChannelAdapter adapter = new MqttPahoMessageDrivenChannelAdapter(mqttConfig.getClientId()+&quot;-&quot;+System.currentTimeMillis(), factory, mqttConfig.getTopic()); adapter.setCompletionTimeout(60000); adapter.setConverter(new DefaultPahoMessageConverter()); adapter.setRecoveryInterval(10000); adapter.setQos(0); adapter.setOutputChannel(mqttInBoundChannel()); return adapter; } /** * mqtt入站消息处理工具，对于指定消息入站通道接收到生产者生产的消息后处理消息的工具。 * * @return */ @Bean @ServiceActivator(inputChannel = &quot;mqttInBoundChannel&quot;) public MessageHandler mqttMessageHandler() { return this.mqttMessageReceiver; } } 数据接收 @Slf4j @AllArgsConstructor @Component public class MqttMessageReceiver implements MessageHandler { @Override public void handleMessage(Message&lt;?&gt; message) throws MessagingException { try { MessageHeaders headers = message.getHeaders(); //获取消息Topic String receivedTopic = (String) headers.get(MqttHeaders.RECEIVED_TOPIC); log.info(&quot;[获取到的消息的topic :]{} &quot;, receivedTopic); //获取消息体 String payload = (String) message.getPayload(); log.info(&quot;[获取到的消息的payload :]{} &quot;, payload); //todo .... } catch (Exception e) { e.printStackTrace(); } } } @Slf4j @AllArgsConstructor @Configuration public class MqttOutboundConfiguration { private MqttConfiguration mqttConfig; private MqttPahoClientFactory factory; @Bean public MessageChannel mqttOutboundChannel() { return new DirectChannel(); } @Bean @ServiceActivator(inputChannel = &quot;mqttOutboundChannel&quot;) public MessageHandler mqttOutbound() { MqttPahoMessageHandler messageHandler = new MqttPahoMessageHandler( mqttConfig.getClientId()+&quot;-&quot;+System.currentTimeMillis() + System.currentTimeMillis(), factory); messageHandler.setDefaultQos(0); //开启异步 messageHandler.setAsync(true); messageHandler.setDefaultTopic(mqttConfig.getTopic()); return messageHandler; } } 发送者 @Component @MessagingGateway(defaultRequestChannel = &quot;mqttOutboundChannel&quot;) public interface MqttGateway { /** * 发送mqtt消息 * @param topic 主题 * @param payload 内容 * @return void */ void sendToMqtt(@Header(MqttHeaders.TOPIC) String topic, String payload); /** * 发送包含qos的消息 * @param topic 主题 * @param qos 对消息处理的几种机制。 * * 0 表示的是订阅者没收到消息不会再次发送，消息会丢失。&lt;br&gt; * * 1 表示的是会尝试重试，一直到接收到消息，但这种情况可能导致订阅者收到多次重复消息。&lt;br&gt; * * 2 多了一次去重的动作，确保订阅者收到的消息有一次。 * @param payload 消息体 * @return void */ void sendToMqtt(@Header(MqttHeaders.TOPIC) String topic, @Header(MqttHeaders.QOS) int qos, String payload); /** * 发送包含qos的消息 * @param topic 主题 * @param qos 对消息处理的几种机制。 * * 0 表示的是订阅者没收到消息不会再次发送，消息会丢失。&lt;br&gt; * * 1 表示的是会尝试重试，一直到接收到消息，但这种情况可能导致订阅者收到多次重复消息。&lt;br&gt; * * 2 多了一次去重的动作，确保订阅者收到的消息有一次。 * @param payload 消息体 * @return void */ void sendToMqtt(@Header(MqttHeaders.TOPIC) String topic, @Header(MqttHeaders.QOS) int qos, byte[] payload); } @Component @AllArgsConstructor public class MqttMessageSender { private MqttGateway mqttGateway; /** * 发送mqtt消息 * @param topic 主题 * @param message 内容 * @return void */ public void send(String topic, String message) { mqttGateway.sendToMqtt(topic, message); } /** * 发送包含qos的消息 * @param topic 主题 * @param qos 质量 * @param messageBody 消息体 * @return void */ public void send(String topic, int qos, JSONObject messageBody){ mqttGateway.sendToMqtt(topic, qos, messageBody.toString()); } /** * 发送包含qos的消息 * @param topic 主题 * @param qos 质量 * @param message 消息体 * @return void */ public void send(String topic, int qos, byte[] message){ mqttGateway.sendToMqtt(topic, qos, message); } } ","link":"https://tianxiawuhao.github.io/R-yFR9BFl/"},{"title":"Docker执行小技巧（批量脚本）","content":"一、批量下载docker镜像并批量导入导出到其它地方 起因是我需要下载一批镜像，但是我使用得Linux机器上无法下载，因为涉及外网，所以我计划在另一台外网机器上完成所有镜像的下载后，再批量导出、导入到目标机器上，开干！ 1、编写一个脚本 images.sh 文件： #!/bin/bash images=( docker.io/cockpit/kubernetes:latest docker.io/openshift/origin-haproxy-router:v3.11 docker.io/openshift/origin-node:v3.11 docker.io/openshift/origin-control-plane:v3.11 docker.io/openshift/origin-deployer:v3.11.0 docker.io/openshift/origin-pod:v3.11.0 docker.io/openshift/origin-web-console:v3.11 docker.io/openshift/origin-docker-registry:v3.11 docker.io/openshift/origin-metrics-server:v3.11 docker.io/openshift/origin-console:v3.11 docker.io/openshift/origin-metrics-heapster:v3.11 docker.io/openshift/origin-metrics-hawkular-metrics:v3.11 docker.io/openshift/origin-metrics-schema-installer:v3.11 docker.io/openshift/origin-metrics-cassandra:v3.11 quay.io/coreos/cluster-monitoring-operator:v0.1.1 quay.io/coreos/prometheus-config-reloader:v0.23.2 quay.io/coreos/prometheus-operator:v0.23.2 docker.io/openshift/prometheus-alertmanager:v0.15.2 docker.io/openshift/prometheus-node-exporter:v0.16.0 docker.io/openshift/prometheus:v2.3.2 docker.io/grafana/grafana:5.2.1 quay.io/coreos/kube-rbac-proxy:v0.3.1 quay.io/coreos/etcd:v3.2.22 quay.io/coreos/kube-state-metrics:v1.3.1 docker.io/openshift/oauth-proxy:v1.1.0 quay.io/coreos/configmap-reload:v0.0.1 quay.io/openshift/origin-node:v3.11 quay.io/openshift/origin-pod:v3.11 ) for imageName in ${images[@]} ; do docker pull $imageName done 2、授予执行权限： chmod +x images.sh 3、执行脚本并打印日志，当然也可以直接执行： nohup ./images.sh &gt; log.file 2&gt;&amp;1 &amp; 下载完成后，查看所有的镜像是否完整： [root@VM-0-12-centos jiguiquan]# docker images --format &quot;table {{.Repository}}\\t{{.Tag}}\\t{{.ID}}&quot; REPOSITORY TAG IMAGE ID openshift/origin-node v3.11 983f6b1c6bd4 openshift/origin-control-plane v3.11 7a2f8e6e6a15 openshift/origin-deployer v3.11.0 e6d4226c7cc0 openshift/origin-haproxy-router v3.11 e3afcd798b00 openshift/origin-pod v3.11.0 560ab7ad8bbe cockpit/kubernetes latest 736c0218e939 openshift/origin-docker-registry v3.11 9dffb2abf1dd openshift/origin-console v3.11 ab1db955ef9e openshift/origin-web-console v3.11 be30b6cce5fa openshift/origin-metrics-server v3.11 8c99f32f40d3 openshift/origin-metrics-heapster v3.11 69421c019449 openshift/origin-metrics-hawkular-metrics v3.11 59e2258250c4 openshift/origin-metrics-schema-installer v3.11 342f50fded7d openshift/origin-metrics-cassandra v3.11 8176cfabc16b quay.io/coreos/cluster-monitoring-operator v0.1.1 4488a207a5bc quay.io/coreos/prometheus-config-reloader v0.23.2 2ed5973a47af quay.io/coreos/prometheus-operator v0.23.2 835a7e260b35 openshift/prometheus-alertmanager v0.15.2 68bbd0006378 openshift/prometheus-node-exporter v0.16.0 f9f775bf6d0e openshift/prometheus v2.3.2 e362c322f000 grafana/grafana 5.2.1 1bfead9ff707 quay.io/coreos/kube-rbac-proxy v0.3.1 992ac1a5e7c7 quay.io/coreos/etcd v3.2.22 ff5dd2137a4f quay.io/coreos/kube-state-metrics v1.3.1 a9c8f313b7aa openshift/oauth-proxy v1.1.0 90c45954eb03 quay.io/coreos/configmap-reload v0.0.1 3129a2ca29d7 4、一键导出所有镜像到 openshift.tar 包： docker save $(docker images | grep -v REPOSITORY | awk 'BEGIN{OFS=&quot;:&quot;;ORS=&quot; &quot;}{print $1,$2}') -o openshift.tar 5、将tar包拷贝到目标服务器上，执行 load 导入命令： docker load -i openshift.tar 二、将当前运行状态的docker容器状态导出为镜像，保存起来 起因是因为，我通过官方教程，快速部署了一个大平台，这个平台是由很多容器一起工作才能支撑，我需要修改前端页面的一些东西，大成本的方案肯定是下载官方源码重新进行改造编译打包镜像等操作； 但是那样太烦了，我就直接进入容器内部修改了一些东西，运行正常后，我将当前状态导出为自己的镜像保存下来，以后再部署就不用再进入修改啦。 docker commit 163950a65bfb registry.cn-hangzhou.aliyuncs.com/jgqk8s/jgq-console:v3.1.1 docker commit命令参数 参数 描述 -a, --author string 作者。 -c, --change list 应用 dockerfile 指令来创建图像。 -m, --message string 提交信息。 -p, --pause 提交期间暂停容器（默认为true）。 ","link":"https://tianxiawuhao.github.io/n4qWAww-f/"},{"title":"Go中的面向对象","content":"相信从你刚开始学习Go时，以及使用Go参与项目，都知道Go中没有面向对象这样的概念。但在实际的项目开发中，为了提高代码的可维护性、可扩展性和可复用性等特点，你不得不使用面向对象的设计理念来编码。 那该怎么实现呢，答案就是使用Go中结构体来实现。 需要注意的是，本文不会单独去讲结构体的语法内容。重点是总结结构体与面向对象的区别。 什么是结构体 在Go语言中，结构体（Struct）是一种自定义数据类型，用于组织和存储一组相关字段的集合。它类似于其他编程语言中的类或对象，是一种将数据和相关方法组合在一起的容器。 结构体的字段就类似于面向对象中的属性。 结构体的方法就类似于面向对象中的方法。 要定义一个结构体非常的简单，其语法格式如下: type 结构体名称 struct { 字段名 字段属性 } func (接受者) 方法名称([方法参数]) [方法返回值] { } 下面我们就来实际定义一个结构体。 package main import &quot;fmt&quot; type Animal struct { name string age string height float64 weight float64 } func (u Animal) run() { fmt.Println(u.name, &quot;跑步的速度很快&quot;) } func main() { animal1 := &amp;Animal{ name: &quot;鸵鸟&quot;, } animal1.run() // output:鸵鸟 跑步的速度很快 } 定义一个名为Animal的结构体，就相当于面向对象中的类。 定义name、age等字段，就相当于面向对象中的属性。 定义run()方法，就相当于面向对象中的方法。记住：方法一定是要有一个接收者的，这里的接收者为结构体Animal，其实就是将这个方法归属到结构体Animal。就好比在类中定义一个方法，这个方法就是属于这个类。 使用&amp;Animal{}，就相当于面向对象的实例化类的过程。 使用animal1.run()，就相当于实例化了一个对象，通过对象去调用方法。 是不是发现使用struct，实现面向对象是非常简单。 对象继承 在具有面向对象的开发语言中，使用对象继承，都是使用关键字extend关键字来实现。在Go中，可以使用结构体嵌套的方式来实现继承关系。 package main import &quot;fmt&quot; type Animal struct { name string age string height float64 weight float64 } type Dog struct { Animal color string } func (u Animal) run() { fmt.Println(u.name, &quot;跑步的速度很快&quot;) } func (d Dog) sleep() { fmt.Println(d.name, &quot;晚上很少睡觉&quot;) } func main() { dog := &amp;Dog{ Animal{name: &quot;小狗&quot;}, } dog.sleep() dog.run() } 上述定义了一个Animal的结构体，作为父类结构体。同时定义了一个run()方法，当做父类的方法，定义了4个字段当做父类的属性。 接着定义了一个Dog结构体，嵌入了一个Animal的结构体，此时两个结构体就实现了一个继承的关系。 Dog结构体自身也定义了一个sleep()的方法。这时，Dog结构体具备父类结构体的所有方法和属性，同时也有自己的属性和方法。 需要注意的是，结构体的嵌套，可以是外部包，也可以是本包。方法的接收者必须是本包的结构体，不能是外部包。 此致，结构体实现对象的特性，就总结的差不多了。记住Go中的机构体不仅能实现面向对象的特性，同时还具有其他更强大的功能。这里简单的举例。 type user struct { name string `json:&quot;name&quot;` } 上面的写法，相信大家在很多的第三方包，以及项目开发中都能遇到。通过定义一个tag，可以将元数据通过tag的定义方式，暴露给外部。 总结 在Go语言中，struct是一种用户自定义的数据类型，用于组织和存储不同字段类型的数据。它类似于其他编程语言中的结构体，但在Go中，struct不具备面向对象编程中类的所有特性。以下是struct与面向对象的主要区别： 继承：Go语言中的struct不支持继承，无法通过一个struct来继承另一个struct的成员和方法。而面向对象编程中，类之间可以通过继承来共享和扩展功能。 封装：在面向对象编程中，类的成员和方法可以进行封装，通过访问修饰符来控制其可见性。而在Go中，struct的成员默认是公开的，可以被外部访问，无法像类一样进行严格封装。 多态：面向对象编程支持多态性，一个对象可以根据不同的上下文表现出不同的行为。而Go语言中的struct不直接支持多态，但可以通过接口来实现类似的效果。 总的来说，虽然Go语言的struct不同于传统的面向对象编程语言中的类，但通过结合使用接口、嵌入等语言特性，我们仍然可以在Go中实现类似面向对象的设计和编程范式。 ","link":"https://tianxiawuhao.github.io/0yq48V3py/"},{"title":"Hadoop3.3.5安装","content":"Hadoop3.3.5安装(Windows) 安装JDK1.8(自行查阅) 1. 下载Hadoop 直接访问官网 [Hadoop下载地址](Index of /hadoop/common/hadoop-3.3.5 (apache.org)) 2. 配置环境变量 将下载的压缩包解压到其他盘，在环境变量中配置好Hadoop的环境变量，新建HADOOP_HOME，如下 path也要加 3. 修改配置文件 在hadoop-3.3.5目录下新建data和tmp两个文件夹，在data目录下新建datanode、namenode、snn三个文件夹。打开hadoop-3.3.5\\etc\\hadoop目录，修改配置文件 core-site.xml文件 &lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; hdfs-site.xml文件 &lt;configuration&gt; &lt;!-- 单机版配置为1 --&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.permissions&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/D:/A-Enviroment/hadoop-3.3.5/data/namenode&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.checkpoint.dir&lt;/name&gt; &lt;value&gt;file:/D:/A-Enviroment/hadoop-3.3.5/data/snn&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.checkpoint.edits.dir&lt;/name&gt; &lt;value&gt;file:/D:/A-Enviroment/hadoop-3.3.5/data/snn&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/D:/A-Enviroment/hadoop-3.3.5/data/datanode&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; mapred-site.xml文件 &lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; yarn-site.xml文件 &lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.local-dirs&lt;/name&gt; &lt;value&gt;/D:/A-Enviroment/hadoop-3.3.5/tmp&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 4. 添加动态库 在hadoop-3.3.5/bin目录下添加hadoop.dll和winutils.exe文件，可以从https://github.com/cdarlint/winutils和https://github.com/steveloughran/winutils获取对应版本的文件。 winutils-master.zip 5. 验证 以管理员身份打开命令提示符 执行hadoop version查看hadoop是否安装/配置成功，如下图： 报错 原因是Hadoop里的Java路径配置不对。打开C:hadoop-3.3.5\\etc\\hadoop\\hadoop-env.cmd这个文件，发现在第25行，配置的JAVA_HOME是系统里的环境变量。 去系统变量里查看JAVA_HOME，发现路径是C:Program Files，其中包含了空格，因此hadoop报错 把JDK移动到另一个不包含空格名称的文件夹下，例如D:\\jdk-17.0.1（安装jdk1.8）。输入命令，成功启动Hadoop 执行hdfs namenode -format验证配置文件是否修改成功 成功 启动 在sbin目录下，以管理员身份运行start-all.cmd 会弹出四个窗口，如下图所示： 如上面是启动失败，这是因为hadoop.dll和winutils.exe与你的版本不匹配的问题，自行百度找版本，将hadoop.dll放到C:\\window\\system32目录下，再将hadoop.dll和winutils.exe添加到hadoop-3.3.5/bin下，以管理员身份重新运行start-all.cmd即可运行成功。 浏览器中输入地址：http://localhost:8088 浏览器中输入地址：http://localhost:9870 ","link":"https://tianxiawuhao.github.io/ZkyB61S7B/"},{"title":"Docker+Jenkins(blueocean)+Gitee构建CICD流水线实战","content":"需求: 使用jenkins搭建流水线实现持续集成持续部署 一、编写docker-compose.yml安装jenkins, jenkins的版本是带blueocean插件的版本 version: '3' services: mysql: image: mysql:8.0.29 container_name: mysql restart: always ports: - 3306:3306 privileged: true environment: TZ: Asia/Shanghai MYSQL_ROOT_PASSWORD: 123456 command: --default-authentication-plugin=mysql_native_password --character-set-server=utf8mb4 --collation-server=utf8mb4_general_ci --explicit_defaults_for_timestamp=true --lower_case_table_names=1 volumes: - /mydata/mysql/data/db:/var/lib/mysql #数据文件挂载 - /mydata/mysql/data/conf:/etc/mysql/conf.d #配置文件挂载 - /mydata/mysql/log:/var/log/mysql #日志文件挂载 redis: image: redis:7 container_name: redis restart: always command: redis-server --requirepass 1234566 --appendonly yes volumes: - /mydata/redis/data:/data - /etc/localtime:/etc/localtime:ro ports: - 6379:6379 nginx: image: nginx:1.22 container_name: nginx restart: always volumes: - /mydata/nginx/nginx.conf:/etc/nginx/nginx.conf #配置文件挂载 - /mydata/nginx/html:/usr/share/nginx/html #静态资源根目录挂载 - /mydata/nginx/log:/var/log/nginx #日志文件挂载 - /etc/localtime:/etc/localtime:ro ports: - 80:80 rabbitmq: image: rabbitmq:3.9-management container_name: rabbitmq restart: always ports: - 5672:5672 - 15672:15672 environment: TZ: Asia/Shanghai volumes: - /mydata/rabbitmq/data:/var/lib/rabbitmq #数据文件挂载 - /etc/localtime:/etc/localtime:ro mongo: image: mongo:4 container_name: mongo restart: always volumes: - /mydata/mongo/db:/data/db ports: - 27017:27017 environment: TZ: Asia/Shanghai MONGO_INITDB_ROOT_USERNAME: root MONGO_INITDB_ROOT_PASSWORD: 123456 command: --auth jenkins: image: jenkinsci/blueocean container_name: jenkins restart: always user: root ports: - 8080:8080 - 50000:50000 volumes: - /mydata/jenkins_home:/var/jenkins_home - /etc/localtime:/etc/localtime:ro - /var/run/docker.sock:/var/run/docker.sock 在/mydata/jenkins_home/appconfig/maven/settings.xml目录配置maven的阿里源 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;settings xmlns=&quot;http://maven.apache.org/SETTINGS/1.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd&quot;&gt; &lt;pluginGroups&gt; &lt;/pluginGroups&gt; &lt;proxies&gt; &lt;/proxies&gt; &lt;servers&gt; &lt;/servers&gt; &lt;mirrors&gt; &lt;mirror&gt; &lt;id&gt;nexus-aliyun&lt;/id&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;name&gt;Nexus aliyun&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt; &lt;/mirror&gt; &lt;/mirrors&gt; &lt;localRepository&gt;/root/.m2&lt;/localRepository&gt; &lt;profiles&gt; &lt;profile&gt; &lt;id&gt;jdk-1.8&lt;/id&gt; &lt;activation&gt; &lt;jdk&gt;1.8&lt;/jdk&gt; &lt;/activation&gt; &lt;properties&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt; &lt;maven.compiler.compilerVersion&gt;1.8&lt;/maven.compiler.compilerVersion&gt; &lt;/properties&gt; &lt;/profile&gt; &lt;/profiles&gt; &lt;/settings&gt; 二、登录xxx:8080配置jenkins 1、初始密码使用 docker logs jenkins 查看获取 2、安装插件 Docker Pipeline, 后面构建流水线需要 3、准备一个项目，提交到Gitee 创建两个文件 Jenkinsfile和Dockerfile pipeline{ agent any environment { WS = &quot;${WORKSPACE}&quot; IMAGE_NAME = &quot;java-devops-demo&quot; } //定义流水线的加工流程 stages { //流水线的所有阶段 stage('1.环境检查'){ steps { sh 'pwd &amp;&amp; ls -alh' sh 'printenv' sh 'docker version' sh 'java -version' sh 'git --version' } } stage('2.编译'){ agent { docker { image 'maven:3-alpine' args '-v maven-repository:/root/.m2' } } steps { sh 'pwd &amp;&amp; ls -alh' sh 'mvn -v' sh 'cd ${WS} &amp;&amp; mvn clean package -s &quot;/var/jenkins_home/appconfig/maven/settings.xml&quot; -Dmaven.test.skip=true' } } stage('3.打包'){ steps { sh 'pwd &amp;&amp; ls -alh' sh 'docker build -t ${IMAGE_NAME} .' } } stage('4.部署'){ // 删除容器和虚悬镜像 steps { sh 'pwd &amp;&amp; ls -alh' sh 'docker rm -f ${IMAGE_NAME} &amp;&amp; docker rmi $(docker images -q -f dangling=true)' sh 'docker run -d -p 8888:8080 --name ${IMAGE_NAME} -v /mydata/logs/${IMAGE_NAME}:/logs/${IMAGE_NAME} ${IMAGE_NAME}' } } } } FROM openjdk:8-jre-alpine # 将当前目录下的jar包复制到docker容器的/目录下 COPY target/*.jar /app.jar # 运行过程中创建一个xx.jar文件 RUN touch /app.jar; ENV TZ=Asia/Shanghai JAVA_OPTS=&quot;-Xms128m -Xmx256m -Djava.security.egd=file:/dev/./urandom&quot; ENV PARAMS=&quot;&quot; # 声明服务运行在8080端口 EXPOSE 8080 # 指定docker容器启动时运行jar包 ENTRYPOINT [ &quot;sh&quot;, &quot;-c&quot;, &quot;java $JAVA_OPTS -jar /app.jar $PARAMS&quot; ] 4、jenkins首页首页左侧的新建任务，输入任务名称，选择流水线 5、配置流水线 6、返回首页点击'打开 Blue Ocean'选择运行 到此整体功能已经完成 三、测试 四、触发远程构建 创建一个jenkins账号用来触发远程构建 构建maven项目 ","link":"https://tianxiawuhao.github.io/uTx-7SqyQ/"},{"title":"Springboot+微信小程序——微信授权登录","content":"在实际小程序开发中，大部分的前后台交互业务操作与前后端分离的webapp没什么区别，都是调用后台接口，使用JSON数据传递，只有在一些特殊的环节才会感觉到微信的存在，比如微信授权登录、微信支付、微信模板消息推送等场景； 这些场景如果理解了原理，其实也都不复杂，本节博文主要讲解，如何使用springboot后台处理微信授权登录： 在处理微信小程序授权登录之前，最后可以先了解一下Oauth2.0授权协议，如果确实不清楚，那么也没事，直接看微信小程序开发官方文档相关章节：小程序登录时序图 说明： 调用 wx.login() 获取 临时登录凭证code ，并回传到开发者服务器； 调用 auth.code2Session 接口，换取 用户唯一标识 OpenID 和 会话密钥 session_key； 之后开发者服务器可以根据用户标识来生成自定义登录态，用于后续业务逻辑中前后端交互时识别用户身份； 注意： 会话密钥 session_key 是对用户数据进行 加密签名 的密钥。为了应用自身的数据安全，开发者服务器不应该把会话密钥下发到小程序，也不应该对外提供这个密钥； 临时登录凭证 code 只能使用一次； 上面把流程理清了，那么下面就开始用代码实现吧：springboot部分：为了减少博文的篇幅，我就不将所有的代码都贴在这里了，只挑核心的贴： 需要用到的核心技术，HttpClient和jwtToken，需要在原有依赖的基础上增加这两个maven依赖： &lt;!-- httpclient --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt; &lt;artifactId&gt;httpclient&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- jwt-token --&gt; &lt;dependency&gt; &lt;groupId&gt;io.jsonwebtoken&lt;/groupId&gt; &lt;artifactId&gt;jjwt&lt;/artifactId&gt; &lt;version&gt;0.7.0&lt;/version&gt; &lt;/dependency&gt; 为HttpClient和jwtToken封装工具类： HttpClientUtil.java package com.wx.video.utils; import java.io.IOException; import java.net.URI; import java.util.ArrayList; import java.util.List; import java.util.Map; import org.apache.http.client.entity.UrlEncodedFormEntity; import org.apache.http.client.methods.CloseableHttpResponse; import org.apache.http.client.methods.HttpGet; import org.apache.http.client.methods.HttpPost; import org.apache.http.client.utils.URIBuilder; import org.apache.http.entity.ContentType; import org.apache.http.entity.StringEntity; import org.apache.http.impl.client.CloseableHttpClient; import org.apache.http.impl.client.HttpClients; import org.apache.http.message.BasicNameValuePair; import org.apache.http.util.EntityUtils; public class HttpClientUtil { public static String doGet(String url, Map&lt;String, String&gt; param) { // 创建Httpclient对象 CloseableHttpClient httpclient = HttpClients.createDefault(); String resultString = &quot;&quot;; CloseableHttpResponse response = null; try { // 创建uri URIBuilder builder = new URIBuilder(url); if (param != null) { for (String key : param.keySet()) { builder.addParameter(key, param.get(key)); } } URI uri = builder.build(); // 创建http GET请求 HttpGet httpGet = new HttpGet(uri); // 执行请求 response = httpclient.execute(httpGet); // 判断返回状态是否为200 if (response.getStatusLine().getStatusCode() == 200) { resultString = EntityUtils.toString(response.getEntity(), &quot;UTF-8&quot;); } } catch (Exception e) { e.printStackTrace(); } finally { try { if (response != null) { response.close(); } httpclient.close(); } catch (IOException e) { e.printStackTrace(); } } return resultString; } public static String doGet(String url) { return doGet(url, null); } public static String doPost(String url, Map&lt;String, String&gt; param) { // 创建Httpclient对象 CloseableHttpClient httpClient = HttpClients.createDefault(); CloseableHttpResponse response = null; String resultString = &quot;&quot;; try { // 创建Http Post请求 HttpPost httpPost = new HttpPost(url); // 创建参数列表 if (param != null) { List&lt;BasicNameValuePair&gt; paramList = new ArrayList&lt;&gt;(); for (String key : param.keySet()) { paramList.add(new BasicNameValuePair(key, param.get(key))); } // 模拟表单 UrlEncodedFormEntity entity = new UrlEncodedFormEntity(paramList); httpPost.setEntity(entity); } // 执行http请求 response = httpClient.execute(httpPost); resultString = EntityUtils.toString(response.getEntity(), &quot;utf-8&quot;); } catch (Exception e) { e.printStackTrace(); } finally { try { response.close(); } catch (IOException e) { e.printStackTrace(); } } return resultString; } public static String doPost(String url) { return doPost(url, null); } public static String doPostJson(String url, String json) { // 创建Httpclient对象 CloseableHttpClient httpClient = HttpClients.createDefault(); CloseableHttpResponse response = null; String resultString = &quot;&quot;; try { // 创建Http Post请求 HttpPost httpPost = new HttpPost(url); // 创建请求内容 StringEntity entity = new StringEntity(json, ContentType.APPLICATION_JSON); httpPost.setEntity(entity); // 执行http请求 response = httpClient.execute(httpPost); resultString = EntityUtils.toString(response.getEntity(), &quot;utf-8&quot;); } catch (Exception e) { e.printStackTrace(); } finally { try { response.close(); } catch (IOException e) { e.printStackTrace(); } } return resultString; } } UserClaims.java package com.wx.video.utils; import java.util.Date; import io.jsonwebtoken.Claims; import io.jsonwebtoken.RequiredTypeException; import io.jsonwebtoken.impl.JwtMap; public class UserClaims extends JwtMap implements Claims { private String grantType = &quot;password&quot;; private Integer uid; private String openid; public Integer getUid() { return uid; } public void setUid(Integer uid) { this.uid = uid; setValue(&quot;uid&quot;, uid); } public String getOpenid() { return openid; } public void setOpenid(String openid) { this.openid = openid; setValue(&quot;openid&quot;, openid); } public String getGrantType() { return grantType; } public void setGrantType(String grantType) { this.grantType = grantType; setValue(&quot;grantType&quot;, this.grantType); } @Override public String getIssuer() { return getString(ISSUER); } @Override public Claims setIssuer(String iss) { setValue(ISSUER, iss); return this; } @Override public String getSubject() { return getString(SUBJECT); } @Override public Claims setSubject(String sub) { setValue(SUBJECT, sub); return this; } @Override public String getAudience() { return getString(AUDIENCE); } @Override public Claims setAudience(String aud) { setValue(AUDIENCE, aud); return this; } @Override public Date getExpiration() { return get(Claims.EXPIRATION, Date.class); } @Override public Claims setExpiration(Date exp) { setDate(Claims.EXPIRATION, exp); return this; } @Override public Date getNotBefore() { return get(Claims.NOT_BEFORE, Date.class); } @Override public Claims setNotBefore(Date nbf) { setDate(Claims.NOT_BEFORE, nbf); return this; } @Override public Date getIssuedAt() { return get(Claims.ISSUED_AT, Date.class); } @Override public Claims setIssuedAt(Date iat) { setDate(Claims.ISSUED_AT, iat); return this; } @Override public String getId() { return getString(ID); } @Override public Claims setId(String jti) { setValue(Claims.ID, jti); return this; } @Override public &lt;T&gt; T get(String claimName, Class&lt;T&gt; requiredType) { Object value = get(claimName); if (value == null) { return null; } if (Claims.EXPIRATION.equals(claimName) || Claims.ISSUED_AT.equals(claimName) || Claims.NOT_BEFORE.equals(claimName)) { value = getDate(claimName); } if (requiredType == Date.class &amp;&amp; value instanceof Long) { value = new Date((Long) value); } if (!requiredType.isInstance(value)) { throw new RequiredTypeException(&quot;Expected value to be of type: &quot; + requiredType + &quot;, but was &quot; + value.getClass()); } return requiredType.cast(value); } } JwtTokenProvider.java package com.wx.video.utils; import javax.crypto.spec.SecretKeySpec; import com.alibaba.fastjson.JSONObject; import io.jsonwebtoken.Claims; import io.jsonwebtoken.CompressionCodecs; import io.jsonwebtoken.Jwts; import io.jsonwebtoken.SignatureAlgorithm; public class JwtTokenProvider { private SecretKeySpec key; public JwtTokenProvider(String key) { SecretKeySpec secretKeySpec = new SecretKeySpec(key.getBytes(), SignatureAlgorithm.HS512.getJcaName()); this.key = secretKeySpec; } public String createToken(Claims claims) { String compactJws = Jwts.builder().setPayload(JSONObject.toJSONString(claims)) .compressWith(CompressionCodecs.DEFLATE).signWith(SignatureAlgorithm.HS512, key).compact(); return compactJws; } public Claims parseToken(String token) { try { return Jwts.parser().setSigningKey(key).parseClaimsJws(token).getBody(); } catch (Exception e) { e.printStackTrace(); } return null; } } JwtUtils.java package com.wx.video.utils; import javax.servlet.http.HttpServletRequest; import org.springframework.beans.factory.annotation.Value; import org.springframework.stereotype.Component; import io.jsonwebtoken.Claims; @Component public class JwtUtils { @Value(&quot;${auth.jwtKey}&quot;) private String jwtKey; /** ** 根据UserClaims创建JwtToken * @param userClaims * @return */ public String createToken(UserClaims userClaims) { JwtTokenProvider tokenProvider = new JwtTokenProvider(jwtKey); return tokenProvider.createToken(userClaims); } /** ** 根据token解析出Claim * @param token * @return */ public Claims parseToken(String token) { JwtTokenProvider tokenProvider = new JwtTokenProvider(jwtKey); return tokenProvider.parseToken(token); } /** ** 根据request请求直接解析出Claim * @param request * @return */ public Claims getUserClaim(HttpServletRequest request) { String token = request.getHeader(&quot;Authorization&quot;); JwtTokenProvider tokenProvider = new JwtTokenProvider(jwtKey); return tokenProvider.parseToken(token); } } 有了上面的工具类，正在的授权登录代码就很简单啦，代码如下（为了方便直接阅读，少贴一些代码，里面需要用到的常量，我就不提取不封装啦，实际工作工程中，可以专门做一个类，存放所有的常量，如appid，secret等，统一管理）： @Controller @RequestMapping(&quot;/api&quot;) public class UserController { @Autowired private UserService userService; @Autowired private VorderService vorderService; @Autowired private JwtUtils jwtUtils; @Autowired private RedisOperator redis; @PostMapping(&quot;/wxlogin&quot;) @ResponseBody public JsonResult wxlogin(@RequestBody Map&lt;String, Object&gt; map){ // 配置请求参数 Map&lt;String, String&gt; param = new HashMap&lt;&gt;(); param.put(&quot;appid&quot;, &quot;wx0fb11123456897544&quot;); param.put(&quot;secret&quot;, &quot;5c5e5efae856768878yb53976b&quot;); param.put(&quot;js_code&quot;, map.get(&quot;code&quot;).toString()); param.put(&quot;grant_type&quot;, &quot;authorization_code&quot;); // 发送请求 String url = &quot;https://api.weixin.qq.com/sns/jscode2session&quot;; String wxResult = HttpClientUtil.doGet(url, param); JSONObject jsonObject = JSONObject.parseObject(wxResult); // 获取参数返回的 String sessionkey = jsonObject.get(&quot;session_key&quot;).toString(); String openid = jsonObject.get(&quot;openid&quot;).toString(); // 根据返回的user实体类，判断用户是否是新用户，不是的话，更新最新登录时间，是的话，将用户信息存到数据库 User user = userService.selectByOpenId(openid); System.out.println(&quot;查询用户结果&quot;+user); if(user != null){ user.setUname(map.get(&quot;uname&quot;).toString()); user.setUavatar(map.get(&quot;uavatar&quot;).toString()); user.setUgender(map.get(&quot;ugender&quot;).toString()); user.setUaddress(map.get(&quot;address&quot;).toString()); user.setSessionkey(sessionkey); //修改sessionKey user.setUpdateTime(new Date()); //修改更新时间 //更新数据库 userService.update(user); }else{ User newUser = new User(); newUser.setOpenid(openid); newUser.setSessionkey(sessionkey); newUser.setUname(map.get(&quot;uname&quot;).toString()); newUser.setUavatar(map.get(&quot;uavatar&quot;).toString()); newUser.setUgender(map.get(&quot;ugender&quot;).toString()); newUser.setUaddress(map.get(&quot;address&quot;).toString()); newUser.setCreateTime(new Date()); // 添加到数据库 int count = userService.insert(newUser); if(count &lt; 0){ return JsonResult.error(&quot;插入数据失败&quot;); } } //获取到当前用户的数据库uid User user1 = userService.selectByOpenId(openid); //生成JWTtoken UserClaims userClaims = new UserClaims(); userClaims.setUid(user1.getUid()); userClaims.setOpenid(openid); String jwttoken = jwtUtils.createToken(userClaims); System.out.println(jwttoken); //将token存入Redis redis.set(jwttoken, sessionkey); // 封装返回小程序 Map&lt;String, String&gt; result = new HashMap&lt;&gt;(); result.put(&quot;token&quot;, jwttoken); return JsonResult.successs(result); } } 注意： 1、上文中用到的appid和secret都是我写的假值，需要替换为自己的； 2、统一返回结果数据结构，这种实现方式很多，就是我里面出现的JsonResult类； 3、需要用到的Redis工具类也自己封装一下，这个实现方法也不麻烦，不影响我们的授权登录流程； 上面的代码就足以完成微信小程序授权登陆啦，小程序获取到token后，记得将token存入本地storage，后面进行登录态校验的请求，在Header头中设置Authorization值为token；后台使用封装好的JwtUtils工具类中的getUserClaim方法，就可以从request中直接解析出Claims对象，里面存放着我们封装进去的uid和openid，然后开始处理我们的正常业务啦； ","link":"https://tianxiawuhao.github.io/pR59olYKQ/"},{"title":"一文彻底弄懂ctx.close()和ctx.channel().close()的区别","content":"一文彻底弄懂ctx.close()和ctx.channel().close()的区别 先说结论： ctx.close()：只会总当前处理器BHandler出发，向前寻找出站Handler，并调用它们的close()方法； ctx.channel().close()：将从整个pipeline的tail尾部向前寻找所有的出站Handler，并调用它们的close()方法！ 一、编写测试代码： 1、需要引入的依赖： &lt;dependency&gt; &lt;groupId&gt;io.netty&lt;/groupId&gt; &lt;artifactId&gt;netty-all&lt;/artifactId&gt; &lt;version&gt;4.1.70.Final&lt;/version&gt; &lt;/dependency&gt; 2、编写一个双向处理器ZidanHandler： /** * ChannelDuplexHandler：双向处理器： * 既继承了ChannelInboundHandlerAdapter类， * 又实现了ChannelOutboundHandler接口。 */ public class ZidanHandler extends ChannelDuplexHandler { @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception { System.out.println(ctx.name() + &quot; channelRead: &quot; + msg); String result = (String) msg; if ((&quot;ctx.close.&quot; + ctx.name()).equals(result)) { ctx.close(); } else if ((&quot;ctx.channel.close.&quot; + ctx.name()).equals(result)) { ctx.channel().close(); } else { ctx.fireChannelRead(msg); } } @Override public void close(ChannelHandlerContext ctx, ChannelPromise promise) throws Exception { System.out.println(ctx.name() + &quot; close&quot;); ctx.close(promise); } @Override public void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise) throws Exception { System.out.println(ctx.name() + &quot; write&quot;); ctx.write(String.format(&quot;[%s]%s&quot;, ctx.name(), msg), promise); } } 3、主入口类NettyServer： public class NettyServer { public static void main(String[] args) throws InterruptedException { NioEventLoopGroup boss = new NioEventLoopGroup(1); NioEventLoopGroup worker = new NioEventLoopGroup(3); new ServerBootstrap() .channel(NioServerSocketChannel.class) .group(boss, worker) .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() { @Override protected void initChannel(SocketChannel socketChannel) throws Exception { socketChannel.pipeline() .addLast(&quot;decoder&quot;, new StringDecoder()) .addLast(&quot;encoder&quot;, new StringEncoder()) .addLast(&quot;A&quot;, new ZidanHandler()) .addLast(&quot;B&quot;, new ZidanHandler()) .addLast(&quot;C&quot;, new ZidanHandler()); } }).bind(8090).sync(); } } 二、启动项目，进行测试看现象 1、项目启动后，我们使用本地的telnet命令作为客户端进行交互： C:\\Users\\admin&gt;telnet localhost 8090 # 重点，进入telnet窗口后，我们需要按下 Ctrl + ]键 欢迎使用 Microsoft Telnet Client Escape 字符为 'CTRL+]' Microsoft Telnet&gt; 2、我们先用telnet发送“ctx.close.B”字符串： 3、我们再用telnet发送“ctx.channel.close.B”字符串： 4、根据现象得出结论： ctx.close()：只会总当前处理器BHandler出发，向前寻找出站Handler，并调用它们的close()方法； ctx.channel().close()：将从整个pipeline的tail尾部向前寻找所有的出站Handler，并调用它们的close()方法！ 三、通过源码弄明白为什么 1、ctx.close() 的源码： abstract class AbstractChannelHandlerContext implements ChannelHandlerContext, ResourceLeakHint { public ChannelFuture close() { return this.close(this.newPromise()); } public ChannelFuture close(final ChannelPromise promise) { if (this.isNotValidPromise(promise, false)) { return promise; } else { // 获取下一个出站Handler（出站是向前，所以实际是前一个） final AbstractChannelHandlerContext next = this.findContextOutbound(4096); EventExecutor executor = next.executor(); if (executor.inEventLoop()) { next.invokeClose(promise); } else { safeExecute(executor, new Runnable() { public void run() { next.invokeClose(promise); } }, promise, (Object)null, false); } return promise; } } private AbstractChannelHandlerContext findContextOutbound(int mask) { AbstractChannelHandlerContext ctx = this; EventExecutor currentExecutor = this.executor(); do { // 找出当前handlerContext的prev前一个handlerContext // 但是会跳过不是Outbound的那些handler ctx = ctx.prev; } while(skipContext(ctx, currentExecutor, mask, 130560)); return ctx; } } 2、ctx.channel().close() 的源码： public abstract class AbstractChannel extends DefaultAttributeMap implements Channel { public ChannelFuture close() { return this.pipeline.close(); } public final ChannelFuture close() { // 其实到这里已经很清晰了，从当前整个pipeline中的tail尾节点开始，向前寻找出站handler， // 并执行他们的close()方法 return this.tail.close(); } } 四、总结： 1、那么我们何时使用ctx.close()，又何时使用ctx.channel().close()方法呢： 如果你正写一个 ChannelHandler, 并且在处理逻辑时，想在这个 handler 中关闭 channel, 则直接调用 ctx.close()； 如果你正准备从一个外部的 handler (例如, 你有一个后台的非I/O线程, 并且你想从该线程中关闭连接)，那么就可以调用ctx.Channel.close()); 但是这种规则也是非绝对的！ 2、其他类似的方法对： ctx.write() 与 ctx.channel().write() ctx.writeAndFlush() 与 ctx.channel().writeAndFlush() ","link":"https://tianxiawuhao.github.io/RokgvWufI/"},{"title":"Actuator监控，并集成Prometheus/Grafana展示（实战）","content":"一、基础概念 1、何为“可观测性Observability”？ 可观测性Observability：对线上的应用进行观测、监控、预警等： 健康状况 Health：【组件状态、存货状态】等； 运行指标 Metrics：【cpu、内存、垃圾回收、吞吐量、响应成功率】等； 链路追踪等； 2、Springboot为我们提供了一个开箱即用的可观测性解决方案： &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; 二、初始化一个Springboot3项目 创建完成后的pom.xml文件如下： &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;3.1.1&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;groupId&gt;com.jiguiquan.www&lt;/groupId&gt; &lt;artifactId&gt;actuator&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;actuator&lt;/name&gt; &lt;description&gt;actuator&lt;/description&gt; &lt;properties&gt; &lt;java.version&gt;17&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.graalvm.buildtools&lt;/groupId&gt; &lt;artifactId&gt;native-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;excludes&gt; &lt;exclude&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;/exclude&gt; &lt;/excludes&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/project&gt; 三、演示Actuator的简单使用/常用端点 1、在 application.yml 中增加如下配置： management: endpoints: web: exposure: include: '*' # 通过web方式，暴露所有端点 2、此时我们在运行，即可看到 /actuator 接口暴露了很多端点： 其中，一切常用的端点介绍： /actuator/health：健康状况的端点； /actuator/metrics：指标数据； /actuator/beans：当前IOC容器中有多少Bean； /actuator/caches：服务当前的缓存情况； /actuator/condition：当前服务中涉及到的条件，哪些条件是满足的，哪些条件是没满足的； 其中最重要的：/actuator/metrics 指标信息： 如果我们想看某个指标的详细信息：/actuator/metrics/指标名 有了这些指标信息，我们以后构建自己的监控平台面板将会变得非常容易！ 3、以下以表格形式列举我们工作中可能会用到的端点： **端点 ** 描述 auditevents 暴露当前应用程序的审核事件信息。需要一个AuditEventRepository组件。 beans 显示应用程序中所有Spring Bean的完整列表。 caches 暴露当前的缓存情况。 conditions 显示自动配置的所有条件信息，包括匹配或不匹配的原因。 configprops 显示所有@ConfigurationProperties。 env 暴露Spring的属性ConfigurableEnvironment flyway 显示已应用的所有Flyway数据库迁移，需要一个或多个Flyway组件。 health 显示应用程序运行状况信息。 httptrace 显示HTTP跟踪信息（默认情况下，最近100个HTTP请求-响应）。需要一个HttpTraceRepository组件。 info 显示应用程序信息。 integrationgraph 显示Spring integrationgraph 。需要依赖spring-integration-core。 loggers 显示和修改应用程序中日志的配置。 liquibase 显示已应用的所有Liquibase数据库迁移。需要一个或多个Liquibase组件。 metrics 显示当前应用程序的“指标”信息。 mappings 显示所有@RequestMapping路径列表。 scheduledtasks 显示应用程序中的计划任务。 sessions 允许从Spring Session支持的会话存储中检索和删除用户会话。需要使用Spring Session的基于Servlet的Web应用程序。 shutdown 使应用程序正常关闭。默认禁用。 startup 显示由ApplicationStartup收集的启动步骤数据。需要使用SpringApplication进行配置BufferingApplicationStartup。 threaddump 执行线程转储。 heapdump 返回hprof堆转储文件。 jolokia 通过HTTP暴露JMX bean（需要引入Jolokia，不适用于WebFlux）。需要引入依赖jolokia-core。 logfile 返回日志文件的内容（如果已设置logging.file.name或logging.file.path属性）。支持使用HTTPRange标头来检索部分日志文件的内容。 prometheus 以Prometheus服务器可以抓取的格式公开指标。需要依赖micrometer-registry-prometheus。 四、自定义监控端点 首先，如果想监控详细的健康状况，我们配置文件中还得增加一些配置： management: endpoints: web: exposure: include: '*' # 通过web方式，暴露所有端点 endpoint: health: # 如果想展示所有组件健康状况的详细信息，下面两项必须配置，否则/health端点只会展示当前服务的存活情况 enabled: true show-details: always 1、自定义健康监控Health端点（ 存活/死亡 ）：核心 HealthIndicator 目标：监控 ZidanService 这个组件的健康状况： 被监控组件：ZidanService.java： @Service public class ZidanService { // 随机返回一个整数，作为健康检查的依据 public int check(){ return new Random().nextInt(); } } 健康状况监控器类： /** * 方法1、只需要实现HealthIndicator接口中的health()方法，即可来定制组件的健康状态； * 方法2、或者继承AbstractHealthIndicator抽象类，实现其doHealthCheck()抽象方法即可！ */ @Component //public class ZidanServiceHealthIndicator implements HealthIndicator { public class ZidanServiceHealthIndicator extends AbstractHealthIndicator { @Autowired private ZidanService zidanService; @Override protected void doHealthCheck(Health.Builder builder) throws Exception { // 调用这个组件的check方法，然后根据返回结果，构建出一个存活结果 int result = zidanService.check(); // 我们就假设，如果result是偶数，则存活，是奇数，则死亡 if ((result % 2) == 0) { builder.up() .withDetail(&quot;code&quot;, &quot;200&quot;) .withDetail(&quot;msg&quot;, &quot;活得很好&quot;) .withDetail(&quot;data&quot;, &quot;我是吉老板&quot;) .build(); } else { builder.down() .withDetail(&quot;code&quot;, &quot;500&quot;) .withDetail(&quot;msg&quot;, &quot;我不太好&quot;) .withDetail(&quot;data&quot;, &quot;嘿嘿&quot;) .build(); } } } 此时，我们再见调用Health端点，就可以看到ZidanService这个组件的健康状况了： 2、自定义指标监控Metrics端点（ 次数、率 ）：核心 MeterRegistry 目标：监控DemoController.demo()方法被调用了多少次！ DemoController.java： @RestController public class DemoController { Counter counter; // 通过构造方法，注入MeterRegistry，从而给counter计数器赋值 public DemoController(MeterRegistry meterRegistry){ counter = meterRegistry.counter(&quot;jiguiquan.demo&quot;); } @GetMapping(&quot;/demo&quot;) public void demo(){ counter.increment(); System.out.println(&quot;demo接口被调用了 &quot; + counter.count() + &quot; 次&quot;); } } 此时，我们访问 /metrics/jiguiquan.demo 端点，就能看到我们自定义的计数指标信息！ 五、整合Prometheus + Grafana 1、需要整合Prometheus，我们首先需要在项目中引入整合Prometheus的依赖： 这个依赖可以是 /metrics/prometheus 端点返回适配Prometheus的数据结构： &lt;dependency&gt; &lt;groupId&gt;io.micrometer&lt;/groupId&gt; &lt;artifactId&gt;micrometer-registry-prometheus&lt;/artifactId&gt; &lt;version&gt;1.10.6&lt;/version&gt; &lt;/dependency&gt; 2、测试 /prometheus 的端点是否正常获取到数据： 3、没有问题，那么我们现在就将我们的代码拷贝到服务器上，进行打包编译： # 将代码拷贝到服务器的/code/目录下 [root actuator]# pwd /code/actuator [root actuator]# ls pom.xml src # 3个命令，将actuator项目打包成二进制可执行文件： mvn clean package -DskipTests mvn spring-boot:process-aot mvn -Pnative native:build 4、通过编译好的二进制文件，快速启动actuator项目： 后台启动： # 二进制文件启动 nohup target/actuator &gt;actuator.log 2&gt;&amp;1 &amp; ## 如果是jar包直接启动： nohup java -jar target/actuator-0.0.1-SNAPSHOT.jar &gt;actuator.log 2&gt;&amp;1 &amp; 服务运行正常！ 六、配置Prometheus，从actuator服务的/prometheus端点上定时拉取数据： 1、将Prometheus下载并解压到服务器的指定位置： [root prometheus]# ls grafana-8.3.6 prometheus-2.37.8 [root prometheus]# cd prometheus-2.37.8/ [root prometheus-2.37.8]# ls console_libraries consoles LICENSE NOTICE prometheus prometheus.yml promtool 2、修改Prometheus的核心配置文件prometheus.yml，添加一个新任务： # my global config global: scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute. evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute. # scrape_timeout is set to the global default (10s). # Alertmanager configuration alerting: alertmanagers: - static_configs: - targets: # - alertmanager:9093 # Load rules once and periodically evaluate them according to the global 'evaluation_interval'. rule_files: # - &quot;first_rules.yml&quot; # - &quot;second_rules.yml&quot; # A scrape configuration containing exactly one endpoint to scrape: # Here it's Prometheus itself. scrape_configs: # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config. - job_name: &quot;prometheus&quot; static_configs: - targets: [&quot;localhost:9090&quot;] # 这就是我们的服务的端口，因为原定的9090和prometheus冲突了，所以我这里已经调整了actuator的端口 - job_name: &quot;actuator&quot; metrics_path: '/actuator/prometheus' #指定抓取的路径 static_configs: - targets: [&quot;localhost:10000&quot;] labels: nodename: 'actuator-demo' 3、nohup后台启动prometheus服务： nohup ./prometheus --config.file=prometheus.yml &gt; prometheus.log 2&gt;&amp;1 &amp; 4、访问 http://ip:9090/targets端点，可以查看数据采集情况： 数据采集正常！ 七、配置Grafana面板，展示actuator服务监控状态 1、依然显示将Grafana下载解压到服务器的指定位置： [root prometheus]# ls grafana-8.3.6 prometheus-2.37.8 [root prometheus]# cd grafana-8.3.6/ [root grafana-8.3.6]# ls bin conf LICENSE NOTICE.md plugins-bundled public README.md scripts VERSION 2、暂时我们不修改Gafana的配置文件，直接nohup后台运行： nohup` `bin``/grafana-server` `&gt; grafana.log 2&gt;&amp;1 &amp; 3、通过 http://ip:3000 访问Grafana服务： Grafana默认账号密码为：admin ：admin，登录时我们可以修改下次密码，也可以Skip跳过！ 4、登录后，我们首先需要先配置一个Prometheus数据源： 5、去官方Dashboard市场找一个好看的仪表板（如果有实力，也可以自己搭建）： 官网Dashboard下载页面：https://grafana.com/grafana/dashboards/ 我选择的是这个：https://grafana.com/grafana/dashboards/12900-springboot-apm-dashboard/ 复制它的Dashboard ID或者下载JSON文件都可以： 6、在Grafana中导入Dashboard： 7、配置完成后的效果如下图所示： 到这里，整个Springboot项目的指标监控全流程就走通了，以后的项目中，可以适当的暴露这些监控端点哦，还可以自定义自己的“健康状况”或“运行指标”信息！ ","link":"https://tianxiawuhao.github.io/VgW7Mfe6w/"},{"title":"基于PostgreSQL的时序数据库TimescaleDB的基本用法和概念","content":"时序数据是指按照时间顺序存储的数据,TimescaleDB是一个开源的、扩展了PostgreSQL的时序数据库扩展,本文就给大家详细的介绍一下基于PostgreSQL的时序数据库TimescaleDB的基本用法和概念,需要的朋友可以参考下 目录 一、TimescaleDB概述 二、安装和配置 三、数据写入和查询 四、连续聚集表 五、分区管理 六. 综合使用示例 七、小结一下 时序数据是指按照时间顺序存储的数据。它在很多领域得到广泛应用，例如物联网、日志分析、金融交易等。为了高效处理时序数据，TimescaleDB应运而生。TimescaleDB是一个开源的、扩展了PostgreSQL的时序数据库扩展，它结合了关系型数据库和时序数据库的优势，提供了更好的时序数据管理和分析能力。 时间序列数据的挑战： 时间序列数据是指按时间顺序收集和记录的数据，如传感器、日志、金融数据等。这类数据在现实生活中广泛存在，但对于传统数据库系统来说，处理大规模和高性能的时间序列数据是一项具有挑战性的任务。 TimescaleDB的出现： TimescaleDB是一个构建在PostgreSQL之上的开源时间序列数据库，它采用了一种创新的方法来解决传统数据库在处理时间序列数据方面的限制。TimescaleDB利用了PostgreSQL的强大功能，并通过扩展超级表的方式提供了更好的性能和可扩展性。 一、TimescaleDB概述 TimescaleDB是一个在PostgreSQL之上构建的时序数据库，它利用了关系型数据库的成熟性和灵活性，并针对时序数据进行了优化。TimescaleDB通过使用分区表（hypertable）和连续聚集表（continuous aggregate）来处理时序数据，使得数据的存储和查询更加高效。 二、安装和配置 1. 安装TimescaleDB插件 安装TimescaleDB可以通过在PostgreSQL上加载TimescaleDB插件来完成。首先，确保已经安装了PostgreSQL数据库，并且具有管理员权限。然后，在命令行中执行以下步骤： # 添加TimescaleDB的APT源 curl https://packagecloud.io/timescale/timescaledb/gpgkey | sudo apt-key add - echo &quot;deb https://packagecloud.io/timescale/timescaledb/ubuntu/ focal main&quot; | sudo tee /etc/apt/sources.list.d/timescale_timescaledb.list # 更新软件源 sudo apt-get update # 安装TimescaleDB插件 sudo apt-get install timescaledb-postgresql-13 2. 创建TimescaleDB数据库 TimescaleDB的工作方式与传统的PostgreSQL数据库相似。我们可以使用createdb命令创建一个新的数据库，并在该数据库上加载TimescaleDB扩展： # 创建一个新的数据库 createdb mydatabase # 连接到该数据库 psql mydatabase # 在数据库中加载TimescaleDB扩展 CREATE EXTENSION IF NOT EXISTS timescaledb CASCADE; 3. 扩展超级表功能 TimescaleDB中的基本数据管理单元称为超级表。超级表是基于普通表的一种特殊类型，它将时间序列数据根据时间进行分区和组织，从而实现更高效的查询性能。以下示例演示了如何创建一个超级表： -- 创建超级表 CREATE TABLE conditions ( time TIMESTAMPTZ NOT NULL, location TEXT NOT NULL, temperature DOUBLE PRECISION NULL, humidity DOUBLE PRECISION NULL ); -- 对超级表进行分区 SELECT create_hypertable('table_name', 'time_column'); 创建Hypertable：TimescaleDB中的核心概念是Hypertable，它是一个逻辑表，负责将普通表划分成不同的时间段。其中，'table_name'是原始表的名称，'time_column'是存储时间信息的列。 三、数据写入和查询 1. 创建超级表 超级表的创建与普通表类似，但需要指定时间列。以下示例创建了一个包含时间列的超级表： CREATE TABLE conditions ( time TIMESTAMPTZ NOT NULL, location TEXT NOT NULL, temperature DOUBLE PRECISION NULL, humidity DOUBLE PRECISION NULL ); 2. 超级表的分区 超级表的分区是TimescaleDB的一个重要特性，它可以根据时间将数据分散到不同的物理表中。通过分区，查询操作仅需要在相关的物理表上执行，从而提高查询性能。以下示例演示了如何为超级表添加分区： -- 对超级表conditions按月份进行分区 SELECT create_hypertable('conditions', 'time', chunk_time_interval =&gt; INTERVAL '1 month'); 3. 超级表的复制和副本 TimescaleDB支持复制和副本功能，可以在多个节点上创建超级表的副本，实现数据冗余和高可用性。以下示例展示了如何创建一个超级表的副本： -- 在节点2上创建conditions超级表的副本 SELECT add_data_node('conditions', '2'); 4. 插入数据 向超级表中插入数据与向普通表中插入数据类似。以下示例向超级表conditions插入一行数据： INSERT INTO conditions (time, location, temperature, humidity) VALUES ('2023-06-29 06:00:00', 'New York', 25.4, 60.2); 5. 更新和删除数据 在超级表中更新和删除数据与普通表相同。以下示例演示了如何更新和删除符合特定条件的数据： -- 更新温度大于30的记录 UPDATE conditions SET temperature = 30.0 WHERE temperature &gt; 30.0; -- 删除湿度小于50的记录 DELETE FROM conditions WHERE humidity &lt; 50.0; 6. 时间序列聚合函数 TimescaleDB提供了一系列内置的时间序列聚合函数，用于计算给定时间范围内的统计信息，如平均值、最大值、最小值等。以下示例展示了如何使用时间序列聚合函数： -- 计算最近一小时的平均温度 SELECT time_bucket('1 hour', time) AS hour, AVG(temperature) AS average_temperature FROM conditions WHERE time &gt; NOW() - INTERVAL '1 hour' GROUP BY hour; 7. 查询和过滤数据 查询和过滤数据时，可以使用常规的SQL查询语句。以下示例展示了如何查询超级表中的数据： -- 查询所有温度大于25的记录 SELECT * FROM conditions WHERE temperature &gt; 25.0; 8. 其他数据查询 TimescaleDB提供了许多用于查询时序数据的功能，例如： 查询最近一小时的数据： SELECT * FROM table_name WHERE time_column &gt; NOW() - INTERVAL '1 hour'; 聚合查询： SELECT time_bucket('5 minutes', time_column) AS bucket, AVG(value_column) AS avg_value FROM table_name GROUP BY bucket ORDER BY bucket; 以上查询将结果按5分钟为一个时间段进行聚合，并计算每个时间段内的平均值。 四、连续聚集表 连续聚集表是TimescaleDB的一个重要特性，它可以在后台自动维护预定义的聚合数据。通过使用连续聚集表，可以极大地提高大规模时序数据的查询性能。以下是创建和使用连续聚集表的示例： 1. 创建连续聚集表： SELECT create_continuous_aggregate('ca_table', 'SELECT time_bucket('5 minutes', time_column) AS bucket, AVG(value_column) AS avg_value FROM table_name GROUP BY bucket'); 2. 查询连续聚集表： SELECT * FROM ca_table; 五、分区管理 TimescaleDB使用分区（partitioning）来管理大规模的时序数据。它可以将表按照时间范围进行自动划分，提高查询性能和数据的可维护性。以下是创建和管理分区的示例： 1. 创建分区： SELECT add_hypertable_partition('table_name', TIMESTAMP '2023-06-01', TIMESTAMP '2023-07-01'); 2. 管理分区： 查询所有分区： SELECT show_partitions('table_name'); 删除分区： SELECT drop_partition('table_name', TIMESTAMP '2023-06-01'); 3. 数据连续性和存储优化 TimescaleDB通过数据连续性来优化存储空间和查询性能。数据连续性指的是调整超级表的分区和存储策略，以确保相邻时间段的数据存储在一起，从而提高查询性能。 4. 数据保留策略 TimescaleDB允许定义数据的保留策略，以自动删除过时的数据。通过设置保留策略，可以控制超级表中数据的保存时长，以及是否自动删除过期数据。 六. 综合使用示例 1. 创建超级表并插入数据 假设我们有一个名为conditions的超级表，包含时间、地点、温度和湿度字段。以下示例演示了如何创建该超级表，并向其中插入一些数据： CREATE TABLE conditions ( time TIMESTAMPTZ NOT NULL, location TEXT NOT NULL, temperature DOUBLE PRECISION NULL, humidity DOUBLE PRECISION NULL ); SELECT create_hypertable('conditions', 'time'); INSERT INTO conditions (time, location, temperature, humidity) VALUES ('2023-06-29 06:00:00', 'New York', 25.4, 60.2), ('2023-06-29 07:00:00', 'New York', 26.8, 58.9), ('2023-06-29 08:00:00', 'New York', 28.3, 57.1); 2. 查询最新的N条数据 假设我们想要查询最新的3条温度数据。以下示例演示了如何使用LIMIT子句和ORDER BY子句进行查询： SELECT * FROM conditions ORDER BY time DESC LIMIT 3; 3. 执行时间范围内的聚合查询 假设我们想要计算过去一小时内每分钟的平均温度。以下示例展示了如何使用时间序列聚合函数和时间戳桶函数进行查询： SELECT time_bucket('1 minute', time) AS minute, AVG(temperature) AS average_temperature FROM conditions WHERE time &gt; NOW() - INTERVAL '1 hour' GROUP BY minute; 七、小结一下 通过使用TimescaleDB，我们可以高效地存储和查询时序数据，并利用连续聚集表和分区管理功能进一步提高性能和可维护性。 Timescale DB跟同类的其他数据库相比具有一些令人兴奋的功能： 它建立在PostgreSQL之上(目前最好的开源关系数据库)。如果您的项目已经在运行PostgreSQL，Timescale可以重点考虑。 通过熟悉的SQL语法进行查询，从而减少了学习难度。 极快的写入速度-每秒数百万次的插入。 数十亿行或PB的数据，对于Timescale来说没什么大不了的。 模式具有真正的灵活性-可以根据需要选择关系模式或无模式。 ","link":"https://tianxiawuhao.github.io/ZZ6BztHhQ/"},{"title":"PostgreSQL概述三","content":"十、事务 10.1 什么是ACID？（常识） 在日常操作中，对于一组相关操作，通常要求要么都成功，要么都失败。在关系型数据库中，称这一组操作为事务。为了保证整体事务的安全性，有ACID这一说： 原子性A：事务是一个最小的执行单位，一次事务中的操作要么都成功，要么都失败。 一致性C：在事务完成时，所有数据必须保持在一致的状态。（事务完成后吗，最终结果和预期结果是一致的） 隔离性：一次事务操作，要么是其他事务操作前的状态，要么是其他事务操作后的状态，不存在中间状态。 持久性：事务提交后，数据会落到本地磁盘，修改是永久性的。 PostgreSQL中，在事务的并发问题里，也是基于MVCC，多版本并发控制去维护数据的一致性。相比于传统的锁操作，MVCC最大的有点就是可以让读写互相不冲突 。 当然，PostgreSQL也支持表锁和行锁，可以解决写写的冲突问题。 PostgreSQL相比于其他数据，有一个比较大的优化，DDL也可以包含在一个事务中。比如集群中的操作，一个事务可以保证多个节点都构建出一个表，才算成功。 10.2 事务的基本使用 首先基于前面的各种操作，应该已经体会到了，PostgreSQL是自动提交事务。跟MySQL是一样的。 可以基于关闭PostgreSQL的自动提交事务来进行操作。 但是上述方式比较麻烦，传统的方式。 就是三个命令： begin：开始事务 commit：提交事务 rollback：回滚事务 -- 开启事务 begin; -- 操作 insert into test values (7,'bbb',12,5); -- 提交事务 commit; 10.3 保存点（了解） 比如项目中有一个大事务操作，不好控制，超时有影响，回滚会造成一切重来，成本太高。 我针对大事务，拆分成几个部分，第一部分完成后，构建一个保存点。如果后面操作失败了，需要回滚，不需要全盘回滚，回滚到之前的保存点，继续重试。 有人会发现，破坏了整体事务的原子性。 But，只要操作合理，可以在保存点的举出上，做重试，只要重试不成功，依然可以全盘回滚。 比如一个电商项目，下订单，扣库存，创建订单，删除购物车，增加用户积分，通知商家…………。这个其实就是一个大事务。可以将扣库存和下订单这种核心功能完成后，增加一个保存点，如果说后续操作有失败的，可以从创建订单成功后的阶段，再做重试。 不过其实上述的业务，基于最终一致性有更好的处理方式，可以保证可用性。 简单操作一下。 -- savepoint操作 -- 开启事务 begin; -- 插入一条数据 insert into test values (8,'铃铛',55,11); -- 添加一个保存点 savepoint ok1; -- 再插入数据,比如出了一场 insert into test values (9,'大唐官府',66,22); -- 回滚到之前的提交点 rollback to savepoint ok1; -- 就可以开始重试操作，重试成功，commit，失败可以rollback; commit; 十一、并发问题 11.1 事务的隔离级别 在不考虑隔离性的前提下，事务的并发可能会出现的问题： 脏读：读到了其他事务未提交的数据。（必须避免这种情况） 不可重复读：同一事务中，多次查询同一数据，结果不一致，因为其他事务修改造成的。（一些业务中这种不可重复读不是问题） 幻读：同一事务中，多次查询同一数据，因为其他事务对数据进行了增删吗，导致出现了一些问题。（一些业务中这种幻读不是问题） 针对这些并发问题，关系型数据库有一些事务的隔离级别，一般用4种。 READ UNCOMMITTED：读未提交（啥用没用，并且PGSQL没有，提供了只是为了完整性） READ COMMITTED：读已提交，可以解决脏读（PGSQL默认隔离级别） REPEATABLE READ：可重复读，可以解决脏读和不可重复读（MySQL默认是这个隔离级别，PGSQL也提供了，但是设置为可重复读，效果还是串行化） SERIALIZABLE：串行化，啥都能解决（锁，效率慢） PGSQL在老版本中，只有两个隔离级别，读已提交和串行化。在PGSQL中就不存在脏读问题。 11.2 MVCC 首先要清楚，为啥要有MVCC。 如果一个数据库，频繁的进行读写操作，为了保证安全，采用锁的机制。但是如果采用锁机制，如果一些事务在写数据，另外一个事务就无法读数据。会造成读写之间相互阻塞。 大多数的数据库都会采用一个机制 多版本并发控制 MVCC 来解决这个问题。 比如你要查询一行数据，但是这行数据正在被修改，事务还没提交，如果此时对这行数据加锁，会导致其他的读操作阻塞，需要等待。如果采用PostgreSQL，他的内部会针对这一行数据保存多个版本，如果数据正在被写入，包就保存之前的数据版本。让读操作去查询之前的版本，不需要阻塞。等写操作的事务提交了，读操作才能查看到最新的数据。 这几个及时可以确保 读写操作没有冲突 ，这个就是MVCC的主要特点。 写写操作，和MVCC没关系，那个就是加锁的方式！ Ps：这里的MVCC是基于 读已提交 去聊的，如果是串行化，那就读不到了。 在操作之前，先了解一下PGSQL中，每张表都会自带两个字段 xmin：给当前事务分配的数据版本。如果有其他事务做了写操作，并且提交事务了，就给xmin分配新的版本。 xmax：当前事务没有存在新版本，xmax就是0。如果有其他事务做了写操作，未提交事务，将写操作的版本放到xmax中。提交事务后，xmax会分配到xmin中，然后xmax归0。 基于上图的操作查看一波效果 事务A -- 左，事务A --1、开启事务 begin; --2、查询某一行数据, xmin = 630,xmax = 0 select xmin,xmax,* from test where id = 8; --3、每次开启事务后，会分配一个事务ID 事务id=631 select txid_current(); --7、修改id为8的数据，然后在本事务中查询 xmin = 631, xmax = 0 update test set name = '铃铛' where id = 8; select xmin,xmax,* from test where id = 8; --9、提交事务 commit; 事务B -- 右，事务B --4、开启事务 begin; --5、查询某一行数据, xmin = 630,xmax = 0 select xmin,xmax,* from test where id = 8; --6、每次开启事务后，会分配一个事务ID 事务id=632 select txid_current(); --8、事务A修改完，事务B再查询 xmin = 630 xmax = 631 select xmin,xmax,* from test where id = 8; --10、事务A提交后，事务B再查询 xmin = 631 xmax = 0 select xmin,xmax,* from test where id = 8; 十二、锁 PostgreSQL中主要有两种锁，一个表锁一个行锁 PostgreSQL中也提供了页锁，咨询锁，But，这个不需要关注，他是为了锁的完整性 12.1 表锁 表锁显而易见，就是锁住整张表。表锁也分为很多中模式。 表锁的模式很多，其中最核心的两个： ACCESS SHARE：共享锁（读锁），读读操作不阻塞，但是不允许出现写操作并行 ACCESS EXCLUSIVE：互斥锁（写锁），无论什么操作进来，都阻塞。 具体的可以查看官网文档：http://postgres.cn/docs/12/explicit-locking.html 表锁的实现： 先查看一波语法 就是基于LOCK开启表锁，指定表的名字name，其次在MODE中指定锁的模式，NOWAIT可以指定是否在没有拿到锁时，一致等待。 -- 111号连接 -- 基于互斥锁，锁住test表 -- 先开启事务 begin; -- 基于默认的ACCESS EXCLUSIVE锁住test表 lock test in ACCESS SHARE mode; -- 操作 select * from test; -- 提交事务，锁释放 commit; 当111号连接基于事务开启后，锁住当前表之后，如果使用默认的ACCESS EXCLUSIVE，其他连接操作表时，会直接阻塞住。 如果111号是基于ACCESS SHARE共享锁时，其他线程查询当前表是不会锁住得 12.2 行锁 PostgreSQL的行锁和MySQL的基本是一模一样的，基于select for update就可以指定行锁。 MySQL中有一个概念，for update时，如果select的查询没有命中索引，可能会锁表。 PostgerSQL有个特点，一般情况，在select的查询没有命中索引时，他不一定会锁表，依然会实现行锁。 PostgreSQL的行锁，就玩俩，一个for update，一个for share。 在开启事务之后，直接执行select * from table where 条件 for update; -- 先开启事务 begin; -- 基于for update 锁住id为3的数据 select * from test where id = 3 for update; update test set name = 'v1' where id = 3; -- 提交事务，锁释放 commit; 其他的连接要锁住当前行，会阻塞住。 十三、备份&amp;恢复 防止数据丢失的第一道防线就是备份。数据丢失有的是硬件损坏，还有人为的误删之类的，也有BUG的原因导致误删数据。 正常备份和恢复，如果公司有DBA，一般咱们不用参与，BUT，学的Java，啥都得会点~~ 在PostgreSQL中，有三种备份方式： SQL备份（逻辑备份） ：其实就是利用数据库自带的类似dump的命令，或者是你用图形化界面执行导入导出时，底层就是基于这个dump命令实现的。备份出来一份sql文件，谁需要就复制给谁。 优点：简单，方便操作，有手就行，还挺可靠。 缺点：数据数据量比较大，这种方式巨慢，可能导出一天，都无法导出完所有数据。 文件系统备份（物理备份） ：其实就是找到当前数据库，数据文件在磁盘存储的位置，将数据文件直接复制一份或多份，存储在不同的物理机上，即便物理机爆炸一个，还有其他物理机。 优点：相比逻辑备份，恢复的速度快。 缺点：在备份数据时，可能数据还正在写入，一定程度上会丢失数据。 在恢复数据时，也需要注意数据库的版本和环境必须保持高度的一致。如果是线上正在运行的数据库，这种复制的方式无法在生产环境实现。 如果说要做数据的迁移，这种方式还不错滴。 归档备份：（也属于物理备份） 先了解几个概念，在PostgreSQL有多个子进程来辅助一些操作 BgWriter进程：BgWriter是将内存中的数据写到磁盘中的一个辅助进程。当向数据库中执行写操作后，数据不会马上持久化到磁盘里。这个主要是为了提升性能。BgWriter会周期性的将内存中的数据写入到磁盘。但是这个周期时间，长了不行，短了也不行。 如果快了，IO操作频繁，效率慢。 如果慢了，有查询操作需要内存中的数据时，需要BgWriter现把数据从内存写到磁盘中，再提供给查询操作作为返回结果。会导致查询操作效率变低。 考虑一个问题： 事务提交了，数据没落到磁盘，这时，服务器宕机了怎么办？ WalWriter进程：WAL就是write ahead log的缩写，说人话就是预写日志（redo log）。其实数据还在内存中时，其实已经写入到WAL日志中一份，这样一来，即便BgWriter进程没写入到磁盘中时，数据也不会存在丢失的问题。 WAL能单独做备份么？单独不行！ 但是WAL日志有个问题，这个日志会循环使用，WAL日志有大小的线程，只能保存指定时间的日志信息，如果超过了，会覆盖之前的日志。 PgArch进程：WAL日志会循环使用，数据会丢失。没关系，还有一个归档的进程，会在切换wal日志前，将WAL日志备份出来。PostgreSQL也提供了一个全量备份的操作。可以根据WAL日志，选择一个事件点，进行恢复。 查看一波WAL日志： 这些就是归档日志 wal日志的名称，是三块内容组成， 每8个字符分成一组，用16进制标识的 00000001 00000000 0000000A 时间线 逻辑id 物理id 查询当前库用的是哪个wal日志 -- 查看当前使用的wal日志 查询到的lsn：0/47233270 select pg_current_wal_lsn(); -- 基于lsn查询具体的wal日志名称 000000010000000000000047 select pg_walfile_name('0/47233270'); 归档默认不是开启的，需要手动开启归档操作，才能保证wal日志的完整性 修改postgresql.conf文件 # 开启wal日志的内容，注释去掉即可 wal_level = replica fsync = on # 开启归档操作 archive_mode = on # 修改一小下命令，修改存放归档日志的路径 archive_command = 'test ! -f /archive/%f &amp;&amp; cp %p /archive/%f' 修改完上述配置文件后，记得重启postgreSQL进程，才会生效！！！！ 归档操作执行时，需要保证/archive存在，并且postgres用户有权限进行w操作 构建/archive路径 # postgres没有权限在/目录下构建目录 # 切换到root，构建目录，将目录的拥有者更改为postgres mkdir /archive chown -R postgres. archive 在当前库中做大量写操作，接入到wal日志，重置切换wal日志，再查看归档情况 发现，将当前的正在使用的wal日志和最新的上一个wal日志归档过来了，但是之前的没归档，不要慌，后期备份时，会执行命令，这个命令会直接要求wal日志立即归档，然后最全量备份。 13.1 逻辑备份&amp;恢复 PostgreSQL提供了pg_dump以及pg_dumpall的命令来实现逻辑备份。 这两命令差不多，看名字猜！ pg_dump这种备份，不会造成用户对数据的操作出现阻塞。 数据库不是很大的时候，pg_dump也不是不成！ 查看一波命令： 这个命令从三块去看：http://postgres.cn/docs/12/app-pgdump.html 连接的信息，指定连接哪个库，用哪个用户~ option的信息有就点多，查看官网。 备份的数据库！ 操作一波。 备份老郑库中的全部数据。 删除当前laozheng库中的表等信息，然后恢复数据 除此之外，也可以通过图形化界面备份，在库的位置点击备份就成，导出一个文本文件。 13.2 物理备份（归档+物理） 这里需要基于前面的文件系统的备份和归档备份实现最终的操作 单独使用文件系统的方式，不推荐毕竟数据会丢失。 这里直接上PostgreSQL提供的pg_basebackup命令来实现。 pg_basebackup会做两个事情、 会将内存中的脏数据落到磁盘中，然后将数据全部备份 会将wal日志直接做归档，然后将归档也备走。 查看一波pg_basebackup命令 先准备一个pg_basebackup的备份命令 # -D 指定备份文件的存储位置 # -Ft 备份文件打个包 # -Pv 输出备份的详细信息 # -U 用户名（要拥有备份的权限） # -h ip地址 -p 端口号 # -R 复制写配置文件 pg_basebackup -D /pg_basebackup -Ft -Pv -Upostgres -h 192.168.11.32 -p 5432 -R 准备测试，走你~ 提前准备出/pg_basebackup目录。记得将拥有者赋予postgres用户 mkdir /pg_basebackup chown -R postgres. /pg_basebackup/ 给postgres用户提供replication的权限，修改pg_hba.conf，记得重启生效 执行备份 pg_basebackup -D /pg_basebackup -Ft -Pv -Upostgres -h 192.168.11.32 -p 5432 -R 需要输入postgres的密码，这里可以设置，重新备份。 执行备份 13.3 物理恢复（归档+物理） 模拟数据库崩盘，先停止postgresql服务，然后直接删掉data目录下的全部内容 将之前备份的两个文件准备好，一个base.tar，一个pg_wal.tar 第一步：将base.tar中的内容，全部解压到 12/data 目录下 第二步：将pg_wal.tar中的内容，全部解压到 /archive 目录下 第三步：在postgresql.auto.conf文件中，指定归档文件的存储位置，以及恢复数据的方式 第四步：启动postgresql服务 systemctl start postgresql-12 第五步：启动后，发现查询没问题，但是执行写操作时，出错，不让写。需要执行一个函数，取消这种恢复数据后的状态，才允许正常的执行写操作。 select pg_wal_replay_resume(); 13.4 物理备份&amp;恢复（PITR-Point in time Recovery） 模拟场景 场景：每天凌晨02:00，开始做全备（PBK），到了第二天，如果有人14:00分将数据做了误删，希望将数据恢复到14:00分误删之前的状态？ 1、恢复全备数据，使用PBK的全备数据恢复到凌晨02:00的数据。（数据会丢失很多） 2、归档恢复：备份中的归档，有02:00~14:00之间的额数据信息，可以基于归档日志将数据恢复到指定的事务id或者是指定时间点，从而实现数据的完整恢复。 准备场景和具体操作 1、构建一张t3表查询一些数据 -- 构建一张表 create table t3 (id int); insert into t3 values (1); insert into t3 values (11); 2、模拟凌晨2点开始做全备操作 pg_basebackup -D /pg_basebackup -Ft -Pv -Upostgres -h 192.168.11.32 -p 5432 -R 3、再次做一些写操作，然后误删数据 -- 凌晨2点已经全备完毕 -- 模拟第二天操作 insert into t3 values (111); insert into t3 values (1111); -- 误删操作 2023年3月20日20:13:26 delete from t3; 4、恢复数据（确认有归档日志） 将当前服务的数据全部干掉，按照之前的全备恢复的套路先走着 然后将全备的内容中的base.tar扔data目录下，归档日志也扔到/archive位置。 5、查看归档日志，找到指定的事务id 查看归档日志，需要基于postgresql提供的一个命令 # 如果命令未找到，说明两种情况，要么没有这个可执行文件，要么是文件在，没设置环境变量 # 咱们这是后者 pg_waldump # 也可以采用全路径的方式 /usr/pgsql-12/bin/pg_waldump 6、修改data目录下的恢复数据的方式 修改postgresql.auto.conf文件 将之前的最大恢复，更换为指定的事务id恢复 基于提供的配置例子，如何指定事务id 修改postgresql.auto.conf文件指定好事务ID 7、启动postgreSQL服务，查看是否恢复到指定事务ID 8、记得执行会后的函数，避免无法执行写操作 select pg_wal_replay_resume(); 十四、数据迁移 PostgreSQL做数据迁移的插件非常多，可以从MySQL迁移到PostgreSQL也可以基于其他数据源迁移到PostgreSQL 这种迁移的插件很多，这里只说一个，pgloader（巨方便） 以MySQL数据迁移到PostgreSQL为例，分为几个操作： 1、准备MySQL服务（防火墙问题，远程连接问题，权限问题） 准备了一个sms_platform的库，里面大概有26W条左右的数据 2、准备PostgreSQL的服务（使用当前一直玩的PostgreSQL） 3、安装pgloader pgloader可以安装在任何位置，比如安装在MySQL所在服务，或者PostgreSQL所在服务，再或者一个独立的服务都可以 我就在PostgreSQL所在服务安装 # 用root用户下载 yum -y install pgloader 4、准备pgloader需要的脚本文件 官方文档： https://pgloader.readthedocs.io/en/latest/ 记住，PostgreSQL的数据库需要提前构建好才可以！！！！ 5、执行脚本，完成数据迁移 先确认pgloader命令可以使用 执行脚本： pgloader 刚刚写好的脚本文件 十五、主从操作 PostgreSQL自身只支持简单的主从，没有主从自动切换，仿照类似Nginx的效果一样，采用keepalived的形式，在主节点宕机后，通过脚本的执行完成主从切换。 15.1 主从实现（异步流复制） 操作方式类似与之前的备份和恢复 **1、准备环境：**准备两台虚拟机，完成上述的环境准备 角色 IP 端口 Master 192.168.11.66 5432 Standby 192.168.11.67 5432 修改好ip，安装好postgresql服务 2、给主准备一些数据 create table t1 (id int); insert into t1 values (111); select * from t1; 3、配置主节点信息（主从都配置，因为后面会有主从切换的操作） 修改 pg_hba.conf 文件 修改 postgresql.conf 文件 提前构建好归档日志和备份目录，并且设置好拥有者 重启PostgreSQL服务 systemctl restart postgresql-12 4、从节点加入到主节点 关闭从节点服务 systemctl stop postgresql-12 删除从节点数据（删除data目录） rm -rf ~/12/data/* 基于pbk去主节点备份数据 # 确认好备份的路径，还有主节点的ip pg_basebackup -D /pgbasebackup -Ft -Pv -Upostgres -h 192.168.11.66 -p 5432 -R 恢复数据操作，解压tar包 cd /pgbasebackuo tar -xf base.tar -C ~/12/data tar -xf pg_wal.tar -C /archive 修改postgresql.auto.conf文件 # 确认有这两个配置，一般第一个需要手写，第二个会自动生成 restore_command = 'cp /archive/%f %p' primary_conninfo = 'user=postgres password=postgres host=192.168.11.66 port=5432 sslmode=prefer sslcompression=0 gssencmode=prefer krbsrvname=postgres target_session_attrs=any' 修改standby.signal文件，开启从节点备份模式 # 开启从节点备份 standby_mode = 'on' 启动从节点服务 systemctl restart postgresql-12 查看主从信息 查看从节点是否有t1表 主节点添加一行数据，从节点再查询，可以看到最新的数据 从节点无法完成写操作，他是只读模式 主节点查看从节点信息 select * from pg_stat_replication 从节点查看主节点信息 select * from pg_stat_wal_receiver 15.2 主从切换（不这么玩） 其实主从的本质就是从节点去主节点不停的备份新的数据。 配置文件的系统其实就是两个： standby.signal文件，这个是从节点开启备份 postgresql.auto.conf文件，这个从节点指定主节点的地址信息 切换就是原主追加上述配置，原从删除上述配追 1、主从节点全部stop停止：……………… 2、原从删除上述配置：………… 3、原从新主启动服务：……… 4、原主新从去原从新主备份一次数据：pg_basebackup操作，同时做解压，然后修改postgresql.conf文件以及standby.signal配置文件 5、启动原主新从查看信息 15.3 主从故障切换 默认情况下，这里的主从备份是异步的，导致一个问题，如果主节点写入的数据还没有备份到从节点，主节点忽然宕机了，导致后面如果基于上述方式实现主从切换，数据可能丢失。 PGSQL在9.5版本后提供了一个pg_rewind的操作，基于归档日志帮咱们做一个比对，比对归档日志，是否有时间差冲突。 实现操作： 1、rewind需要开启一项配置才可以使用 修改postgresql.conf中的 wal_log_hints = ‘on’ 2、为了可以更方便的使用rewind，需要设置一下 /usr/pgsql-12/bin/ 的环境变量 vi /etc/profile 追加信息 export PATH=/usr/pgsql-12/bin/:$PATH source /etc/profile 3、模拟主库宕机，直接对主库关机 4、从节点切换为主节点 # 因为他会去找$PGDATA，我没配置，就基于-D指定一下PGSQL的data目录 pg_ctl promote -D ~/12/data/ 5、将原主节点开机，执行命令，搞定归档日志的同步 启动虚拟机 停止PGSQL服务 pg_ctl stop -D ~/12/data 基于pg_rewind加入到集群 pg_rewind -D ~/12/data/ --source-server='host=192.168.11.66 user=postgres password=postgres' 如果上述命令失败，需要启动再关闭PGSQL，并且在执行，完成归档日志的同步 pg_ctl start -D ~/12/data pg_ctl stop -D ~/12/data pg_rewind -D ~/12/data/ --source-server='host=192.168.11.66 user=postgres password=postgres' 6、修改新从节点的配置，然后启动 构建standby.signal standby_mode = 'on' 修改postgresql.auto.conf文件 # 注意ip地址 primary_conninfo = 'user=postgres password=postgres host=192.168.11.66 port=5432 sslmode=prefer sslcompression=0 gssencmode=prefer krbsrvname=postgres target_session_attrs=any' restore_command = 'cp /archive/%f %p' 启动新的从节点 pg_ctl start -D ~/12/data/ ","link":"https://tianxiawuhao.github.io/uSSvVMkpS/"},{"title":"PostgreSQL概述二","content":"七、数据类型 PGSQL支持的类型特别丰富，大多数的类型和MySQL都有对应的关系 名称 说明 对比MySQL 布尔类型 boolean，标准的布尔类型，只能存储true，false MySQL中虽然没有对应的boolean，但是有替换的类型，数值的tinyint类型，和PGSQL的boolean都是占1个字节。 整型 smallint（2字节），integer（4字节），bigint（8字节） 跟MySQL没啥区别。 浮点型 decimal，numeric（和decimal一样一样的，精准浮点型），real（float），double precision（double），money（货币类型） 和MySQL基本也没区别，MySQL支持float，double，decimal。MySQL没有这个货币类型。 字符串类型 varchar(n)（character varying），char(n)（character），text 这里和MySQL基本没区别。PGSQL存储的varchar类型，可以存储一个G。MySQL好像存储64kb（应该是）。 日期类型 date（年月日），time（时分秒），timestamp（年月日时分秒）（time和timestamp可以设置时区） 没啥说的，和MySQL基本没区别。MySQL有个datetime。 二进制类型 bytea-存储二进制类型 MySQL也支持，MySQL中是blob 位图类型 bit(n)（定长位图），bit varying(n)（可变长度位图） 就是存储0，1。MySQL也有，只是这个类型用的不多。 枚举类型 enum，跟Java的enum一样 MySQL也支持。 几何类型 点，直线，线段，圆………… MySQL没有，但是一般开发也用不到 数组类型 在类型后，追加[]，代表存储数组 MySQL没有~~~ JSON类型 json（存储JSON数据的文本），jsonb（存储JSON二进制） 可以存储JSON，MySQL8.x也支持 ip类型 cidr（存储ip地址） MySQL也不支持~ 八、PostgreSQL基本操作&amp;数据类型 8.1 单引号和双引号 在PGSQL中，写SQL语句时，单引号用来标识实际的值。双引号用来标识一个关键字，比如表名，字段名。 -- 单引号写具体的值，双引号类似MySQL的``标记，用来填充关键字 -- 下面的葡萄牙会报错，因为葡萄牙不是关键字 select 1.414,'卡塔尔',&quot;葡萄牙&quot;; 8.2 数据类型转换 第一种方式：只需要在值的前面，添加上具体的数据类型即可 -- 将字符串转成位图类型 select bit '010101010101001'; 第二种方式：也可以在具体值的后面，添加上 ::类型 ，来指定 -- 数据类型 select '2011-11-11'::date; select '101010101001'::bit(20); select '13'::int; 第三种方式：使用CAST函数 -- 类型转换的完整写法 select CAST(varchar '100' as int); 8.3 布尔类型 布尔类型简单的丫批，可以存储三个值，true，false，null -- 布尔类型的约束没有那么强，true，false大小写随意，他会给你转，同时yes，no这种他也认识，但是需要转换 select true,false,'yes'::boolean,boolean 'no',True,FaLse,NULL::boolean; boolean类型在做and和or的逻辑操作时，结果 字段A 字段B a and b a or b true true true true true false false true true NULL NULL true false false false false false NULL false NULL 8.4 数值类型 8.4.1 整型 整型比较简单，主要就是三个： smallint、int2：2字节 integer、int、int4：4字节 bigint、int8：8字节 正常没啥事就integer，如果要存主键，比如雪花算法，那就bigint。空间要节约，根据情况smallint 8.4.2 浮点型 浮点类型就关注2个（其实是一个） decimal(n,m)：本质就是numeric，PGSQL会帮你转换 numeric(n,m)：PGSQL本质的浮点类型 针对浮点类型的数据，就使用 numeric 8.4.3 序列 MySQL中的主键自增，是基于auto_increment去实现。MySQL里没有序列的对象。 PGSQL和Oracle十分相似，支持序列：sequence。 PGSQL可没有auto_increment。 序列的正常构建方式： create sequence laozheng.table_id_seq; -- 查询下一个值 select nextval('laozheng.table_id_seq'); -- 查询当前值 select currval('laozheng.table_id_seq'); 默认情况下，seqeunce的起始值是0，每次nextval递增1，最大值9223372036854775807 告诉缓存，插入的数据比较多，可以指定告诉缓存，一次性计算出20个后续的值，nextval时，就不可以不去计算，直接去高速缓存拿值，效率会有一内内的提升。 序列大多数的应用，是用作表的主键自增效果。 -- 表自增 create table laozheng.xxx( id int8 default nextval('laozheng.table_id_seq'), name varchar(16) ); insert into laozheng.xxx (name) values ('xxx'); select * from laozheng.xxx; 上面这种写法没有问题，但是很不爽~很麻烦。 PGSQL提供了序列的数据类型，可以在声明表结构时，直接指定序列的类型即可。 bigserial相当于给bigint类型设置了序列实现自增。 smallserial serial bigserial -- 表自增（爽） create table laozheng.yyy( id bigserial, name varchar(16) ); insert into laozheng.yyy (name) values ('yyy'); 在drop表之后，序列不会被删除，但是序列会变为不可用的状态。 因为序列在使用serial去构建时，会绑定到指定表的指定列上。 如果是单独构建序列，再构建表，使用传统方式实现，序列和表就是相对独立的。 8.4.4 数值的常见操作 针对数值咱们可以实现加减乘除取余这5个操作 还有其他的操作方式 操作符 描述 示例 结果 ^ 幂 2 ^ 3 8 |/ 平方根 |/ 36 6 @ 绝对值 @ -5 5 &amp; 与 31 &amp; 16 16 | 或 31|32 63 &lt;&lt; 左移 1&lt;&lt;1 2 &gt;&gt; 右移 16&gt;&gt;1 8 数值操作也提供了一些函数，比如pi()，round(数值，位数)，floor()，ceil() 8.5 字符串类型 字符串类型用的是最多的一种，在PGSQL里，主要支持三种： character（就是MySQL的char类型），定长字符串。（最大可以存储1G） character varying（varchar），可变长度的字符串。（最大可以存储1G） text（跟MySQL异常）长度特别长的字符串。 操作没什么说的，但是字符串常见的函数特别多。 字符串的拼接一要要使用||来拼接。 其他的函数，可以查看 http://www.postgres.cn/docs/12/functions-string.html 8.6 日期类型 在PGSQL中，核心的时间类型，就三个。 timestamp（时间戳，覆盖 年月日时分秒） date（年月日） time（时分秒） 在PGSQL中，声明时间的方式。 只需要使用字符串正常的编写 yyyy-MM-dd HH:mm:ss 就可以转换为时间类型。 直接在字符串位置使用之前讲到的数据类型转换就可以了。 当前系统时间 ： 可以使用now作为当前系统时间（没有时区的概念） select timestamp 'now'; -- 直接查询now，没有时区的概念 select time with time zone 'now' at time zone '08:00:00' 也可以使用current_timestamp的方式获取（推荐，默认东八区） 日期类型的运算 正常对date类型做+，-操作，默认单位就是天~ date + time = timestamp~~~ select date '2011-11-11' + time '12:12:12' ; 可以针对timestamp使用interval的方式进行 +，-操作，在查询以时间范围为条件的内容时，可以使用 select timestamp '2011-11-11 12:12:12' + interval '1day' + interval '1minute' + interval '1month'; 8.7 枚举类型 枚举类型MySQL也支持，只是没怎么用，PGSQL同样支持这种数据类型 可以声明枚举类型作为表中的字段类型，这样可以无形的给表字段追加诡异的规范。 -- 声明一个星期的枚举，值自然只有周一~周日。 create type week as enum ('Mon','Tues','Sun'); -- 声明一张表，表中的某个字段的类型是上面声明的枚举。 drop table test; create table test( id bigserial , weekday week ); insert into test (weekday) values ('Mon'); insert into test (weekday) values ('Fri'); 8.8 IP类型 PGSQL支持IP类型的存储，支持IPv4，IPv6这种，甚至Mac这种诡异类型也支持 这种IP类型，可以在存储IP时，帮助做校验，其次也可以针对IP做范围查找。 IP校验的效果 IP也支持范围查找。 8.9 JSON&amp;JSONB类型 JSON在MySQL8.x中也做了支持，但是MySQL支持的不好，因为JSON类型做查询时，基本无法给JSON字段做索引。 PGSQL支持JSON类型以及JSONB类型。 JSON和JSONB的使用基本没区别。 撇去JSON类型，本质上JSON格式就是一个字符串，比如MySQL5.7不支持JSON的情况的下，使用text也可以，但是字符串类型无法校验JSON的格式，其次单独的字符串没有办法只获取JSON中某个key对应的value。 JSON和JSONB的区别： JSON类型无法构建索引，JSONB类型可以创建索引。 JSON类型的数据中多余的空格会被存储下来。JSONB会自动取消多余的空格。 JSON类型甚至可以存储重复的key，以最后一个为准。JSONB不会保留多余的重复key（保留最后一个）。 JSON会保留存储时key的顺序，JSONB不会保留原有顺序。 JSON中key对应的value的数据类型 JSON PGSQL String text number numeric boolean boolean null (none) [ {&quot;name&quot;: &quot;张三&quot;}, {&quot;name&quot;: { &quot;info&quot;: &quot;xxx&quot; }} ] 操作JSON： 上述的四种JSON存储的类型： select '9'::JSON,'null'::JSON,'&quot;laozheng&quot;'::JSON,'true'::json; select '9'::JSONB,'null'::JSONB,'&quot;laozheng&quot;'::JSONB,'true'::JSONB; JSON数组 select '[9,true,null,&quot;我是字符串&quot;]'::JSON; JSON对象 select '{&quot;name&quot;: &quot;张三&quot;,&quot;age&quot;: 23,&quot;birthday&quot;: &quot;2011-11-11&quot;,&quot;gender&quot;: null}'::json; select '{&quot;name&quot;: &quot;张三&quot;,&quot;age&quot;: 23,&quot;birthday&quot;: &quot;2011-11-11&quot;,&quot;gender&quot;: null}'::jsonb; 构建表存储JSON create table test( id bigserial, info json, infob jsonb ); insert into test (info,infob) values ('{&quot;name&quot;: &quot;张三&quot; ,&quot;age&quot;: 23,&quot;birthday&quot;: &quot;2011-11-11&quot;,&quot;gender&quot;: null}', '{&quot;name&quot;: &quot;张三&quot; ,&quot;age&quot;: 23,&quot;birthday&quot;: &quot;2011-11-11&quot;,&quot;gender&quot;: null}') select * from test; 构建索引的效果 create index json_index on test(info); create index jsonb_index on test(infob); JSON还支持很多函数。可以直接查看 http://www.postgres.cn/docs/12/functions-json.html 函数太多了，不分析了。 8.10 复合类型 复合类型就好像Java中的一个对象，Java中有一个User，User和表做了一个映射，User中有个人信息对象。可以基于符合类型对映射上个人信息。 public class User{ private Integer id; private Info info; } class Info{ private String name; private Integer age; } 按照上面的情况，将Info构建成一个复合类型 -- 构建复合类型，映射上Info create type info_type as (name varchar(32),age int); -- 构建表，映射User create table tb_user( id serial, info info_type ); -- 添加数据 insert into tb_user (info) values (('张三',23)); insert into tb_user (info) values (('露丝',233)); insert into tb_user (info) values (('jack',33)); insert into tb_user (info) values (('李四',24)); select * from tb_user; 8.11 数组类型 数组还是要依赖其他类型，比如在设置住址，住址可能有多个住址，可以采用数组类型去修饰字符串。 PGSQL中，指定数组的方式就是[]，可以指定一维数组，也支持二维甚至更多维数组。 构建数组的方式： drop table test; create table test( id serial, col1 int[], col2 int[2], col3 int[][] ); -- 构建表指定数组长度后，并不是说数组内容只有2的长度，可以插入更多数据 -- 甚至在你插入数据，如果将二维数组结构的数组扔到一维数组上，也可以存储。 -- 数组编写方式 select '{{how,are},{are,you}}'::varchar[]; select array[[1,2],[3,4]]; insert into test (col1,col2,col3) values ('{1,2,3}','{4,5,6}','{7,8,9}'); insert into test (col1,col2,col3) values ('{1,2,3}','{4,5,6}',array[[1,2],[3,4]]); insert into test (col1,col2,col3) values ('{1,2,3}','{4,5,6}','{{1,2},{3,4}}'); select * from test; 如果现在要存储字符串数组，如果存储的数组中有双引号怎么办，有大括号怎么办。 -- 如果存储的数组中的值，有单引号怎么办？ -- 使用两个单引号，作为一个单引号使用 select '{''how''}'::varchar[]; -- 如果存储的数组中的值，有逗号怎么办？(PGSQL中的数组索引从1开始算，写0也是从1开始算。) -- 用双引号将数组的数据包起来~ select ('{&quot;how,are&quot;}'::varchar[])[2]; -- 如果存储的数组中的值，有双引号怎么办？ -- 如果要添加双引号，记得转义。 select ('{&quot;\\&quot;how\\&quot;,are&quot;}'::varchar[])[1]; 数组的比较方式 -- 包含 select array[1,2] @&gt; array[1]; -- 被包含 select array[1,2] &lt;@ array[1,2,4]; -- 是否有相同元素 select array[2,4,4,45,1] &amp;&amp; array[1]; 九、表 表的构建语句，基本都会。 核心在于构建表时，要指定上一些约束。 9.1 约束 9.1.1 主键 -- 主键约束 drop table test; create table test( id bigserial primary key , name varchar(32) ); 9.1.2 非空 -- 非空约束 drop table test; create table test( id bigserial primary key , name varchar(32) not null ); 9.1.3 唯一 drop table test; create table test( id bigserial primary key , name varchar(32) not null, id_card varchar(32) unique ); insert into test (name,id_card) values ('张三','333333333333333333'); insert into test (name,id_card) values ('李四','333333333333333333'); insert into test (name,id_card) values (NULL,'433333333333333333'); 9.1.4 检查 -- 检查约束 -- 价格的表，price，discount_price drop table test; create table test( id bigserial primary key, name varchar(32) not null, price numeric check(price &gt; 0), discount_price numeric check(discount_price &gt; 0), check(price &gt;= discount_price) ); insert into test (name,price,discount_price) values ('粽子',122,12); 9.1.5 外键（不玩） 9.1.6 默认值 一般公司内，要求表中除了主键和业务字段之外，必须要有5个字段 created，create_id，updated，update_id，is_delete -- 默认值 create table test( id bigserial primary key, created timestamp default current_timestamp ); 9.2 触发器 触发器Trigger，是由事件出发的一种存储过程 当对标进行insert，update，delete，truncate操作时，会触发表的Trigger（看触发器的创建时指定的事件） 构建两张表，学生信息表，学生分数表。 在删除学生信息的同时，自动删除学生的分数。 先构建表信息，填充数据 create table student( id int, name varchar(32) ); create table score( id int, student_id int, math_score numeric, english_score numeric, chinese_score numeric ); insert into student (id,name) values (1,'张三'); insert into student (id,name) values (2,'李四'); insert into score (id,student_id,math_score,english_score,chinese_score) values (1,1,66,66,66); insert into score (id,student_id,math_score,english_score,chinese_score) values (2,2,55,55,55); select * from student; select * from score; 为了完成级联删除的操作，需要编写pl/sql。 先查看一下PGSQL支持的plsql，查看一下PGSQL的plsql语法 [ &lt;&lt;label&gt;&gt; ] [ DECLARE declarations ] BEGIN statements END [ label ]; 构建一个存储函数，测试一下plsql -- 优先玩一下plsql -- $$可以理解为是一种特殊的单引号，避免你在declare，begin，end中使用单引号时，出现问题， -- 需要在编写后，在$$之后添加上当前内容的语言。 create function test() returns int as $$ declare money int := 10; begin return money; end; $$ language plpgsql; select test(); 在简单了解了一下plpgsql的语法后，编写一个触发器函数。 触发器函数允许使用一些特殊变量 NEW 数据类型是RECORD；该变量为行级触发器中的INSERT/UPDATE操作保持新数据行。在语句级别的触发器以及DELETE操作，这个变量是null。 OLD 数据类型是RECORD；该变量为行级触发器中的UPDATE/DELETE操作保持新数据行。在语句级别的触发器以及INSERT操作，这个变量是null。 构建一个删除学生分数的触发器函数。 -- 构建一个删除学生分数的触发器函数。 create function trigger_function_delete_student_score() returns trigger as $$ begin delete from score where student_id = old.id; return old; end; $$ language plpgsql; 开始构建触发器，在学生信息表删除时，执行前面声明的触发器函数 CREATE [ OR REPLACE ] [ CONSTRAINT ] TRIGGER name { BEFORE | AFTER | INSTEAD OF } { event [ OR ... ] } ON table_name [ FROM referenced_table_name ] [ NOT DEFERRABLE | [ DEFERRABLE ] [ INITIALLY IMMEDIATE | INITIALLY DEFERRED ] ] [ REFERENCING { { OLD | NEW } TABLE [ AS ] transition_relation_name } [ ... ] ] [ FOR [ EACH ] { ROW | STATEMENT } ] [ WHEN ( condition ) ] EXECUTE { FUNCTION | PROCEDURE } function_name ( arguments ) where event can be one of: INSERT UPDATE [ OF column_name [, ... ] ] DELETE TRUNCATE 当 CONSTRAINT选项被指定，这个命令会创建一个 约束触发器 。这和一个常规触发器相同，不过触发该触发器的时机可以使用SET CONSTRAINTS调整。约束触发器必须是表上的 AFTER ROW触发器。它们可以在导致触发器事件的语句末尾被引发或者在包含该语句的事务末尾被引发。在后一种情况中，它们被称作是被 延迟 。一个待处理的延迟触发器的引发也可以使用 SET CONSTRAINTS立即强制发生。当约束触发器实现的约束被违背时，约束触发器应该抛出一个异常。 描绘一波~~ -- 编写触发器，指定在删除某一行学生信息时，触发当前触发器，执行触发器函数 create trigger trigger_student after delete on student for each row execute function trigger_function_delete_student_score(); -- 测试效果 select * from student; select * from score; delete from student where id = 1; 9.3 表空间（问题填坑） 在存储数据时，数据肯定要落到磁盘上，基于构建的tablespace，指定数据存放在磁盘上的物理地址。 如果没有自己设计tablespace，PGSQL会自动指定一个位置作为默认的存储点。 可以通过一个函数，查看表的物理数据存放在了哪个磁盘路径下。 -- 查询表存储的物理地址 select pg_relation_filepath('student'); 这个位置是在$PG_DATA后的存放地址 $PG_DATA == /var/lib/pgsql/12/data/ 41000其实就是存储数据的物理文件 构建表空间，指定数据存放位置 -- 构建表空间,构建表空间需要用户权限是超级管理员，其次需要指定的目录已经存在 create tablespace tp_test location '/var/lib/pgsql/12/tp_test'; 构建数据库，以及表，指定到这个表空间中 其实指定表空间的存储位置后，PGSQL会在$PG_DATA目录下存储一份，同时在咱们构建tablespace时，指定的路径下也存储一份。 这两个绝对路径下的文件都有存储表中的数据信息。 /var/lib/pgsql/12/data/pg_tblspc/41015/PG_12_201909212/41016/41020 /var/lib/pgsql/12/lz_tp_test/PG_12_201909212/41016/41020 进一步会发现，其实在PGSQL的默认目录下，存储的是一个link，连接文件，类似一个快捷方式 9.4 视图 跟MySQL的没啥区别，把一些复杂的操作封装起来，还可以隐藏一些敏感数据。 视图对于用户来说，就是一张真实的表，可以直接基于视图查询一张或者多张表的信息。 视图对于开发来说，就是一条SQL语句。 在PGSQL中，简单（单表）的视图是允许写操作的。 但是强烈不推荐对视图进行写操作，虽然PGSQL默认允许（简单的视图）。 写入的时候，其实修改的是表本身 -- 构建一个简单视图 create view vw_score as (select id,math_score from score); select * from vw_score; update vw_score set math_score = 99 where id = 2; 多表视图 -- 复杂视图(两张表关联) create view vw_student_score as (select stu.id as id ,stu.name as name ,score.math_score from student stu,score score where stu.id = score.student_id); select * from vw_student_score; update vw_student_score set math_score =999 where id = 2; 9.5 索引 9.5.1 索引的基本概念 先了解概念和使用 索引是数据库中快速查询数据的方法。 索引能提升查询效率的同时，也会带来一些问题 增加了存储空间 写操作时，花费的时间比较多 索引可以提升效率，甚至还可以给字段做一些约束 9.5.2 索引的分类 B-Tree索引：最常用的索引。 Hash索引：跟MySQL类似，做等值判断，范围凉凉~ GIN索引：针对字段的多个值的类型，比如数组类型。 9.5.3 创建索引看效果 准备大量测试数据，方便查看索引效果 -- 测试索引效果 create table tb_index( id bigserial primary key, name varchar(64), phone varchar(64)[] ); -- 添加300W条数据测试效果 do $$ declare i int := 0; begin while i &lt; 3000000 loop i = i + 1; insert into tb_index (name,phone) values (md5(random()::text || current_timestamp::text)::uuid,array[random()::varchar(64),random()::varchar(64)]); end loop; end; $$ language plpgsql; 在没有索引的情况下，先基于name做等值查询，看时间，同时看执行计划 -- c0064192-1836-b019-c649-b368c2be31ca select * from tb_index where id = 2222222; select * from tb_index where name = 'c0064192-1836-b019-c649-b368c2be31ca'; explain select * from tb_index where name = 'c0064192-1836-b019-c649-b368c2be31ca'; -- Seq Scan 这个代表全表扫描 -- 时间大致0.3秒左右 在有索引的情况下，再基于name做等值查询，看时间，同时看执行计划 -- name字段构建索引（默认就是b-tree） create index index_tb_index_name on tb_index(name); -- 测试效果 select * from tb_index where name = 'c0064192-1836-b019-c649-b368c2be31ca'; explain select * from tb_index where name = 'c0064192-1836-b019-c649-b368c2be31ca'; -- Index Scan 使用索引 -- 0.1s左右 测试GIN索引效果 在没有索引的情况下，基于phone字段做包含查询 -- phone：{0.6925242730781953,0.8569644964711074} select * from tb_index where phone @&gt; array['0.6925242730781953'::varchar(64)]; explain select * from tb_index where phone @&gt; array['0.6925242730781953'::varchar(64)]; -- Seq Scan 全表扫描 -- 0.5s左右 给phone字段构建GIN索引，在查询 -- 给phone字符串数组类型字段构建一个GIN索引 create index index_tb_index_phone_gin on tb_index using gin(phone); -- 查询 select * from tb_index where phone @&gt; array['0.6925242730781953'::varchar(64)]; explain select * from tb_index where phone @&gt; array['0.6925242730781953'::varchar(64)]; -- Bitmap Index 位图扫描 -- 0.1s以内完成 9.6 物化视图 前面说过普通视图，本质就是一个SQL语句，普通的视图并不会本地磁盘存储任何物理。 每次查询视图都是执行这个SQL。效率有点问题。 物化视图从名字上就可以看出来，必然是要持久化一份数据的。使用套路和视图基本一致。这样一来查询物化视图，就相当于查询一张单独的表。相比之前的普通视图，物化视图就不需要每次都查询复杂SQL，每次查询的都是真实的物理存储地址中的一份数据（表）。 物化视图因为会持久化到本地，完全脱离原来的表结构。 而且物化视图是可以单独设置索引等信息来提升物化视图的查询效率。 But，有好处就有坏处，更新时间不太好把控。 如果更新频繁，对数据库压力也不小。 如果更新不频繁，会造成数据存在延迟问题，实时性就不好了。 如果要更新物化视图，可以采用触发器的形式，当原表中的数据被写后，可以通过触发器执行同步物化视图的操作。或者就基于定时任务去完成物化视图的数据同步。 look 一下语法。 干活！ -- 构建物化视图 create materialized view mv_test as (select id,name,price from test); -- 操作物化视图和操作表的方式没啥区别。 select * from mv_test; -- 操作原表时，对物化视图没任何影响 insert into test values (4,'月饼',50,10); -- 物化视图的添加操作(不允许写物化视图)，会报错 insert into mv_test values (5,'大阅兵',66); 物化视图如何从原表中进行同步操作。 PostgreSQL中，对物化视图的同步，提供了两种方式，一种是全量更新，另一种是增量更新。 全量更新语法，没什么限制，直接执行，全量更新 -- 查询原来物化视图的数据 select * from mv_test; -- 全量更新物化视图 refresh materialized view mv_test; -- 再次查询物化视图的数据 select * from mv_test; 增量更新，增量更新需要一个唯一标识，来判断哪些是增量，同时也会有行数据的版本号约束。 -- 查询原来物化视图的数据 select * from mv_test; -- 增量更新物化视图，因为物化视图没有唯一索引，无法判断出哪些是增量数据 refresh materialized view concurrently mv_test; -- 给物化视图添加唯一索引。 create unique index index_mv_test on mv_test(id); -- 增量更新物化视图 refresh materialized view concurrently mv_test; -- 再次查询物化视图的数据 select * from mv_test; -- 增量更新时，即便是修改数据，物化视图的同步，也会根据一个xmin和xmax的字段做正常的数据同步 update test set name = '汤圆' where id = 5; insert into test values (5,'猪头肉',99,40); select * from test; ","link":"https://tianxiawuhao.github.io/IKbaKSlgE/"},{"title":"PostgreSQL概述一","content":"一、PostgreSQL介绍 PostgreSQL是一个功能强大的 开源 的关系型数据库。底层基于C实现。 PostgreSQL的开源协议和Linux内核版本的开源协议是一样的。。BDS协议，这个协议基本和MIT开源协议一样，说人话，就是你可以对PostgreSQL进行一些封装，然后商业化是收费。 PostgreSQL的名字咋来的。之前叫Ingres，后面为了解决一些ingres中的一些问题，作为后面的ingres，就起名叫postgre。 PostgreSQL版本迭代的速度比较快，现在最新的正式的发布版本，已经到了16.RELEASE。 PGSQL的版本选择一般有两种： 如果为了稳定的运行，推荐使用12.x版本。 如果想体验新特性，推荐使用15.x版本。 PGSQL允许跨版本升级，而且没有什么大问题。 PGSQL社区特别活跃，基本是三个月一发版。意味着很多常见的BUG都可以得到及时的修复。 PGSQL其实在国外使用的比较多，国内暂时还是以MySQL为主。 但是国内很多国产数据库都是基于PGSQL做的二次封装：比如华为GaussDB还有腾讯的Tbase等等。其实很多公司原来玩的Oracle，直接平转到PGSQL。同时国内的很多云产品都支持PGSQL了。 PGSQL因为开源，有很多做数据迁移的工具，可以让你快速的从MySQL，SQLServer，Oracle直接平转到PGSQL中，比如pgloader这样的数据迁移工具。 PGSQL的官方地址：https://www.postgresql.org/ PGSQL的国内社区：http://www.postgres.cn/v2/home 二、PostgreSQL和MySQL的区别 技术没有好坏之分，知识看一下是否符合你的业务，能否解决你的业务需求。其次也要查看社区的活跃度以及更新的频次。 MySQL不支持的几点内容： MySQL的数据类型不够丰富。 MySQL不支持序列概念，Sequence。 使用MySQL时，网上缺少比较好用的插件。 MySQL的性能优化监控工具不是很多，定位问题的成本是比较高。 MySQL的主从复制没有一个官方的同步策略，同步问题难以解决。 MySQL虽然开源，but，不够彻底。 PostgreSQL相对MySQL上述问题的特点： PostgreSQL的数据类型嘎嘎丰富。 PostgreSQL是有序列的概念的。 PostgreSQL的插件特别丰富。 PostgreSQL支持主从复制的同步操作，可以实现数据的0丢失。 PostgreSQL的MVCC实现和MySQL不大一样。PostgreSQL一行数据会存储多个版本。最多可以存储40亿个事务版本。 三、PostgreSQL的安装 咱们只在Linux中安装，不推荐大家在Windows下安装。 Linux的版本尽量使用7.x版本，最好是7.6或者是7.8版本。 去官网找按照的方式 选择好PGSQL的版本，已经Linux的发行版本 拿到命令，麻也不管，直接扔到Linux中运行即可 # 下载PGSQL的rpm包 sudo yum install -y https://download.postgresql.org/pub/repos/yum/reporpms/EL-7-x86_64/pgdg-redhat-repo-latest.noarch.rpm # 安装PGSQL12的软件程序，需要下载，需要等一会，一般不会失败，即便失败，他也会重新帮你找镜像 sudo yum install -y postgresql12-server # 数据库初始化 sudo /usr/pgsql-12/bin/postgresql-12-setup initdb # 设置开启启动项，并设置为开启自行启动 sudo systemctl enable postgresql-12 # 启动PGSQL sudo systemctl start postgresql-12 这种属于Windows下的傻瓜式安装，基本不会出错。如果出错，可能是那些问题： 安装Linux的时候，一定要选择最小安装 你的Linux不能连接外网 Linux中的5432端口，可能被占用了 PostgreSQL不推荐使用root管理，在安装成功postgreSQL后，他默认会给你创建一个用户：postgres 玩PGSQL前，先切换到postgres su postgres 切换到postgres用户后，直接输入psql即可进入到postgreSQL提供的客户端 # 进入命令行 psql # 查看有哪些库，如果是新安装的，有三个库，一个是postgres，template0，template1she 其次不推荐下载Windows版本去玩 如果非要下载：https://sbp.enterprisedb.com/getfile.jsp?fileid=1258242 四、PostgreSQL的配置 要搞两个配置信息，一个关于postgreSQL的远程连接配置以及postgreSQL的日志配置。 PostgreSQL的主要配置放在数据目录下的， postgresql.conf 以及 pg_hba.conf 配置文件 这些配置文件都放在了 # 这个目录下 /var/lib/pgsql/12/data 上图可以看到，postgreSQL的核心文件，都属于postgres用户，操作的时候，尽可能的别用root用户，容易玩出坑，尽可能先切换到postgres用户去玩。 4.1 远程连接配置 PostgreSQL默认情况下不支持远程连接的，这个跟MySQL几乎一样 MySQL给mysql.user追加用户，一般是采用grant的命令去玩。 PostgreSQL要基于配置文件修改，才能制定用户是否可以远程连接。 直接去修改pg_hba.conf配置文件 用户以及对应数据库和连接方式的编写模板 # 第一块 local：代表本地连接，host代表可以指定连接的ADDRESS # 第二块 database编写数据库名，如果写all，代表所有库都可以连接 # 第三块 user编写连接的用户，可以写all，代表所有用户 # 第四块 address代表那些IP地址可以连接 # 第五块 method加密方式，这块不用过多关注，直接md5 # 直接来个痛快的配置吗，允许任意地址的全部用户连接所有数据库 host all all 0.0.0.0/0 md5 为了实现远程连接，除了用户级别的这种配置，还要针对服务级别修改一个配置 服务级别的配置在postgresql.conf 发现默认情况下，PGSQL只允许localhost连接，直接配置为*即可解决问题 记得，为了生效，一定要重启 # postgres密码不管，直接root用户 sudo systemctl restart postgresql-12 4.2 配置数据库的日志 查看postgresql.conf文件 postgreSQL默认情况下，只保存7天的日志，循环覆盖。 # 代表日志是开启的。 logging_collector = on # 日志存放的路径，默认放到当前目录下的log里 log_directory = 'log' # 日志的文件名，默认是postgresql为前缀，星期作为后缀 log_filename = 'postgresql-%a.log' # 默认一周过后，日志文件会被覆盖 log_truncate_on_rotation = on # 一天一个日志文件 log_rotation_age = 1d # 一个日志文件，没有大小限制 log_rotation_size = 0 五、PostgreSQL的基操 只在psql命令行（客户端）下，执行了一次\\l，查看了所有的库信息 可以直接基于psql查看一些信息，也可以基于psql进入到命令行后，再做具体操作 可以直接基于psql去玩 可以数据psql --help，查看psql的命令 可以直接进入到命令行的原因，是psql默认情况下，就是以postgres用户去连接本地的pgsql，所以可以直接进入 下面的图是默认的连接方式 后面都基于psql的命令行（客户端）去进行操作 命令绝对不要去背，需要使用的时候，直接找帮助文档，在psql命令行中，直接注入 \\help，即可查看到数据库级别的一些命令 \\?，可以查看到服务级别的一些命令 5.1 用户操作 构建用户命令巨简单 # 区别就是create user默认有连接权限，create role没有，不过可以基于选项去设置 CREATE USER 名称 [ [ WITH ] 选项 [ ... ] ] create role 名称 [ [ WITH ] 选项 [ ... ] ] 构建一个超级管理员用户 create user root with SUPERUSER PASSWORD 'root'; 退出psql命令行 编写psql命令尝试去用root用户登录 psql -h 192.168.11.32 -p 5432 -U root -W 发现，光有用户不让登录，得让用户有一个数据库，直接构建一个root库 create database root; 可以在不退出psql的前提下，直接切换数据库 也可以退出psql，重新基于psql命令去切换用户以及数据库 如果要修改用户信息，或者删除用户，可以查看 # 修改用户，直接基于ALTER命令操作 # 删除用户，直接基于DROP命令操作 如果要查看现在的全部用户信息 5.2 权限操作 权限操作前，要先掌握一下PGSQL的逻辑结构 可以看到PGSQL一个数据库中有多个schema，在每个schema下都有自己的相应的库表信息，权限粒度会比MySQL更细一些。 在PGSQL中，权限的管理分为很多多层 server、cluster、tablespace级别：这个级别一般是基于pg_hba.conf去配置 database级别：通过命令级别操作，grant namespace、schema级别：玩的不多……不去多了解这个~~ 对象级别：通过grant命令去设置 后面如果需要对database或者是对象级别做权限控制，直接基于grant命令去操作即可 # 查看grant命令 \\help grant 小任务 构建一个用户（你自己名字） 构建一个数据库 在这个数据库下构建一个schema（数据库默认有一个public的schema） 将这个schema的权限赋予用户 在这个schema下构建一个表 将表的select，update，insert权限赋予用户 完成上述操作 -- 准备用户 create user laozheng with password 'laozheng'; -- 准备数据库 create database laozheng; -- 切换数据库 \\c laozheng; -- 构建schema create schema laozheng; -- 将schema的拥有者修改为laozheng用户 alter schema laozheng owner to laozheng; -- 将laozheng库下的laozheng的schema中的表的增，改，查权限赋予给laozheng用户 grant select,insert,update on all tables in schema laozheng to laozheng; -- 用postgres用户先构建一张表 create table laozheng.test(id int); -- 切换到laozheng用户。 \\c laozheng -laozheng -- 报错： -- 致命错误: 对用户&quot;-laozheng&quot;的对等认证失败 -- Previous connection kept -- 上述方式直接凉凉，原因是匹配连接方式时，基于pg_hba.conf文件去从上往下找 -- 找到的第一个是local，匹配上的。发现连接方式是peer。 -- peer代表用当前系统用户去连接PostgreSQL -- 当前系统用户只有postgres，没有laozheng，无法使用peer连接 -- 想构建laozheng用户时，发现postgreSQL的所有文件拥有者和所属组都是postgres，并且能操作的只有拥有者 -- 基于上述问题，不采用本地连接即可。 -- 采用远程连接。 psql -h 192.168.11.32 -p 5432 -U laozheng -W -- 这样依赖，跳过了local链接方式的匹配，直接锁定到后面的host，host的连接方式是md5，md5其实就是密码加密了。 -- 登录后，直接输入 \\dn -- 查看到当前database下有两个schema 这种权限的赋予方式，可以用管理员用户去构建整体表结构，如此一来，分配指定用户，赋予不同的权限，这样一来，就不怕用户误操了。 六、图形化界面安装 图形化界面可以连接PGSQL的很多，Navicat（收费）。 也可以直接使用PostgreSQL官方提供的图形化界面。（完全免费） 官方提供的：https://www.pgadmin.org/ 直接点击就可以下载~~~https://www.postgresql.org/ftp/pgadmin/pgadmin4/v6.9/windows/ 傻瓜式安装~~~ 打开pgAdmin 添加一个新的连接 直接save，就可以连接到老郑的信息 可以切换语言 ","link":"https://tianxiawuhao.github.io/IKbaKSlgE1/"},{"title":"Go语言指针详解","content":"与 Java 和 .NET 等编程语言不同，Go语言为程序员提供了控制数据结构指针的能力，但是，并不能进行指针运算。Go语言允许你控制特定集合的数据结构、分配的数量以及内存访问模式，这对于构建运行良好的系统是非常重要的。指针对于性能的影响不言而喻，如果你想要做系统编程、操作系统或者网络应用，指针更是不可或缺的一部分。 指针（pointer）在Go语言中可以被拆分为两个核心概念： 类型指针，允许对这个指针类型的数据进行修改，传递数据可以直接使用指针，而无须拷贝数据，类型指针不能进行偏移和运算。 切片，由指向起始元素的原始指针、元素数量和容量组成。 受益于这样的约束和拆分，Go语言的指针类型变量即拥有指针高效访问的特点，又不会发生指针偏移，从而避免了非法修改关键性数据的问题。同时，垃圾回收也比较容易对不会发生偏移的指针进行检索和回收。 切片比原始指针具备更强大的特性，而且更为安全。切片在发生越界时，运行时会报出宕机，并打出堆栈，而原始指针只会崩溃。 C/C++中的指针 说到 C/C++ 中的指针，会让许多人“谈虎色变”，尤其是对指针的偏移、运算和转换。 其实，指针是 C/C++ 语言拥有极高性能的根本所在，在操作大块数据和做偏移时即方便又便捷。因此，操作系统依然使用C语言及指针的特性进行编写。 C/C++ 中指针饱受诟病的根本原因是指针的运算和内存释放，C/C++ 语言中的裸指针可以自由偏移，甚至可以在某些情况下偏移进入操作系统的核心区域，我们的计算机操作系统经常需要更新、修复漏洞的本质，就是为解决指针越界访问所导致的“缓冲区溢出”的问题。 要明白指针，需要知道几个概念：指针地址、指针类型和指针取值，下面将展开详细说明。 认识指针地址和指针类型 一个指针变量可以指向任何一个值的内存地址，它所指向的值的内存地址在 32 和 64 位机器上分别占用 4 或 8 个字节，占用字节的大小与所指向的值的大小无关。当一个指针被定义后没有分配到任何变量时，它的默认值为 nil。指针变量通常缩写为 ptr。 每个变量在运行时都拥有一个地址，这个地址代表变量在内存中的位置。Go语言中使用在变量名前面添加&amp;操作符（前缀）来获取变量的内存地址（取地址操作），格式如下： ptr := &amp;v // v 的类型为 T 其中 v 代表被取地址的变量，变量 v 的地址使用变量 ptr 进行接收，ptr 的类型为*T，称做 T 的指针类型，*代表指针。 指针实际用法，可以通过下面的例子了解： package main import ( &quot;fmt&quot; ) func main() { var cat int = 1 var str string = &quot;banana&quot; fmt.Printf(&quot;%p %p&quot;, &amp;cat, &amp;str) } 运行结果： 0xc042052088 0xc0420461b0 代码说明如下： var cat int = 1，声明整型变量 cat。 var str string = &quot;banana&quot;，声明字符串变量 str。 fmt.Printf(&quot;%p %p&quot;, &amp;cat, &amp;str)，使用 fmt.Printf 的动词%p打印 cat 和 str 变量的内存地址，指针的值是带有0x十六进制前缀的一组数据。 提示：变量、指针和地址三者的关系是，每个变量都拥有地址，指针的值就是地址。 从指针获取指针指向的值 当使用&amp;操作符对普通变量进行取地址操作并得到变量的指针后，可以对指针使用*操作符，也就是指针取值，代码如下。 package main import ( &quot;fmt&quot; ) func main() { // 准备一个字符串类型 var house = &quot;Malibu Point 10880, 90265&quot; // 对字符串取地址, ptr类型为*string ptr := &amp;house // 打印ptr的类型 fmt.Printf(&quot;ptr type: %T\\n&quot;, ptr) // 打印ptr的指针地址 fmt.Printf(&quot;address: %p\\n&quot;, ptr) // 对指针进行取值操作 value := *ptr // 取值后的类型 fmt.Printf(&quot;value type: %T\\n&quot;, value) // 指针取值后就是指向变量的值 fmt.Printf(&quot;value: %s\\n&quot;, value) } 运行结果： ptr type: *string address: 0xc0420401b0 value type: string value: Malibu Point 10880, 90265 取地址操作符&amp;和取值操作符*是一对互补操作符，&amp;取出地址，*根据地址取出地址指向的值。 变量、指针地址、指针变量、取地址、取值的相互关系和特性如下： 对变量进行取地址操作使用&amp;操作符，可以获得这个变量的指针变量。 指针变量的值是指针地址。 对指针变量进行取值操作使用*操作符，可以获得指针变量指向的原变量的值。 使用指针修改值 通过指针不仅可以取值，也可以修改值。 前面已经演示了使用多重赋值的方法进行数值交换，使用指针同样可以进行数值交换，代码如下： package main import &quot;fmt&quot; // 交换函数 func swap(a, b *int) { // 取a指针的值, 赋给临时变量t t := *a // 取b指针的值, 赋给a指针指向的变量 *a = *b // 将a指针的值赋给b指针指向的变量 *b = t } func main() { // 准备两个变量, 赋值1和2 x, y := 1, 2 // 交换变量值 swap(&amp;x, &amp;y) // 输出变量值 fmt.Println(x, y) } 运行结果： 2 1 *操作符作为右值时，意义是取指针的值，作为左值时，也就是放在赋值操作符的左边时，表示 a 指针指向的变量。其实归纳起来，*操作符的根本意义就是操作指针指向的变量。当操作在右值时，就是取指向变量的值，当操作在左值时，就是将值设置给指向的变量。 如果在 swap() 函数中交换操作的是指针值，会发生什么情况？可以参考下面代码： package main import &quot;fmt&quot; func swap(a, b *int) { b, a = a, b } func main() { x, y := 1, 2 swap(&amp;x, &amp;y) fmt.Println(x, y) } 运行结果： 1 2 结果表明，交换是不成功的。上面代码中的 swap() 函数交换的是 a 和 b 的地址，在交换完毕后，a 和 b 的变量值确实被交换。但和 a、b 关联的两个变量并没有实际关联。这就像写有两座房子的卡片放在桌上一字摊开，交换两座房子的卡片后并不会对两座房子有任何影响。 示例：使用指针变量获取命令行的输入信息 Go语言内置的 flag 包实现了对命令行参数的解析，flag 包使得开发命令行工具更为简单。 下面的代码通过提前定义一些命令行指令和对应的变量，并在运行时输入对应的参数，经过 flag 包的解析后即可获取命令行的数据。 【示例】获取命令行输入： package main // 导入系统包 import ( &quot;flag&quot; &quot;fmt&quot; ) // 定义命令行参数 var mode = flag.String(&quot;mode&quot;, &quot;&quot;, &quot;process mode&quot;) func main() { // 解析命令行参数 flag.Parse() // 输出命令行参数 fmt.Println(*mode) } 将这段代码命名为 main.go，然后使用如下命令行运行： go run main.go --mode=fast 命令行输出结果如下： fast 代码说明如下： var mode = flag.String(&quot;mode&quot;, &quot;&quot;, &quot;process mode&quot;)，通过 flag.String，定义一个 mode 变量，这个变量的类型是 *string。后面 3 个参数分别如下： 参数名称：在命令行输入参数时，使用这个名称。 参数值的默认值：与 flag 所使用的函数创建变量类型对应，String 对应字符串、Int 对应整型、Bool 对应布尔型等。 参数说明：使用 -help 时，会出现在说明中。 flag.Parse()，解析命令行参数，并将结果写入到变量 mode 中。 fmt.Println(*mode)，打印 mode 指针所指向的变量。 由于之前已经使用 flag.String 注册了一个名为 mode 的命令行参数，flag 底层知道怎么解析命令行，并且将值赋给 mode*string 指针，在 Parse 调用完毕后，无须从 flag 获取值，而是通过自己注册的这个 mode 指针获取到最终的值。代码运行流程如下图所示。 图：命令行参数与变量的关系 创建指针的另一种方法——new() 函数 Go语言还提供了另外一种方法来创建指针变量，格式如下： new(类型) 一般这样写： str := new(string) *str = &quot;Go语言教程&quot; fmt.Println(*str) new() 函数可以创建一个对应类型的指针，创建过程会分配内存，被创建的指针指向默认值。 package main import &quot;fmt&quot; type A struct { Ptr1 *B Ptr2 *B Val B Val2 string Val3 int } type B struct { Str string } func main() { a := A{ Ptr1: &amp;B{&quot;ptr-str-1&quot;}, Ptr2: &amp;B{&quot;ptr-str-2&quot;}, Val: B{&quot;val-str&quot;}, Val2: &quot;main&quot;, Val3: 0, } fmt.Printf(&quot;main a addr: %p\\n&quot;, &amp;a) //a地址 fmt.Printf(&quot;main a addr: %p\\n&quot;, a) //a地址 fmt.Printf(&quot;main Ptr1 addr: %p, point to addr: %p, value: %v\\n&quot;, &amp;a.Ptr1, a.Ptr1, *a.Ptr1) //a地址，ptr1地址，B数据ptr-str-1 fmt.Printf(&quot;main Ptr2 addr: %p, point to addr: %p, value: %v\\n&quot;, &amp;a.Ptr2, a.Ptr2, *a.Ptr2) //ptr2地址，ptr2-B地址，B数据ptr-str-2 fmt.Printf(&quot;main Val addr: %p, value: %v\\n&quot;, &amp;a.Val, a.Val) //a.Val地址，a.Val值 fmt.Printf(&quot;main Val2 addr: %p, value: %v\\n&quot;, &amp;a.Val2, a.Val2) //a.Val2地址，a.Val2值 fmt.Printf(&quot;main Val3 addr: %p, value: %v\\n\\n&quot;, &amp;a.Val3, a.Val3) //a.Val3地址，a.Val3值 demo1(a) fmt.Println(&quot;after demo1:&quot;) fmt.Printf(&quot;main Ptr1 addr: %p, point to addr: %p, value: %v\\n&quot;, &amp;a.Ptr1, a.Ptr1, *a.Ptr1) fmt.Printf(&quot;main Ptr2 addr: %p, point to addr: %p, value: %v\\n&quot;, &amp;a.Ptr2, a.Ptr2, *a.Ptr2) fmt.Printf(&quot;main Val addr: %p, value: %v\\n&quot;, &amp;a.Val, a.Val) fmt.Printf(&quot;main Val2 addr: %p, value: %v\\n&quot;, &amp;a.Val2, a.Val2) fmt.Printf(&quot;main Val3 addr: %p, value: %v\\n\\n&quot;, &amp;a.Val3, a.Val3) demo2(&amp;a) fmt.Println(&quot;after demo2:&quot;) fmt.Printf(&quot;main Ptr1 addr: %p, point to addr: %p, value: %v\\n&quot;, &amp;a.Ptr1, a.Ptr1, *a.Ptr1) fmt.Printf(&quot;main Ptr2 addr: %p, point to addr: %p, value: %v\\n&quot;, &amp;a.Ptr2, a.Ptr2, *a.Ptr2) fmt.Printf(&quot;main Val addr: %p, value: %v\\n&quot;, &amp;a.Val, a.Val) fmt.Printf(&quot;main Val2 addr: %p, value: %v\\n&quot;, &amp;a.Val2, a.Val2) fmt.Printf(&quot;main Val3 addr: %p, value: %v\\n\\n&quot;, &amp;a.Val3, a.Val3) } func demo1(a A) { fmt.Printf(&quot;demo1 a addr:%p\\n&quot;, &amp;a) //a地址 // Update a value of a pointer and changes will persist a.Ptr1.Str = &quot;demo1-ptr-str1&quot; fmt.Printf(&quot;demo1 Ptr1 addr: %p, point to addr: %p, value: %v\\n&quot;, &amp;a.Ptr1, a.Ptr1, *a.Ptr1) //a地址，Ptr1地址，值demo1-ptr-str1 // Use an entirely new B object and changes won't persist a.Ptr2 = &amp;B{&quot;demo1-ptr-str-2&quot;} fmt.Printf(&quot;demo1 Ptr2 addr: %p, point to addr: %p, value: %v\\n&quot;, &amp;a.Ptr2, a.Ptr2, *a.Ptr2) //a.Ptr2地址，Ptr2地址，值demo1-ptr-str-2 //a.Val.Str = &quot;new-val-str&quot; a.Val = B{&quot;demo1-val-str&quot;} fmt.Printf(&quot;demo1 Val addr: %p, value: %v\\n&quot;, &amp;a.Val, a.Val) //a.Val地址，demo1-val-str fmt.Printf(&quot;demo1 Val2 before addr: %p, value: %v\\n&quot;, &amp;a.Val2, a.Val2) a.Val2 = &quot;demo1&quot; fmt.Printf(&quot;demo1 Val2 after addr: %p, value: %v\\n&quot;, &amp;a.Val2, a.Val2) //地址不变，值变 fmt.Printf(&quot;demo1 Val3 before addr: %p, value: %v\\n&quot;, &amp;a.Val3, a.Val3) a.Val3 = 1 fmt.Printf(&quot;demo1 Val3 after addr: %p, value: %v\\n&quot;, &amp;a.Val3, a.Val3) //地址不变，值变 } func demo2(a *A) { fmt.Printf(&quot;demo2 a addr:%p\\n&quot;, &amp;a) fmt.Printf(&quot;demo2 a point to addr:%p\\n&quot;, a) //在go语言中通过指针去访问指针所对应的地址处的值时，*允许不写。 a.Ptr1.Str = &quot;demo2-ptr-str1&quot; fmt.Printf(&quot;demo1 Ptr1 addr: %p, point to addr: %p, value: %v\\n&quot;, &amp;a.Ptr1, a.Ptr1, *a.Ptr1) a.Ptr2 = &amp;B{&quot;demo2-ptr-str-2&quot;} fmt.Printf(&quot;demo2 Ptr2 addr: %p, point to addr: %p, value: %v\\n&quot;, &amp;a.Ptr2, a.Ptr2, *a.Ptr2) //a.Val.Str = &quot;new-val-str&quot; a.Val = B{&quot;demo2-val-str&quot;} fmt.Printf(&quot;demo2 Val addr: %p, value: %v\\n&quot;, &amp;a.Val, a.Val) a.Val2 = &quot;demo2&quot; fmt.Printf(&quot;demo2 Val2 addr: %p, value: %v\\n&quot;, &amp;a.Val2, a.Val2) a.Val3 = 2 fmt.Printf(&quot;demo2 Val3 addr: %p, value: %v\\n&quot;, &amp;a.Val3, a.Val3) } main a addr: 0xc000078040 main a addr: %!p(main.A={0xc00006c250 0xc00006c260 {val-str} main 0}) main Ptr1 addr: 0xc000078040, point to addr: 0xc00006c250, value: {ptr-str-1} main Ptr2 addr: 0xc000078048, point to addr: 0xc00006c260, value: {ptr-str-2} main Val addr: 0xc000078050, value: {val-str} main Val2 addr: 0xc000078060, value: main main Val3 addr: 0xc000078070, value: 0 demo1 a addr:0xc0000780c0 demo1 Ptr1 addr: 0xc0000780c0, point to addr: 0xc00006c250, value: {demo1-ptr-str1} demo1 Ptr2 addr: 0xc0000780c8, point to addr: 0xc00006c2c0, value: {demo1-ptr-str-2} demo1 Val addr: 0xc0000780d0, value: {demo1-val-str} demo1 Val2 before addr: 0xc0000780e0, value: main demo1 Val2 after addr: 0xc0000780e0, value: demo1 demo1 Val3 before addr: 0xc0000780f0, value: 0 demo1 Val3 after addr: 0xc0000780f0, value: 1 after demo1: main Ptr1 addr: 0xc000078040, point to addr: 0xc00006c250, value: {demo1-ptr-str1} main Ptr2 addr: 0xc000078048, point to addr: 0xc00006c260, value: {ptr-str-2} main Val addr: 0xc000078050, value: {val-str} main Val2 addr: 0xc000078060, value: main main Val3 addr: 0xc000078070, value: 0 demo2 a addr:0xc00000a030 demo2 a point to addr:0xc000078040 demo1 Ptr1 addr: 0xc000078040, point to addr: 0xc00006c250, value: {demo2-ptr-st tr1} demo2 Ptr2 addr: 0xc000078048, point to addr: 0xc00006c360, value: {demo2-ptr-st tr-2} demo2 Val addr: 0xc000078050, value: {demo2-val-str} demo2 Val2 addr: 0xc000078060, value: demo2 demo2 Val3 addr: 0xc000078070, value: 2 after demo2: main Ptr1 addr: 0xc000078040, point to addr: 0xc00006c250, value: {demo2-ptr-str r1} main Ptr2 addr: 0xc000078048, point to addr: 0xc00006c360, value: {demo2-ptr-str r-2} main Val addr: 0xc000078050, value: {demo2-val-str} main Val2 addr: 0xc000078060, value: demo2 main Val3 addr: 0xc000078070, value: 2 ","link":"https://tianxiawuhao.github.io/FPlmZ5XzY/"},{"title":"Go语言学习（简单与Java作比较）","content":"序言 ​ 之前有过一年多的Java开发经验，主要学习了Java基础（包含面向对象语言特点——封装继承多态、异常处理、常用类、数组和集合、IO流），JVM内存机制，设计模式，数据库设计，以及在工作中用到的一些开发框架，了解并熟悉别人的框架有助于简化自己的开发。本篇文件较为啰嗦，且无侧重点，慎入。 不管是学习C语言、C++、Java还是Go、Python，前面学到的东西并不会白费，有些时候反而能互补，比如有些设计思想。 搭建环境那都是一样的就不说了。下面就主要以Java和Go语言作对比，Java是面向对象的语言，Go语言既不是面向对象的语言也不是面向过程的语言，但Go语言在其他语言的身上能找到自己的影子。 Java所有开发都是以对象来的，虽然Java的基本数据类型**char，boolean，byte，short，int，long，float，double，**但它们都有对象包装类——**Character，Boolean，Byte，Short，Integer，Long，Float，Double，**实例化一个对象只需要new出来，它就能创建在JVM的堆里，至于使用完了垃圾回收大家可以自己去查阅资料。这里需要注意的一个地方就是，基本类型int转换为包装类型Integer时，在[-128,127]内转换的数据相等，在其他范围相同的int数据转换的包装类值不相等。这是由于Integer类为内部整数维护一个内部静态类IntegerCache缓存并保存从-128到127的整数对象，所以在这区间转换出来的是相等的对象。 public static void main(String[] args) { System.out.println(&quot;实例化相同值对象结果=&quot;+(new Integer(127) == new Integer(127))); // false System.out.println(&quot;实例化相同值对象结果=&quot;+(new Integer(128) == new Integer(128))); //false System.out.println(&quot;将基本类型int转换为包装类型Integer结果=&quot;+(Integer.valueOf(127) == Integer.valueOf(127))); //true System.out.println(&quot;将基本类型int转换为包装类型Integer结果=&quot;+(Integer.valueOf(128) == Integer.valueOf(128))); //false Integer a1 = 127; Integer a2 = 127; Integer b1 = 128; Integer b2 = 128; System.out.println(a1 == a2); //true System.out.println(b1 == b2); //false } Java和Go语言区别 数据类型 上面其实也提到过Java的基本数据类型，其实Java除了八大基本数据类型（char，boolean，byte，short，int，long，float，double）外还有引用数据类型，如对象（包含string）、数组。 Go语言数据类型包含布尔型（bool），数字型（uint8、uint16、uint32、uint64、int8、int16、int32、int64、float32、float64、complex64、complex128），字符串型，派生类型（指针Pointer、数组、结构化类型、Channel、函数、切片、接口、Map），布尔型、数字型、字符串型的默认值和Java差不多，不过Java的引用数据类型Java是null，Go的派生类型是nil。 Java中占用字节大小：char-2字节，boolean-1字节（boolean类型数组的访问与修改共用byte类型数组的baload和bastore指令时，boolean数组是1个字节；boolean值在编译后使用Java虚拟机中的int数据类型来代替时boolean值是4个字节，根据JVM是否按照规范来判断），byte-1字节，short-2字节，int-4字节，long-8字节，float-4字节，double-8字节。Go中的数据类型占几个字节特别好算吧，直接除以8bit，比如uint16即16/8=2字节。 变量、方法的访问控制 Java会有访问控制符和非访问控制符，访问控制符可以使类、变量、方法和构造方法具有不同的访问权限，public &gt; protected &gt; default &gt; private（当前类、同包子类、不同包子类、不同包其他类的访问）。而非访问控制符有static修饰符（可修饰类、变量、方法，用来声明独立于对象的静态变量），final修饰符（类——不能被继承、方法——不能被继承类重定义、变量——不可修改），abstract修饰符（创建抽象类和抽象方法，不能与final一起用），synchronized修饰符（保证被修饰的方法线程安全），transient修饰符（实现Serialize接口的类中被修饰的变量JVM不进行序列化，但静态变量不管有没有修饰符都不能被序列化，且实现Externalizable接口的类与transient无关），volatile修饰符（被修饰的变量线程共享，保证线程可见性和禁止指令重排序） Go中不同包下的不同变量和方法只需要通过大小写就可以进行访问控制，大写字母开头的变量和方法是公开的，而小写的是私有的。 代码块执行顺序 Java代码块的执行顺序是：父类静态代码块(1次)——&gt;子类静态代码块(1次)——&gt;父类非静态代码块 ——&gt; 父类构造函数 ——&gt; 子类非静态代码块 ——&gt; 子类构造函数 Go代码块的执行顺序是：其他包go文件定义变量初始化 ——&gt; 其他包go文件定义的init()函数执行 ——&gt; 本包go文件定义变量初始化 ——&gt; 本包go文件定义的init()函数执行 ——&gt; 本包go文件定义的main()函数执行 循环结构 Java循环结构有while循环，do...while循环，for循环，用到的控制语句有break、continue。 Go语言中循环结构主要就是for循环以及循环嵌套，里面会用到的控制语句有break、continue、goto，多了一条goto语句，它可以无条件地转移到过程中指定的行，但是在结构化程序设计中一般不主张使用 goto 语句， 以免造成程序流程的混乱，使理解和调试程序都产生困难。 Go语言中的for循环除了for (init;condition;increment){conditional code}外，还可以使用for (key,value := range oldMap){newMap[key] = value}对slice、map、数组、字符串等进行遍历。Go语言里()可以省略，有兴趣的同学还可以拿Go的for range遍历和Java的Iterator迭代器进行比较。 条件语句 Java的条件语句主要有if语句，if...else语句以及if嵌套语句。 Go的条件语句有if语句，if...else语句，if嵌套语句，switch语句（switch 变量+case分支+default分支，默认匹配后不会执行其他case，使用 fallthrough 会强制执行后面的 case 语句，fallthrough 不会判断下一条 case 的表达式结果是否为 true），select语句（Go的一个控制结构，类似switch，但它会随机执行一个可运行的 case。如果没有 case 可运行，它将阻塞，直到有 case 可运行。一个默认的子句应该总是可运行的。） 函数 Java虽然没有函数的概念，但是通常可以用类中的方法去进行实现各个功能模块。 Go语言中至少有一个main()函数，也可以加一个init()初始化函数。 Go语言中函数参数可以用是值传递也可以是引用传递，值传递在函数内对参数进行修改不会影响到实际参数，引用传递在函数内对参数进行修改会影响到实际参数。这个和Java方法参数的值传递（基本数据类型）和引用传递（引用数据类型）一样。 Go语言函数一个函数可以作为另一个函数的实参，如下图左边是工具包里的go文件，声明了一个函数类型func(int) int，符合该函数类型的所有函数都可以作为TestCallBack的参数，右边是函数调用。 package util import &quot;fmt&quot; //声明一个函数类型 type callBackType func(int) int func CallBack(x int) int{ fmt.Printf(&quot;我是回调函数，x：%d\\n&quot;,x) return x } func TestCallBack(x int, function callBackType){ function(x) } package main import( &quot;fmt&quot; &quot;godemo/src/gocode/callbackDemo/util&quot; ) func main(){ //CallBack函数作为实参传入到TestCallBack函数 util.TestCallBack(1, util.CallBack) //匿名函数作为实参传入到TestCallBack函数中，类似于Java的匿名内部类 util.TestCallBack(2, func(x int) int { fmt.Printf(&quot;我是回调，x：%d\\n&quot;, x) return x }) } Java中虽然一个方法不能作为另一个方法的实参，但是Java 8引入的新特性函数式接口可以将不同的条件的方法作为参数。Predicate接口通过@FunctionalInterface实现函数式接口，它可以定义一组条件并确定指定对象是否符合这些条件。下面的eval方法中的第二参数可以传入不同的条件。 public static void main(String args[]){ List&lt;Integer&gt; list = Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9); /** * Predicate&lt;Integer&gt; predicate = n -&gt; true * n 是一个参数传递到 Predicate 接口的 test 方法 * n 如果存在则 test 方法返回 true */ System.out.print(&quot;输出所有数:&quot;); eval(list, n-&gt;true); /** * Predicate&lt;Integer&gt; predicate1 = n -&gt; n%2 == 0 * n 是一个参数传递到 Predicate 接口的 test 方法 * 如果 n%2 为 0 test 方法返回 true */ System.out.print(&quot;输出所有偶数:&quot;); eval(list, n-&gt; n%2 == 0 ); // Predicate&lt;Integer&gt; predicate2 = n -&gt; n &gt; 3 // n 是一个参数传递到 Predicate 接口的 test 方法 // 如果 n 大于 3 test 方法返回 true System.out.print(&quot;输出大于 3 的所有数字:&quot;); eval(list, n-&gt; n &gt; 3 ); } public static void eval(List&lt;Integer&gt; list, Predicate&lt;Integer&gt; predicate) { for(Integer n: list) { if(predicate.test(n)) { System.out.println(n + &quot; &quot;); } } System.out.println(); } } 数组 Java和Go的数组基本上是没有区别的，声明和初始化数组的方式基本类似，也都是通过索引访问数组的元素，这里就不多说了。 指针 Java中是没有指针概念的，Java语言想让程序员更关注于业务，一个对象不实例化它就是null不会占用内存空间，也就不会有内存地址。实例化的对象它们的地址也有可能发生变化，比如GC复制算法在进行对象的垃圾回收时，有些还在使用的对象的地址可能会发生变化。Java中的unsafe类可以获取相对初始地址的偏移量来查看当前对象目前的地址，一般不需要关注。 Go语言中和C/C++一样也用到了指针，使用指针可以更简单的执行任务，指针也带来了更高的效率。指针被定义后没有分配到任何变量时它就是nil指针。 Java中对某个对象/数组进行修改时，需要通过方法的返回值的对象/数组来接收新的对象/数组，而Go语言有指针后可以向函数传入指针，直接通过指针修改对应的值。 结构体 Go中结构体用来定义不同的数据类型，语法type struct_name struct{member definition ... member definition ... } 切片 Go语言切片是对数组的抽象。数组长度不可变，但切片长度和容量是可变的。 切片包含三个属性：指针（Pointer）、长度（Length）、容量（Capacity），可以使用内建函数make([]type, length, capacity)创建特定类型的切片，也可以从其他切片中进行截取。 这里需要区分一下数组和切片，数组是长度不可变的，而切片是长度可变的。数组的类型是[length] type，切片的类型是[] type。 创建一个切片事实上是指向一个底层数组，如果对切片进行截取后，修改截取后的切片的元素值实际上改变的是底层数组的元素值，截取前的切片元素值也会发生改变。 切片可以通过内建函数copy()和append()进行切片拷贝以及切片元素追加。 切片元素追加时需要注意，如果原切片的容量大于长度，则直接在原切片的基础上追加，切片容量不变且长度+1；如果原切片的容量等于长度，则会对原切片进行2倍扩容且长度+1，这时将会生成一个新的底层数组，即原切片和扩容后的切片不再是共用一个底层数组，修改扩容后的切片的元素值将不会影响原切片的元素值。示例如下， old_slice := []int{11,22,33,44,55,66,77,88} test_slice := old_slice[1:3:3] //从1开始切，长度3-1，容量3-1，即{22,33} fmt.Printf(&quot;追加前的slice=%v,长度=%v,容量=%v\\n&quot;,test_slice,len(test_slice),cap(test_slice)) app_slice := append(test_slice, 100) fmt.Printf(&quot;追加后的slice=%v,长度=%v,容量=%v\\n&quot;,app_slice,len(app_slice),cap(app_slice)) app_slice[0] = 300 fmt.Printf(&quot;修改扩容后的切片的第一个元素后，追加前slice=%v, 追加后slice=%v\\n&quot;,test_slice, app_slice) 还需注意的是底层数组扩容时，长度不超过1024则按照当前底层数组的长度的2倍进行扩容，并生成新数组；长度超过1024时，按照25%的比率扩容，也就是1024个元素存在时，容量扩展为1280（本机的go版本是1.15.7） Map集合 -它是一种无序的键值对集合。但是其底层实现是一个散列表，散列表中有两个结构体hmap（a header for a go map）+ bmap（a bucket for a go map，通常也叫bucket）。 Go中它是数组+链表，使用链表的原因和Java一样是为了避免哈希冲突。 而Java在Jdk1.8后是数组+链表+红黑树，在桶元素个数大于64或小于6时会从链表和红黑树结构相互转换，引入红黑树主要是为了提高查询效率。 Go map进行扩容时也是根据加载因子（loadFactor）进行判断的，Go语言中的加载因子=map长度/2^B(B代表bmap数组的长度，B是通过key进行hash运算的低位的位数)，而Java语言中的加载因子默认是0.75f。 递归函数 递归，就是在运行的过程中调用自己。 不管是Go、Java、还是其他语言都可以实现递归调用，最常见的递归案例就是实现斐波那契数列。 接口 Go的接口是对其他类型行为的抽象和概括；它通过type interface_name interface定义。而Java中接口要优先于实现类，通过implements 关键字绑定接口和实现类的关系。 Go是通过func(struct_name_variable struct_name) method_name()[return_type]{ //方法实现}来重写方法的。Java是通过接口实现类去实现接口并重写（Override）方法的。 异常处理 我们知道Java异常分为Error（程序无法处理的严重错误，编译器不做检查，通常JVM会终止线程的动作）和Exception（运行时异常和编译期异常以及自定义异常），而Exception异常通常可以通过try{}catch(Exception){}语句进行捕获处理。 Go语言中进行异常处理时，可以通过defer+recover()+panic()来处理异常。defer是延迟语句，当主函数退出后才进行调用，遵循FIFO原则；recover()和panic()都是builtin包下的内建函数。在defer的函数中，执行recover()函数调用会取回传至panic调用的错误值，恢复正常执行，停止恐慌过程（可以理解为Java中的中断），若recover在defer的函数之外被调用，它将不会停止恐慌过程序列；panic()函数调用后会终止程序，错误情况会被报告，包括引发该恐慌的实参值。 下面给出一个Go语言使用defer+recover()捕获和处理运行时异常的栗子（出现异常后，主函数后面的语句将正常执行）： import( &quot;fmt&quot; &quot;errors&quot; ) func test(){ //使用defer+recover捕获和处理异常 defer catchError() num1 := 10 num2 := 0 res := num1/num2 fmt.Println(&quot;res = &quot;,res) } //捕获异常 func catchError(){ err := recover() //recover()也是内建函数 if err!=nil{ //err!=nil说明捕获到异常 fmt.Println(&quot;异常信息=&quot;,err) } } func main(){ test() fmt.Println(&quot;main()下面的代码......&quot;) } 下面给出一个自定义异常的处理，直接使用panic()内建函数终止程序，会输出自定义错误，但后面的代码将不会再执行： import( &quot;fmt&quot; &quot;errors&quot; ) //支持自定义错误，使用errors.New和panic内建函数 func readConf(name string)(err error){ if name == &quot;config.ini&quot;{ return nil //读取返回空... } else{ return errors.New(&quot;读取文件错误...&quot;) } } func test(){ err := readConf(&quot;confi.ini&quot;) if err != nil{ panic(err) //如果读取文件发生错误，输出错误并终止程序 } fmt.Println(&quot;test()继续执行...&quot;) } func main(){ test() fmt.Println(&quot;main()下面的代码......&quot;) } 并发 Java中我们只需要了解进程和线程，而Go语言里我们需要知道进程、线程和协程。进程是任务调度的最小单位，每个进程各自都独立一块内存；线程是程序执行流的最小单位，是处理器调度和分派的最小单位；协程是一种用户态的轻量级线程，协程拥有自己的寄存器上下文和栈； 线程和进程都是同步机制，而协程是异步；协程能保留上一次调用时的状态，每次重入相当进入上一次的状态。 goroutine是协程的go语言实现，它是轻量级线程，goroutine 的调度是由 Golang 运行时进行管理的。 Java语言通过Thread类去创建线程，而Go语言通过 go 语句开启一个新的运行期线程， 即 goroutine。 Go语言可用通道来传递数据，channel就是用于两个 goroutine 之间通过传递一个指定类型的值来同步运行和通讯。默认的通道是不带缓冲区的，如果需要设置缓冲区可以通过make()函数去设置。 需要注意的是出现协程死锁的情况（fatal error: all goroutines are asleep - deadlock），无缓冲信道不会用来存储数据，只会用来给不同协程传输数据。主函数也是一个协程（main goroutine）。下面给出一个简单的demo。 package utils import( &quot;fmt&quot; &quot;time&quot; ) func SaySomething(s string){ for i:=0;i&lt;5;i++{ time.Sleep(100 * time.Millisecond) fmt.Printf(&quot;这是某某第%v次说%v\\n&quot;,i+1,s) } } func Sum(s []int, c chan int){ sum := 0 for _,v := range s{ sum += v } //把sum发送到通道c c &lt;- sum } func Fibonacci(n int, c chan int){ x, y := 0, 1 for i:=0;i&lt;n;i++{ //把x发送到通道c中 c &lt;- x x, y = y, x+y } close(c) } package main import( &quot;fmt&quot; &quot;goDemo/src/gocode/goroutine/utils&quot; ) func main(){ str1 := &quot;hello&quot; str2 := &quot;world&quot; //输出的 hello 和 world 是没有固定先后顺序。因为它们是两个 goroutine 在执行 go utils.SaySomething(str1) utils.SaySomething(str2) fmt.Println(&quot;then I can say&quot;) //通过两个 goroutine 来计算数字之和，在 goroutine 完成计算后，它会计算两个结果的和： s := []int{7, 2, 8, -9, 4, 0} c := make(chan int) fmt.Printf(&quot;信道地址=%v,信道容量=%v\\n&quot;,c, cap(c)) go utils.Sum(s[:len(s)/2],c) go utils.Sum(s[len(s)/2:],c) //从通道c接收数据并赋值给x，y x, y := &lt;- c , &lt;- c fmt.Printf(&quot;后半部分求和=%v,前半部分求和=%v,总和=%v\\n&quot;,x,y,x+y) // 这里我们定义了一个可以存储整数类型的带缓冲通道，缓冲区大小为2 ch := make(chan int, 2) // 因为 ch 是带缓冲的通道，我们可以同时发送两个数据 // 而不用立刻需要去同步读取数据 ch &lt;- 1 ch &lt;- 2 // ch &lt;- 3 //如果再发送一个数据则会出现异常 fatal error: all goroutines are asleep - deadlock! // 获取这两个数据 fmt.Println(&lt;-ch) fmt.Println(&lt;-ch) // 接收了数据之后缓存里才能继续缓冲数据，否则就会出现协程死锁 ch &lt;- 3 //go遍历通道和关闭通道 cc := make(chan int,10) go utils.Fibonacci(cap(cc), cc) for i := range cc{ fmt.Printf(&quot;%v\\t&quot;,i) } //无缓冲信道不会用来存储数据，只负责数据的流通，两个goroutine可以用无缓冲信道传输数据 go func(){c &lt;- 1}() test := &lt;- c fmt.Println(test) // c &lt;- 1 //无缓冲信道容量为0，不能在main goroutine中传数据，否则会出现deadlock } ","link":"https://tianxiawuhao.github.io/4lr2-CaPP/"},{"title":"Centos7部署FastDFS","content":"一、FastDFS的介绍与原理 官方Github地址：https://github.com/happyfish100 FastDFS 是一个开源的高性能分布式文件系统（DFS）。 它的主要功能包括：文件存储，文件同步和文件访问，以及高容量和负载平衡。 主要解决了海量数据存储问题，特别适合以中小文件（建议范围：4KB &lt; file_size &lt;500MB）为载体的在线服务。 FastDFS 系统有三个角色：跟踪服务器(Tracker Server)、存储服务器(Storage Server)和客户端(Client)。 **Tracker Server：**跟踪服务器，主要做调度工作，起到均衡的作用；负责管理所有的 storage server和 group，每个 storage 在启动后会连接 Tracker，告知自己所属 group 等信息，并保持周期性心跳。 **Storage Server：**存储服务器，主要提供容量和备份服务；以 group 为单位，每个 group 内可以有多台 storage server，数据互为备份。 **Client：**客户端，上传下载数据的服务器，也就是我们自己的项目所部署在的服务器，在使用的过程中，只需要配置Tranker的地址即可。 二、FastDFS的安装前环境准备 需要用到的相关软件与简单介绍 centos 7.x libfatscommon FastDFS分离出的一些公用函数包 FastDFS FastDFS本体 fastdfs-nginx-module FastDFS和nginx的关联模块 nginx nginx1.20.2 1、检查安装编译环境： yum install git gcc gcc-c++ make automake autoconf libtool pcre pcre-devel zlib zlib-devel openssl-devel wget vim -y 2、统一准备好文件夹位置： 说明 位置 所有安装包 /usr/local/fastdfs 数据存储位置 /data/fastdfs/ data/logs都存在了dfs [root@tcosmo-test-szls-baas01 /]# mkdir /usr/local/fastdfs [root@tcosmo-test-szls-baas01 /]# mkdir -p /data/fastdfs ## 进入 [root@tcosmo-test-szls-baas01 fastdfs]# cd /usr/local/fastdfs/ 三、安装FastDFS的四大软件 由于github经常无法访问，我们可以提前下载好，拷贝进/usr/local/fastdfs目录下 1、下载并安装libfatscommon： git clone https://github.com/happyfish100/libfastcommon.git --depth 1 cd libfastcommon/ ./make.sh &amp;&amp; ./make.sh install #编译安装 2、安装FastDFS本体： git clone https://github.com/happyfish100/fastdfs.git --depth 1 cd fastdfs/ ./make.sh &amp;&amp; ./make.sh install #编译安装 # 查看默认提供的配置文件： [root@tcosmo-test-szls-baas01 fdfs]# ll /etc/fdfs/ total 32 -rw-r--r-- 1 root root 1909 Feb 14 11:25 client.conf -rw-r--r-- 1 root root 10246 Feb 14 11:25 storage.conf -rw-r--r-- 1 root root 620 Feb 14 11:25 storage_ids.conf -rw-r--r-- 1 root root 9138 Feb 14 11:25 tracker.conf # 提前将nginx的配置文件也拷贝到fdfs的配置文件文件夹下： cp /usr/local/fastdfs/fastdfs/conf/http.conf /etc/fdfs/ #供nginx访问使用 cp /usr/local/fastdfs/fastdfs/conf/mime.types /etc/fdfs/ #供nginx访问使用 当fastdfs安装完成后，会在 /usr/local/fastdfs/fastdfs/init.d 目录下生成对应的运行脚本： [root@i-zqovu8av init.d]# ll /usr/local/fastdfs/fastdfs/init.d total 8 -rwxrwxrwx 1 root root 961 Dec 31 02:52 fdfs_storaged -rwxrwxrwx 1 root root 963 Dec 31 02:52 fdfs_trackerd 如果没有运行权限，我们就手动chmod！ 3、安装fastdfs-nginx-module： git clone https://github.com/happyfish100/fastdfs-nginx-module.git --depth 1 cp /usr/local/fastdfs/fastdfs-nginx-module/src/mod_fastdfs.conf /etc/fdfs 4、安装Nginx并添加fastdfs-nginx-module： ## 下载 wget ## 解压 &amp; 进入： tar -zxvf nginx-1.20.2.tar.gz cd nginx-1.20.2/ ## 添加fastdfs-nginx-module模块 ./configure --add-module=/usr/local/fastdfs/fastdfs-nginx-module/src/ ## 编译安装 make &amp;&amp; make install 四、FastDFS的单机部署配置 1、tracker的配置： #服务器ip为 139.198.191.246 vim /etc/fdfs/tracker.conf #需要修改的内容如下 port=22122 # tracker服务器端口（默认22122,一般不修改） base_path=/data/fastdfs # 存储日志和数据的根目录 2、storage的配置： vim /etc/fdfs/storage.conf #需要修改的内容如下 port=23000 # storage服务端口（默认23000,一般不修改） base_path=/data/fastdfs # 数据和日志文件存储根目录 store_path0=/data/fastdfs # 第一个存储目录 tracker_server=139.198.191.246:22122 # tracker服务器IP和端口 http.server_port=8888 # http访问文件的端口(默认8888,看情况修改,和nginx中保持一致) 3、nginx的配置： vim /etc/fdfs/mod_fastdfs.conf #需要修改的内容如下 tracker_server=139.198.191.246:22122 #tracker服务器IP和端口 url_have_group_name=true store_path0=/data/fastdfs #配置nginx.config vim /usr/local/nginx/conf/nginx.conf #添加如下配置 server { listen 8888; ## 该端口为storage.conf中的http.server_port相同 server_name localhost; location ~/group[0-9]/ { ngx_fastdfs_module; } error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } } 4、启动服务： # 关闭防火墙 systemctl stop firewalld.service # 启动tracker /usr/local/fastdfs/fastdfs/init.d/fdfs_trackerd start Starting FastDFS tracker server: # 启动storage /usr/local/fastdfs/fastdfs/init.d/fdfs_storaged start Starting FastDFS storage server: # 重启nginx nginx -s reload 五、使用自带的client测试文件的上传与访问 1、配置自带的client的配置文件： vim /etc/fdfs/client.conf #需要修改的内容如下 base_path=/data/fastdfs tracker_server=139.198.191.246:22122 #tracker服务器IP和端口 2、测试上传一个文件，我们事先在某个位置准备一张图片即可： [root@i-zqovu8av fdfs]# fdfs_upload_file /etc/fdfs/client.conf /root/testfdfs.jpg group1/M00/00/00/i8a_9mIKAMKAPKgPAAA2E69InJY486.jpg #返回图片位置，则表示成功。 3、上传成功后，我们就能在 /data/fastdfs 目录下，看到我们上传的文件： [root@i-zqovu8av 00]# pwd /data/fastdfs/data/00/00 [root@i-zqovu8av 00]# ls i8a_9mIKAMKAPKgPAAA2E69InJY486.jpg 4、我们还可以通过配置的Nginx对文件进行直接访问： http://139.198.191.246:8888/group1/M00/00/00/i8a_9mIKAMKAPKgPAAA2E69InJY486.jpg 至此，FastDFS的单机版安装与测试完成. 补充一：FastDFS与Nginx的开机自启动 考虑到每次启动之后都要重新启动一下fastdfs 和 nginx服务，比较麻烦，所以增加开机自启动 1、配置 rc.local 文件： vim /etc/rc.d/rc.local # 增加fastdfs start /usr/bin/fdfs_trackerd /etc/fdfs/tracker.conf restart /usr/bin/fdfs_storaged /etc/fdfs/storage.conf restart # 增加nginx start /usr/local/nginx/sbin/nginx 2、由于在centos7中, /etc/rc.d/rc.local 文件的权限被降低了，需要给rc.local 文件增加可执行的权限： chmod +x /etc/rc.d/rc.local ","link":"https://tianxiawuhao.github.io/8j32dqs0d/"},{"title":"在Centos7.9手动安装Nexus3","content":" dockers安装很方便，但是有时候我们不可以选择docker安装的时候，就手动安装吧。 一、安装JDK 首先新的环境虚拟机有自己自带的openjdk，在安装jdk前需要先把虚拟机的卸掉，防止多个jdk冲突，下面是卸载步骤： #查看已安装的openjdk： rpm -qa | grep openjdk #结果显示存在的openjdk： java-1.8.0-openjdk-1.8.0.161-2.b14.el7.x86_64 #版本显示不一 #执行批量删除命令，删除 openjdk rpm -qa | grep openjdk |xargs rpm -e --nodeps #再次查看rpm -qa | grep openjdk 是否还存在 下载地址：Java Downloads | Oracle #解压命令 tar -zxvf jdk-8u181-linux-x64.tar.gz #重命名 mv jdk-8u181-linux-x64 JAVA 配置环境变量 #配置JDK环境变量 vim /etc/profile,在文件末追加如下内容: export JAVA_HOME=jdk的根目录 export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar export PATH=$PATH:$JAVA_HOME/bin 保存退出 #使添加在文件中的配置信息马上生效: source /etc/profile #查看JDK安装是否成功： java -version 二、安装MAVEN 进入maven官网下载最新maven，下载后放到/usr/local目录，解压（具体版本号改为下载对应的） 1. 解压 tar -xzf apache-maven-3.9.0-bin.tar.gz 2. 移动目录 mv apache-maven-3.9.0 /opt/maven 3. 配置环境变量 vi /etc/profile export MAVEN_HOME=/opt/maven/ export PATH=$MAVEN_HOME/bin:$PATH #加载 source /etc/profile 4. 查看maven是否配置成功 mvn -v 5. 配置 cd /opt/maven/conf vi settings.xml &lt;localRepository&gt;/usr/local/maven/repo&lt;/localRepository&gt; 6. 配置阿里云镜像 &lt;mirror&gt; &lt;id&gt;alimaven&lt;/id&gt; &lt;name&gt;aliyun maven&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;/mirror&gt; 三、下载安装Nexus3 1、下载最新Nexus3安装包 https://my.sonatype.com/ 在 Latest Releases 标签下， 下载最新nexus repository安装包 2、解压到 /usr/local/nexus/ 目录下 tar -zxvf nexus-3.37.3-02-unix.tar.gz [root@tcosmo-szls01 nexus]# pwd /usr/local/nexus # 解压完会有两个目录: [root@tcosmo-szls01 nexus]# ll total 0 drwxr-xr-x. 10 root root 181 Jan 26 11:08 nexus-3.37.3-02 ## 包含了 Nexus 运行所需要的文件。是 Nexus 运行必须的 drwxr-xr-x. 3 root root 20 Jan 26 11:08 sonatype-work ## 包含了 Nexus 生成的配置文件、日志文件、仓库文件等。当我们需要备份 Nexus 的时候默认备份此目录即可 3、配置环境变量，并使其生效 [root@tcosmo-szls01 nexus]# vim /etc/profile export NEXUS_HOME=/usr/local/nexus/nexus-3.37.3-02 export PATH=$PATH:$NEXUS_HOME/bin [root@tcosmo-szls01 nexus]# source /etc/profile [root@tcosmo-szls01 nexus]# nexus -version WARNING: ************************************************************ WARNING: Detected execution as &quot;root&quot; user. This is NOT recommended! WARNING: ************************************************************ Usage: /usr/local/nexus/nexus-3.37.3-02/bin/nexus {start|stop|run|run-redirect|status|restart|force-reload} 4、修改默认启动用户 vim /usr/local/nexus/nexus-3.37.3-02/bin/nexus.rc run_as_user=&quot;root&quot; #内容就这一行，放开注释，填写用户即可 5、修改启动端口 vim /usr/local/nexus/nexus-3.37.3-02/etc/nexus-default.properties #默认就是8081，可以不修改 6、启动nexus并查看运行状态 [root@tcosmo-szls01 nexus]# nexus start WARNING: ************************************************************ WARNING: Detected execution as &quot;root&quot; user. This is NOT recommended! WARNING: ************************************************************ Starting nexus [root@tcosmo-szls01 nexus]# nexus status WARNING: ************************************************************ WARNING: Detected execution as &quot;root&quot; user. This is NOT recommended! WARNING: ************************************************************ nexus is running. 7、访问系统 访问http://ip:8081，登陆用户admin 密码存放在：/usr/local/nexus/sonatype-work/nexus3/admin.password 目录 [root@tcosmo-szls01 nexus]# cat /usr/local/nexus/sonatype-work/nexus3/admin.password easdsccc-64ab-45c4-9d8a-abh******jfe19 8、配置nexus开机自启动 vim /etc/rc.d/rc.local /usr/local/nexus/nexus-3.18.1/bin/nexus start #添加这一行内容 chmod 755 /etc/rc.d/rc.local 四、Nexus3的使用 1、在mvn配置中添加nexus的认证信息 &lt;server&gt; &lt;id&gt;mynexus-releases&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;******&lt;/password&gt; &lt;/server&gt; &lt;server&gt; &lt;id&gt;mynexus-snapshots&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;******&lt;/password&gt; &lt;/server&gt; 2、在项目的pom.xml中添加nexus的仓库地址 （如果是多module应用，可以直接在父pom里面配置） &lt;distributionManagement&gt; &lt;repository&gt; &lt;id&gt;mynexus-releases&lt;/id&gt; &lt;name&gt;maven-releases&lt;/name&gt; &lt;url&gt;http://10.206.73.157:8081/repository/maven-releases/&lt;/url&gt; &lt;/repository&gt; &lt;snapshotRepository&gt; &lt;id&gt;mynexus-snapshots&lt;/id&gt; &lt;name&gt;maven-snapshots&lt;/name&gt; &lt;url&gt;http://10.206.73.157:8081/repository/maven-snapshots/&lt;/url&gt; &lt;/snapshotRepository&gt; &lt;/distributionManagement&gt; 3、执行deploy命令，完成打包与推送 mvn clean deploy # 如果像强行要求使用者更新本地依赖，可以 -U ![1643178542(https://tianxiawuhao.github.io/post-images/1643178548537135.png) 4、现在可以在maven-public中查看到我们推送的jar包 5、在Maven项目中的使用 如果我们只有个别依赖需要从我们自己的nexus下载，那么只需要在项目的pom.xml文件中进行配置即可： &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;releases&lt;/id&gt; &lt;name&gt;maven-public&lt;/name&gt; &lt;url&gt;http://10.206.73.157:8081/repository/maven-public/&lt;/url&gt; &lt;/repository&gt; &lt;/repositories&gt; 如果我们有很多依赖都需要从我们自己的nexus下载，那么可以通过修改maven的配置文件，setting.xml增加镜像来实现； &lt;mirror&gt; &lt;id&gt;myNexus&lt;/id&gt; &lt;mirrorOf&gt;*&lt;/mirrorOf&gt; &lt;name&gt;myNexus&lt;/name&gt; &lt;url&gt;http://10.206.73.157:8081/repository/maven-public/&lt;/url&gt; &lt;/mirror&gt; 但是，需要注意的是： 当maven的版本高于3.6之后，我们在项目pom.xml中是没有办法使用http协议的仓库地址的，会报错： maven-default-http-blocker (http://0.0.0.0/): Blocked mirror for repositories 此时，我们要么使用https协议的仓库地址，要么在maven的setting.xml中进行配置； 当maven的配置setting.xml中配置了多个mirror镜像的时候，是没有办法同时生效，依次检查的，只有第一个会生效； 这很尴尬，因为我们一般会喜欢在setting.xml中配置阿里云的镜像仓库，不想完全替换成我们自己的。 补充1：Nexus中默认的那些仓库分别是什么作用，有什么区别？ Type类型的理解： proxy：是远程仓库的代理。比如说在nexus中配置了一个central repository的proxy，当用户向这个proxy请求一个artifact，这个proxy就会先在本地查找，如果找不到的话，就会从远程仓库下载，然后返回给用户，相当于起到一个中转的作用。 Hosted：是宿主仓库，用户可以把自己的一些构件，deploy到hosted中，也可以手工上传构件到hosted里。比如说oracle的驱动程序，ojdbc6.jar，在central repository是获取不到的，就需要手工上传到hosted里，一般用来存放公司自己的jar包； Group：是仓库组，在maven里没有这个概念，是nexus特有的。目的是将上述多个仓库聚合，对用户暴露统一的地址，这样用户就不需要在pom中配置多个地址，只要统一配置group的地址就可以了右边那个Repository Path可以点击进去，看到仓库中artifact列表。不过要注意浏览器缓存，当你的项目希望在多个repository使用资源时就不需要多次引用了，只需要引用一个group即可。 四大仓库的理解： maven-central：代理仓库，使用了proxy模式； maven-release：用来存放release版本的jar包；（我上面就是推送到了release库里面） maven-snapshot：用来存放snapshot版本的jar包； maven-public：maven-central、maven-release和maven-snapshot三个库的合集，对外使用，只要提供这一个group的地址即可。 补充二：文件描述符Ulimit问题修复 vim /etc/security/limits.conf # 添加以下内容（如果我们是root用户启动的话）： @root - nofile 65536 ## 保存后重启系统： reboot 之后，此问题即可被修复！ ","link":"https://tianxiawuhao.github.io/vdJWpe4ZR/"},{"title":"图的遍历方法","content":"从图中某一顶点出发访遍图中其余顶点，且使每一个顶点仅被访问一次，这一过程就叫做图的遍历（Traversing Graph） 访问过的顶点打上标记，避免访问多次而不自知；可以通过设置一个访问数组visited[n]，n是图中顶点个数，初值为0，访问之后设置为1 图遍历要避免因回路陷入死循环，通常有两种遍历次序方案： 深度优先遍历 广度优先遍历 深度优先遍历 深度优先遍历（Depth_First_Search），也有称为深度优先搜索，简称DFS 如上图，如何从顶点A开始走遍所有的图顶点并作上标记？ 从顶点A开始，始终向右手边走，注意，走到最后，还有一个I顶点没走。 深度优先遍历其实就是一个递归的过程，从图右边路径可以看出类似一棵树的前序遍历，确实如此。 从图中某个顶点v出发，访问此顶点，然后从v的未被访问的邻接点出发深度优先遍历图，直至图中所有和v有路径想通的顶点都被访问到。 非连通图，只需要对它的连通分量分别进行深度优先遍历，即在先前一个顶点进行一次深度优先遍历后，若图中尚有顶点未被访问，则选图中一个未被访问的顶点作起始点，重复上述过程，直至图中所有顶点都被访问到为止。 邻接矩阵深度优先遍历 邻接矩阵表示如下图： /** * 邻接矩阵的深度优先遍历（注意I点的遍历比较特殊） */ public class DFSTest { Boolean[] visited = new Boolean[9]; //邻接矩阵的深度优先递归算法 void DFS(Graph graph,int i){ int j; visited[i] = true; System.out.println(graph.verxs[i]);//打印顶点,也可以进行其他操作 for(j = 0;j&lt;graph.numVertexes;j++){ if(graph.arc[i][j] == 1 &amp;&amp; !visited[j]){ DFS(graph, j); } } } //邻接矩阵的深度遍历操作 void DFSTraverse(Graph graph){ int i; for(i = 0;i&lt;graph.numVertexes;i++){ visited[i] = false;//初始化所有顶点状态都是未访问状态 } for(i=0;i&lt;graph.numVertexes;i++){ if(!visited[i]){//对未访问过的顶点调用DFS，若是连通图，则只会执行一次 DFS(graph, i); } } } } 邻接表深度优先遍历 public class DFSTest { //邻接表深度优先递归算法 void DFS(GraphAdjList gl,int i){ EdgeNode p = null; visited[i] = true; System.out.println(gl.vertexNodes[i].data);//打印顶点 p = gl.vertexNodes[i].firstEdge; while(p != null){ if(!visited[p.next.adjvex]){ DFS(gl, p.next.adjvex); } p = p.next; } } //邻接表的深度遍历操作 void DFSTraverse(GraphAdjList gl){ int i; for(i = 0;i&lt;gl.numVertexes;i++){ visited[i] = false;//初始化所有顶点状态都是未访问状态 } for(i=0;i&lt;gl.numVertexes;i++){ if(!visited[i]){//对未访问过的顶点调用DFS，若是连通图，则只会执行一次 DFS(gl, i); } } } } 总结：两个不同存储结构深度优先遍历算法 邻接矩阵是二维数组，要查找每个顶点的邻接点需要访问矩阵中的元素，因此需要O(n^2)的时间 邻接表做存储结构，需要时间取决于顶点和边的数量，所以是O(n+e) 显然对于点多边少的稀疏图来说，邻接表结构使得算法在时间效率上大大提高。 广度优先遍历 广度优先遍历（Breadth First Search），又称广度优先搜索，简称BFS。 深度优先遍历未必是最佳方案，它意味着要彻底查找完一个地方，然后才查找另一个地方。 DFS类似于树的前序遍历，BFS类似于树的层序遍历。 如上图所示，顶点A作为第一层，A的所有边顶点BF作为第二层，BF的所有边顶点CIGE作为第三层，依次类推。 邻接矩阵广度优先遍历 public class BFSTest { //邻接表深度优先递归算法 //邻接矩阵广度遍历算法 void BFSTraverse(Graph g){ int i,j; Queue&lt;Integer&gt; q; for(i=0;i&lt;g.numVertexes;i++){ visited[i] = false; } q = new LinkedList&lt;Integer&gt;(); for(i=0;i&lt;g.numVertexes;i++){ if(!visited[i]){//未访问就处理 visited[i] = true; System.out.println(g.verxs[i]);//打印顶点 q.offer(i);//此顶点入队 while(!q.isEmpty()){//若当前队列不为空 i = q.poll();//出队并记录此顶点在数组中的下标 for(j=0;j&lt;g.numVertexes;j++){ //判断其他顶点若与当前顶点存在边，并且从未访问过 if(g.arc[i][j] == 1 &amp;&amp; !visited[j]){ visited[j] = true; //打印顶点 System.out.println(g.verxs[j]); //将找到的顶点入队列 q.offer(j); } } } } } } } 邻接表广度优先遍历 public class BFSTest { //邻接表广度遍历算法 void BFSTraverse(GraphAdjList g){ int i; EdgeNode p; Queue&lt;Integer&gt; q; for(i=0;i&lt;g.numVertexes;i++){ visited[i] = false; } q = new LinkedList&lt;Integer&gt;(); for(i=0;i&lt;g.numVertexes;i++){ if(!visited[i]){ visited[i] = true; //打印顶点 System.out.println(g.vertexNodes[i].data); q.offer(i); while(!q.isEmpty()){ i = q.poll(); //找到当前顶点边表链表头指针 p = g.vertexNodes[i].firstEdge; while(p != null){ if(!visited[p.adjvex]){ visited[p.adjvex] = true; System.out.println(g.vertexNodes[p.adjvex].data); q.offer(p.adjvex); } p = p.next;//指针指向下一个邻接点 } } } } } } 总结 两种遍历时间复杂度是相同的，不同的是对顶点的访问顺序不同。 两种算法没有优劣之分，视不同的情况选择不同的算法。 深度优先更适合目标比较明确，以找到目标为主要目的的情况 广度优先更适合在不断扩大遍历范围时找到相对最优解的情况 ","link":"https://tianxiawuhao.github.io/w4yOxhPAW/"},{"title":"Centos7部署MinIO","content":" 英文官网：https://min.io/ 中文官网：http://minio.org.cn/（更新不及时，容易被坑） 一、为什么要使用MinIO 其实抛开Ceph这类复杂的存储解决方案外，小公司小团队的选择一般都会是FastDFS或者MinIO，那么这里我们就简单对比下MinIO相较于FastDFS的优势！ 1、安装部署(运维)复杂度 虽然我觉得FastDFS安装也不算很复杂，但是MinIO的安装可以说更简单； FastDFS安装： 2、文档 FastDFS可以说发展了十几年几乎没有官方文档，都是靠热心网友投稿； 而MinIO文档还是比较全的，中文文档：http://docs.minio.org.cn/docs/master/minio-monitoring-guide 3、开源项目运营组织 FastDFS是阿里余庆大神的个人项目，而MinIO背靠的是Apache开源组织； 4、UI界面 FastDFS默认是不带UI界面的，而MinIO服务部署时候，是自带UI界面的，开箱即用； 5、性能 MinIO号称是世界上速度最快的对象存储服务器，标准硬件设备上，读/写速度都是GB级别的；FastDFS差多了； 6、容器化支持 MinIO提供了与k8s、etcd、docker等容器化技术深度集成方案，可以说就是为了云环境而生的。这点是FastDFS不具备的。 7、丰富的SDK支持 MinIO支持的不同语言SDK多于FastDFS，尤其文档，更是完胜； 8、AWS S3标准兼容 Amazon的S3 API是对象存储领域的事实标准。MinIO是S3兼容性的事实上的标准，是第一个采用API和第一个添加对S3 Select支持的标准之一。包括微软Azure在内的750多家公司使用MinIO的S3网关，这一数字超过了业内其他公司的总和。 什么意思？就是说你现在为了节约成本使用MinIO，等你的公司壮大了、有钱了。不想自己运维基础设施了，你就可以把对象存储放到云上，只要云厂商支持S3标准，你的应用程序是不需要重新开发的。 二、MinIO的部署之Centos单机部署 此单机模式，每一份对象数据，minio直接在data目录下存储一份数据，不会建立副本，也不会启用纠删码机制（数据恢复），存在单点故障； 1、直接下载二进制执行文件，并启动： 文件下载地址：https://min.io/download#/linux wget https://dl.min.io/server/minio/release/linux-amd64/minio chmod +x minio MINIO_ROOT_USER=admin MINIO_ROOT_PASSWORD=password ./minio server /mnt/data --console-address &quot;:9001&quot; 后端调用就是9000端口，访问前端页面就是9001端口： 一切都是如此的简单！ 编写一个后台运行的nohup脚本runminio.sh: #!/bin/bash export MINIO_ROOT_USER=admin export MINIO_ROOT_PASSWORD=Haier2022 nohup /usr/local/minio/minio server /mnt/data --console-address &quot;:9001&quot; &gt; log.file 2&gt;&amp;1 &amp; #默认服务端口为9000，如果端口冲突，可以修改端口 nohup /usr/local/minio/minio server /mnt/data --address &quot;:9999&quot; --console-address &quot;:9001&quot; &gt; log.file 2&gt;&amp;1 &amp; nohup命令 关闭当前session不会中断程序，可以通过kill等命令终止。 示例及说明 nohup command &gt; output.log 2&gt;&amp;1 &amp; 其中 2&gt;&amp;1是用来将标准错误2重定向到标准输出1中。1前面的&amp;是为了让bash将1解释成标准输出而不是文件1。而最后一个&amp;是为了让bash在后台执行。 2、配置Minio的开机自启： 创建配置文件：minio.conf [root@localhost minio]# mkdir /usr/local/minio/conf [root@localhost minio]# vim /usr/local/minio/conf/minio.conf MINIO_VOLUMES=&quot;/data/minio&quot; MINIO_OPTS=&quot;--console-address :9001&quot; MINIO_ROOT_USER=&quot;admin&quot; MINIO_ROOT_PASSWORD=&quot;Wolong2022&quot; MINIO_SERVER_URL=&quot;http://192.168.0.117:9000&quot; 在/etc/systemd/system目录下新建一个服务minio.service [root@localhost minio]# vim /etc/systemd/system/minio.service [Unit] Description=MinIO Documentation=https://docs.min.io Wants=network-online.target After=network-online.target AssertFileIsExecutable=/usr/local/minio/minio [Service] User=root Group=root EnvironmentFile=/usr/local/minio/conf/minio.conf ExecStart=/usr/local/minio/minio server $MINIO_OPTS $MINIO_VOLUMES # Let systemd restart this service always Restart=always # Specifies the maximum file descriptor number that can be opened by this process LimitNOFILE=65536 # Disable timeout logic and wait until process is stopped TimeoutStopSec=infinity SendSIGKILL=no [Install] WantedBy=multi-user.target 3、启动minio并设置开机自启： [root@localhost minio]# systemctl start minio [root@localhost minio]# systemctl enable minio Created symlink from /etc/systemd/system/multi-user.target.wants/minio.service to /etc/systemd/system/minio.service. [root@localhost minio]# systemctl status minio ● minio.service - MinIO Loaded: loaded (/etc/systemd/system/minio.service; enabled; vendor preset: disabled) Active: active (running) since 二 2022-11-01 15:19:19 CST; 14s ago Docs: https://docs.min.io Main PID: 34443 (minio) CGroup: /system.slice/minio.service └─34443 /usr/local/minio/minio server --console-address :9001 /data/minio 三、MinIO的部署之Docker单机部署 此单机模式，每一份对象数据，minio直接在data目录下存储一份数据，不会建立副本，也不会启用纠删码机制（数据恢复），存在单点故障； 直接执行docker命令： docker run -d \\ -p 9000:9000 \\ -p 9001:9001 \\ --name minio \\ -e &quot;MINIO_ROOT_USER=admin&quot; \\ -e &quot;MINIO_ROOT_PASSWORD=password&quot; \\ -v /mnt/data:/data \\ -v /mnt/config:/root/.minio \\ quay.io/minio/minio server /data --console-address &quot;:9001&quot; 四、MinIO的部署之纠删码模式部署 该部署模式使用纠删码（Reasure Code）和校验checksum来保护数据免受硬件故障或无声数据损坏，即使你丢了一般数量（N/2）的硬盘，你仍可以恢复数据！ 纠删码技术的官方描述资料：http://docs.minio.org.cn/docs/master/minio-erasure-code-quickstart-guide 使用docker进行单机版的纠删码模式部署： docker run -d \\ -p 9000:9000 \\ -p 9001:9001 \\ --name minio \\ -e &quot;MINIO_ROOT_USER=admin&quot; \\ -e &quot;MINIO_ROOT_PASSWORD=password&quot; \\ -v /mnt/data1:/data1 \\ -v /mnt/data2:/data2 \\ -v /mnt/data3:/data3 \\ -v /mnt/data4:/data4 \\ -v /mnt/data5:/data5 \\ -v /mnt/data6:/data6 \\ -v /mnt/data7:/data7 \\ -v /mnt/data8:/data8 \\ -v /mnt/config:/root/.minio \\ quay.io/minio/minio server /data{1...8} --console-address &quot;:9001&quot; 执行成功后，我们登录到后台，然后上传几个文件看看效果： [root@node2 /]# tree /mnt /mnt ├── config │ └── certs │ └── CAs ├── data │ └── haier │ ├── harbor-offline-installer-v1.10.10.tgz │ ├── maven.jpeg │ └── nexus3.jpg ├── data1 │ └── haier │ ├── maven.jpeg │ │ └── xl.meta │ └── nexus3.jpg │ └── xl.meta ├── data2 │ └── haier │ ├── maven.jpeg │ │ └── xl.meta │ └── nexus3.jpg │ └── xl.meta ├── data3 │ └── haier │ ├── maven.jpeg │ │ └── xl.meta │ └── nexus3.jpg │ └── xl.meta ├── data4 │ └── haier │ ├── maven.jpeg │ │ └── xl.meta │ └── nexus3.jpg │ └── xl.meta ├── data5 │ └── haier │ ├── maven.jpeg │ │ └── xl.meta │ └── nexus3.jpg │ └── xl.meta ├── data6 │ └── haier │ ├── maven.jpeg │ │ └── xl.meta │ └── nexus3.jpg │ └── xl.meta ├── data7 │ └── haier │ ├── maven.jpeg │ │ └── xl.meta │ └── nexus3.jpg │ └── xl.meta └── data8 └── haier ├── maven.jpeg │ └── xl.meta └── nexus3.jpg └── xl.meta 37 directories, 19 files 此时，我做了一次简单的测试，删盘（其实是删除文件夹）： 当我删除4个文件夹时候，数据依然可以正常访问； 当我删除第5个文件夹时候，数据文法正常访问，UI页面无法登录，报错： 五、MinIO的部署之分部署集群部署 启动一个分布式Minio实例，你只需要把硬盘位置做为参数传给minio server命令即可，然后，你需要在所有其它节点运行同样的命令。 分布式Minio里所有的节点需要有同样的access秘钥和secret秘钥，这样这些节点才能建立联接。为了实现这个，你需要在执行minio server命令之前，先将access秘钥和secret秘钥export成环境变量，新版本使用MINIO_ROOT_USER 和 MINIO_ROOT_PASSWORD。 分布式Minio使用的磁盘里必须是干净的，里面没有数据。 下面示例里的IP仅供示例参考，你需要改成你真实用到的IP和文件夹路径。 分布式Minio里的节点时间差不能超过3秒，你可以使用NTP 来保证时间一致。 在Windows下运行分布式Minio处于实验阶段，请悠着点使用。 比如我打算用三台机器部署分布式集群，挂载6块盘 1、首先，在三台机器的 /usr/local/minio 目录下下载好我们的minio程序，并添加环境变量： chmod +x minio export MINIO_ROOT_USER=admin export MINIO_ROOT_PASSWORD=Haier2022 2、在每台机器上，都执行以下命令，启动集群： nohup /usr/local/minio/minio server --config-dir /minio/config --address ':9000' --console-address ':9001' \\ http://10.206.73.154/minio/data1 http://10.206.73.154/minio/data2\\ http://10.206.73.155/minio/data1 http://10.206.73.155/minio/data2\\ http://10.206.73.156/minio/data1 http://10.206.73.156/minio/data2&gt; /usr/local/minio/log.file 2&gt;&amp;1 &amp; 但是当我们执行时，经常会碰到这个问题： 即必须要使用新的挂载磁盘，不可以使用与系统root目录相同的磁盘；这时候我们就要想办法先给系统挂载一块新磁盘到 /minio 目录上，之后才能进行之后的操作了！ 3、挂载完了，但是我只有一块机器有磁盘，将就一下： nohup /usr/local/minio/minio server --config-dir /minio/config --address ':9000' --console-address ':9001' \\ http://10.206.73.156/minio/data1 http://10.206.73.156/minio/data2 \\ http://10.206.73.156/minio/data3 http://10.206.73.156/minio/data4 \\ http://10.206.73.156/minio/data5 http://10.206.73.156/minio/data6&gt; /usr/local/minio/log.file 2&gt;&amp;1 &amp; 4、在minio的9001面板中，对服务进行监控： ","link":"https://tianxiawuhao.github.io/CIqmuU7wS/"},{"title":"存储系统搭建—Ceph集群","content":"一、Ceph基础简介 nfs存储方式只能用于开发测试，生产环节绝对不可以用，我们选择Ceph！ 1、Ceph简介 Ceph是一种为优秀的性能、可靠性和可扩展性而设计的统一的、分布式文件系统。 2、Ceph集群的安装方式选择 官方安装文档：https://docs.ceph.com/en/pacific/install Cephadm安装（推荐）： Rook安装（推荐）：使用Rook部署一个能管理Ceph集群的系统，一步到位！Rook官网： 其他方式（手动-不推荐） 3、Rook简介：https://www.rook.io/docs/rook/v1.7/quickstart.html Rook：存储编排系统； K8s：容器编排系统； Rook的工作原理： Rook的架构设计： 二、Rook+Ceph的安装 1、查看前提条件——硬件要求 Raw devices (no partitions or formatted filesystems)； 原始磁盘，无分区或者格式化 Raw partitions (no formatted filesystem)；原始分区，无格式化文件系统 意思就是不要使用自己系统已经用过的磁盘，最好是干净的几块磁盘，因为Ceph底层有自己的文件系统（是基于LVM2可拓展的文件系统） fdisk -l 找到自己挂载的磁盘 如： Disk /dev/vdc: 107.4 GB, 107374182400 bytes, 209715200 sectors # 查看满足要求的 lsblk -f ## 结果： vda └─vda1 xfs 9cff3d69-3769-4ad9-8460-9c54050583f9 / vdb swap YUNIFYSWAP 48eb1df6-1663-4a52-ab30-040d552c2d76 vdc #没有任何文件系统的干净磁盘，可用 #云厂商都这么磁盘清0 dd if=/dev/zero of=/dev/vdc bs=1M status=progress 2、升级内核 升级内核（可选） 如果你将使用 Ceph 共享文件系统（CephFS）来创建卷（Volumes），则 rook 官方建议使用至少 4.17 版本的 Linux 内核。如果你使用的内核版本低于 4.17，则请求的 Persistent Volume Claim（PVC）大小将不会被强制执行。存储配额（Storage quotas）只能在较新的内核上得到强制执行。 在 Kubernetes 中，PVC 用于向存储系统请求指定大小的存储空间。如果请求的 PVC 大小无法得到强制执行，则无法保证所请求的存储空间大小。由于存储配额在较旧的内核上无法得到强制执行，因此在使用 CephFS 创建卷时，如果使用较旧的内核版本，则可能无法正确地管理和分配存储空间。因此，rook 官方建议使用至少 4.17 版本的内核。 在所有k8s节点升级内核，之前忘记考虑这个事情，以后得提前升级好内核再搭建k8s集群，这种情况下升级内核，很难保证不会对k8s集群带来直接影响。注意了，生产环境可不能这么玩，得提前做好规划和准备。 我的centos7内核当前版本： [root@k8s-a-node01 ~]# uname -r 3.10.0-1160.el7.x86_64 开始升级 rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org rpm -Uvh https://www.elrepo.org/elrepo-release-7.el7.elrepo.noarch.rpm yum --enablerepo=elrepo-kernel install kernel-ml-devel kernel-ml-headers kernel-ml -y grub2-set-default 0 grub2-mkconfig -o /boot/grub2/grub.cfg reboot uname -sr # 升级完成后查看内核，已经满足条件 [root@k8s-a-node10 ~]# uname -sr Linux 6.2.9-1.el7.elrepo.x86_64 # 看看k8s是否正常，我的没问题，非常顺利。 kubectl get nodes kubectl get pod -n kube-system 3、克隆Rook的git仓库代码 git clone --single-branch --branch v1.11.2 https://github.com/rook/rook.git cd rook/deploy/examples 4、修改安装operator.yaml： 把文件中的默认镜像修改为自己的： ##首先把ceph镜像换成我们自己的 rook/ceph:v1.6.3 换成 xxxx/rook-ceph:v1.6.3 ## 建议修改以下的东西。在operator.yaml里面 ROOK_CSI_CEPH_IMAGE: &quot;xxxx/cephcsi:v3.3.1&quot; ROOK_CSI_REGISTRAR_IMAGE: &quot;xxxx/csi-node-driver-registrar:v2.0.1&quot; ROOK_CSI_RESIZER_IMAGE: &quot;xxxx/csi-resizer:v1.0.1&quot; ROOK_CSI_PROVISIONER_IMAGE: &quot;xxxx/csi-provisioner:v2.0.4&quot; ROOK_CSI_SNAPSHOTTER_IMAGE: &quot;xxxx/csi-snapshotter:v4.0.0&quot; ROOK_CSI_ATTACHER_IMAGE: &quot;xxxx/csi-attacher:v3.0.2&quot; 安装operator： kubectl create -f crds.yaml -f common.yaml -f operator.yaml # 在继续操作之前，验证 rook-ceph-operator 是否正常运行 kubectl get deployment -n rook-ceph kubectl get pod -n rook-ceph 5、修改 cluster.yaml中的集群信息： 因为默认是使用所有节点，如果我们不适用所有节点，则需要做以下修改： storage: # cluster level storage configuration and selection # resources 生产环境尽量配置大一点，测试环境不用动。 # useAllNodes: false 是否用所有节点 默认用所有 # useAllDevices: false 是否用所有设备 默认用所有 useAllNodes: false useAllDevices: false config: osdsPerDevice: &quot;3&quot; #每个设备osd数量 nodes: - name: &quot;k8s-node3&quot; devices: - name: &quot;vdc&quot; - name: &quot;k8s-node1&quot; devices: - name: &quot;vdc&quot; - name: &quot;k8s-node2&quot; devices: - name: &quot;vdc&quot; 6、正式安装集群： kubectl apply -f cluster.yaml 漫长等待，部署成功后的pod数量： [root@master01 examples]# kubectl get pod -n rook-ceph NAME READY STATUS RESTARTS AGE IP NODE csi-cephfsplugin-74xkg 3/3 Running 0 125m 10.233.96.16 node2 csi-cephfsplugin-98xmt 3/3 Running 0 125m 10.233.90.14 node1 csi-cephfsplugin-provisioner-5d498c4bdd-vq2pc 6/6 Running 0 125m 10.233.90.15 node1 csi-cephfsplugin-provisioner-5d498c4bdd-x97g7 6/6 Running 0 125m 10.233.96.17 node2 csi-cephfsplugin-zknsc 3/3 Running 0 118m 10.233.70.14 master csi-rbdplugin-provisioner-7bd657db4c-6cl6r 6/6 Running 0 125m 10.233.96.15 node2 csi-rbdplugin-provisioner-7bd657db4c-tg4d2 6/6 Running 0 125m 10.233.90.13 node1 csi-rbdplugin-skx2m 3/3 Running 0 118m 10.233.70.12 master csi-rbdplugin-t86nx 3/3 Running 0 125m 10.233.96.14 node2 csi-rbdplugin-vww89 3/3 Running 0 125m 10.233.90.12 node1 rook-ceph-crashcollector-master-58b49b5db6-m9m5v 1/1 Running 0 11m 10.233.70.21 master rook-ceph-crashcollector-node1-5965f9db96-ncgm8 1/1 Running 0 12m 10.233.90.28 node1 rook-ceph-crashcollector-node2-7d67bb865-sh2ch 1/1 Running 0 11m 10.233.96.39 node2 rook-ceph-mgr-a-7bd599b466-bgkqg 1/1 Running 0 12m 10.233.96.35 node2 rook-ceph-mon-a-6ff5f6b6cb-99mcz 1/1 Running 0 18m 10.233.90.25 node1 rook-ceph-mon-c-5cf8b995b5-pxssc 1/1 Running 0 14m 10.233.96.33 node2 rook-ceph-mon-d-6cd9c57459-pr5dp 1/1 Running 0 11m 10.233.70.22 master rook-ceph-operator-5bbbb569df-5nbh9 1/1 Running 0 150m 10.233.96.12 node2 rook-ceph-osd-0-64b57fd54c-trmt5 1/1 Running 0 11m 10.233.90.29 node1 rook-ceph-osd-1-6b7568d5c7-bzt9v 1/1 Running 0 11m 10.233.96.38 node2 rook-ceph-osd-prepare-node1-2vhbd 0/1 Completed 0 10m 10.233.90.33 node1 rook-ceph-osd-prepare-node2-646sj 0/1 Completed 0 10m 10.233.96.41 node2 7、验证ceph集群 要验证群集是否处于正常状态，可以连接到 toolbox 工具箱并运行命令 kubectl create -f toolbox.yaml kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- bash # 进去之后就可以执行各种命令了： ceph status ceph osd status ceph df rados df # 看看我的 [root@k8s-a-master examples]kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- bash bash-4.4$ ceph -s cluster: id: 0b64957b-9aaa-43ef-9946-211ca911dc92 health: HEALTH_OK services: mon: 3 daemons, quorum a,b,c (age 3m) mgr: a(active, since 64s), standbys: b osd: 3 osds: 3 up (since 116s), 3 in (since 2m) data: pools: 1 pools, 1 pgs objects: 2 objects, 449 KiB usage: 234 MiB used, 9.8 TiB / 9.8 TiB avail pgs: 1 active+clean bash-4.4$ ceph osd status ID HOST USED AVAIL WR OPS WR DATA RD OPS RD DATA STATE 0 k8s-a-node02 23.9M 999G 0 0 0 0 exists,up 1 k8s-a-node03 20.9M 999G 0 0 0 0 exists,up 2 k8s-a-node01 23.8M 999G 0 0 0 0 exists,up 每个 OSD 都有一个状态，可以是以下几种之一： up：表示该 OSD 正常运行，并且可以处理数据请求。 down：表示该 OSD 当前无法运行或不可用，可能由于硬件故障或软件问题。 out：表示该 OSD 不参与数据存储或恢复，这通常是由于管理员手动将其标记为不可用。 exists：表示该 OSD 配置存在，但尚未启动或加入集群。 8、确保mgr-dashboard这个service是可以访问的 [root@master ceph]# kubectl get svc -n rook-ceph NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE csi-cephfsplugin-metrics ClusterIP 10.233.24.242 &lt;none&gt; 8080/TCP,8081/TCP 129m csi-rbdplugin-metrics ClusterIP 10.233.1.189 &lt;none&gt; 8080/TCP,8081/TCP 129m rook-ceph-mgr ClusterIP 10.233.57.190 &lt;none&gt; 9283/TCP 15m rook-ceph-mgr-dashboard ClusterIP 10.233.59.74 &lt;none&gt; 8443/TCP 15m rook-ceph-mon-a ClusterIP 10.233.2.171 &lt;none&gt; 6789/TCP,3300/TCP 22m rook-ceph-mon-c ClusterIP 10.233.51.168 &lt;none&gt; 6789/TCP,3300/TCP 18m rook-ceph-mon-d ClusterIP 10.233.58.171 &lt;none&gt; 6789/TCP,3300/TCP 14m rook-ceph-mgr：是 Ceph 的管理进程（Manager），负责集群的监控、状态报告、数据分析、调度等功能，它默认监听 9283 端口，并提供了 Prometheus 格式的监控指标，可以被 Prometheus 拉取并用于集群监控。 rook-ceph-mgr-dashboard：是 Rook 提供的一个 Web 界面，用于方便地查看 Ceph 集群的监控信息、状态、性能指标等。 rook-ceph-mon：是 Ceph Monitor 进程的 Kubernetes 服务。Ceph Monitor 是 Ceph 集群的核心组件之一，负责维护 Ceph 集群的状态、拓扑结构、数据分布等信息，是 Ceph 集群的管理节点。 9、使用 NodePort 类型的Service暴露dashboard dashboard-external-https.yaml apiVersion: v1 kind: Service metadata: name: rook-ceph-mgr-dashboard-external-https namespace: rook-ceph labels: app: rook-ceph-mgr rook_cluster: rook-ceph spec: ports: - name: dashboard port: 8443 protocol: TCP targetPort: 8443 selector: app: rook-ceph-mgr rook_cluster: rook-ceph sessionAffinity: None type: NodePort 10、创建后NodePort类的svc后查看 [root@k8s-a-master examples]# kubectl get svc -n rook-ceph NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE rook-ceph-mgr ClusterIP 10.96.58.93 &lt;none&gt; 9283/TCP 28m rook-ceph-mgr-dashboard ClusterIP 10.103.131.77 &lt;none&gt; 8443/TCP 28m # 记得干掉 rook-ceph-mgr-dashboard-external-https NodePort 10.107.247.53 &lt;none&gt; 8443:32564/TCP 9m38s rook-ceph-mon-a ClusterIP 10.98.168.35 &lt;none&gt; 6789/TCP,3300/TCP 29m rook-ceph-mon-b ClusterIP 10.103.127.134 &lt;none&gt; 6789/TCP,3300/TCP 29m rook-ceph-mon-c ClusterIP 10.111.46.187 &lt;none&gt; 6789/TCP,3300/TCP 29m 三、Ceph的后续使用 Ceph的后台的管理员账号为admin，为了方便，我还是换成了NodePort的方式暴露Dashboard了！ 1、获取Ceph后台的默认访问密码： [root@master ceph]# kubectl -n rook-ceph get secret rook-ceph-dashboard-password -o jsonpath=&quot;{['data']['password']}&quot; | base64 --decode &amp;&amp; echo JXr&quot;@`8)_?]7%GwP,(C^ 后台如图： 登录完，右上角修改以下密码！ 2、创建存储池+StorageClass——block-ceph.yaml——用于块存储的StorageClass apiVersion: ceph.rook.io/v1 kind: CephBlockPool metadata: name: replicapool namespace: rook-ceph spec: failureDomain: host #容灾模式，host或者osd replicated: size: 2 #数据副本数量 --- apiVersion: storage.k8s.io/v1 kind: StorageClass #存储驱动 metadata: name: rook-ceph-block # Change &quot;rook-ceph&quot; provisioner prefix to match the operator namespace if needed provisioner: rook-ceph.rbd.csi.ceph.com parameters: # clusterID is the namespace where the rook cluster is running clusterID: rook-ceph # Ceph pool into which the RBD image shall be created pool: replicapool # (optional) mapOptions is a comma-separated list of map options. # For krbd options refer # https://docs.ceph.com/docs/master/man/8/rbd/#kernel-rbd-krbd-options # For nbd options refer # https://docs.ceph.com/docs/master/man/8/rbd-nbd/#options # mapOptions: lock_on_read,queue_depth=1024 # (optional) unmapOptions is a comma-separated list of unmap options. # For krbd options refer # https://docs.ceph.com/docs/master/man/8/rbd/#kernel-rbd-krbd-options # For nbd options refer # https://docs.ceph.com/docs/master/man/8/rbd-nbd/#options # unmapOptions: force # RBD image format. Defaults to &quot;2&quot;. imageFormat: &quot;2&quot; # RBD image features. Available for imageFormat: &quot;2&quot;. CSI RBD currently supports only `layering` feature. imageFeatures: layering # The secrets contain Ceph admin credentials. csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph # Specify the filesystem type of the volume. If not specified, csi-provisioner # will set default as `ext4`. Note that `xfs` is not recommended due to potential deadlock # in hyperconverged settings where the volume is mounted on the same node as the osds. csi.storage.k8s.io/fstype: ext4 # Delete the rbd volume when a PVC is deleted reclaimPolicy: Delete allowVolumeExpansion: true 3、*创建存储池**+StorageClass**——cephfs-ceph.yaml——用于共享文件存储的StorageClass* apiVersion: ceph.rook.io/v1 kind: CephFilesystem metadata: name: myfs namespace: rook-ceph # namespace:cluster spec: # The metadata pool spec. Must use replication. metadataPool: replicated: size: 2 requireSafeReplicaSize: true parameters: # Inline compression mode for the data pool # Further reference: https://docs.ceph.com/docs/nautilus/rados/configuration/bluestore-config-ref/#inline-compression compression_mode: none # gives a hint (%) to Ceph in terms of expected consumption of the total cluster capacity of a given pool # for more info: https://docs.ceph.com/docs/master/rados/operations/placement-groups/#specifying-expected-pool-size #target_size_ratio: &quot;.5&quot; # The list of data pool specs. Can use replication or erasure coding. dataPools: - failureDomain: host replicated: size: 2 # Disallow setting pool with replica 1, this could lead to data loss without recovery. # Make sure you're *ABSOLUTELY CERTAIN* that is what you want requireSafeReplicaSize: true parameters: # Inline compression mode for the data pool # Further reference: https://docs.ceph.com/docs/nautilus/rados/configuration/bluestore-config-ref/#inline-compression compression_mode: none # gives a hint (%) to Ceph in terms of expected consumption of the total cluster capacity of a given pool # for more info: https://docs.ceph.com/docs/master/rados/operations/placement-groups/#specifying-expected-pool-size #target_size_ratio: &quot;.5&quot; # Whether to preserve filesystem after CephFilesystem CRD deletion preserveFilesystemOnDelete: true # The metadata service (mds) configuration metadataServer: # The number of active MDS instances activeCount: 1 # Whether each active MDS instance will have an active standby with a warm metadata cache for faster failover. # If false, standbys will be available, but will not have a warm cache. activeStandby: true # The affinity rules to apply to the mds deployment placement: # nodeAffinity: # requiredDuringSchedulingIgnoredDuringExecution: # nodeSelectorTerms: # - matchExpressions: # - key: role # operator: In # values: # - mds-node # topologySpreadConstraints: # tolerations: # - key: mds-node # operator: Exists # podAffinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - rook-ceph-mds # topologyKey: kubernetes.io/hostname will place MDS across different hosts topologyKey: kubernetes.io/hostname preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: app operator: In values: - rook-ceph-mds # topologyKey: */zone can be used to spread MDS across different AZ # Use &lt;topologyKey: failure-domain.beta.kubernetes.io/zone&gt; in k8s cluster if your cluster is v1.16 or lower # Use &lt;topologyKey: topology.kubernetes.io/zone&gt; in k8s cluster is v1.17 or upper topologyKey: topology.kubernetes.io/zone # A key/value list of annotations annotations: # key: value # A key/value list of labels labels: # key: value resources: # The requests and limits set here, allow the filesystem MDS Pod(s) to use half of one CPU core and 1 gigabyte of memory # limits: # cpu: &quot;500m&quot; # memory: &quot;1024Mi&quot; # requests: # cpu: &quot;500m&quot; # memory: &quot;1024Mi&quot; # priorityClassName: my-priority-class mirroring: enabled: false --- apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: rook-cephfs # annotations: # storageclass.kubernetes.io/is-default-class: &quot;true&quot; # Change &quot;rook-ceph&quot; provisioner prefix to match the operator namespace if needed provisioner: rook-ceph.cephfs.csi.ceph.com parameters: # clusterID is the namespace where operator is deployed. clusterID: rook-ceph # CephFS filesystem name into which the volume shall be created fsName: myfs # Ceph pool into which the volume shall be created # Required for provisionVolume: &quot;true&quot; pool: myfs-data0 # The secrets contain Ceph admin credentials. These are generated automatically by the operator # in the same namespace as the cluster. csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph reclaimPolicy: Delete allowVolumeExpansion: true 上面两个StorageClass都创建完成后，可以再后台存储池中看到： 4、当StorageClass都准备好之后，我们就可以查看到了： [root@master ceph]# kubectl get sc -A NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE local (default) openebs.io/local Delete WaitForFirstConsumer false 24h rook-ceph-block rook-ceph.rbd.csi.ceph.com Delete Immediate true 22m rook-cephfs rook-ceph.cephfs.csi.ceph.com Delete Immediate true 3m26s 之后，在申请pvc的时候，我们只需要指定对应的StorageClass即可！ ","link":"https://tianxiawuhao.github.io/nfaGbmKb_/"},{"title":"Clickhouse优缺点及性能情况","content":"优点： 1，为了高效的使用CPU，数据不仅仅按列存储，同时还按向量进行处理； 2，数据压缩空间大，减少IO；处理单查询高吞吐量每台服务器每秒最多数十亿行； 3，索引非B树结构，不需要满足最左原则；只要过滤条件在索引列中包含即可；即使在使用的数据不在索引中，由于各种并行处理机制ClickHouse全表扫描的速度也很快； 4，写入速度非常快，50-200M/s，对于大量的数据更新非常适用。 缺点： 1，不支持事务，不支持真正的删除/更新； 2，不支持高并发，官方建议qps为100，可以通过修改配置文件增加连接数，但是在服务器足够好的情况下； 3，SQL满足日常使用80%以上的语法，join写法比较特殊；最新版已支持类似SQL的join，但性能不好； 4，尽量做1000条以上批量的写入，避免逐行insert或小批量的insert，update，delete操作，因为ClickHouse底层会不断的做异步的数据合并，会影响查询性能，这个在做实时数据写入的时候要尽量避开； 5，Clickhouse快是因为采用了并行处理机制，即使一个查询，也会用服务器一半的CPU去执行，所以ClickHouse不能支持高并发的使用场景，默认单查询使用CPU核数为服务器核数的一半，安装时会自动识别服务器核数，可以通过配置文件修改该参数。 全量数据导入：数据导入临时表 -&gt; 导入完成后，将原表改名为tmp1 -&gt; 将临时表改名为正式表 -&gt; 删除原表 增量数据导入： 增量数据导入临时表 -&gt; 将原数据除增量外的也导入临时表 -&gt; 导入完成后，将原表改名为tmp1-&gt; 将临时表改成正式表-&gt; 删除原数据表 相关优化： 1，关闭虚拟内存，物理内存和虚拟内存的数据交换，会导致查询变慢。 2，为每一个账户添加join_use_nulls配置，左表中的一条记录在右表中不存在，右表的相应字段会返回该字段相应数据类型的默认值，而不是标准SQL中的Null值。 3，JOIN操作时一定要把数据量小的表放在右边，ClickHouse中无论是Left Join 、Right Join还是Inner Join永远都是拿着右表中的每一条记录到左表中查找该记录是否存在，所以右表必须是小表。 4，批量写入数据时，必须控制每个批次的数据中涉及到的分区的数量，在写入之前最好对需要导入的数据进行排序。无序的数据或者涉及的分区太多，会导致ClickHouse无法及时对新导入的数据进行合并，从而影响查询性能。 5，尽量减少JOIN时的左右表的数据量，必要时可以提前对某张表进行聚合操作，减少数据条数。有些时候，先GROUP BY再JOIN比先JOIN再GROUP BY查询时间更短。 6，ClickHouse的分布式表性能性价比不如物理表高，建表分区字段值不宜过多，防止数据导入过程磁盘可能会被打满。 7，CPU一般在50%左右会出现查询波动，达到70%会出现大范围的查询超时，CPU是最关键的指标，要非常关注。 性能情况 1,单个查询吞吐量：如果数据被放置在page cache中，则一个不太复杂的查询在单个服务器上大约能够以2-10GB／s（未压缩）的速度进行处理（对于简单的查询，速度可以达到30GB／s）。如果数据没有在page cache中的话，那么速度将取决于你的磁盘系统和数据的压缩率。例如，如果一个磁盘允许以400MB／s的速度读取数据，并且数据压缩率是3，则数据的处理速度为1.2GB/s。这意味着，如果你是在提取一个10字节的列，那么它的处理速度大约是1-2亿行每秒。对于分布式处理，处理速度几乎是线性扩展的，但这受限于聚合或排序的结果不是那么大的情况下。 2，处理短查询的延时时间：数据被page cache缓存的情况下，它的延迟应该小于50毫秒(最佳情况下应该小于10毫秒)。 否则，延迟取决于数据的查找次数。延迟可以通过以下公式计算得知： 查找时间（10 ms） * 查询的列的数量 * 查询的数据块的数量。 3，处理大量短查询：ClickHouse可以在单个服务器上每秒处理数百个查询（在最佳的情况下最多可以处理数千个）。但是由于这不适用于分析型场景。建议每秒最多查询100次。 4，数据写入性能：建议每次写入不少于1000行的批量写入，或每秒不超过一个写入请求。当使用tab-separated格式将一份数据写入到MergeTree表中时，写入速度大约为50到200MB/s。如果您写入的数据每行为1Kb，那么写入的速度为50，000到200，000行每秒。如果您的行更小，那么写入速度将更高。为了提高写入性能，您可以使用多个INSERT进行并行写入，这将带来线性的性能提升。 count: 千万级别，500毫秒，1亿 800毫秒 2亿 900毫秒 3亿 1.1秒 group: 百万级别 200毫米，千万 1秒，1亿 10秒，2亿 20秒，3亿 30秒 join：千万-10万 600 毫秒， 千万 -百万：10秒，千万-千万 150秒 ClickHouse并非无所不能，查询语句需要不断的调优，可能与查询条件有关，不同的查询条件表是左join还是右join也是很有讲究的。 其他补充： 1，MySQL单条SQL是单线程的，只能跑满一个core，ClickHouse相反，有多少CPU，吃多少资源，所以飞快； 2，ClickHouse不支持事务，不存在隔离级别。ClickHouse的定位是分析性数据库，而不是严格的关系型数据库。 3，IO方面，MySQL是行存储，ClickHouse是列存储，后者在count()这类操作天然有优势，同时，在IO方面，MySQL需要大量随机IO，ClickHouse基本是顺序IO。 有人可能觉得上面的数据导入的时候，数据肯定缓存在内存里了，这个的确，但是ClickHouse基本上是顺序IO。对IO基本没有太高要求，当然，磁盘越快，上层处理越快，但是99%的情况是，CPU先跑满了（数据库里太少见了，大多数都是IO不够用）。 ","link":"https://tianxiawuhao.github.io/lhZ2-aE1I/"},{"title":"Centos7安装Harbor","content":" Harbor作为docker镜像仓库，自然是要依赖于docker环境的，所以，我们在安装Harbor之前，必须要安装docker环境，同时再安装下docker-compose！ docker与docker-compose的安装教程参考：Docker、Docker-compose的安装 一、Harbor的离线安装 1、下载Harbor的离线安装包： 下载地址：https://github.com/goharbor/harbor/releases 我下载的是当前最新版：v1.10.10 2、上传并解压到 /usr/local/目录下： [root@node1 ~]# tar -zxvf harbor-offline-installer-v1.10.10.tgz [root@node1 ~]# mv harbor /usr/local/ [root@node1 ~]# cd /usr/local/harbor/ [root@node1 harbor]# ls common.sh harbor.v1.10.10.tar.gz harbor.yml install.sh LICENSE prepare 3、修改harbor的核心配置文件： [root@node1 harbor]# vim harbor.yml ## 唯一必须修改项 hostname: 10.206.73.155 #如果有计划好的域名，可以直接使用域名 # http related config http: # port for http, default is 80. If https enabled, this port will redirect to https port port: 80 # 因为我暂时不需要https访问，所以https的配置去除 #https: #port: 443 #certificate: /your/certificate/path #private_key: /your/private/key/path harbor_admin_password: Harbor12345 #需要修改 4、检查并安装： # 检查 ./prepare # 安装 ./install 如果碰到iptable相关的error，关闭 firewalld，并重启docker服务 systemctl stop firewalld service docker restart 5、查看、启动、停止： 在harbor目录下使用docker-compose命令： ## 查看 [root@node1 harbor]# docker-compose ps Name Command State Ports --------------------------------------------------------------------------------------------- harbor-core /harbor/harbor_core Up (healthy) harbor-db /docker-entrypoint.sh Up (healthy) 5432/tcp harbor-jobservice /harbor/harbor_jobservice ... Up (healthy) harbor-log /bin/sh -c /usr/local/bin/ ... Up (healthy) 127.0.0.1:1514-&gt;10514/tcp harbor-portal nginx -g daemon off; Up (healthy) 8080/tcp nginx nginx -g daemon off; Up (healthy) 0.0.0.0:80-&gt;8080/tcp redis redis-server /etc/redis.conf Up (healthy) 6379/tcp registry /home/harbor/entrypoint.sh Up (healthy) 5000/tcp registryctl /home/harbor/start.sh Up (healthy) # docker-compose stop ## 停止 # docker-compose up -d ## 后台启动 6、通过yml中配置的ip+port，以及admin的账号密码访问Harbor： 补充一、内部使用Harbor时，只有ip端口，没有域名怎么办？ Error response from daemon: Get https://10.206.73.155/v2/: dial tcp 10.206.73.155:443: connect: connection refused 我们可以在docker的核心配置文件中，增加上对特定ip的允许即可： [root@master ~]# vim /etc/docker/daemon.json { &quot;registry-mirrors&quot;:[ &quot;https://otvq3lq9.mirror.aliyuncs.com&quot; ], &quot;insecure-registries&quot;:[ &quot;10.206.73.155&quot; ], &quot;max-concurrent-downloads&quot;:10, &quot;log-driver&quot;:&quot;json-file&quot;, &quot;log-level&quot;:&quot;warn&quot;, &quot;log-opts&quot;:{ &quot;max-size&quot;:&quot;50m&quot;, &quot;max-file&quot;:&quot;3&quot; }, &quot;data-root&quot;:&quot;/var/lib/docker&quot; } 重启docker守护线程和docker服务： [root@node2 ~]# systemctl daemon-reload [root@node2 ~]# systemctl restart docker ","link":"https://tianxiawuhao.github.io/XWPPx5pbS/"},{"title":"GitLab的安装与简单使用","content":"Gitlab是什么就不用多讲了，直接上手安装——使用Docker安装，参考官方文档 一、Gitlab的安装 1、在宿主机创建配置、日志、数据的目录： mkdir -p /home/gitlab/config 创建config目录 mkdir -p /home/gitlab/logs 创建logs目录 mkdir -p /home/gitlab/data 创建data目录 2、使用以下将本拉取并启动gitlab： docker run --detach \\ --hostname 192.168.56.11 \\ --publish 443:443 --publish 80:80 --publish 222:22 \\ --name gitlab \\ --restart always \\ --volume /home/gitlab/config:/etc/gitlab \\ --volume /home/gitlab/logs:/var/log/gitlab \\ --volume /home/gitlab/data:/var/opt/gitlab \\ gitlab/gitlab-ce:latest 拉取镜像时间较长，需耐心等待，镜像大小达2.29G！ 第一次启动时候，由于要安装很多东西，而且还要检查健康状况，所以会非常慢，耐心！ 3、获得Gitlab初始root账号密码： [root@mycentos2 ~]# sudo docker exec -it gitlab grep 'Password:' /etc/gitlab/initial_root_password Password: Ho0rM3Gm9LpuIARVnNE/6YoQrXC05g+JwYITLm0y2oo= 登录成功后，立即修改掉root密码： 重新登录！ 二、Gitlab的使用小妙招 1、快速迁移git仓库（所有分支） git clone --mirror &lt;URL to my OLD repo location&gt; cd 文件夹下 git remote set-url origin &lt;URL to my NEW repo location&gt; git push -f origin 三、创建新项目 全局设置 git config --global user.name &quot;xxx&quot; git config --global user.email &quot;xxx@163.com&quot; 在本地创建一个新的git仓库 git clone git@qdxwgitlab.qingdao.cosmoplat.com:jiguiquan/openiot-k8s.git cd openiot-k8s touch README.md git add README.md git commit -m &quot;add README&quot; git push -u origin master 上传现有文件夹 cd existing_folder git init git remote add origin git@qdxwgitlab.qingdao.cosmoplat.com:xxx/openiot-k8s.git git add . git commit -m &quot;Initial commit&quot; git push -u origin master 上传现有本地仓库 cd existing_repo git remote rename origin old-origin git remote add origin git@qdxwgitlab.qingdao.cosmoplat.com:xxx/openiot-k8s.git git push -u origin --all git push -u origin --tags 四、配置git的ssh密钥 默认的密钥对在: 用户目录/.ssh/ 下 1、任意地方cmd执行下面命令，生成公钥私钥密钥对： ssh-keygen -t rsa -C &quot;jiguiquan@haier.com&quot; 需要输入的地方都直接回车即可！ 2、到github或者gitlab上添加ssh凭证即可！ 五、git提交时候的CRLF转化问题 1、当我们提交git add的时候，git会给出提示： $ git add . warning: LF will be replaced by CRLF in .env. The file will have its original line endings in your working directory warning: LF will be replaced by CRLF in .idea/.gitignore. The file will have its original line endings in your working directory 2、有时候我们不希望自动转化，所以我们可以关闭自动转化的功能 提前全局设置gitconfig，关闭此功能； git config --global core.autocrlf false ","link":"https://tianxiawuhao.github.io/dpVtZ7y41/"},{"title":"Gitlab的手动安装","content":" 对于这一类的基础服务，我还是绝得手动安装得方式更能让我心安！ docker安装的文章，参考这里： GitLab的安装与简单使用 一、使用清华镜像源安装 清华镜像源：https://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/ 清华镜像源给的使用教程：https://mirror.tuna.tsinghua.edu.cn/help/gitlab-ce/ 开始安装！ 1、配置yum源： vim /etc/yum.repos.d/gitlab-ce.repo # 复制粘贴以下内容： [gitlab-ce] name=Gitlab CE Repository baseurl=https://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el$releasever/ gpgcheck=0 enabled=1 2、再执行以下两步： sudo yum makecache sudo yum install gitlab-ce #自动安装最新版 # sudo yum install gitlab-ce-x.x.x #安装指定版本 #查看gitlab的版本： cat /opt/gitlab/embedded/service/gitlab-rails/VERSION #卸载当前版本： gitlab-ctl uninstall yum remove gitlab-ce #安装指定版本： #具体版本信息可通过该链接查看：https://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el7/ sudo yum install gitlab-ce-14.7.0-ce.0.el7.x86_64 二、Gitlab的启动和常用命令 1、Gitlab的配置文件位置：/etc/gitlab/gitlab.rb vim /etc/gitlab/gitlab.rb 配置外部访问域名： external_url 'http://gitlab.jiguiquan.com' 2、使配置重新生效并启动 gitlab-ctl reconfigure # 关于初始账号密码的提示： Notes: Default admin account has been configured with following details: Username: root Password: You didn't opt-in to print initial root password to STDOUT. Password stored to /etc/gitlab/initial_root_password. This file will be cleaned up in first reconfigure run after 24 hours. # 查看状态 gitlab-ctl status 3、gitlab的其它常用命令： gitlab-ctl start # 启动所有 gitlab 组件； gitlab-ctl stop # 停止所有 gitlab 组件； gitlab-ctl restart # 重启所有 gitlab 组件； gitlab-ctl status # 查看服务状态； vim /etc/gitlab/gitlab.rb # 修改gitlab配置文件； gitlab-ctl reconfigure # 重新编译gitlab的配置； gitlab-rake gitlab:check SANITIZE=true --trace # 检查gitlab； gitlab-ctl tail # 查看日志； gitlab-ctl tail nginx/gitlab_access.log 三、Gitlab502问题解决思路 因为Gitlab里面涉及到的服务太多了，所以可能导致的原因很多很多，所以只将思路； 1、纯粹就是启动慢，一定要给它时间！ 2、正常情况下最容易碰到的问题是，端口被占用： 2.1、查看服务状态 [root@tcosmo-szls01 gitlab]# gitlab-ctl status run: alertmanager: (pid 47345) 3083s; run: log: (pid 39732) 4609s run: gitaly: (pid 47374) 3083s; run: log: (pid 38290) 4745s run: gitlab-exporter: (pid 47436) 3083s; run: log: (pid 39461) 4629s run: gitlab-workhorse: (pid 47464) 3082s; run: log: (pid 39111) 4650s run: grafana: (pid 60395) 567s; run: log: (pid 40223) 4561s run: logrotate: (pid 47533) 3081s; run: log: (pid 38080) 4763s run: nginx: (pid 60308) 568s; run: log: (pid 39286) 4645s run: node-exporter: (pid 47624) 3080s; run: log: (pid 39395) 4635s run: postgres-exporter: (pid 47630) 3080s; run: log: (pid 39834) 4605s run: postgresql: (pid 47640) 3080s; run: log: (pid 38465) 4733s run: prometheus: (pid 47644) 3079s; run: log: (pid 39641) 4617s run: puma: (pid 62674) 27s; run: log: (pid 38876) 4664s run: redis: (pid 47731) 3079s; run: log: (pid 38133) 4752s run: redis-exporter: (pid 47740) 3078s; run: log: (pid 39567) 4621s run: sidekiq: (pid 60222) 585s; run: log: (pid 38910) 4658s 2.2、看到puma服务启动时间不正常，所以我们详细的查看puma服务的日志： [root@tcosmo-szls01 gitlab]# gitlab-ctl tail puma 2022-01-25_04:25:18.93538 /opt/gitlab/embedded/bin/puma:23:in `&lt;top (required)&gt;' 2022-01-25_04:25:19.82282 {&quot;timestamp&quot;:&quot;2022-01-25T04:25:19.822Z&quot;,&quot;pid&quot;:62825,&quot;message&quot;:&quot;Puma starting in cluster mode...&quot;} 2022-01-25_04:25:19.82285 {&quot;timestamp&quot;:&quot;2022-01-25T04:25:19.822Z&quot;,&quot;pid&quot;:62825,&quot;message&quot;:&quot;* Puma version: 5.5.2 (ruby 2.7.5-p203) (\\&quot;Zawgyi\\&quot;)&quot;} 2022-01-25_04:25:19.82285 {&quot;timestamp&quot;:&quot;2022-01-25T04:25:19.822Z&quot;,&quot;pid&quot;:62825,&quot;message&quot;:&quot;* Min threads: 4&quot;} 2022-01-25_04:25:19.82286 {&quot;timestamp&quot;:&quot;2022-01-25T04:25:19.822Z&quot;,&quot;pid&quot;:62825,&quot;message&quot;:&quot;* Max threads: 4&quot;} 2022-01-25_04:25:19.82286 {&quot;timestamp&quot;:&quot;2022-01-25T04:25:19.822Z&quot;,&quot;pid&quot;:62825,&quot;message&quot;:&quot;* Environment: production&quot;} 2022-01-25_04:25:19.82287 {&quot;timestamp&quot;:&quot;2022-01-25T04:25:19.822Z&quot;,&quot;pid&quot;:62825,&quot;message&quot;:&quot;* Master PID: 62825&quot;} 2022-01-25_04:25:19.82289 {&quot;timestamp&quot;:&quot;2022-01-25T04:25:19.822Z&quot;,&quot;pid&quot;:62825,&quot;message&quot;:&quot;* Workers: 80&quot;} 2022-01-25_04:25:19.82290 {&quot;timestamp&quot;:&quot;2022-01-25T04:25:19.822Z&quot;,&quot;pid&quot;:62825,&quot;message&quot;:&quot;* Restarts: (✔) hot (✖) phased&quot;} 2022-01-25_04:25:19.82293 {&quot;timestamp&quot;:&quot;2022-01-25T04:25:19.822Z&quot;,&quot;pid&quot;:62825,&quot;message&quot;:&quot;* Preloading application&quot;} 2022-01-25_04:25:53.14976 {&quot;timestamp&quot;:&quot;2022-01-25T04:25:53.149Z&quot;,&quot;pid&quot;:62825,&quot;message&quot;:&quot;* Listening on unix:///var/opt/gitlab/gitlab-rails/sockets/gitlab.socket&quot;} 2022-01-25_04:25:53.14993 bundler: failed to load command: puma (/opt/gitlab/embedded/bin/puma) 2022-01-25_04:25:53.14999 Errno::EADDRINUSE: Address already in use - bind(2) for &quot;127.0.0.1&quot; port 8080 ## 罪魁祸首在这里，puma服务默认需要使用8080端口，但是8080端口被占用了 2.3、解决问题 如果不是主要服务，我们就可以把那个服务停掉； 如果不能停，那我们就去修改puma服务默认的端口 vim /etc/gitlab/gitlab.rb # 修改puma服务默认的端口 ### Advanced settings # puma['listen'] = '127.0.0.1' puma['port'] = 8090 ## 这里 # puma['socket'] = '/var/opt/gitlab/gitlab-rails/sockets/gitlab.socket' # puma['somaxconn'] = 1024 # 之后我们让配置生效并重启服务 gitlab-ctl reconfigure 之后服务就可以正常访问了： 四、Gitlab的使用 1、获得初始密码，并修改初始root密码： [root@tcosmo-szls01 gitlab]# cat /etc/gitlab/initial_root_password|grep Password: Password: bimCiS46btIEfFn/03A0gVA8EuriTjuRrqtJhwvl5vo= 登录后立即修改初始密码： 2、Gitlab的几个重要目录 配置文件目录：/etc/gitlab/ 日志文件目录：/var/log/gitlab/ data数据目录：/var/opt/gitlab/ 补充一：重要：Gitlab的数据备份： 1、手动备份： gitlab-rake gitlab:backup:create # 上面的命令会将gitlab的数据备份到//var/opt/gitlab/backups目录下，包含：gitlab仓库、数据库、用户、用户组、用户密钥、权限等完整信息。 [root@tcosmo-szls01 backups]# pwd /var/opt/gitlab/backups [root@tcosmo-szls01 backups]# ll -h total 243M -rw------- 1 git git 243M Apr 19 18:19 1650363597_2022_04_19_14.7.0_gitlab_backup.tar 2、自动备份（每天凌晨4点执行备份，且只保留7天） 2.1、修改备份有效期，只保留7天 vim /etc/gitlab/gitlab.rb # 显示行号 :set nu # 大概在564行： gitlab_rails['backup_keep_time'] = 604800 ##配置为7天 配置完成后，执行以下命令使配置生效： gitlab-ctl reconfigure 2.2、借助系统crontab命令实现每天凌晨4点定时备份： vim /etc/crontab 0 4 * * * root /opt/gitlab/bin/gitlab-rake gitlab:backup:create CRON=1 重启crontab服务： systemctl restart crond 3、接下来就是重要的恢复时刻啦： 3.1、首先将其它地方的备份文件拷贝到目标gitlab的 /var/opt/gitlab/backups/ 目录下： [root@tcosmo-szlstb03 backups]# pwd /var/opt/gitlab/backups [root@tcosmo-szlstb03 backups]# chmod 777 1652558427_2022_05_15_14.7.0_gitlab_backup.tar #修改下此文件的权限 [root@tcosmo-szlstb03 backups]# ll total 463020 -rwxrwxrwx 1 root root 474132480 May 15 14:00 1652558427_2022_05_15_14.7.0_gitlab_backup.tar 3.2、停止数据库连接服务： [root@tcosmo-szlstb03 backups]# gitlab-ctl stop puma &amp;&amp; gitlab-ctl stop sidekiq ok: down: puma: 0s, normally up ok: down: sidekiq: 0s, normally up [root@tcosmo-szlstb03 backups]# gitlab-ctl status run: alertmanager: (pid 127725) 2368s; run: log: (pid 125865) 2462s run: gitaly: (pid 127446) 2376s; run: log: (pid 124540) 2594s run: gitlab-exporter: (pid 127391) 2379s; run: log: (pid 125602) 2482s run: gitlab-kas: (pid 134202) 1829s; run: log: (pid 124881) 2579s run: gitlab-workhorse: (pid 127361) 2380s; run: log: (pid 125421) 2499s run: grafana: (pid 134276) 1828s; run: log: (pid 126913) 2407s run: logrotate: (pid 124415) 2609s; run: log: (pid 124450) 2608s run: nginx: (pid 134237) 1828s; run: log: (pid 125463) 2493s run: node-exporter: (pid 127384) 2380s; run: log: (pid 125570) 2488s run: postgres-exporter: (pid 127739) 2367s; run: log: (pid 126033) 2456s run: postgresql: (pid 124683) 2586s; run: log: (pid 124695) 2585s run: prometheus: (pid 131596) 2155s; run: log: (pid 125676) 2470s down: puma: 29s, normally up; run: log: (pid 125345) 2511s run: redis: (pid 124479) 2603s; run: log: (pid 124491) 2602s run: redis-exporter: (pid 127393) 2379s; run: log: (pid 125625) 2476s down: sidekiq: 29s, normally up; run: log: (pid 125370) 2505s 3.3、之前restore备份恢复操作： [root@tcosmo-szlstb03 backups]# gitlab-rake gitlab:backup:restore BACKUP=1652558427_2022_05_15_14.7.0 2022-05-15 14:15:58 +0800 -- Unpacking backup ... 2022-05-15 14:15:59 +0800 -- Unpacking backup ... done GitLab version mismatch: Your current GitLab version (14.10.2) differs from the GitLab version in the backup! Please switch to the following version and try again: version: 14.7.0 这里的意思是，目标GItlab版本与备份文件版本不一致，所以Gitlab的备份恢复必须保持Gitlab版本一致！ 乖乖切换对应的Gitlab版本吧！ 3.4、最后重启下Gitlab吧： gitlab-ctl start 补充二、Gitlab一些细节配置 1、Gitlab仓库允许的最大上传文件配置： 修改核心配置文件/etc/gitlab/gitlab.rb中的Nginx配置： vim /etc/gitlab/gitlab.rb nginx['client_max_body_size'] = '1024m' #然后重启服务 gitlab-ctl reconfigure 补充三、Gitlab之Git LFS大文件存储 1、修改gitlab核心配置文件： vim config/gitlab.yml: ### Git LFS gitlab_rails['lfs_enabled'] = true #默认位置：`/var/opt/gitlab/gitlab-rails/shared/lfs-objects` gitlab_rails['lfs_storage_path'] = &quot;/data/gitlab/lfs-objects&quot; 2、重启gitlab： gitlab-ctl reconfigure 3、之后正常情况下，所有的项目就都支持LFS了，也可以关闭： 4、通过管理员账号Admin Area——&gt;overview——&gt;project时，还可以看到项目的详细预览信息： 5、在推送代码时，需要执行git lfs track 命令指定存入lfs的文件后缀： 现在我将要测试上传一个大文件 + 小文件到gitlab，同时让大文件存放到“/data/gitlab/lfs-objects” # 标记哪些文件需要通过lfs存储 $ git lfs track &quot;*.vep&quot; Tracking &quot;*.vep&quot; # 查看自动生成得.gitattributes文件 $ cat .gitattributes *.vep filter=lfs diff=lfs merge=lfs -text # 添加文件 $ git add . # commit提交 $ git commit -m &quot;first test large file&quot; [main 3357c89] first test large file 3 files changed, 5 insertions(+) create mode 100644 .gitattributes create mode 100644 big-file.vep create mode 100644 small-file.txt # push到远程gitlab $ git push origin main info: detecting host provider for 'http://10.206.73.159/'... info: detecting host provider for 'http://10.206.73.159/'... info: detecting host provider for 'http://10.206.73.159/'... info: detecting host provider for 'http://10.206.73.159/'... Locking support detected on remote &quot;origin&quot;. Consider enabling it with: $ git config lfs.http://10.206.73.159/jiguiquan/test-big-file.git/info/lfs.locksverify true Uploading LFS objects: 0% (0/1), 37 MB | 3.3 MB/s ... Uploading LFS objects: 0% (0/1), 862 MB | 3.6 MB/s ... Uploading LFS objects: 100% (1/1), 1.3 GB | 3.7 MB/s, done. Enumerating objects: 6, done. Counting objects: 100% (6/6), done. Delta compression using up to 4 threads Compressing objects: 100% (3/3), done. Writing objects: 100% (5/5), 540 bytes | 270.00 KiB/s, done. Total 5 (delta 0), reused 0 (delta 0), pack-reused 0 To http://10.206.73.159/jiguiquan/test-big-file.git ac6319f..3357c89 main -&gt; main 6、上传文成后，我们去Gitlab中查看，可以看到对应的大文件有个特殊小标志： 同时，LFS大文件时无法在代码中直接删除的！ 7、到我们配置得LFS路径下看看，是不是确实有这个大文件： [root@node1 lfs-objects]# pwd /data/gitlab/lfs-objects [root@node1 lfs-objects]# ls a9 tmp [root@node1 lfs-objects]# tree . ├── a9 │ └── d3 │ └── 9518c7f4f5426781bbbd90cf50169581e03b2f265d0b2443dff35c81ce80 └── tmp ├── cache ├── uploads └── work 6 directories, 1 file [root@node1 lfs-objects]# cd a9/d3/ [root@node1 d3]# ll -h total 1.3G -rw-r--r--. 1 git git 1.3G Jul 14 16:43 9518c7f4f5426781bbbd90cf50169581e03b2f265d0b2443dff35c81ce80 通过文件大小直到，肯定即使我们上传得那个大文件！ 8、最后，肯定还得要测一下clone下载正不正常（git lfs clone …）： # 通过git lfs clone 完成对包含LFS的项目的clone $ git lfs clone http://10.206.73.159/jiguiquan/test-big-file.git WARNING: 'git lfs clone' is deprecated and will not be updated with new flags from 'git clone' 'git clone' has been updated in upstream Git to have comparable speeds to 'git lfs clone'. Cloning into 'test-big-file'... info: detecting host provider for 'http://10.206.73.159/'... info: detecting host provider for 'http://10.206.73.159/'... remote: Enumerating objects: 8, done. remote: Counting objects: 100% (8/8), done. remote: Compressing objects: 100% (4/4), done. remote: Total 8 (delta 0), reused 0 (delta 0), pack-reused 0 Receiving objects: 100% (8/8), done. info: detecting host provider for 'http://10.206.73.159/'... info: detecting host provider for 'http://10.206.73.159/'... Downloading LFS objects: 0% (0/1), 92 MB | 3.7 MB/s ... Downloading LFS objects: 0% (0/1), 501 MB | 2.0 MB/s ... Receiving objects: 100% (8/8), done. info: detecting host provider for 'http://10.206.73.159/'... info: detecting host provider for 'http://10.206.73.159/'... Downloading LFS objects: 100% (1/1), 1.3 GB | 1.6 MB/s 大文件Clone正常： ","link":"https://tianxiawuhao.github.io/DfTPV-OWf/"},{"title":"easeCode","content":"controller.java.vm ##定义初始变量 #set($tableName = $tool.append($tableInfo.name, &quot;Controller&quot;)) ##设置回调 $!callback.setFileName($tool.append($tableName, &quot;.java&quot;)) $!callback.setSavePath($tool.append($tableInfo.savePath, &quot;/controller&quot;)) ##拿到主键 #if(!$tableInfo.pkColumn.isEmpty()) #set($pk = $tableInfo.pkColumn.get(0)) #end #if($tableInfo.savePackageName)package $!{tableInfo.savePackageName}.#{end}controller; import $!{tableInfo.savePackageName}.entity.$!{tableInfo.name}; import $!{tableInfo.savePackageName}.entity.to.$!{tableInfo.name}TO; import $!{tableInfo.savePackageName}.exception.basic.APIResponse; import $!{tableInfo.savePackageName}.service.$!{tableInfo.name}Service; import com.haier.dtsimrs.entity.PageInfo; import io.swagger.v3.oas.annotations.Operation; import io.swagger.v3.oas.annotations.tags.Tag; import jakarta.annotation.Resource; import jakarta.validation.Valid; import org.springframework.web.bind.annotation.*; import java.util.List; /** * $!{tableInfo.comment}($!{tableInfo.name})表控制层 * * @author wuhao * @since $!time.currTime() */ @Tag(name = &quot;$!{tableInfo.comment}&quot;,description = &quot;$!{tableInfo.comment}&quot;) @RestController @RequestMapping(&quot;$!tool.firstLowerCase($tableInfo.name)&quot;) public class $!{tableName} { /** * 服务对象 */ @Resource private $!{tableInfo.name}Service $!tool.firstLowerCase($tableInfo.name)Service; /** * 通过条件查询数据 * * @param id 筛选条件 * @return 实例对象 */ @GetMapping(value = &quot;/queryByOne&quot;) @Operation(summary = &quot;根据条件获取单条数据&quot;, description = &quot;根据条件获取单条数据&quot;) public APIResponse&lt;$!{tableInfo.name}&gt; queryById(@PathVariable(&quot;id&quot;) $!pk.shortType id) { return APIResponse.ok(this.$!{tool.firstLowerCase($tableInfo.name)}Service.getById(id)); } /** * 分页查询 * * @param $!{tool.firstLowerCase($tableInfo.name)} 筛选条件 * @return 查询结果 */ @PostMapping(value = &quot;/queryByPage&quot;) @Operation(summary = &quot;获取分页数据&quot;, description = &quot;获取分页数据&quot;) public APIResponse&lt;PageInfo&lt;$!{tableInfo.name}&gt;&gt; queryByPage(@RequestBody $!{tableInfo.name}TO $!{tool.firstLowerCase($tableInfo.name)}) { return APIResponse.ok(this.$!{tool.firstLowerCase($tableInfo.name)}Service.queryByPage($!{tool.firstLowerCase($tableInfo.name)})); } /** * 通过条件查询批量数据 * * @param $!{tool.firstLowerCase($tableInfo.name)} 筛选条件 * @return 单条数据 */ @PostMapping(value = &quot;/queryByList&quot;) @Operation(summary = &quot;通过条件查询批量数据&quot;, description = &quot;通过条件查询批量数据&quot;) public APIResponse&lt;List&lt;$!{tableInfo.name}&gt;&gt; queryByList(@RequestBody $!{tableInfo.name} $!{tool.firstLowerCase($tableInfo.name)}) { return APIResponse.ok(this.$!{tool.firstLowerCase($tableInfo.name)}Service.queryByList($!{tool.firstLowerCase($tableInfo.name)})); } /** * 新增数据 * * @param $!{tool.firstLowerCase($tableInfo.name)} 实体 * @return 新增结果 */ @PostMapping(value = &quot;/saveOrUpdate&quot;) @Operation(summary = &quot;新增或编辑数据&quot;, description = &quot;新增或编辑数据&quot;) public APIResponse&lt;Boolean&gt; saveOrUpdate(@RequestBody @Valid $!{tableInfo.name} $!{tool.firstLowerCase($tableInfo.name)}) { return APIResponse.ok(this.$!{tool.firstLowerCase($tableInfo.name)}Service.saveOrUpdate($!{tool.firstLowerCase($tableInfo.name)})); } /** * 批量新增数据 * * @param list 实体 * @return 新增结果 */ @PostMapping(value = &quot;/saveOrUpdateBatch&quot;) @Operation(summary = &quot;批量新增或编辑数据&quot;, description = &quot;批量新增或编辑数据&quot;) public APIResponse&lt;Boolean&gt; saveOrUpdateBatch(@RequestBody @Valid List&lt;$!{tableInfo.name}&gt; list) { return APIResponse.ok(this.$!{tool.firstLowerCase($tableInfo.name)}Service.saveOrUpdateBatch(list)); } /** * 删除数据 * * @param id 主键 * @return 删除是否成功 */ @DeleteMapping(value = &quot;/deleteById&quot;) @Operation(summary = &quot;根据主键删除数据&quot;, description = &quot;根据主键删除数据&quot;) public APIResponse&lt;Boolean&gt; deleteById($!pk.shortType id) { return APIResponse.ok(this.$!{tool.firstLowerCase($tableInfo.name)}Service.removeById(id)); } /** * 批量删除数据 * * @param ids 主键 * @return 删除是否成功 */ @DeleteMapping(value = &quot;/removeByIds&quot;) @Operation(summary = &quot;根据主键批量删除数据&quot;, description = &quot;根据主键批量删除数据&quot;) public APIResponse&lt;Boolean&gt; deleteById(List&lt;$!pk.shortType&gt; ids) { return APIResponse.ok(this.$!{tool.firstLowerCase($tableInfo.name)}Service.removeByIds(ids)); } } service.java.vm ##定义初始变量 #set($tableName = $tool.append($tableInfo.name, &quot;Service&quot;)) ##设置回调 $!callback.setFileName($tool.append($tableName, &quot;.java&quot;)) $!callback.setSavePath($tool.append($tableInfo.savePath, &quot;/service&quot;)) ##拿到主键 #if(!$tableInfo.pkColumn.isEmpty()) #set($pk = $tableInfo.pkColumn.get(0)) #end #if($tableInfo.savePackageName)package $!{tableInfo.savePackageName}.#{end}service; import $!{tableInfo.savePackageName}.entity.$!{tableInfo.name}; import $!{tableInfo.savePackageName}.entity.to.$!{tableInfo.name}TO; import com.baomidou.mybatisplus.extension.service.IService; import java.util.List; import com.haier.dtsimrs.entity.PageInfo; /** * $!{tableInfo.comment}($!{tableInfo.name})表服务接口 * * @author wuhao * @since $!time.currTime() */ public interface $!{tableName} extends IService&lt;$!{tableInfo.name}&gt;{ /** * 通过条件查询批量数据 * * @param $!tool.firstLowerCase($!{tableInfo.name}) 筛选条件 * @return 查询结果 */ List&lt;$!{tableInfo.name}&gt; queryByList($!{tableInfo.name} $!tool.firstLowerCase($!{tableInfo.name})); /** * 分页查询 * * @param $!tool.firstLowerCase($!{tableInfo.name}) 筛选条件 * @return 查询结果 */ PageInfo&lt;$!{tableInfo.name}&gt; queryByPage($!{tableInfo.name}TO $!{tool.firstLowerCase($tableInfo.name)}); } serviceImpl.java.vm ##定义初始变量 #set($tableName = $tool.append($tableInfo.name, &quot;ServiceImpl&quot;)) ##设置回调 $!callback.setFileName($tool.append($tableName, &quot;.java&quot;)) $!callback.setSavePath($tool.append($tableInfo.savePath, &quot;/service/impl&quot;)) ##拿到主键 #if(!$tableInfo.pkColumn.isEmpty()) #set($pk = $tableInfo.pkColumn.get(0)) #end #if($tableInfo.savePackageName)package $!{tableInfo.savePackageName}.#{end}service.impl; import $!{tableInfo.savePackageName}.entity.$!{tableInfo.name}; import $!{tableInfo.savePackageName}.entity.to.$!{tableInfo.name}TO; import $!{tableInfo.savePackageName}.mapper.$!{tableInfo.name}Mapper; import $!{tableInfo.savePackageName}.service.$!{tableInfo.name}Service; import com.baomidou.mybatisplus.core.toolkit.Wrappers; import com.baomidou.mybatisplus.extension.plugins.pagination.Page; import com.haier.dtsimrs.entity.PageInfo; import org.springframework.stereotype.Service; import com.baomidou.mybatisplus.extension.service.impl.ServiceImpl; import jakarta.annotation.Resource; import java.util.List; import java.util.Objects; import org.springframework.util.StringUtils; import com.baomidou.mybatisplus.core.conditions.query.LambdaQueryWrapper; import org.springframework.beans.BeanUtils; /** * $!{tableInfo.comment}($!{tableInfo.name})表服务实现类 * * @author wuhao * @since $!time.currTime() */ @Service(&quot;$!tool.firstLowerCase($!{tableInfo.name})Service&quot;) public class $!{tableName} extends ServiceImpl&lt;$!{tableInfo.name}Mapper, $!{tableInfo.name}&gt; implements $!{tableInfo.name}Service { @Resource private $!{tableInfo.name}Mapper $!tool.firstLowerCase($!{tableInfo.name})Mapper; /** * 通过条件查询批量数据 * * @param $!tool.firstLowerCase($!{tableInfo.name}) 筛选条件 * @return 实例对象 */ @Override public List&lt;$!{tableInfo.name}&gt; queryByList($!{tableInfo.name} $!tool.firstLowerCase($!{tableInfo.name})) { return this.$!{tool.firstLowerCase($!{tableInfo.name})}Mapper.selectList(getWrapper($!tool.firstLowerCase($!{tableInfo.name}))); } /** * 分页查询 * * @param $!{tool.firstLowerCase($tableInfo.name)} 筛选条件 * @return 查询结果 */ @Override public PageInfo&lt;$!{tableInfo.name}&gt; queryByPage($!{tableInfo.name}TO $!{tool.firstLowerCase($tableInfo.name)}) { //开启分页 Page&lt;$!{tableInfo.name}&gt; page = new Page&lt;&gt;($!{tool.firstLowerCase($tableInfo.name)}.getPage(), $!{tool.firstLowerCase($tableInfo.name)}.getSize()); $!{tableInfo.name} build = $!{tableInfo.name}.builder().build(); BeanUtils.copyProperties($!{tool.firstLowerCase($tableInfo.name)},build); return new PageInfo&lt;&gt;(this.$!{tool.firstLowerCase($tableInfo.name)}Mapper.selectPage(page, getWrapper(build))); } private LambdaQueryWrapper getWrapper($!{tableInfo.name} $!tool.firstLowerCase($!{tableInfo.name})){ return Wrappers.&lt;$!{tableInfo.name}&gt;lambdaQuery() #foreach($column in $tableInfo.fullColumn) #if($!{tool.getClsNameByFullName($column.type)}==&quot;String&quot;) .eq(StringUtils.hasLength($!{tool.firstLowerCase($!{tableInfo.name})}.get$!{tool.firstUpperCase($!{column.name})}()), $!{tableInfo.name}::get$!{tool.firstUpperCase($!{column.name})}, $!{tool.firstLowerCase($!{tableInfo.name})}.get$!{tool.firstUpperCase($!{column.name})}()) #end #if($!{tool.getClsNameByFullName($column.type)}!=&quot;String&quot;) .eq(Objects.nonNull($!{tool.firstLowerCase($!{tableInfo.name})}.get$!{tool.firstUpperCase($!{column.name})}()), $!{tableInfo.name}::get$!{tool.firstUpperCase($!{column.name})},$!{tool.firstLowerCase($!{tableInfo.name})}.get$!{tool.firstUpperCase($!{column.name})}()) #end #end ; } } mapper.java.vm ##引入mybatis支持 $!{mybatisSupport.vm} ##设置保存名称与保存位置 $!callback.setFileName($tool.append($!{tableInfo.name}, &quot;Mapper.xml&quot;)) $!callback.setSavePath($tool.append($modulePath, &quot;/src/main/resources/mapper&quot;)) ##拿到主键 #if(!$tableInfo.pkColumn.isEmpty()) #set($pk = $tableInfo.pkColumn.get(0)) #end &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt; &lt;mapper namespace=&quot;$!{tableInfo.savePackageName}.mapper.$!{tableInfo.name}Mapper&quot;&gt; &lt;resultMap type=&quot;$!{tableInfo.savePackageName}.entity.$!{tableInfo.name}&quot; id=&quot;$!{tableInfo.name}Map&quot;&gt; #foreach($column in $tableInfo.fullColumn) &lt;result property=&quot;$!column.name&quot; column=&quot;$!column.obj.name&quot; jdbcType=&quot;$!column.ext.jdbcType&quot;/&gt; #end &lt;/resultMap&gt; &lt;!--查询指定行数据--&gt; &lt;select id=&quot;queryAllByLimit&quot; resultMap=&quot;$!{tableInfo.name}Map&quot;&gt; select #allSqlColumn() from $!tableInfo.obj.name &lt;where&gt; #foreach($column in $tableInfo.fullColumn) &lt;if test=&quot;$!column.name != null#if($column.type.equals(&quot;java.lang.String&quot;)) and $!column.name != ''#end&quot;&gt; and $!column.obj.name = #{$!column.name} &lt;/if&gt; #end &lt;/where&gt; &lt;/select&gt; &lt;insert id=&quot;insertBatch&quot; keyProperty=&quot;$!pk.name&quot; useGeneratedKeys=&quot;true&quot;&gt; insert into $!{tableInfo.obj.name}(#foreach($column in $tableInfo.otherColumn)$!column.obj.name#if($velocityHasNext), #end#end) values &lt;foreach collection=&quot;entities&quot; item=&quot;entity&quot; separator=&quot;,&quot;&gt; (#foreach($column in $tableInfo.otherColumn)#{entity.$!{column.name}}#if($velocityHasNext), #end#end) &lt;/foreach&gt; &lt;/insert&gt; &lt;!--需要通过在数据库连接URL中指定allowMultiQueries参数值为true告诉数据库以支持”;&quot;号分隔的多条语句的执行--&gt; &lt;update id=&quot;updateBatch&quot;&gt; &lt;foreach collection=&quot;entities&quot; item=&quot;entity&quot; separator=&quot;;&quot;&gt; update $!{tableInfo.obj.name} &lt;set&gt; #foreach($column in $tableInfo.otherColumn) &lt;if test=&quot;entity.$!column.name != null#if($column.type.equals(&quot;java.lang.String&quot;)) and entity.$!column.name != ''#end&quot;&gt; $!column.obj.name = #{entity.$!column.name}, &lt;/if&gt; #end &lt;/set&gt; where $!pk.obj.name = #{entity.$!pk.name} &lt;/foreach&gt; &lt;/update&gt; &lt;/mapper&gt; dao.java.vm ##定义初始变量 #set($tableName = $tool.append($tableInfo.name, &quot;Mapper&quot;)) ##设置回调 $!callback.setFileName($tool.append($tableName, &quot;.java&quot;)) $!callback.setSavePath($tool.append($tableInfo.savePath, &quot;/mapper&quot;)) ##拿到主键 #if(!$tableInfo.pkColumn.isEmpty()) #set($pk = $tableInfo.pkColumn.get(0)) #end #if($tableInfo.savePackageName)package $!{tableInfo.savePackageName}.#{end}mapper; import $!{tableInfo.savePackageName}.entity.$!{tableInfo.name}; import com.baomidou.mybatisplus.core.mapper.BaseMapper; import org.apache.ibatis.annotations.Mapper; import org.apache.ibatis.annotations.Param; import java.util.List; /** * $!{tableInfo.comment}($!{tableInfo.name})表数据库访问层 * * @author wuhao * @since $!time.currTime() */ @Mapper public interface $!{tableInfo.name}Mapper extends BaseMapper&lt;$!{tableInfo.name}&gt;{ } entity.java.vm ##引入宏定义 $!{define.vm} ##使用宏定义设置回调（保存位置与文件后缀） #save(&quot;/entity&quot;, &quot;.java&quot;) ##使用宏定义设置包后缀 #setPackageSuffix(&quot;entity&quot;) ##使用全局变量实现默认包导入 $!{autoImport.vm} import com.baomidou.mybatisplus.annotation.IdType; import com.baomidou.mybatisplus.annotation.TableId; import jakarta.persistence.*; import lombok.*; import lombok.experimental.Accessors; import io.swagger.v3.oas.annotations.media.Schema; import java.io.Serializable; ##使用宏定义实现类注释信息 #tableComment(&quot;实体类&quot;) @Data @EqualsAndHashCode(callSuper = false) @Accessors(chain = true) @Schema(title=&quot;$!{tableInfo.name}对象&quot;, description=&quot;$!{tableInfo.comment}&quot;) @Builder @AllArgsConstructor @NoArgsConstructor @Entity @Table(name = &quot;$!{tool.hump2Underline($tableInfo.name)}&quot;, schema = &quot;$!{tableInfo.comment}&quot;) public class $!{tableInfo.name} implements Serializable { private static final long serialVersionUID = $!tool.serial(); #foreach($column in $tableInfo.fullColumn) #if(${column.comment})/** * ${column.comment} */#end #if($!{column.name}==&quot;id&quot;)@Schema(title = &quot;${column.comment}&quot;) @TableId(value = &quot;id&quot;, type = IdType.AUTO) @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private $!{tool.getClsNameByFullName($column.type)} $!{column.name}; #end#if($!{column.name}!=&quot;id&quot;)@Schema(title = &quot;${column.comment}&quot;) #if($!{column.shortType}==&quot;String&quot;) @Column(columnDefinition = &quot;varchar(255) COMMENT '$!{column.comment}'&quot;, name = &quot;$!{tool.hump2Underline($column.name)}&quot;) #end #if($!{column.shortType}==&quot;Integer&quot;) @Column(columnDefinition = &quot;int COMMENT '$!{column.comment}'&quot;, name = &quot;$!{tool.hump2Underline($column.name)}&quot;) #end #if($!{column.shortType}==&quot;Long&quot;) @Column(columnDefinition = &quot;bigint COMMENT '$!{column.comment}'&quot;, name = &quot;$!{tool.hump2Underline($column.name)}&quot;) #end #if($!{column.shortType}==&quot;Double&quot;) @Column(columnDefinition = &quot;double(5, 3) COMMENT '$!{column.comment}'&quot;, name = &quot;$!{tool.hump2Underline($column.name)}&quot;) #end #if($!{column.shortType}==&quot;Date&quot;) @Column(columnDefinition = &quot;datetime COMMENT '$!{column.comment}'&quot;, name = &quot;$!{tool.hump2Underline($column.name)}&quot;) #end #if($!{column.shortType}==&quot;DateTime&quot;) @Column(columnDefinition = &quot;datetime COMMENT '$!{column.comment}'&quot;, name = &quot;$!{tool.hump2Underline($column.name)}&quot;) #end #if($!{column.shortType}==&quot;LocalDateTime&quot;) @Column(columnDefinition = &quot;datetime COMMENT '$!{column.comment}'&quot;, name = &quot;$!{tool.hump2Underline($column.name)}&quot;) #end #if($!{column.shortType}==&quot;Boolean&quot;) @Column(columnDefinition = &quot;int(1) COMMENT '$!{column.comment}'&quot;, name = &quot;$!{tool.hump2Underline($column.name)}&quot;) #end private $!{tool.getClsNameByFullName($column.type)} $!{column.name}; #end #end } entityTO.java.vm ##引入宏定义 $!{define.vm} ##使用宏定义设置回调（保存位置与文件后缀） #save(&quot;/entity/to&quot;, &quot;TO.java&quot;) ##使用宏定义设置包后缀 #setPackageSuffix(&quot;entity.to&quot;) ##使用全局变量实现默认包导入 $!{autoImport.vm} import com.baomidou.mybatisplus.annotation.IdType; import com.baomidou.mybatisplus.annotation.TableId; import com.haier.dtsimrs.entity.SearchPage; import lombok.*; import lombok.experimental.Accessors; import io.swagger.v3.oas.annotations.media.Schema; import java.io.Serializable; ##使用宏定义实现类注释信息 #tableComment(&quot;实体TO类&quot;) @Data @EqualsAndHashCode(callSuper = false) @Accessors(chain = true) @Schema(title=&quot;$!{tableInfo.name}TO对象&quot;, description=&quot;$!{tableInfo.comment}&quot;) @Builder @AllArgsConstructor @NoArgsConstructor public class $!{tableInfo.name}TO extends SearchPage implements Serializable { private static final long serialVersionUID = $!tool.serial(); #foreach($column in $tableInfo.fullColumn) #if(${column.comment})/** * ${column.comment} */#end #if($!{column.name}==&quot;id&quot;)@Schema(title = &quot;${column.comment}&quot;) @TableId(value = &quot;id&quot;, type = IdType.AUTO) private $!{tool.getClsNameByFullName($column.type)} $!{column.name}; #end#if($!{column.name}!=&quot;id&quot;)@Schema(title = &quot;${column.comment}&quot;) private $!{tool.getClsNameByFullName($column.type)} $!{column.name}; #end #end } entityVO.java.vm ##引入宏定义 $!{define.vm} ##使用宏定义设置回调（保存位置与文件后缀） #save(&quot;/entity/vo&quot;, &quot;VO.java&quot;) ##使用宏定义设置包后缀 #setPackageSuffix(&quot;entity.vo&quot;) ##使用全局变量实现默认包导入 $!{autoImport.vm} import com.baomidou.mybatisplus.annotation.IdType; import com.baomidou.mybatisplus.annotation.TableId; import lombok.*; import lombok.experimental.Accessors; import io.swagger.v3.oas.annotations.media.Schema; import java.io.Serializable; ##使用宏定义实现类注释信息 #tableComment(&quot;实体VO类&quot;) @Data @EqualsAndHashCode(callSuper = false) @Accessors(chain = true) @Schema(title=&quot;$!{tableInfo.name}VO对象&quot;, description=&quot;$!{tableInfo.comment}&quot;) @Builder @AllArgsConstructor @NoArgsConstructor public class $!{tableInfo.name}VO implements Serializable { private static final long serialVersionUID = $!tool.serial(); #foreach($column in $tableInfo.fullColumn) #if(${column.comment})/** * ${column.comment} */#end #if($!{column.name}==&quot;id&quot;)@Schema(title = &quot;${column.comment}&quot;) @TableId(value = &quot;id&quot;, type = IdType.AUTO) private $!{tool.getClsNameByFullName($column.type)} $!{column.name}; #end#if($!{column.name}!=&quot;id&quot;)@Schema(title = &quot;${column.comment}&quot;) private $!{tool.getClsNameByFullName($column.type)} $!{column.name}; #end #end } APIResponse import cn.hutool.core.util.StrUtil; import com.fasterxml.jackson.annotation.JsonIgnore; import io.swagger.annotations.ApiModel; import io.swagger.annotations.ApiModelProperty; import lombok.AllArgsConstructor; import lombok.Data; import lombok.experimental.Accessors; import java.io.Serializable; @Data @Accessors(chain = true) @AllArgsConstructor @ApiModel(&quot;基础响应信息&quot;) public class APIResponse&lt;E&gt; implements Serializable { @ApiModelProperty(value = &quot;响应码 200:成功，其他:失败&quot;, required = true) private Integer code; @ApiModelProperty(value = &quot;响应信息&quot;) private String msg; @ApiModelProperty(value = &quot;响应数据&quot;) private E data; @JsonIgnore private Object[] args; /** * 获取响应消息 * @return 响应消息 */ public String getMsg() { return (null != args &amp;&amp; null != msg) ? StrUtil.format(msg, args) : msg; } public APIResponse() { this.code = ResponseCode.SUCCESS.getCode(); } private APIResponse(ResponseCode code) { this.code = code.getCode(); this.msg = code.getMsg(); } public static APIResponse ok() { return new APIResponse(ResponseCode.SUCCESS); } public static &lt;E&gt; APIResponse&lt;E&gt; ok(E data) { APIResponse&lt;E&gt; res = new APIResponse(ResponseCode.SUCCESS); res.setData(data); return res; } public static &lt;E&gt; APIResponse&lt;E&gt; ok(E data, String msg) { APIResponse res = ok(data); res.setMsg(msg); return res; } private static APIResponse fail() { return new APIResponse(ResponseCode.FAIL); } /** * 失败响应 * @param responseCode 异常码 * @return */ public static APIResponse fail(IResponseCode responseCode) { return fail(responseCode.getCode(), responseCode.getMsg()); } /** * 失败响应 * @param responseCode 异常码 * @param args 参数 * @return */ public static APIResponse fail(IResponseCode responseCode, Object... args) { APIResponse res = fail(responseCode.getCode(), responseCode.getMsg()); res.setArgs(args); return res; } @Deprecated public static APIResponse fail(String msg) { APIResponse res = fail(); res.setMsg(msg); return res; } @Deprecated public static APIResponse fail(String format, Object... msg) { APIResponse res = fail(); res.setMsg(StrUtil.format(format, msg)); return res; } public static APIResponse fail(Integer code, String msg) { APIResponse res = fail(msg); res.setCode(code); return res; } public APIResponse msg(String msg) { this.msg = msg; return this; } /** * 响应是否成功 * * @return */ @ApiModelProperty(value = &quot;请求是否成功&quot;, hidden = true) public boolean isOk() { return this.code == ResponseCode.SUCCESS.getCode(); } } IResponseCode public interface IResponseCode { /** * 获取响应码 * @return */ int getCode(); /** * 获取响应信息 * * @return */ String getMsg(); } IResponseCode import lombok.Getter; @Getter public enum ResponseCode implements IResponseCode { SUCCESS(&quot;成功&quot;, 200), NEED_CHANGE_PAAS(&quot;需要修改初始密码&quot;, 220), FAIL(&quot;失败&quot;, 500), EXCEPTION(&quot;异常&quot;, 505), TOKEN_INVALID(&quot;token失效&quot;, 501), ILLEGAL_TOKEN(&quot;非法token&quot;, 502), NOTFUND_TOKEN(&quot;请求头缺少token&quot;, 503), FORBIDDEN_ACCESS(&quot;无访问权限&quot;, 403), ACCOUNT_NOTFOUND( &quot;账号不存在&quot;,404), ACCOUNT_EXIST( &quot;账号已存在&quot;,410), LOGIN_INVALID(&quot;账号或密码错误&quot;,405), ACCOUNT_EXPIRED(&quot;账号过期&quot;,406), PASSWORD_EXPIRED( &quot;密码过期&quot;,407), LICENSE_OUTTIME(&quot;license许可已经过期&quot;,408), ACCOUNT_LOCKED( &quot;无法登录，请与管理员联系&quot;,409), CODE_NOT_SEND( &quot;短信验证码未发送&quot;,411), CODE_ERROR( &quot;短信验证码错误&quot;,412), ; int code; String msg; ResponseCode(int code) { this.code = code; } ResponseCode(String msg,int code) { this(code); this.msg = msg; } @Override public int getCode() { return code; } @Override public String getMsg() { return msg; } } SearchPage import io.swagger.annotations.ApiModelProperty; import lombok.AllArgsConstructor; import lombok.Data; import lombok.EqualsAndHashCode; import lombok.NoArgsConstructor; import lombok.experimental.Accessors; @Data @EqualsAndHashCode(callSuper = false) @Accessors(chain = true) @AllArgsConstructor @NoArgsConstructor public class SearchPage { @ApiModelProperty(value = &quot;页数&quot;) private int page = 0; @ApiModelProperty(value = &quot;每页条数&quot;) private int size = 10; } page import com.baomidou.mybatisplus.extension.plugins.pagination.Page; import lombok.Data; import java.util.List; @Data public class PageInfo&lt;T&gt;{ protected long total; protected List&lt;? extends T&gt; list; public static final int DEFAULT_NAVIGATE_PAGES = 8; private long pageNum; private long pageSize; private long startRow; private long endRow; public PageInfo(Page&lt;T&gt; page) { this.list = page.getRecords(); this.total = page.getTotal(); this.pageNum = page.getCurrent(); this.pageSize = page.getSize(); this.startRow = 0L; this.endRow = page.getTotal() &gt; 0 ? (page.getTotal()/page.getSize()) : 0L; } public String toString() { String sb = &quot;PageInfo{&quot; + &quot;pageNum=&quot; + this.pageNum + &quot;, pageSize=&quot; + this.pageSize + &quot;, startRow=&quot; + this.startRow + &quot;, endRow=&quot; + this.endRow + &quot;, total=&quot; + this.total + &quot;, list=&quot; + this.list + '}'; return sb; } } xml spring: jpa: hibernate: ddl-auto: update show-sql: true properties: hibernate: hbm2ddl: auto: update ","link":"https://tianxiawuhao.github.io/OxvAGnY87/"},{"title":"Java实现License授权许可和验证","content":"TrueLicense TrueLicense是一个开源的证书管理引擎，使用场景：当项目交付给客户之后用签名来保证客户不能随意使用项目 默认校验了开始结束时间，可扩展增加mac地址校验等。 其中还有ftp的校验没有尝试，本demo详细介绍的是本地校验 license授权机制的原理： 生成密钥对，方法有很多。我们使用trueLicense来做软件产品的保护，我们主要使用它的LicenseManager类来生成证书文件、安装证书文件、验证证书文件. 原理 首先需要生成密钥对，方法有很多，JDK中提供的KeyTool即可生成。 授权者保留私钥，使用私钥对包含授权信息（如截止日期，MAC地址等）的license进行数字签名。 公钥交给使用者（放在验证的代码中使用），用于验证license是否符合使用条件。 使用Keytool命令生成密钥对 Keytool 是一个Java 数据证书的管理工具 ,Keytool 将密钥（key）和证书（certificates）存在一个称为keystore的文件中 在keystore里，包含两种数据： 密钥实体（Key entity）——密钥（secret key）又或者是私钥和配对公钥（采用非对称加密） 可信任的证书实体（trusted certificate entries）——只包含公钥 在接触代码前，我们先来大概熟悉下密钥生成的流程吧 首先要用KeyTool工具来生成私匙库：（-alias别名 –validity 3650表示10年有效） keytool -genkey -alias privatekey -keystore privateKeys.store -keysize 1024 -validity 3650 -storepass &quot;XXXX&quot; -keypass &quot;XXXX&quot; -dname &quot;CN=test, OU=test, O=Space, L=SH, ST=SH, C=CN&quot; 这里密码我使用XXXX 注意！！！默认的密码策略是6位数字与字母，如果不遵守会报错 这个时候，会在打开命令行的地方创建出一个文件，privateKeys.store 然后把私匙库内的证书导出到一个文件当中： keytool -export -alias privatekey -file certfile.cer -keystore privateKeys.store 生成certfile.cer(证书)，生成公钥库后就没什么用了 然后再把这个证书文件导入到公匙库： keytool -import -alias publiccert -file certfile.cer -keystore publicCerts.store 生成 publicCerts.store privateKeys.keystore：私钥，这个我们自己留着，不能泄露给别人。 publicCerts.keystore：公钥，这个给客户用的。在我们程序里面就是用他配合license进行授权信息的校验的。 certfile.cer：这个文件没啥用，可以删掉。 最后自行将生成文件privateKeys.store、publicCerts.store拷贝出来备用。 实现代码 - 证书生成 maven依赖 &lt;!--truelicense 依赖--&gt; &lt;!-- https://mvnrepository.com/artifact/de.schlichtherle.truelicense/truelicense-core --&gt; &lt;dependency&gt; &lt;groupId&gt;de.schlichtherle.truelicense&lt;/groupId&gt; &lt;artifactId&gt;truelicense-core&lt;/artifactId&gt; &lt;version&gt;1.33&lt;/version&gt; &lt;/dependency&gt; 首先从整个流程上来讲，现在这步是证书生成，证书生成需要私钥库和证书参数 在这个引擎中，公/私钥库默认是存储在项目中的。但是，我们实际生产环境中，都是将配置文件等脱离项目部署的，所以我们需要重写它获取公/私钥库的地方。 CustomKeyStoreParam.java import de.schlichtherle.license.AbstractKeyStoreParam; import org.springframework.util.ResourceUtils; import java.io.*; /** * 自定义KeyStoreParam，用于将公私钥存储文件存放到其他磁盘位置而不是项目中。现场使用的时候公钥大部分都不会放在项目中的 */ public class CustomKeyStoreParam extends AbstractKeyStoreParam { /** * 公钥/私钥在磁盘上的存储路径 */ private final String storePath; private final String alias; private final String storePwd; private final String keyPwd; public CustomKeyStoreParam(Class clazz, String resource, String alias, String storePwd, String keyPwd) { super(clazz, resource); this.storePath = resource; this.alias = alias; this.storePwd = storePwd; this.keyPwd = keyPwd; } @Override public String getAlias() { return alias; } @Override public String getStorePwd() { return storePwd; } @Override public String getKeyPwd() { return keyPwd; } /** * AbstractKeyStoreParam里面的getStream()方法默认文件是存储的项目中。 * 用于将公私钥存储文件存放到其他磁盘位置而不是项目中 */ @Override public InputStream getStream() throws IOException { // return new FileInputStream(new File(storePath)); File file = ResourceUtils.getFile(storePath); if (file.exists()) { return new FileInputStream(file); } else { throw new FileNotFoundException(storePath); } } } 证书参数可以用配置文件配置，也可以写成类，这个方法用的就是类的方式 License.java import lombok.Data; import java.io.Serializable; import java.util.Date; /** * License生成类需要的参数 */ @Data public class License implements Serializable { private static final long serialVersionUID = -7793154252684580872L; /** * 证书subject */ private String subject; /** * 私钥别称 */ private String privateAlias; /** * 私钥密码（需要妥善保管，不能让使用者知道） */ private String keyPass; /** * 访问私钥库的密码 */ private String storePass; /** * 证书生成路径 */ private String licensePath; /** * 私钥库存储路径 */ private String privateKeysStorePath; /** * 证书生效时间 */ private Date issuedTime = new Date(); /** * 证书失效时间 */ private Date expiryTime; /** * 用户类型 */ private String consumerType = &quot;user&quot;; /** * 用户数量 */ private Integer consumerAmount = 1; /** * 描述信息 */ private String description = &quot;&quot;; /** * 额外的服务器硬件校验信息 */ private LicenseExtraModel licenseExtraModel; } 其中的扩展参数类 /** * 自定义需要校验的License参数，可以增加一些额外需要校验的参数，比如项目信息，ip地址信息等等，待完善 */ public class LicenseExtraModel { // 这里可以添加一些往外的自定义信息，比如我们可以增加项目验证，客户电脑sn码的验证等等 } 由于引擎本身默认只验证了有效期，当我们需要自定义一个继承于LicenseManager的自定义证书管理器。 额外的信息的校验可以加在validate()方法里 CustomLicenseManager.java import de.schlichtherle.license.*; import de.schlichtherle.xml.GenericCertificate; import de.schlichtherle.xml.XMLConstants; import org.apache.logging.log4j.LogManager; import org.apache.logging.log4j.Logger; import java.beans.XMLDecoder; import java.io.BufferedInputStream; import java.io.ByteArrayInputStream; import java.io.UnsupportedEncodingException; import java.util.Date; /** * 自定义LicenseManager，用于增加额外的信息校验(除了LicenseManager的校验，我们还可以在这个类里面添加额外的校验信息) */ public class CustomLicenseManager extends LicenseManager { private static Logger logger = LogManager.getLogger(CustomLicenseManager.class); public CustomLicenseManager(LicenseParam param) { super(param); } /** * 复写create方法 */ @Override protected synchronized byte[] create(LicenseContent content, LicenseNotary notary) throws Exception { initialize(content); this.validateCreate(content); final GenericCertificate certificate = notary.sign(content); return getPrivacyGuard().cert2key(certificate); } /** * 复写install方法，其中validate方法调用本类中的validate方法，校验IP地址、Mac地址等其他信息 */ @Override protected synchronized LicenseContent install(final byte[] key, final LicenseNotary notary) throws Exception { final GenericCertificate certificate = getPrivacyGuard().key2cert(key); notary.verify(certificate); final LicenseContent content = (LicenseContent) this.load(certificate.getEncoded()); this.validate(content); setLicenseKey(key); setCertificate(certificate); return content; } /** * 复写verify方法，调用本类中的validate方法，校验IP地址、Mac地址等其他信息 */ @Override protected synchronized LicenseContent verify(final LicenseNotary notary) throws Exception { // Load license key from preferences, final byte[] key = getLicenseKey(); if (null == key) { throw new NoLicenseInstalledException(getLicenseParam().getSubject()); } GenericCertificate certificate = getPrivacyGuard().key2cert(key); notary.verify(certificate); final LicenseContent content = (LicenseContent) this.load(certificate.getEncoded()); this.validate(content); setCertificate(certificate); return content; } /** * 校验生成证书的参数信息 */ protected synchronized void validateCreate(final LicenseContent content) throws LicenseContentException { final LicenseParam param = getLicenseParam(); final Date now = new Date(); final Date notBefore = content.getNotBefore(); final Date notAfter = content.getNotAfter(); if (null != notAfter &amp;&amp; now.after(notAfter)) { throw new LicenseContentException(&quot;证书失效时间不能早于当前时间&quot;); } if (null != notBefore &amp;&amp; null != notAfter &amp;&amp; notAfter.before(notBefore)) { throw new LicenseContentException(&quot;证书生效时间不能晚于证书失效时间&quot;); } final String consumerType = content.getConsumerType(); if (null == consumerType) { throw new LicenseContentException(&quot;用户类型不能为空&quot;); } } /** * 复写validate方法，用于增加我们额外的校验信息 */ @Override protected synchronized void validate(final LicenseContent content) throws LicenseContentException { //1. 首先调用父类的validate方法 super.validate(content); //2. 然后校验自定义的License参数，去校验我们的license信息 // LicenseExtraModel expectedCheckModel = (LicenseExtraModel) content.getExtra(); // 做我们自定义的校验 } /** * 重写XMLDecoder解析XML */ private Object load(String encoded) { BufferedInputStream inputStream = null; XMLDecoder decoder = null; try { inputStream = new BufferedInputStream(new ByteArrayInputStream(encoded.getBytes(XMLConstants.XML_CHARSET))); decoder = new XMLDecoder(new BufferedInputStream(inputStream, XMLConstants.DEFAULT_BUFSIZE), null, null); return decoder.readObject(); } catch (UnsupportedEncodingException e) { e.printStackTrace(); } finally { try { if (decoder != null) { decoder.close(); } if (inputStream != null) { inputStream.close(); } } catch (Exception e) { logger.error(&quot;XMLDecoder解析XML失败&quot;, e); } } return null; } } 前面的所有可以说都是为了整个流程在铺垫，现在开始是真正开始生成License证书的代码 LicenseCreator.java import de.schlichtherle.license.*; import org.apache.logging.log4j.LogManager; import org.apache.logging.log4j.Logger; import javax.security.auth.x500.X500Principal; import java.io.File; import java.text.MessageFormat; import java.util.prefs.Preferences; /** * License生成类 -- 用于license生成 */ public class LicenseCreator { private final static X500Principal DEFAULT_HOLDER_AND_ISSUER = new X500Principal(&quot;CN=Haier, OU=Haier, O=Space, L=SH, ST=SH, C=CN&quot;); private static final Logger logger = LogManager.getLogger(LicenseCreator.class); private final License license; public LicenseCreator(License license) { this.license = license; } /** * 生成License证书 */ public boolean generateLicense() { try { LicenseManager licenseManager = new CustomLicenseManager(initLicenseParam()); LicenseContent licenseContent = initLicenseContent(); licenseManager.store(licenseContent, new File(license.getLicensePath())); return true; } catch (Exception e) { logger.error(MessageFormat.format(&quot;证书生成失败：{0}&quot;, license), e); return false; } } /** * 初始化证书生成参数 */ private LicenseParam initLicenseParam() { Preferences preferences = Preferences.userNodeForPackage(LicenseCreator.class); //设置对证书内容加密的秘钥 CipherParam cipherParam = new DefaultCipherParam(license.getStorePass()); KeyStoreParam privateStoreParam = new CustomKeyStoreParam(LicenseCreator.class, license.getPrivateKeysStorePath(), license.getPrivateAlias(), license.getStorePass(), license.getKeyPass()); return new DefaultLicenseParam(license.getSubject(), preferences, privateStoreParam, cipherParam); } /** * 设置证书生成正文信息 */ private LicenseContent initLicenseContent() { LicenseContent licenseContent = new LicenseContent(); licenseContent.setHolder(DEFAULT_HOLDER_AND_ISSUER); licenseContent.setIssuer(DEFAULT_HOLDER_AND_ISSUER); licenseContent.setSubject(license.getSubject()); licenseContent.setIssued(license.getIssuedTime()); licenseContent.setNotBefore(license.getIssuedTime()); licenseContent.setNotAfter(license.getExpiryTime()); licenseContent.setConsumerType(license.getConsumerType()); licenseContent.setConsumerAmount(license.getConsumerAmount()); licenseContent.setInfo(license.getDescription()); //扩展校验，这里可以自定义一些额外的校验信息(也可以用json字符串保存) if (license.getLicenseExtraModel() != null) { licenseContent.setExtra(license.getLicenseExtraModel()); } return licenseContent; } } 测试 - 证书生成 环境工具类都准备好了，接下来直接开始测试，看看能否生成 @Test void generateLicense() { // 生成license需要的一些参数 License param = new License(); // 证书授权主体 param.setSubject(&quot;licenseTest&quot;); // 私钥别名 param.setPrivateAlias(&quot;privateKey&quot;); // 私钥密码（需要妥善保管，不能让使用者知道） param.setKeyPass(&quot;xxxx&quot;); // 访问私钥库的密码 param.setStorePass(&quot;xxxx&quot;); // 证书存储地址 param.setLicensePath(&quot;E:\\\\license2\\\\license.lic&quot;); // 私钥库所在地址 param.setPrivateKeysStorePath(&quot;E:\\\\license\\\\privateKeys.store&quot;); // 证书生效时间 Calendar issueCalendar = Calendar.getInstance(); param.setIssuedTime(issueCalendar.getTime()); // 证书失效时间 Calendar expiryCalendar = Calendar.getInstance(); // 设置当前时间 expiryCalendar.setTime(new Date()); // 往后延长一年 = 授权一年时间 expiryCalendar.add(Calendar.YEAR,1); param.setExpiryTime(expiryCalendar.getTime()); // 用户类型 param.setConsumerType(&quot;user&quot;); // 用户数量 param.setConsumerAmount(1); // 描述 param.setDescription(&quot;测试&quot;); LicenseCreator licenseCreator = new LicenseCreator(param); // 生成license licenseCreator.generateLicense(); } 代码实现 - 证书安装和校验 就像证书生成，验证也需要一个专门的类 LicenseVerify.java import de.schlichtherle.license.*; import org.apache.logging.log4j.LogManager; import org.apache.logging.log4j.Logger; import java.io.File; import java.text.DateFormat; import java.text.MessageFormat; import java.text.SimpleDateFormat; import java.util.prefs.Preferences; /** * License校验类 */ public class LicenseVerify { private static Logger logger = LogManager.getLogger(LicenseVerify.class); /** * 证书subject */ private final String subject; /** * 公钥别称 */ private final String publicAlias; /** * 访问公钥库的密码 */ private final String storePass; /** * 证书生成路径 */ private final String licensePath; /** * 密钥库存储路径 */ private final String publicKeysStorePath; /** * LicenseManager */ private LicenseManager licenseManager; /** * 标识证书是否安装成功 */ private boolean installSuccess; public LicenseVerify(String subject, String publicAlias, String storePass, String licensePath, String publicKeysStorePath) { this.subject = subject; this.publicAlias = publicAlias; this.storePass = storePass; this.licensePath = licensePath; this.publicKeysStorePath = publicKeysStorePath; } /** * 安装License证书，读取证书相关的信息, 在bean加入容器的时候自动调用 */ public void installLicense() { try { Preferences preferences = Preferences.userNodeForPackage(LicenseVerify.class); CipherParam cipherParam = new DefaultCipherParam(storePass); KeyStoreParam publicStoreParam = new CustomKeyStoreParam(LicenseVerify.class, publicKeysStorePath, publicAlias, storePass, null); LicenseParam licenseParam = new DefaultLicenseParam(subject, preferences, publicStoreParam, cipherParam); licenseManager = new CustomLicenseManager(licenseParam); licenseManager.uninstall(); LicenseContent licenseContent = licenseManager.install(new File(licensePath)); DateFormat format = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); installSuccess = true; logger.info(&quot;------------------------------- 证书安装成功 -------------------------------&quot;); logger.info(MessageFormat.format(&quot;证书有效期：{0} - {1}&quot;, format.format(licenseContent.getNotBefore()), format.format(licenseContent.getNotAfter()))); } catch (Exception e) { installSuccess = false; logger.error(&quot;------------------------------- 证书安装失败 -------------------------------&quot;); logger.error(e); } } /** * 卸载证书，在bean从容器移除的时候自动调用 */ public void unInstallLicense() { if (installSuccess) { try { licenseManager.uninstall(); } catch (Exception e) { // ignore } } } /** * 校验License证书 */ public boolean verify() { try { licenseManager.verify(); return true; } catch (Exception e) { return false; } } } 我们之前说了，除了项目许多的配置文件我们一般是需要放在服务器单独的路径下的，除了公钥和私钥库，还有我们验证需要配置的一些参数 application.yml #License相关配置 license: subject: licenseTest #主体 - 注意主体要与生成证书的主体一致一致，不然验证通过不了 publicAlias: publicCert #公钥别称 storePass: 123456q #访问公钥的密码 licensePath: E:\\license2\\license.lic #license位置 publicKeysStorePath: E:\\license\\publicCerts.store #公钥位置 这些参数代码获取如下 LicenseConfig.java import lombok.Data; import org.springframework.boot.context.properties.ConfigurationProperties; import org.springframework.context.annotation.Bean; import org.springframework.stereotype.Component; @Data @Component @ConfigurationProperties(prefix = &quot;license&quot;) public class LicenseConfig { /** * 证书subject */ private String subject; /** * 公钥别称 */ private String publicAlias; /** * 访问公钥库的密码 */ private String storePass; /** * 证书生成路径 */ private String licensePath; /** * 密钥库存储路径 */ private String publicKeysStorePath; @Bean(initMethod = &quot;installLicense&quot;, destroyMethod = &quot;unInstallLicense&quot;) public LicenseVerify licenseVerify() { return new LicenseVerify(subject, publicAlias, storePass, licensePath, publicKeysStorePath); } } 以上代码是读取yml的配置，以及将LicenseConfig加入Spring容器，在加入Spring容器的同时，执行licenseVerify里的安装方法 这样，程序就会在启动时，自动安装证书，校验时就可以用了 @SpringBootTest @RunWith(SpringRunner.class) public class LicenseVerifyTest { private LicenseVerify licenseVerify; @Autowired public void setLicenseVerify(LicenseVerify licenseVerify) { this.licenseVerify = licenseVerify; } @Test public void licenseVerify() { System.out.println(&quot;licese是否有效：&quot; + licenseVerify.verify()); } } ","link":"https://tianxiawuhao.github.io/GEPNPC2k-/"},{"title":"切面链实现","content":"切面链实现 /** * 服务切面接口，用来干预服务 */ public interface ServiceAspect { /** * 上传，成功返回文件信息，失败返回 null */ default serviceInfo serviceAround(ServiceAspectChain chain,ServiceInfo serviceInfo) { return chain.next(service); } } /** * 服务的切面调用链 */ @Getter @Setter public class ServiceAspectChain { private ServiceAspectChainCallback callback; private Iterator&lt;serviceAspect&gt; aspectIterator; public UploadAspectChain(Iterable&lt;ServiceAspect&gt; aspects,ServiceAspectChainCallback callback) { this.aspectIterator = aspects.iterator(); this.callback = callback; } /** * 调用下一个切面(遍历执行最后一个) */ public ServiceInfo next(ServiceInfo serviceInfo) { if (aspectIterator.hasNext()) {//还有下一个 return aspectIterator.next().serviceAround(this,serviceInfo); } else { return callback.run(serviceInfo); } } /** * 调用下一个切面(遍历执行所有) */ public ServiceInfo next(ServiceInfo serviceInfo) { if (aspectIterator.hasNext()) {//还有下一个 service = aspectIterator.next().serviceAround(this,serviceInfo); return callback.run(serviceInfo); } else { return callback.run(serviceInfo); } } } /** * 切面调用链结束回调 */ public interface ServiceAspectChainCallback { ServiceInfo run(ServiceInfo serviceInfo); } private CopyOnWriteArrayList&lt;serviceAspect&gt; aspectList; private ServiceInfo serviceInfo; //处理切面 new ServiceAspectChain(aspectList,(_serviceInfo) -&gt; { //业务逻辑 return _serviceInfo; }).next(serviceInfo); /** * 接口实现,使用切面打印日志 */ @Slf4j @Component public class LogServiceAspect implements ServiceAspect { @Override public serviceInfo serviceAround(ServiceAspectChain chain,ServiceInfo serviceInfo) { log.info(&quot;上传文件 before -&gt; {}&quot;,serviceInfo); serviceInfo = chain.next(serviceInfo); log.info(&quot;上传文件 after -&gt; {}&quot;,serviceInfo); return serviceInfo; } } { //获得切面 List CopyOnWriteArrayList&lt;ServiceAspect&gt; list = new ArrayList(); //增加 LogServiceAspect aspect = new LogServiceAspect(); list.add(aspect); //删除 list.remove(aspect); //条件删除 list.removeIf(item -&gt; item instanceof LogServiceAspect); } ","link":"https://tianxiawuhao.github.io/DCnCuY10S/"},{"title":"发布JAR包到远程中央仓库","content":"声明：经过下面一系列操作之后，以后想发布新版本，只要修改好要升级的版本，然后在 Maven的 Lifecycle 里双击 deploy 即可~ 前言 自使用maven以来，没少使用maven中央仓库中的各种jar包，方便有效，但是咱们也不能总是只取不予，也应该懂得奉献，当你写好了一个十分好用的jar包，想贡献出去给大家使用的时候，应该怎么做呢？当然是发布到maven的中央仓库了，不过要说这个发布过程，还真是比较复杂，本文就来详细说下如何发布jar包到maven中央仓库。 注意事项 工单管理：​ ​https://issues.sonatype.org/secure/Dashboard.jspa​​ 说明：注册账号、创建和管理issue，Jar包的发布是以解决issue的方式起步的。这里的用户名与密码是非常重要的，后面会用到，一定要保存好。 构件仓库：​ ​https://oss.sonatype.org/#welcome​​ 说明：算是正式发布前的一个过段仓库，使用maven提交后的jar包先到这个库中。 镜像仓库：​ ​http://search.maven.org​​ 说明：最终成功发布的jar可以在这里搜到。 一、创建工单 在上述的​​工单管理​​​的地址中进行创建，如果没有账号，需要先注册一个，记住用户名密码，后边要配置到​​setting.xml​​中。 Create Issue 填写内容说明： ===Step 1=== Project：Community Support - Open Source Project Repository Hosting Issue Type：New Project ===Step 2=== Summary：JAR包名称，如：requestjson Group Id：你懂得，不用多说，如com.luxsuen Project URL：项目站点，如：https://github.com/LuxSun/requestjson SCM url：项目源码仓库，如：https://github.com/LuxSun/requestjson.git 其他内容不用填写，创建Issue后需要等待一小段时间，Sonatype的工作人员审核处理，速度还是很快的，一般一个工作日以内，当Issue的Status变为RESOLVED后，就可以进行下一步操作了，否则，就等待… 到这里，需要跟客服唠嗑一下~（以前这一步审核很简单，跟客服说是你的即可，现在开始要提供证明） 如果是用Github，就无需注册域名，直接根据客服提示证明一下即可（新手推荐）。 如果是用自己的域名，就需要根据客服的提示配置下DNS。 ​ Ps：无论以上是哪种情况，最后只要呈现如下图所示，即代表成功！ 二、配置Maven（pom.xml） 接下来就是重头戏了，pom.xml是一个maven项目的重点配置，一个项目的所有配置都可以由这个文件来描述，文件中的所有配置都有默认值，也就是说所有的配置都是可选配置，但是为了把构件发布到中央仓库，我们必须配置一些关键信息，否则再发布时是不会通过了。 在工程的pom.xml文件中，引入Sonatype官方的一个通用配置​​oss-parent​​，这样做的好处是很多pom.xml的发布配置不需要自己配置了： &lt;parent&gt; &lt;groupId&gt;org.sonatype.oss&lt;/groupId&gt; &lt;artifactId&gt;oss-parent&lt;/artifactId&gt; &lt;version&gt;7&lt;/version&gt; &lt;/parent&gt; 并增加Licenses、SCM、Developers信息： &lt;licenses&gt; &lt;license&gt; &lt;name&gt;The Apache Software License, Version 2.0&lt;/name&gt; &lt;url&gt;http://www.apache.org/licenses/LICENSE-2.0.txt&lt;/url&gt; &lt;distribution&gt;repo&lt;/distribution&gt; &lt;/license&gt; &lt;/licenses&gt; &lt;developers&gt; &lt;developer&gt; &lt;name&gt;Lux Sun&lt;/name&gt; &lt;email&gt;28554482@qq.com&lt;/email&gt; &lt;organization&gt;Lux Sun&lt;/organization&gt; &lt;url&gt;https://github.com/LuxSun&lt;/url&gt; &lt;/developer&gt; &lt;/developers&gt; &lt;scm&gt; &lt;url&gt;https://github.com/LuxSun/requestjson&lt;/url&gt; &lt;connection&gt;https://github.com/LuxSun/requestjson.git&lt;/connection&gt; &lt;/scm&gt; 修改maven配置文件setting.xml，在servers中增加server配置。 &lt;servers&gt; &lt;server&gt; &lt;id&gt;sonatype-nexus-snapshots&lt;/id&gt; &lt;username&gt;Sonatype 账号&lt;/username&gt; &lt;password&gt;Sonatype 密码&lt;/password&gt; &lt;/server&gt; &lt;server&gt; &lt;id&gt;sonatype-nexus-staging&lt;/id&gt; &lt;username&gt;Sonatype 账号&lt;/username&gt; &lt;password&gt;Sonatype 密码&lt;/password&gt; &lt;/server&gt; &lt;/servers&gt; 根据官方指南，这里需要4个插件， maven-source-plugin 用来生成Source Jar文件 maven-javadoc-plugin 用来生成 javadoc 文档 maven-gpg-plugin 用来对工程文件进行自动签名 nexus-staging-maven-plugin 用来将工程发布到中央仓库 另外注意生成javadoc文档时需要指定关闭doclint，不然可能因为使用了不规范的javadoc注解而导致失败，完整配置如下。 完整版 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.luxsuen&lt;/groupId&gt; &lt;artifactId&gt;requestjson&lt;/artifactId&gt; &lt;version&gt;1.0.1&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;requestjson&lt;/name&gt; &lt;description&gt;RequestJson&lt;/description&gt; &lt;url&gt;https://github.com/LuxSun/requestjson&lt;/url&gt; &lt;properties&gt; &lt;spring.version&gt;4.3.2.RELEASE&lt;/spring.version&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;maven.compiler.encoding&gt;UTF-8&lt;/maven.compiler.encoding&gt; &lt;/properties&gt; &lt;licenses&gt; &lt;license&gt; &lt;name&gt;The Apache Software License, Version 2.0&lt;/name&gt; &lt;url&gt;http://www.apache.org/licenses/LICENSE-2.0.txt&lt;/url&gt; &lt;distribution&gt;repo&lt;/distribution&gt; &lt;/license&gt; &lt;/licenses&gt; &lt;developers&gt; &lt;developer&gt; &lt;name&gt;Lux Sun&lt;/name&gt; &lt;email&gt;28554482@qq.com&lt;/email&gt; &lt;organization&gt;Lux Sun&lt;/organization&gt; &lt;url&gt;https://github.com/LuxSun&lt;/url&gt; &lt;/developer&gt; &lt;/developers&gt; &lt;scm&gt; &lt;url&gt;https://github.com/LuxSun/requestjson&lt;/url&gt; &lt;connection&gt;https://github.com/LuxSun/requestjson.git&lt;/connection&gt; &lt;/scm&gt; &lt;distributionManagement&gt; &lt;snapshotRepository&gt; &lt;id&gt;ossrh&lt;/id&gt; &lt;name&gt;oss Snapshots Repository&lt;/name&gt; &lt;url&gt;https://oss.sonatype.org/content/repositories/snapshots&lt;/url&gt; &lt;/snapshotRepository&gt; &lt;repository&gt; &lt;id&gt;ossrh&lt;/id&gt; &lt;name&gt;oss Staging Repository&lt;/name&gt; &lt;url&gt;https://oss.sonatype.org/service/local/staging/deploy/maven2/&lt;/url&gt; &lt;/repository&gt; &lt;/distributionManagement&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.1&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt; &lt;version&gt;3.0.2&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;default-jar&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;jar&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;configuration&gt; &lt;excludes&gt; &lt;exclude&gt;**/spring/&lt;/exclude&gt; &lt;exclude&gt;**/com/luxsuen/requestjson/entity/&lt;/exclude&gt; &lt;exclude&gt;**/com/luxsuen/requestjson/web/&lt;/exclude&gt; &lt;exclude&gt;**/META-INF/*.kotlin_module&lt;/exclude&gt; &lt;/excludes&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;profiles&gt; &lt;profile&gt; &lt;id&gt;release&lt;/id&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-source-plugin&lt;/artifactId&gt; &lt;version&gt;2.2.1&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;attach-sources&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;jar-no-fork&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;configuration&gt; &lt;excludes&gt; &lt;exclude&gt;**/spring/&lt;/exclude&gt; &lt;exclude&gt;**/com/luxsuen/requestjson/entity/&lt;/exclude&gt; &lt;exclude&gt;**/com/luxsuen/requestjson/web/&lt;/exclude&gt; &lt;exclude&gt;**/META-INF/*.kotlin_module&lt;/exclude&gt; &lt;/excludes&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-javadoc-plugin&lt;/artifactId&gt; &lt;version&gt;2.9.1&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;attach-javadocs&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;jar&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;configuration&gt; &lt;excludePackageNames&gt;com.luxsuen.requestjson.entity:com.luxsuen.requestjson.web&lt;/excludePackageNames&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-gpg-plugin&lt;/artifactId&gt; &lt;version&gt;1.5&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;sign-artifacts&lt;/id&gt; &lt;phase&gt;verify&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;sign&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.sonatype.plugins&lt;/groupId&gt; &lt;artifactId&gt;nexus-staging-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.6.7&lt;/version&gt; &lt;extensions&gt;true&lt;/extensions&gt; &lt;configuration&gt; &lt;serverId&gt;ossrh&lt;/serverId&gt; &lt;nexusUrl&gt;https://oss.sonatype.org/&lt;/nexusUrl&gt; &lt;autoReleaseAfterClose&gt;true&lt;/autoReleaseAfterClose&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; &lt;/profiles&gt; &lt;/project&gt; 注意1：以上 pom.xml 必须包括：name、description、url、licenses、developers、scm 等基本信息，此外，使用了 Maven 的 profile 功能，只有在 release 的时候，创建源码包、创建文档包、使用 GPG 进行数字签名。此外，snapshotRepository 与 repository 中的 id 一定要与 setting.xml 中 server 的 id 保持一致。 注意2：这里在 nexus-staging-maven-plugin 插件里开启了自动 Release。也可以关掉，然后登录构件仓库 ​ ​https://oss.sonatype.org​​ 手动去 close 然后 relseae。 三、配置gpg-key 1、可视化配置 Windows的话先到 ​ ​https://gpg4win.org/download.html​​ 下载最新的版本，直接安装即可打开即可。 1.1、新建密钥对（OpenPGP类型） 这里需要填入名字和电子邮件，然后注意可以在高级设置那里将有效期一栏的小勾取消掉，即有效期无限，如下。 1.2、确认信息并输入密码 1.3、选择将公钥上传到目录服务 注意：如果这里不点的话后面分发后将无法在服务器找到对应公钥。 1.4、右键选择生成的证书在服务器上发布 1.5、根据密钥ID在服务器上查找公钥 最终必须保证这一步成功！如下。 Ps：注意：这里有个bug，如果生成的密钥ID的第一个字符是数字0的话，在下面使用maven-gpg插件的时候可能会被自动去掉导致密钥找不到，所以保险起见遇到这种情况就重新生成证书。 2、命令行配置 Ps：Windows系统使用“gpg”命令开头，而Mac系统使用“gpg2”命令开头。 如果是使用的windows，可以下载gpg4win，地址：​​https://www.gpg4win.org/download.html​​​，安装后在命令行中执行 ​​gpg --gen-key​​生成，过程中需要填写名字、邮箱等，其他步骤可以使用默认值，不过有个叫：Passphase的参数需要记住，这个相当于是是密钥的密码，下一步发布过程中进行签名操作的时候会用到。 建议大家下载 Gpg4win-Vanilla 版本，因为它仅包括 GnuPG，这个工具才是我们所需要的。 安装 GPG 软件后，打开命令行窗口，依次做以下操作： 2.1、查看是否安装成功 gpg --version 能够显示 GPG 的版本信息，说明安装成功了。 2.2、生成密钥对 gpg --gen-key 此时需要输入姓名、邮箱等字段，其它字段可使用默认值，此外，还需要输入一个 Passphase，相当于一个密钥库的密码，一定不要忘了，也不要告诉别人，最好记下来，因为后面会用到。 2.3、查看公钥 gpg --list-keys 输出如下信息（新版本可能格式会有点区别，不用担心，是正常的）： C:/Users/huangyong/AppData/Roaming/gnupg/pubring.gpg ---------------------------------------------------- pub 2048R/82DC852E 2014-04-24 uid hy_think &lt;hy_think@163.com&gt; sub 2048R/3ACA39AF 2014-04-24 可见这里的公钥的 ID 是：82DC852E，很明显是一个 16 进制的数字，马上就会用到。 2.4、将公钥发布到 PGP 密钥服务器（后面还需要操作此步骤） gpg --keyserver hkp://pool.sks-keyservers.net --send-keys 82DC852E 此后，可使用本地的私钥来对上传构件进行数字签名，而下载该构件的用户可通过上传的公钥来验证签名，也就是说，大家可以验证这个构件是否由本人上传的，因为有可能该构件被坏人给篡改了。 2.5、查询公钥是否发布成功 gpg --keyserver hkp://pool.sks-keyservers.net --recv-keys 82DC852E 实际上就是从 key server 上通过公钥 ID 来接收公钥，此外，也可以到 sks-keyservers.net 上通过公钥 ID 去查询。 四、上传构件到 OSS 中 这步就简单了，就是一套命令： mvn clean deploy -P sonatype-oss-release -Darguments=&quot;gpg.passphrase=密钥密码&quot; 如果使用eclipse的mvn插件发布的话，配置如下（IDEA类似）： 如果发布成功，就可以到构件仓库中查看了。 或者 先通过 cmd 进入到项目的这个目录下。 再执行下面命令。 mvn clean deploy -P release 当执行以上 Maven 命令时，会自动弹出一个对话框，需要输入上面提到的 Passphase，它就是通过 GPG 密钥对的密码，只有自己才知道。随后会看到大量的 upload 信息，而且速度比较慢，经常会 timeout，需要反复尝试。 注意：此时上传的构件并未正式发布到中央仓库中，只是部署到 OSS 中了，下面才是真正的发布。 五、在 OSS 中发布构件 进入​​https://oss.sonatype.org/#stagingRepositories​​​查看发布好的构件，点击左侧的​​Staging Repositories​​​，一般最后一个就是刚刚发布的jar了，此时的构件状态为​​open​​。 打开命令行窗口，查看gpg key并上传到第三方的key验证库： E:\\98_code\\workSpace\\marathon-client&gt;gpg --list-keys C:/Users/VF/AppData/Roaming/gnupg/pubring.gpg --------------------------------------------- pub 2048R/824B4D7A 2016-01-06 uid [ultimate] cloudnil &lt;cloudnil@126.com&gt; sub 2048R/7A10AD69 2016-01-06 E:\\98_code\\workSpace\\marathon-client&gt;gpg --keyserver hkp://keyserver.ubuntu.com:11371 --send-keys 824B4D7A gpg: sending key 824B4D7A to hkp server keyserver.ubuntu.com E:\\98_code\\workSpace\\marathon-client&gt; 回到 ​ ​https://oss.sonatype.org/#stagingRepositories​​，选中刚才发布的构件，并点击上方的close–&gt;Confirm，在下边的Activity选项卡中查看状态，当状态变成closed后，执行Release–&gt;Confirm，并在下边的Activity选项卡中查看状态，成功后构件自动删除。 需要在曾经创建的 Issue 下面回复一条“构件已成功发布”的评论，这是为了通知 Sonatype 的工作人员为需要发布的构件做审批，发布后会关闭该 Issue。 一小段时间（约1-2个小时）后即可同步到​ ​https://search.maven.org​​​中央仓库（Maven中央仓库可能需要24-48小时：​ ​https://mvnrepository.com​​）。 Ps：注意这里有个大坑，之前很多教程都会把这个步骤提前到最开始（我不是说生成密钥这个环节，这个密钥生成环节的确是一开始就可以弄了的），但下面这句解析需要到这一步再开始搞，之前一直在https://oss.sonatype.org/#stagingRepositories这里审核一直失败，大坑呀~ 最后，想说一句：第一次都是很痛的，以后就舒服了。没错，只有第一次发布才如此痛苦，以后 deploy 的构件会自动部发布到中央仓库，无需再这样折腾了。（附加：成功效果图） 附： 在编码过程中，有些通用的代码模块，有时候我们不想通过复制拷贝来粗暴地复用，因为这样不仅体现不了变化，也不利于统一管理。这里我们使用maven deploy的方式，将通用的模块打成jar包，发布到nexus，让其他的项目来引用，以更简洁高效的方式来实现复用和管理。 第一：maven的settings.xml文件中设置标签 &lt;server&gt; &lt;id&gt;my-deploy-release&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin123&lt;/password&gt; &lt;/server&gt; &lt;server&gt; &lt;id&gt;my-deploy-snapshot&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin123&lt;/password&gt; &lt;/server&gt; 此处设置的用户名和密码都是nexus的登陆配置。 第二：在项目的pom.xml文件中设置 &lt;distributionManagement&gt; &lt;repository&gt; &lt;id&gt;my-deploy-release&lt;/id&gt; &lt;url&gt;http://192.168.1.123:8081/nexus/content/repositories/releases/&lt;/url&gt; &lt;/repository&gt; &lt;snapshotRepository&gt; &lt;id&gt;my-deploy-snapshot&lt;/id&gt; &lt;url&gt;http://192.168.1.123:8081/nexus/content/repositories/snapshots/&lt;/url&gt; &lt;/snapshotRepository&gt; &lt;/distributionManagement&gt; 在此，url都是nexus相应仓库的链接地址，这一步做完之后，已经完成了发布所需要的基本配置。【试试命令：mvn deploy】 注意：中的要和、的一致，maven在发布时，会根据此id来查找相应的用户名密码进行登录验证并上传文件。 第三：发布的灵活性配置 maven会判断版本后面是否带了-SNAPSHOT，如果带了就发布到snapshots仓库，否则发布到release仓库。这里我们可以在pom.xml文件中，设置。 &lt;groupId&gt;com.test&lt;/groupId&gt; &lt;artifactId&gt;my-test&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;version&gt;${project.release.version}&lt;/version&gt; &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;project.release.version&gt;1.0-SNAPSHOT&lt;/project.release.version&gt; &lt;/properties&gt; &lt;profiles&gt; &lt;profile&gt; &lt;id&gt;product&lt;/id&gt; &lt;properties&gt; &lt;project.release.version&gt;1.0&lt;/project.release.version&gt; &lt;/properties&gt; &lt;/profile&gt; &lt;/profiles&gt; 说明：通过占位符${project.release.version}来控制需要发布的版本，用命令mvn deploy -P product，发布my-test的1.0版本到releases库。如果使用命令mvn deploy，则默认使用 1.0-SNAPSHOT版本号，将发布my-test的1.0-SNAPSHOT版本到snapshots库。 第四：发布时遇到的一些问题 部署到snapshot仓库时，jar包会带上时间戳，这没关系，maven会自动取相应版本最新的jar包； Failed to execute goal org.apache.maven.plugins:maven-deploy-plugin:2.8.2:deploy (default-deploy) on project my-test: Failed to deploy artifacts: Could not transfer artifact...from/to release...部署到release仓库时，相同版本的jar包不能提交。 原因：因为release的部署策略是【disable redeploy】，不允许覆盖更新组件。 解决办法：修改一下版本号，再提交即可。之前其实还可以通过人工客服聊天帮忙删除不想要的Jar包，但是现在不行了，只能通过上传新版本，所以有些童鞋想做测试的需要注意这一点。 打包指定的文件/文件夹在代码中也有体现（含：doc、sources、class）。 Maven deploy 时，为什么没有生成对应的 javadoc 和 sources （前提已经有插件部署好）？ ","link":"https://tianxiawuhao.github.io/vU42syr2A/"},{"title":"Kubernetes的存储","content":"一、K8s的存储的基础知识与分类 参考官方文档：https://kubernetes.io/zh/docs/concepts/storage/ 1、基础知识： K8s的基本单位为Pod，但是一个Pod中实际工作的是容器containers，所以要挂载的卷其实也是给容器使用的；K8s卷的设计： 容器Container：申请容器内的哪个目录/文件将被挂载出来；（但是如何挂载，他不做细节干涉） Pod：为其中的每个容器声明出来的挂载，指定详细的挂载详情；（有很多种挂载模式，可以是本地的、可以是远程的、甚至还可以是云服务上提供的对象存储服务） 2、数据卷分类： 数据卷实现分类有很多，可通过explain查看所有暂时支持的实现分类（多达30种） kubectl explain pod.spec.volumes 但是我们大概可以粗分为以下三大类： 配置信息：类似于独立环境变量文件的方式，将文件种的键值对挂载到容器内； 临时存储：这些存储虽然挂出来了，但是如果容器被删除，或者在其他机器上被拉起，这些挂载文件将无法继续被读取到； 持久化存储：真正的持久化，K8s将自己不擅长的存储实现，交给了别的擅长于存储的服务商或软件；——这也是我们以后学习的重点难点！ 二、NFS网络文件系统的安装与调试 NFS（Network File System）即网络文件系统，它允许网络中的计算机之间通过网络共享资源。将NFS主机分享的目录，挂载到本地客户端当中，本地NFS的客户端应用可以透明地读写位于远端NFS服务器上的文件，在客户端端看起来，就像访问本地文件一样。 NFS设计成C/S架构，通过RPC远程过程调用的方式实现数据同步！ 1、在Master节点安装NFS Server（当然，其他节点也可以） yum install -y nfs-utils #执行命令 vi /etc/exports，创建 exports 文件，文件内容如下： echo &quot;/nfs/data/ *(insecure,rw,sync,no_root_squash)&quot; &gt; /etc/exports # 执行以下命令，启动 nfs 服务;创建共享目录 mkdir -p /nfs/data systemctl enable rpcbind systemctl enable nfs-server systemctl start rpcbind systemctl start nfs-server exportfs -r #检查配置是否生效 [root@k8s-01 /]# exportfs /nfs/data &lt;world&gt; ## 说明已经nfs-server服务已经启动，生效 2、在我们两台需要共享nfs server的机器上，安装NFS Client yum install -y nfs-utils #执行以下命令检查 nfs 服务器端是否有设置共享目录 # showmount -e $(nfs服务器的IP) showmount -e 192.168.56.20 # 输出结果如下所示 Export list for 192.168.56.20 /nfs/data * #执行以下命令挂载 nfs 服务器上的共享目录到本机路径 /root/nfsmount mkdir /root/nfsmount # mount -t nfs $(nfs服务器的IP):/root/nfs_root /root/nfsmount mount -t nfs 192.168.56.20:/nfs/data /root/nfsmount 3、测试NFS服务是否在3台服务器上已经生效 #1、 client节点执行命令，看本地的文件系统列表： [root@k8s-02 ~]# df -Th Filesystem Type Size Used Avail Use% Mounted on devtmpfs devtmpfs 1.9G 0 1.9G 0% /dev tmpfs tmpfs 1.9G 0 1.9G 0% /dev/shm tmpfs tmpfs 1.9G 9.4M 1.9G 1% /run tmpfs tmpfs 1.9G 0 1.9G 0% /sys/fs/cgroup /dev/sda1 xfs 40G 11G 30G 27% / tmpfs tmpfs 379M 0 379M 0% /run/user/0 192.168.56.20:/nfs/data nfs4 40G 6.3G 34G 16% /root/nfsmount ## 可以看到，有一个是远程的网络文件系统，挂载了我们自己本地的/root/nfsmount 目录上 #2、在server上目录种创建文件aaa.txt #3、在clinet上目录中创建文件bbb.txt 可以看到，3台主机上对应的文件已经立即得到了同步！ 4、补充，有时候，我们即使安装了nfs-utils，依然无法使用showmount -e ip 我们可以直接在客户端配置fstab挂载： vi /etc/fstab ##追加nfs master的信息 10.130.59.50:/nfs/data /nfs/data nfs defaults 0 0 执行以下命令，让修改后的fstab生效： mount -a 之后查看： df -Th ##存在以下行： 10.130.59.50:/nfs/data nfs4 50G 4.4G 46G 9% /nfs/data 之后创建文件，测试正常！ 三、使用一个Nginx测试Pod使用NFS进行挂载 1、创建nginxnfs.yaml文件： #测试Pod直接挂载NFS了 apiVersion: v1 kind: Pod metadata: name: vol-nfs namespace: mytest spec: containers: - name: mynginx image: nginx volumeMounts: - name: html mountPath: /usr/share/nginx/html/ volumes: - name: html nfs: path: /nfs/data server: 192.168.56.20 2、执行此yaml文件，部署一台nginx： [root@k8s-01 mytest]# kubectl apply -f nginxnfs.yaml pod/vol-nfs created [root@k8s-01 mytest]# kubectl get pod -n mytest -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES vol-nfs 1/1 Running 0 68s 10.244.165.226 k8s-03 &lt;none&gt; &lt;none&gt; 3、nfs server的对应目录下，创建一个index.html文件，然后访问： [root@k8s-01 mytest]# echo 12345 &gt; /nfs/data/index.html [root@k8s-01 mytest]# curl 10.244.165.226 12345 可以看到，明明部署在k8s-03节点上面的pod，依然可以正常使用k8s-01节点上的文件系统进行文件挂载！ 四、PV、PVC、StorageClass的基础知识 背景：为什么要把一个存储事情分成这么多层？如此复杂？ 答：存储的管理是一个与计算实例的管理完全不同的问题。 因为整个Pod部署的yaml文件可能都是由我们开发人员编写的，但是我们以后部署的服务器什么情况我们肯定是不知道的，服务器上能支持的文件存储方式我们也是不知道的，那我们的Pod肯定没有办法一气呵成地完成所有的事情！ 有了PV（Persistent Volume）、PVC（Persistent Volume Claim）后，我们就可以让运维人员提前准备好很多的PV，这样我们在部署Pod的yaml文件种，只需要添加一个PVC（PV的使用申请），这样K8s就能够自动地为Pod匹配上可以使用的PV存储卷了； ****程序员：****负责Pod + PVC的yaml文件即可； **K8s运维人员：**负责准备好PV；—— 这里还分为静态供应、自动供应 两类； 1、什么是PV（*Persistent Volume 持久化存储卷*）？ PV 卷的供应有两种方式：静态供应或动态供应。 静态供应：所有的事情，尤其是PV的创建，我是由管理员手动创建的，如果没有现成的可用的PV存在，我们的PVC申请将一直处于Pending状态； 动态供应：PV的创建是自动化完成的，主要依赖于底层的StorageClass来实现；—— 后面详述 手动创建PV（mypv.yaml）—— 我一次创建了3个PV： apiVersion: v1 kind: PersistentVolume metadata: name: mypv-10m labels: type: local spec: storageClassName: my-nfs-storage capacity: storage: 10m accessModes: - ReadWriteOnce nfs: #使用nfs存储系统，挂载的目录为192.168.56.20:/nfs/data/one server: 192.168.56.20 path: /nfs/data/one --- apiVersion: v1 kind: PersistentVolume metadata: name: mypv-20m labels: type: local spec: storageClassName: my-nfs-storage capacity: storage: 20m accessModes: - ReadWriteOnce nfs: #使用nfs存储系统，挂载的目录为192.168.56.20:/nfs/data/one server: 192.168.56.20 path: /nfs/data/two --- apiVersion: v1 kind: PersistentVolume metadata: name: mypv-50m labels: type: local spec: storageClassName: my-nfs-storage capacity: storage: 50m accessModes: - ReadWriteOnce nfs: #使用nfs存储系统，挂载的目录为192.168.56.20:/nfs/data/one server: 192.168.56.20 path: /nfs/data/three 应用此yaml文件后，将产生3个pv存储卷： [root@k8s-01 mytest]# kubectl apply -f mypv.yaml persistentvolume/mypv-10m created persistentvolume/mypv-20m created persistentvolume/mypv-50m created [root@k8s-01 mytest]# kubectl get pv -n mytest NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE mypv-10m 10m RWO Retain Available my-nfs-storage 11s mypv-20m 20m RWO Retain Available my-nfs-storage 11s mypv-50m 50m RWO Retain Available my-nfs-storage 11s 注：其中重要的字段： ACCESS MODES：访问模式 RWO – ReadWriteOnce：允许单节点读写 ROX – ReadOnlyMany：允许多节点只读（读写分离时可用） RWX – ReadWriteMany：允许多节点读写（集群常用） RWOP – ReadWriteOncePod：只允许单Pod读写（比RWO更精细） RECLAIM POLICY：回收策略 Retain — 手动回收，PVC被删除，PV纹丝不动，自己手动控制，最安全； Recycle — 基本擦除 (rm -rf /thevolume/*)，卷里面的内容将会被清除，卷变为Released状态，依然无法被其他其他PVC申领； Delete — 诸如 AWS EBS、GCE PD、Azure Disk 或 OpenStack Cinder 卷这类关联存储资产也被删除； 目前，仅 NFS 和 HostPath 支持回收（Recycle）。 AWS EBS、GCE PD、Azure Disk 和 Cinder 卷都支持删除（Delete）。： pv自己也跟着删除 STATUS：当前状态 Available（可用）– 卷是一个空闲资源，尚未绑定到任何申领； Bound（已绑定）– 该卷已经绑定到某申领； Released（已释放）– 所绑定的PVC已被删除，但是资源尚未被集群回收； Failed（失败）– 卷的自动回收操作失败。 CLAIM：当前正被哪个PVC绑定 —— 当某个PV被PVC申领之后，该列就会有数值了！ STORAGECLASS：存储类——动态供应的重点，每个StorageClass都对应着自己的一套自己的存储实现； 2、什么是PVC（Persistent Volume Claim 持久化存储卷申明/申请）？ PVC其实就是我们程序员编写的，对于PV资源的一个申请，当有合适的PV的时候，我们即可以进行绑定，如果没有合适的PV，我们申请的PVC将一直处于Pending状态，直到有合适的PV被创建出来！ 手动创建一个PVC（mypvc.yaml） apiVersion: v1 kind: PersistentVolumeClaim metadata: name: mypvc namespace: mytest labels: app: nginx-pvc spec: storageClassName: my-nfs-storage #必须与PV的该项对应，K8s将会从该SC对应的pv资源种为我们匹配合适的资源 accessModes: - ReadWriteOnce resources: requests: storage: 15m #我故意写的15m，看看K8s是否会帮我们挑选一个最合适的PV资源 我们应用此yaml文件后，查看pvc和pv的状态： [root@k8s-01 mytest]# kubectl apply -f mypvc.yaml persistentvolumeclaim/mypvc created [root@k8s-01 mytest]# kubectl get pvc -n mytest NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE mypvc Bound mypv-20m 20m RWO my-nfs-storage 14s [root@k8s-01 mytest]# kubectl get pv -n mytest NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE mypv-10m 10m RWO Retain Available my-nfs-storage 2m36s mypv-20m 20m RWO Retain Bound mytest/mypvc my-nfs-storage 2m36s mypv-50m 50m RWO Retain Available my-nfs-storage 2m36s 可以看到，pv和pvc的状态都变为了Bound状态；同时，K8s为我们挑选的时最合适的PV资源； 3、我们再编写一个Nginx的Pod，来使用上面创建的pvc申请（pvcnginx.yaml） apiVersion: v1 kind: Pod metadata: name: &quot;mypvc-nginx&quot; namespace: mytest labels: app: &quot;mypvc-nginx&quot; spec: containers: - name: mypvc-nginx image: &quot;nginx&quot; ports: - containerPort: 80 name: http volumeMounts: - name: localtime mountPath: /etc/localtime - name: html mountPath: /usr/share/nginx/html volumes: - name: localtime hostPath: path: /usr/share/zoneinfo/Asia/Shanghai - name: html persistentVolumeClaim: claimName: mypvc #我自己创建的pvc的名字 restartPolicy: Always 我们应用此yaml后，对应的nginx即可被创建，然后我们在对应的two文件夹种创建index.html文件即可看到效果： [[root@k8s-01 mytest]# kubectl get all -n mytest NAME READY STATUS RESTARTS AGE pod/mypvc-nginx 1/1 Running 0 3m55s pod/vol-nfs 1/1 Running 0 5h25m [root@k8s-01 mytest]# echo mypvc-nginx &gt; /nfs/data/two/index.html [root@k8s-01 mytest]# curl 10.244.165.227 mypvc-nginx 这样，整个”静态供应“的流程就完成了！ 4、什么是StorageClass（存储类）？ 整个静态供应的过程中，我们好像都没有 StorageClass 什么事情，除了就用它来隔绝PV资源一样，没有了其他作用！ 每个 StorageClass 都包含 provisioner、parameters 和 reclaimPolicy 字段， 这些字段会在 StorageClass 需要动态分配 PersistentVolume 时会使用到。 provisioner：我们一般称之为 供应商； 当我们创建StorageClass的时候，会指定使用哪个provisioner来为我们供应服务（自动创建PV） 而在我们的PVC中，我们会选择使用哪一个StorageClass来为我们服务； 这样，整个”动态供应“的模型就出来了！我们程序员只需要编写 Pod、PVC的yaml文件，在PVC中指定StorageClassName即可，至于StorageClass对应的是自动供应PV，还是手动创建PV，都跟我们没有关系了！ 五、借助NFS存储服务实现一个动态供应案例 再来回顾一下”动态供应“的流程：（其中重点就是：当没有合适的PV时候，K8s会获取StorageClass信息，帮我们创建合适的PV资源） 1、参考 nfs-subdir-external-provisioner 动态供应的文档创建yaml 官方文档地址：https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner 我们把这三个文件写在同一个 pvc-provisioner.yaml 文件中，便于移植： apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: managed-nfs-storage annotations: storageclass.kubernetes.io/is-default-class: &quot;true&quot; provisioner: k8s-sigs.io/nfs-subdir-external-provisioner parameters: archiveOnDelete: &quot;true&quot; --- apiVersion: apps/v1 kind: Deployment metadata: name: nfs-client-provisioner labels: app: nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: mynfs spec: replicas: 1 strategy: type: Recreate selector: matchLabels: app: nfs-client-provisioner template: metadata: labels: app: nfs-client-provisioner spec: serviceAccountName: nfs-client-provisioner containers: - name: nfs-client-provisioner image: registry.cn-hangzhou.aliyuncs.com/jgqk8s/nfs-subdir-external-provisioner:v4.0.2 volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: k8s-sigs.io/nfs-subdir-external-provisioner - name: NFS_SERVER value: 10.206.114.4 - name: NFS_PATH value: /nfs/data volumes: - name: nfs-client-root nfs: server: 10.206.114.4 path: /nfs/data --- apiVersion: v1 kind: ServiceAccount metadata: name: nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: mynfs --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: nfs-client-provisioner-runner rules: - apiGroups: [&quot;&quot;] resources: [&quot;nodes&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;persistentvolumes&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;delete&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;persistentvolumeclaims&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;update&quot;] - apiGroups: [&quot;storage.k8s.io&quot;] resources: [&quot;storageclasses&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;events&quot;] verbs: [&quot;create&quot;, &quot;update&quot;, &quot;patch&quot;] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: run-nfs-client-provisioner subjects: - kind: ServiceAccount name: nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: mynfs roleRef: kind: ClusterRole name: nfs-client-provisioner-runner apiGroup: rbac.authorization.k8s.io --- kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: mynfs rules: - apiGroups: [&quot;&quot;] resources: [&quot;endpoints&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;] --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: mynfs subjects: - kind: ServiceAccount name: nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: mynfs roleRef: kind: Role name: leader-locking-nfs-client-provisioner apiGroup: rbac.authorization.k8s.io 2、执行上面的yaml文件： [root@k8s-01 mytest]# kubectl apply -f pvc-provisioner.yaml storageclass.storage.k8s.io/managed-nfs-storage created deployment.apps/nfs-client-provisioner created serviceaccount/nfs-client-provisioner created clusterrole.rbac.authorization.k8s.io/nfs-client-provisioner-runner created clusterrolebinding.rbac.authorization.k8s.io/run-nfs-client-provisioner created role.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created rolebinding.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created [root@k8s-01 mytest]# kubectl get storageclass NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE managed-nfs-storage (default) k8s-sigs.io/nfs-subdir-external-provisioner Delete Immediate false 94s 其中的default，就代表该StorageClass为默认SC； 如果刚开始的Yaml文件中配置配置为 default 默认SC，我们可以通过以下两种方式修改； 通过 kubectl edit 命令修改： kubectl edit sc managed-nfs-storage 通过 kubectl patch 命令修改： kubectl patch storageclass managed-nfs-storage -p '{&quot;metadata&quot;: {&quot;annotations&quot;:{&quot;storageclass.kubernetes.io/is-default-class&quot;:&quot;true&quot;}}}' 3、创建一个Pod + PVC，使用上面的 SC 完成动态供应 apiVersion: v1 kind: Pod metadata: name: &quot;auto-pv-nginx&quot; namespace: mytest labels: app: &quot;auto-pv-nginx&quot; spec: containers: - name: auto-pv-nginx image: &quot;nginx&quot; ports: - containerPort: 80 name: http volumeMounts: - name: localtime mountPath: /etc/localtime - name: html mountPath: /usr/share/nginx/html volumes: - name: localtime hostPath: path: /usr/share/zoneinfo/Asia/Shanghai - name: html persistentVolumeClaim: claimName: auto-pvc #下方对应的pvc的名字 restartPolicy: Always --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: auto-pvc namespace: mytest labels: app: auto-pvc spec: storageClassName: managed-nfs-storage #如果我们已经配置了默认的StorageClass，该行可以缺省 accessModes: - ReadWriteOnce resources: requests: storage: 30m #待会看看是不是会自动创建一个30m的PV资源 应用该yaml后，看看是否自动创建了一个30m的PV资源： [root@k8s-01 mytest]# kubectl apply -f auto-pv-nginx.yaml pod/auto-pv-nginx created persistentvolumeclaim/auto-pvc created [root@k8s-01 mytest]# kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE mypv-10m 10m RWO Retain Available my-nfs-storage 78m mypv-20m 20m RWO Retain Bound mytest/mypvc my-nfs-storage 78m mypv-50m 50m RWO Retain Available my-nfs-storage 78m pvc-b7ecbdff-89f4-4135-91b9-cf8d227ce86f 30m RWO Delete Bound mytest/auto-pvc managed-nfs-storage 26s 4、我们将刚刚部署的测试用的pod和pvc删除，检验该SC的回收策略？ [root@k8s-01 mytest]# ls /nfs/data/ aaa.txt bbb.txt index.html mytest-auto-pvc-pvc-b7ecbdff-89f4-4135-91b9-cf8d227ce86f one three two [root@k8s-01 mytest]# kubectl delete -f auto-pv-nginx.yaml pod &quot;auto-pv-nginx&quot; deleted persistentvolumeclaim &quot;auto-pvc&quot; deleted [root@k8s-01 mytest]# ls /nfs/data/ aaa.txt archived-mytest-auto-pvc-pvc-b7ecbdff-89f4-4135-91b9-cf8d227ce86f bbb.txt index.html one three two 上面的现象足以证明： 我们此StorageClass创建出来的PV的回收策略为DELETE（PVC删除时候，PV也被删除） 但是虽然PV被删除了，但是删除前，该SC帮我们做了数据备份；（原文件夹前增加了archived-前缀）—— archiveOnDelete 字段为true ","link":"https://tianxiawuhao.github.io/Ija68kq2u/"},{"title":"序列化工具Protobuf","content":"Netty的编码和解码 编写网络应用程序时，因为数据在网络中传输的都是二进制字节码数据，在发送数据时就需要编码，接收数据时就需要解码 codec(编解码器) 的组成部分有两个：decoder(解码器)和 encoder(编码器)。encoder 负责把业务数据转换成字节码数据，decoder 负责把字节码数据转换成业务数据。 Netty提供的编码器和解码器 Netty 提供的编码器 StringEncoder，对字符串数据进行编码 ObjectEncoder，对 Java 对象进行编码 Netty 提供的解码器 StringDecoder, 对字符串数据进行解码 ObjectDecoder，对 Java 对象进行解码 Netty本身的编解码的机制和问题分析 Netty 本身自带的 ObjectDecoder 和 ObjectEncoder 可以用来实现 POJO 对象或各种业务对象的编码和解码，底层使用的仍是 Java 序列化技术 , 而Java 序列化技术本身效率就不高，存在如下问题 无法跨语言 序列化后的体积太大，是二进制编码的 5 倍多。 序列化性能太低 Protobuf简介 Protobuf是用来将对象序列化的，相类似的技术还有Json序列化等等。它是一种高效的结构化数据存储格式，可以用于结构化数据串行化（序列化）。 Protobuf 是以 message 的方式来管理数据的 支持跨平台、跨语言，即[客户端和服务器端可以是不同的语言编写的] （支持目前绝大多数语言，例如 C++、C#、Java、python 等 高性能，高可靠性 使用 protobuf 编译器能自动生成代码，Protobuf 是将类的定义使用.proto 文件进行描述。说明，在idea 中编写 .proto 文件时，会自动提示是否下载 .ptotot 编写插件. 可以让语法高亮。 然后通过 protoc.exe 编译器根据.proto 自动生成.java 文件 Protobuf.exe下载 https://github.com/protocolbuffers/protobuf/releases protobuf中的变量类型和其他语言对照表 Protobuf的使用 定义proto文件 syntax = &quot;proto3&quot;; //版本 option optimize_for = SPEED; //加快解析 option java_outer_classname = &quot;MyDataInfo&quot;; //生成的外部类名，同时也是文件名 message MyMessage { //定义一个枚举类型 enum DateType { StudentType = 0; //在proto3中，要求enum的编号从0开始 WorkerType = 1; } //相当于message的属性，用data_type来标识传的是哪一个枚举类型，1表示属性序号1 DateType data_type = 1; //标识每次枚举类型最多只能出现其中的一个类型，节省空间 oneof dataBody { Student stuent = 2; Worker worker = 3; } } message Student { //会在StudentPojo 外部类生成一个内部类Student，他是真正发送的pojo对象 int32 id = 1; //Student类中有一个属性名字为ID，类型为int32（protobuf类型），1表示序号，不是值 string name = 2; } message Worker { string name = 1; int32 age = 2; } 生成java文件 protoc.exe --java_out=. xxx.proto pom &lt;!--序列化工具protobuf--&gt; &lt;dependency&gt; &lt;groupId&gt;com.google.protobuf&lt;/groupId&gt; &lt;artifactId&gt;protobuf-java&lt;/artifactId&gt; &lt;version&gt;3.15.3&lt;/version&gt; &lt;/dependency&gt; 注意protoc.exe下载的版本是3.15.3，这里protobuf的pom依赖也得是对应的版本，否则protoc.exe生成的java文件放入项目中报错。 客户端 public class Client { //属性 private final String host; private final int port; public Client(String host, int port) { this.port = port; this.host = host; } public void run() throws InterruptedException { NioEventLoopGroup eventExecutors = new NioEventLoopGroup(); try { Bootstrap bootstrap = new Bootstrap() .group(eventExecutors) .channel(NioSocketChannel.class) .handler(new ChannelInitializer&lt;SocketChannel&gt;() { @Override protected void initChannel(SocketChannel ch) throws Exception { ChannelPipeline pipeline = ch.pipeline(); //加入Handler pipeline.addLast(&quot;encoder&quot;,new ProtobufEncoder()); pipeline.addLast(new GroupChatClientHandler()); } }); ChannelFuture channelFuture = bootstrap.connect(host, port).sync(); //得到channel Channel channel = channelFuture.channel(); System.out.println(&quot;--------&quot; + channel.localAddress() + &quot;---------&quot;); //客户端需要输入信息，创建一个扫描器 Scanner scanner = new Scanner(System.in); MyDataInfo.MyMessage message = null; while (scanner.hasNextLine()){ String msg = scanner.nextLine(); if (&quot;1&quot;.equals(msg)){ message = MyDataInfo.MyMessage. newBuilder(). setDataType(MyDataInfo.MyMessage.DateType.StudentType) .setStuent(MyDataInfo.Student .newBuilder() .setId(5) .setName(&quot;王五&quot;) .build()) .build(); }else { message = MyDataInfo.MyMessage. newBuilder(). setDataType(MyDataInfo.MyMessage.DateType.WorkerType). setWorker(MyDataInfo.Worker. newBuilder(). setName(&quot;李四&quot;). setAge(11). build()).build(); } channel.writeAndFlush(message); } } finally { eventExecutors.shutdownGracefully(); } } public static void main(String[] args) throws InterruptedException { new Client(&quot;127.0.0.1&quot;, 7000).run(); } } 服务端 package com.pl.netty.protobuf; public class Server { private int port; public Server(int port) { this.port = port; } public void run(){ //bossGroup 用于接收连接 NioEventLoopGroup bossGroup = new NioEventLoopGroup(1); //workerGroup 用于具体的处理 NioEventLoopGroup workerGroup = new NioEventLoopGroup(); try { // Bootstrap 类是客户端程序的启动引导类，ServerBootstrap 是服务端启动引导类 ServerBootstrap serverBootstrap = new ServerBootstrap(); serverBootstrap.group(bossGroup,workerGroup) //Netty 网络通信的组件，能够用于执行网络 I/O 操作。 选择哪种channel 异步的服务器端 TCP Socket 连接 .channel(NioServerSocketChannel.class) //ChannelOption.SO_BACKLOG 对应 TCP/IP 协议 listen 函数中的 backlog 参数，用来初始化服务器可连接队列大小。 //服务端处理客户端连接请求是顺序处理的，所以同一时间只能处理一个客户端连接。多个客户端来的时候，服务端将不能处理的客户端连接请求放在队列中等待处理，backlog 参数指定了队列的大小。 //Socket参数，服务端接受连接的队列长度，如果队列已满，客户端连接将被拒绝。默认值，Windows为200，其他为128。 .option(ChannelOption.SO_BACKLOG,128) //Netty中的option主要是设置的ServerChannel的一些选项，而childOption主要是设置的ServerChannel的子Channel的选项。 //SO_KEEPALIVE 连接保活，默认值为False。启用该功能时，TCP会主动探测空闲连接的有效性。可以将此功能视为TCP的心跳机制，需要注意的是：默认的心跳间隔是7200s即2小时。Netty默认关闭该功能。 .childOption(ChannelOption.SO_KEEPALIVE, true) //定义worker .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() { @Override protected void initChannel(SocketChannel socketChannel) throws Exception { ChannelPipeline pipeline = socketChannel.pipeline(); pipeline.addLast(&quot;decoder&quot;,new ProtobufDecoder(MyDataInfo.MyMessage.getDefaultInstance())); pipeline.addLast(new GroupChatServerHandler()); } }); System.out.println(&quot;netty 服务端启动&quot;); //Netty 中所有的 IO 操作都是异步的，不能立刻得知消息是否被正确处理。但是可以过一会等它执行完成或者直接注册一个监听，具体的实现就是通过 Future 和 ChannelFutures，他们可以注册一个监听，当操作执行成功或失败时监听会自动触发注册的监听事件 ChannelFuture channelFuture = serverBootstrap.bind(port).sync(); //监听关闭事件 channelFuture.channel().closeFuture().sync(); }catch (InterruptedException e) { e.printStackTrace(); }finally { bossGroup.shutdownGracefully(); workerGroup.shutdownGracefully(); } } public static void main(String[] args) { Server server = new Server(7000); server.run(); } } GroupChatServerHandler package com.pl.netty.protobuf; /** * @Description: TODO * @ClassName GroupchatServerHandler 继承 SimpleChannelInboundHandler 入栈，指定信息泛型为MyDataInfo.MyMessage * @Version V1.0.0 */ public class GroupChatServerHandler extends SimpleChannelInboundHandler&lt;MyDataInfo.MyMessage&gt; { //定义一个Channel组，管理所有的channel //GlobalEventExecutor.INSTANCE 是全局事件执行器，是一个单例 private static ChannelGroup channelGroup = new DefaultChannelGroup(GlobalEventExecutor.INSTANCE); SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); //此方法表示连接建立，一旦建立连接，就第一个被执行 @Override public void handlerAdded(ChannelHandlerContext ctx) throws Exception { Channel channel = ctx.channel(); //该方法会将 channelGroup 中所有 channel 遍历，并发送消息，而不需要我们自己去遍历 channelGroup.writeAndFlush(&quot;[客户端]&quot; + channel.remoteAddress() + sdf.format(new Date()) + &quot;加入聊天\\n&quot;); //将当前的Channel加入到 ChannelGroup channelGroup.add(channel); } //表示 channel 处于活动状态，提示 xxx 上线 @Override public void channelActive(ChannelHandlerContext ctx) throws Exception { System.out.println(ctx.channel().remoteAddress() + &quot; &quot; + sdf.format(new Date()) + &quot;上线了~&quot;); } //表示 channel 处于不活动状态，提示 xxx 离线 @Override public void channelInactive(ChannelHandlerContext ctx) throws Exception { System.out.println(ctx.channel().remoteAddress() + &quot; &quot; + sdf.format(new Date()) + &quot;离线了~&quot;); } @Override protected void channelRead0(ChannelHandlerContext channelHandlerContext, MyDataInfo.MyMessage msg) throws Exception { MyDataInfo.MyMessage.DateType dataType = msg.getDataType(); if (dataType == MyDataInfo.MyMessage.DateType.StudentType){ MyDataInfo.Student student = msg.getStuent(); System.out.println(&quot;学生Id = &quot; + student.getId() + student.getName()); }else if (dataType == MyDataInfo.MyMessage.DateType.WorkerType){ MyDataInfo.Worker worker = msg.getWorker(); System.out.println(&quot;工人：name = &quot; + worker.getName() + worker.getAge()); }else { System.out.println(&quot;输入的类型不正确&quot;); } } } ","link":"https://tianxiawuhao.github.io/A4ZOEAhc9/"},{"title":"minio新版搭建和数据迁移","content":"MinIO 是一个基于Apache License v2.0开源协议的对象存储服务，是一个非常轻量级的服务，非常适合于存储大容量非结构化的数据，例如图片、视频、日志文件、备份数据和容器/虚拟机镜像等，而一个对象文件可以是任意大小，从几kb到最大5T不等 minio搭建 获取最新版本minio docker pull minio/minio 启动minio docker run --name minio -p 9000:9000 -p 9090:9090 -d --restart=always -e &quot;MINIO_ROOT_USER=admin&quot; -e &quot;MINIO_ROOT_PASSWORD=admin123&quot; -v C:\\installation\\minio\\data:/data -v C:\\installation\\minio\\config:/root/.minio minio/minio server /data --console-address &quot;:9000&quot; --address &quot;:9090&quot; 数据迁移 Rclone Rclone 是一个命令行程序，用于管理云存储上的文件。它是云供应商 Web 存储接口的功能丰富的替代品。超过 40 种云存储产品支持 rclone，包括 S3 对象存储、商业和消费者文件存储服务以及标准传输协议。 Rclone 广泛用于 Linux、Windows 和 Mac。第三方开发人员使用 rclone 命令行或 API 创建创新的备份、恢复、GUI 和业务流程解决方案。 将文件备份（和加密）到云存储 从云存储恢复（和解密）文件 将云数据镜像到其他云服务或本地 将数据迁移到云端，或在云存储供应商之间迁移 将多个、加密、缓存或多样化的云存储挂载为磁盘 使用lsf、ljson、size、ncdu分析和说明云存储中保存的数据 将文件系统联合在一起以将多个本地和/或云文件系统呈现为一个 安装Rclone curl https://rclone.org/install.sh | sudo bash 如果网络不好建议先下载到本地，然后上传到服务器 # 下载文件 wget https://downloads.rclone.org/v1.57.0/rclone-v1.57.0-linux-amd64.zip # 解压文件 unzip rclone-v1.57.0-linux-amd64.zip # 给一下权限 chmod 0755 ./rclone-v1.57.0-linux-amd64/rclone # 拷贝到 /usr/bin/ 可以直接使用 rclone命令 cp ./rclone-v1.57.0-linux-amd64p/rclone /usr/bin/ # 删除源文件 rm -rf ./rclone-v1.56.0-linux-amd64.zip windows下载 #官网下载： http://rclone.org/downloads/ #GitHub下载： http://github.com/rclone/rclone 配置 rclone config - 进入交互式配置选项，进行添加、删除、管理网盘等操作。 rclone config file - 显示配置文件的路径，一般配置文件在 ~/.config/rclone/rclone.conf，更换服务器可直接copy该文件。 rclone config show - 显示配置文件信息 cat ~/.config/rclone/rclone.conf # 备注：可以在一个配置文件中配置多份，使用[name]来区分 [oldminio] type = s3 provider = Minio env_auth = false access_key_id = minio secret_access_key = 123 region = cn-east-1 endpoint = http://IP:PORT location_constraint = server_side_encryption = [newminio] type = s3 provider = Minio env_auth = false access_key_id = minio secret_access_key = 123 region = cn-east-1 endpoint = http://IP:PORT location_constraint = server_side_encryption = rclone sync checksum oldminio:model-file-bucket newminio:model-file-bucket 常用功能 # 将阿里云minio的数据迁移到腾讯的cos上 # aliyun 配置文件的名称 # wechat-root-files 存储桶名称 # files 要传输的文件 # -P 显示实时传输进度 # --transfers=N - 并行文件数，默认为 4。在比较小的内存的 VPS 上建议调小这个参数，比如 128M 的小鸡上使用建议设置为 1 ./rclone sync aliyun:wechat-root-files/files tencent:sj-data-testwechat-1307267266/files -P --transfers=200 # 删除文件 ./rclone delete tencent:sj-data-testwechat-1307267266/wechat-root-files -P --transfers=100 常用功能选项 rclone copy - 复制 rclone move - 移动，如果要在移动后删除空源目录，请加上 --delete-empty-src-dirs 参数 rclone sync - 同步：将源目录同步到目标目录，只更改目标目录。 rclone delete - 删除路径下的文件内容。 rclone purge - 删除路径及其所有文件内容。 rclone mkdir - 创建目录。 rclone rmdir - 删除目录。 rclone rmdirs - 删除指定灵境下的空目录。如果加上 --leave-root 参数，则不会删除根目录。 rclone check - 检查源和目的地址数据是否匹配。 rclone ls - 列出指定路径下的所有的文件以及文件大小和路径。 rclone lsl - 比上面多一个显示上传时间。 rclone lsd 列出指定路径下的目录 rclone lsf - 列出指定路径下的目录和文件 # 日志 # rclone 有 4 个级别的日志记录，ERROR，NOTICE，INFO 和 DEBUG。 # 默认情况下，rclone 将生成 ERROR 和 NOTICE 级别消息。 常用参数 -n = --dry-run - 测试运行，用来查看 rclone 在实际运行中会进行哪些操作。 -P = --progress - 显示实时传输进度。 --cache-chunk-size SizeSuffi - 块的大小，默认 5M，理论上是越大上传速度越快，同时占用内存也越多。如果设置得太大，可能会导致进程中断。 --cache-chunk-total-size SizeSuffix - 块可以在本地磁盘上占用的总大小，默认 10G。 --transfers=N - 并行文件数，默认为 4。在比较小的内存的 VPS 上建议调小这个参数，比如 128M 的小鸡上使用建议设置为 1。 --config string - 指定配置文件路径，string 为配置文件路径。比如在使用宝塔面板输入命令操作时可能会遇到找不到配置文件的问题，这时就需要指定配置文件路径。 --s3-provider Other --max-backlog #N # 在 sync/copy/move 时使用，占用 N 倍 KB 内存 --buffer-size=SIZE # 加速 sync/copy/move --bwlimit UP:DOWN # 上传下载限速，b|k|M|G --size-only 使用sync功能配合此参数，表示只有文件大小有变化才会同步文件，可能存在文件大小未变但是文件已经发生变化，不推荐使用； --checksum 通过md5判断文件是否有变化，md5发生变化时才会同步文件；如果source是本地磁盘，这会带来较多的磁盘和CPU消耗；如果source和destination都是对象存储，则推荐使用这个参数； --update --use-server-modtime 通过mtime判断文件是否变化，只有当本地文件的mtime较新时，文件才会上传 --fast-list rclone默认的遍历方式是单独处理每个目录，每个目录调用1次API。使用这个参数将会将所有文件信息加入内存（1000个文件进行1次API调用），1个文件消耗1k内存。 --no-traverse 当destination中文件较多时，使用此参数将会直接查找这个文件，而不是通过文件列表 --max-age 限制文件最大age，用来上传最近的文件 --s3-no-head 默认上传后会通过head检测文件是否已经上传，通过此参数可关闭。 --s3-upload-cutoff 文件大于这个值会使用分片上传，默认值是200M，最大值是5G https:、、rclone.org/s3/ #s3-upload-cutoff --s3-disable-checksum 不计算md5 --s3-upload-concurrency 默认值4，有多少chunck同时上传，如果传少量大文件，提高这个参数可以提升带宽。 --s3-chunk-size 默认值 5M --s3-max-upload-parts 默认值10000 --s3-force-path-style 默认值true --s3-v2-auth 默认值false --s3-list-chunk 默认值1000，每次list返回的key数量 日志 # rclone 有 4 个级别的日志记录，ERROR，NOTICE，INFO 和 DEBUG。 # 默认情况下，rclone 将生成 ERROR 和 NOTICE 级别消息。 # -q rclone 将仅生成 ERROR 消息。 # -v rclone 将生成 ERROR，NOTICE 和 INFO 消息，推荐此项。 # -vv rclone 将生成 ERROR，NOTICE，INFO 和 DEBUG 消息。 # --log-level LEVEL 标志控制日志级别。 # 输出日志到文件 # 使用 --log-file=FILE 选项，rclone 会将 Error，Info 和 Debug 消息以及标准错误重定向到 FILE，这里的 FILE 是你指定的日志文件路径。 # 另一种方法是使用系统的指向命令，比如 # rclone sync -v Onedrive:/DRIVEX Gdrive:/DRIVEX &gt; &quot;~/DRIVEX.log&quot; 2&gt;&amp;1 # 文件过滤 # --exclude 排除文件或目录。比如 --exclude *.bak，排除所有 bak 文件。 # --include 包含文件或目录。比如 --include *.{png,jpg} ，包含所有 png 和 jpg 文件，排除其他文件。 # --delete-excluded 删除排除的文件。需配合过滤参数使用，否则无效。 # 目录过滤 # --exclude .git/ 排除所有目录下的 .git 目录。 # --exclude /.git/ 只排除根目录下的 .git 目录。 ","link":"https://tianxiawuhao.github.io/SznSzjoX7/"},{"title":"spring启动流程","content":"Spring的启动流程可以归纳为三个步骤： 1、初始化Spring容器，注册内置BeanPostProcessor的BeanDefinition到容器中 2、将配置类的BeanDefinition注册到容器中 3、调用refresh()方法刷新容器 因为是基于 java-config 技术分析源码，所以这里的入口是 AnnotationConfigApplicationContext ，如果是使用 xml 分析，那么入口即为 ClassPathXmlApplicationContext ，它们俩的共同特征便是都继承了 AbstractApplicationContext 类，而大名鼎鼎的 refresh()便是在这个类中定义的。我们接着分析 AnnotationConfigApplicationContext 类，源码如下： // 初始化容器 public AnnotationConfigApplicationContext(Class&lt;?&gt;... annotatedClasses) { // 注册 Spring 内置后置处理器的 BeanDefinition 到容器 this(); // 注册配置类 BeanDefinition 到容器 register(annotatedClasses); // 加载或者刷新容器中的Bean refresh(); } 所以整个Spring容器的启动流程可以绘制成如下流程图： 接着我们主要从这三个入口详细分析一下Spring的启动流程： 一、初始化流程： 1、spring容器的初始化时，通过this()调用了无参构造函数，主要做了以下三个事情： （1）实例化BeanFactory【DefaultListableBeanFactory】工厂，用于生成Bean对象 （2）实例化BeanDefinitionReader注解配置读取器，用于对特定注解（如@Service、@Repository）的类进行读取转化成 BeanDefinition 对象，（BeanDefinition 是 Spring 中极其重要的一个概念，它存储了 bean 对象的所有特征信息，如是否单例，是否懒加载，factoryBeanName 等） （3）实例化ClassPathBeanDefinitionScanner路径扫描器，用于对指定的包目录进行扫描查找 bean 对象 2、核心代码剖析： （1）向容器添加内置组件：org.springframework.context.annotation.AnnotationConfigUtils#registerAnnotationConfigProcessors： 根据上图分析，代码运行到这里时候，Spring 容器已经构造完毕，那么就可以为容器添加一些内置组件了，其中最主要的组件便是 ConfigurationClassPostProcessor 和 AutowiredAnnotationBeanPostProcessor ，前者是一个 beanFactory 后置处理器，用来完成 bean 的扫描与注入工作，后者是一个 bean 后置处理器，用来完成 @AutoWired 自动注入。 二、注册SpringConfig配置类到容器中： 1、将SpringConfig注册到容器中：org.springframework.context.annotation.AnnotatedBeanDefinitionReader#doRegisterBean： 这个步骤主要是用来解析用户传入的 Spring 配置类，解析成一个 BeanDefinition 然后注册到容器中，主要源码如下： &lt;T&gt; void doRegisterBean(Class&lt;T&gt; annotatedClass, @Nullable Supplier&lt;T&gt; instanceSupplier, @Nullable String name, @Nullable Class&lt;? extends Annotation&gt;[] qualifiers, BeanDefinitionCustomizer... definitionCustomizers) { // 解析传入的配置类，实际上这个方法既可以解析配置类，也可以解析 Spring bean 对象 AnnotatedGenericBeanDefinition abd = new AnnotatedGenericBeanDefinition(annotatedClass); // 判断是否需要跳过，判断依据是此类上有没有 @Conditional 注解 if (this.conditionEvaluator.shouldSkip(abd.getMetadata())) { return; } abd.setInstanceSupplier(instanceSupplier); ScopeMetadata scopeMetadata = this.scopeMetadataResolver.resolveScopeMetadata(abd); abd.setScope(scopeMetadata.getScopeName()); String beanName = (name != null ? name : this.beanNameGenerator.generateBeanName(abd, this.registry)); // 处理类上的通用注解 AnnotationConfigUtils.processCommonDefinitionAnnotations(abd); if (qualifiers != null) { for (Class&lt;? extends Annotation&gt; qualifier : qualifiers) { if (Primary.class == qualifier) { abd.setPrimary(true); } else if (Lazy.class == qualifier) { abd.setLazyInit(true); } else { abd.addQualifier(new AutowireCandidateQualifier(qualifier)); } } } // 封装成一个 BeanDefinitionHolder for (BeanDefinitionCustomizer customizer : definitionCustomizers) { customizer.customize(abd); } BeanDefinitionHolder definitionHolder = new BeanDefinitionHolder(abd, beanName); // 处理 scopedProxyMode definitionHolder = AnnotationConfigUtils.applyScopedProxyMode(scopeMetadata, definitionHolder, this.registry); // 把 BeanDefinitionHolder 注册到 registry BeanDefinitionReaderUtils.registerBeanDefinition(definitionHolder, this.registry); } 三、refresh()容器刷新流程： refresh()主要用于容器的刷新，Spring 中的每一个容器都会调用 refresh() 方法进行刷新，无论是 Spring 的父子容器，还是 Spring Cloud Feign 中的 feign 隔离容器，每一个容器都会调用这个方法完成初始化。refresh()可划分为12个步骤，其中比较重要的步骤下面会有详细说明。 1、refresh()方法的源码：org.springframework.context.support.AbstractApplicationContext#refresh： public void refresh() throws BeansException, IllegalStateException { synchronized (this.startupShutdownMonitor) { // 1. 刷新前的预处理 prepareRefresh(); // 2. 获取 beanFactory，即前面创建的【DefaultListableBeanFactory】 ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); // 3. 预处理 beanFactory，向容器中添加一些组件 prepareBeanFactory(beanFactory); try { // 4. 子类通过重写这个方法可以在 BeanFactory 创建并与准备完成以后做进一步的设置 postProcessBeanFactory(beanFactory); // 5. 执行 BeanFactoryPostProcessor 方法，beanFactory 后置处理器 invokeBeanFactoryPostProcessors(beanFactory); // 6. 注册 BeanPostProcessors，bean 后置处理器 registerBeanPostProcessors(beanFactory); // 7. 初始化 MessageSource 组件（做国际化功能；消息绑定，消息解析） initMessageSource(); // 8. 初始化事件派发器，在注册监听器时会用到 initApplicationEventMulticaster(); // 9. 留给子容器（子类），子类重写这个方法，在容器刷新的时候可以自定义逻辑，web 场景下会使用 onRefresh(); // 10. 注册监听器，派发之前步骤产生的一些事件（可能没有） registerListeners(); // 11. 初始化所有的非单实例 bean finishBeanFactoryInitialization(beanFactory); // 12. 发布容器刷新完成事件 finishRefresh(); } ... } } 我们总结一下refresh()方法每一步主要的功能 1,prepareRefresh()刷新前的预处理： （1）initPropertySources()：初始化一些属性设置，子类自定义个性化的属性设置方法； （2）getEnvironment().validateRequiredProperties()：检验属性的合法性 （3）earlyApplicationEvents = new LinkedHashSet()：保存容器中的一些早期的事件； 2,obtainFreshBeanFactory()：获取在容器初始化时创建的BeanFactory： （1）refreshBeanFactory()：刷新BeanFactory，设置序列化ID； （2）getBeanFactory()：返回初始化中的GenericApplicationContext创建的BeanFactory对象，即【DefaultListableBeanFactory】类型 3,prepareBeanFactory(beanFactory)：BeanFactory的预处理工作，向容器中添加一些组件： （1）设置BeanFactory的类加载器、设置表达式解析器等等 （2）添加BeanPostProcessor【ApplicationContextAwareProcessor】 （3）设置忽略自动装配的接口：EnvironmentAware、EmbeddedValueResolverAware、ResourceLoaderAware、ApplicationEventPublisherAware、MessageSourceAware、ApplicationContextAware； （4）注册可以解析的自动装配类，即可以在任意组件中通过注解自动注入：BeanFactory、ResourceLoader、ApplicationEventPublisher、ApplicationContext （5）添加BeanPostProcessor【ApplicationListenerDetector】 （6）添加编译时的AspectJ； （7）给BeanFactory中注册的3个组件：environment【ConfigurableEnvironment】、systemProperties【Map&lt;String, Object&gt;】、systemEnvironment【Map&lt;String, Object&gt;】 4,postProcessBeanFactory(beanFactory)：子类重写该方法，可以实现在BeanFactory创建并预处理完成以后做进一步的设置 5,invokeBeanFactoryPostProcessors(beanFactory)：在BeanFactory标准初始化之后执行BeanFactoryPostProcessor的方法，即BeanFactory的后置处理器： （1）先执行BeanDefinitionRegistryPostProcessor： postProcessor.postProcessBeanDefinitionRegistry(registry) ① 获取所有的实现了BeanDefinitionRegistryPostProcessor接口类型的集合 ② 先执行实现了PriorityOrdered优先级接口的BeanDefinitionRegistryPostProcessor ③ 再执行实现了Ordered顺序接口的BeanDefinitionRegistryPostProcessor ④ 最后执行没有实现任何优先级或者是顺序接口的BeanDefinitionRegistryPostProcessors （2）再执行BeanFactoryPostProcessor的方法：postProcessor.postProcessBeanFactory(beanFactory) ① 获取所有的实现了BeanFactoryPostProcessor接口类型的集合 ② 先执行实现了PriorityOrdered优先级接口的BeanFactoryPostProcessor ③ 再执行实现了Ordered顺序接口的BeanFactoryPostProcessor ④ 最后执行没有实现任何优先级或者是顺序接口的BeanFactoryPostProcessor 6,registerBeanPostProcessors(beanFactory)：向容器中注册Bean的后置处理器BeanPostProcessor，它的主要作用是干预Spring初始化bean的流程，从而完成代理、自动注入、循环依赖等功能 （1）获取所有实现了BeanPostProcessor接口类型的集合： （2）先注册实现了PriorityOrdered优先级接口的BeanPostProcessor； （3）再注册实现了Ordered优先级接口的BeanPostProcessor； （4）最后注册没有实现任何优先级接口的BeanPostProcessor； （5）最r终注册MergedBeanDefinitionPostProcessor类型的BeanPostProcessor：beanFactory.addBeanPostProcessor(postProcessor); （6）给容器注册一个ApplicationListenerDetector：用于在Bean创建完成后检查是否是ApplicationListener，如果是，就把Bean放到容器中保存起来：applicationContext.addApplicationListener((ApplicationListener&lt;?&gt;) bean); 此时容器中默认有6个默认的BeanProcessor(无任何代理模式下)：【ApplicationContextAwareProcessor】、【ConfigurationClassPostProcessorsAwareBeanPostProcessor】、【PostProcessorRegistrationDelegate】、【CommonAnnotationBeanPostProcessor】、【AutowiredAnnotationBeanPostProcessor】、【ApplicationListenerDetector】 7,initMessageSource()：初始化MessageSource组件，主要用于做国际化功能，消息绑定与消息解析： （1）看BeanFactory容器中是否有id为messageSource 并且类型是MessageSource的组件：如果有，直接赋值给messageSource；如果没有，则创建一个DelegatingMessageSource； （2）把创建好的MessageSource注册在容器中，以后获取国际化配置文件的值的时候，可以自动注入MessageSource； 8,initApplicationEventMulticaster()：初始化事件派发器，在注册监听器时会用到： （1）看BeanFactory容器中是否存在自定义的ApplicationEventMulticaster：如果有，直接从容器中获取；如果没有，则创建一个SimpleApplicationEventMulticaster （2）将创建的ApplicationEventMulticaster添加到BeanFactory中，以后其他组件就可以直接自动注入 9,onRefresh()：留给子容器、子类重写这个方法，在容器刷新的时候可以自定义逻辑 10,registerListeners()：注册监听器：将容器中所有的ApplicationListener注册到事件派发器中，并派发之前步骤产生的事件： （1）从容器中拿到所有的ApplicationListener （2）将每个监听器添加到事件派发器中：getApplicationEventMulticaster().addApplicationListenerBean(listenerBeanName); （3）派发之前步骤产生的事件applicationEvents：getApplicationEventMulticaster().multicastEvent(earlyEvent); 11,finishBeanFactoryInitialization(beanFactory)：初始化所有剩下的非单实例bean，核心方法是preInstantiateSingletons()，会调用getBean()方法创建对象； （1）获取容器中的所有beanDefinitionName，依次进行初始化和创建对象 （2）获取Bean的定义信息RootBeanDefinition，它表示自己的BeanDefinition和可能存在父类的BeanDefinition合并后的对象 （3）如果Bean满足这三个条件：非抽象的，单实例，非懒加载，则执行单例Bean创建流程： （4）所有Bean都利用getBean()创建完成以后，检查所有的Bean是否为SmartInitializingSingleton接口的，如果是；就执行afterSingletonsInstantiated()； 12,finishRefresh()：发布BeanFactory容器刷新完成事件： （1）initLifecycleProcessor()：初始化和生命周期有关的后置处理器：默认从容器中找是否有lifecycleProcessor的组件【LifecycleProcessor】，如果没有，则创建一个DefaultLifecycleProcessor()加入到容器； （2）getLifecycleProcessor().onRefresh()：拿到前面定义的生命周期处理器（LifecycleProcessor）回调onRefresh()方法 （3）publishEvent(new ContextRefreshedEvent(this))：发布容器刷新完成事件； （4）liveBeansView.registerApplicationContext(this); ","link":"https://tianxiawuhao.github.io/E_iSRsJL1/"},{"title":"gitlab使用runner完成CI","content":"gitlab安装 docker下载镜像 docker pull gitlab/gitlab-ce:latest gitlab容器部署 docker run -d -p 8443:443 -p 80:80 -p 8022:22 --restart always --name gitlab -v C:\\installation\\gitlab\\config:/etc/gitlab -v C:\\installation\\gitlab\\logs:/var/log/gitlab -v C:\\installation\\gitlab\\data:/var/opt/gitlab 修改配置 vi /etc/gitlab/gitlab.rb ------------------------------------------------- # gitlab访问地址，可以写域名。如果端口不写的话默认为80端口 external_url 'http://gitlab.test' # ssh主机ip gitlab_rails['gitlab_ssh_host'] = 'gitlab.test' # ssh连接端口 gitlab_rails['gitlab_shell_ssh_port'] = 8022 # 是否启用 gitlab_rails['smtp_enable'] = true # SMTP服务的地址 gitlab_rails['smtp_address'] = &quot;smtp.qq.com&quot; # 端口 gitlab_rails['smtp_port'] = 465 # 你的QQ邮箱（发送账号） gitlab_rails['smtp_user_name'] = &quot;*******@qq.com&quot; # 授权码 gitlab_rails['smtp_password'] = &quot;********&quot; # 域名 gitlab_rails['smtp_domain'] = &quot;smtp.qq.com&quot; # 登录验证 gitlab_rails['smtp_authentication'] = &quot;login&quot; # 使用了465端口，就需要配置下面三项 gitlab_rails['smtp_enable_starttls_auto'] = true gitlab_rails['smtp_tls'] = true gitlab_rails['smtp_openssl_verify_mode'] = 'none' # 你的QQ邮箱（发送账号） gitlab_rails['gitlab_email_from'] = '********@qq.com' # 修改http和ssh配置 vi /opt/gitlab/embedded/service/gitlab-rails/config/gitlab.yml -------------------------------------------------------------- gitlab: host: gitlab.test port: 80 https: false #修改hosts c:\\windows\\system32\\drivers\\etc 127.0.0.1 gitlab.test gitlab-runner安装 url和token通过gitlab的CI/CD菜单项查看 docker exec -it gitlab-runner gitlab-runner register -n --url http://172.17.0.2/ --registration-token GR1348941smuNaFuN-Cf68ne2X-tP --executor docker --description &quot;Docker Runner&quot; --docker-image=maven:latest --tag-list=&quot;dev-release&quot; 注册runner docker exec -it gitlab-runner gitlab-runner register -n --url http://172.17.0.2/ --registration-token GR1348941smuNaFuN-Cf68ne2X-tP --executor docker --description &quot;Docker Runner&quot; --docker-image=maven:latest --tag-list=&quot;allen-devolop-tag&quot; 项目添加.gitlab-ci.yml文件 # runner使用doker时指定镜像 image: maven:3.6.3-jdk-8 # 定义缓存 # 如果gitlab runner是shell或者docker，此缓存功能没有问题 # 如果是k8s环境，要确保已经设置了分布式文件服务作为缓存 cache: key: minio paths: - .m2/repository/ - minio/target/*.jar # 本次构建的阶段：build package stages: - package - build # 生产jar的job make_jar: tags: - dev-release only: - dev-release image: maven:3.6.3-jdk-8 stage: package script: - echo &quot;=============== 开始编译源码，在target目录生成jar文件 ===============&quot; - mvn clean &amp;&amp; mvn package -Dmaven.test.skip=true - echo &quot;target文件夹&quot; `ls minio/target/` # 生产镜像的job make_image: tags: - dev-release only: - dev-release image: docker:latest stage: build script: - VERSION=`date +%Y%m%d%H%M` - echo &quot;VERSION=$VERSION&quot; &gt; .version - echo $VERSION - echo &quot;从缓存中恢复的target文件夹&quot; `ls minio/target/` - echo &quot;=============== 登录Harbor ===============&quot; - cd minio - docker login -u user -p password https://harbor.com - echo &quot;=============== 打包Docker镜像 ： &quot; harbor.com/test/minio-connect:$VERSION &quot;===============&quot; - docker build -t harbor.com/test/minio-connect:$VERSION . - echo &quot;=============== 推送到镜像仓库 ===============&quot; - docker push harbor.com/test/minio-connect:$VERSION - echo &quot;=============== 登出 ===============&quot; - docker logout - echo &quot;清理掉本次构建的jar文件&quot; - rm -rf target/*.jar ","link":"https://tianxiawuhao.github.io/0SH2PgZRJ/"},{"title":"springboot启动流程","content":"SpringBoot 启动流程 new SpringApplication阶段 一般的springboot的启动类长这个样子。 @SpringBootApplication public class SampleApplication { public static void main(String[] args) { SpringApplication.run(SampleApplication.class, args); } } 可以看到这个main方法里面只有简单的一行code，这一行就是理解springboot启动过程的入口了。点进去看一看它做了什么事情，大致上可以认为分为两个部分，首先是new了一个SpringApplication对象，然后调用了它的run方法。 /** * Static helper that can be used to run a {@link SpringApplication} from the * specified source using default settings. * @param primarySource the primary source to load * @param args the application arguments (usually passed from a Java main method) * @return the running {@link ApplicationContext} */ public static ConfigurableApplicationContext run(Class&lt;?&gt; primarySource, String... args) { return run(new Class&lt;?&gt;[] { primarySource }, args); } /** * Static helper that can be used to run a {@link SpringApplication} from the * specified sources using default settings and user supplied arguments. * @param primarySources the primary sources to load * @param args the application arguments (usually passed from a Java main method) * @return the running {@link ApplicationContext} */ public static ConfigurableApplicationContext run(Class&lt;?&gt;[] primarySources, String[] args) { return new SpringApplication(primarySources).run(args); } new SpringApplication 阶段做了哪些事情 结合code我们看一下在new SpringApplication时，SpringBoot框架做了哪些事情 public SpringApplication(ResourceLoader resourceLoader, Class&lt;?&gt;... primarySources) { this.resourceLoader = resourceLoader; Assert.notNull(primarySources, &quot;PrimarySources must not be null&quot;); //设置primarySources this.primarySources = new LinkedHashSet&lt;&gt;(Arrays.asList(primarySources)); //确定application类型 this.webApplicationType = WebApplicationType.deduceFromClasspath(); //读取配置在spring.factories文件中的ApplicationContextInitializer，需要注意一下，在这个方法里面，springboot会开始读取spring.factories里面的配置并且根据ClassLoader的不同，缓存到内存中。这个之后再展开讲。 setInitializers((Collection) getSpringFactoriesInstances(ApplicationContextInitializer.class)); //读取配置在spring.factories文件中的ApplicationListener。 setListeners((Collection) getSpringFactoriesInstances(ApplicationListener.class)); //确定入口类 this.mainApplicationClass = deduceMainApplicationClass(); } 确定应用程序类型(servlet) 首先就是确定一下我们这个项目的类型 判断方法比较简单粗暴，就是看一下你项目中有没有对应类型的标志性的类。举个例子，如果你有WEBFLUX_INDICATOR_CLASS这个类的话，就认为你的项目类型是REACTIVE。 private static final String WEBMVC_INDICATOR_CLASS = &quot;org.springframework.web.servlet.DispatcherServlet&quot;; private static final String WEBFLUX_INDICATOR_CLASS = &quot;org.springframework.web.reactive.DispatcherHandler&quot;; private static final String JERSEY_INDICATOR_CLASS = &quot;org.glassfish.jersey.servlet.ServletContainer&quot;; private static final String SERVLET_APPLICATION_CONTEXT_CLASS = &quot;org.springframework.web.context.WebApplicationContext&quot;; private static final String REACTIVE_APPLICATION_CONTEXT_CLASS = &quot;org.springframework.boot.web.reactive.context.ReactiveWebApplicationContext&quot;; static WebApplicationType deduceFromClasspath() { if (ClassUtils.isPresent(WEBFLUX_INDICATOR_CLASS, null) &amp;&amp; !ClassUtils.isPresent(WEBMVC_INDICATOR_CLASS, null) &amp;&amp; !ClassUtils.isPresent(JERSEY_INDICATOR_CLASS, null)) { return WebApplicationType.REACTIVE; } for (String className : SERVLET_INDICATOR_CLASSES) { if (!ClassUtils.isPresent(className, null)) { return WebApplicationType.NONE; } } return WebApplicationType.SERVLET; } spi加载spring.factories 然后就是通过SPI机制获取spring.factories的配置 在构造器中，我们可以很明显的看到有两次对于getSpringFactoriesInstances方法的调用，这个方法兜兜转转，最后会调用到SpringFactoriesLoader的loadSpringFactories的方法。在这个方法中，对于相同的classsloader，只会尝试load一次spring.factories文件中的配置。 private static Map&lt;String, List&lt;String&gt;&gt; loadSpringFactories(@Nullable ClassLoader classLoader) { MultiValueMap&lt;String, String&gt; result = cache.get(classLoader); if (result != null) { return result; } try { Enumeration&lt;URL&gt; urls = (classLoader != null ? classLoader.getResources(FACTORIES_RESOURCE_LOCATION) : ClassLoader.getSystemResources(FACTORIES_RESOURCE_LOCATION)); result = new LinkedMultiValueMap&lt;&gt;(); while (urls.hasMoreElements()) { URL url = urls.nextElement(); UrlResource resource = new UrlResource(url); Properties properties = PropertiesLoaderUtils.loadProperties(resource); for (Map.Entry&lt;?, ?&gt; entry : properties.entrySet()) { String factoryTypeName = ((String) entry.getKey()).trim(); for (String factoryImplementationName : StringUtils.commaDelimitedListToStringArray((String) entry.getValue())) { result.add(factoryTypeName, factoryImplementationName.trim()); } } } cache.put(classLoader, result); return result; } catch (IOException ex) { throw new IllegalArgumentException(&quot;Unable to load factories from location [&quot; + FACTORIES_RESOURCE_LOCATION + &quot;]&quot;, ex); } } 加载ApplicationContextInitializer(初始化器)和ApplicationListener(监听器) 从load好的spring.factorie中获取配置好的ApplicationContextInitializer和ApplicationListener 这个就没啥好说了。需要注意的是，这些ApplicationListener是在调用run方法之前就已经加载进来的，所以这些ApplicationListener不仅仅能够接收ApplicationStartedEvent以及之后的ApplicationEvent，之前的比如ApplicationStartingEvent、ApplicationEnvironmentPreparedEvent等等事件类型也都可以接收到。 确定主类 这个的逻辑也比较简单，首先获取当前的方法调用栈（这点还是有疑问为啥不通过Thread.currentThread().getStackTrace()这种看起来更加正式的方法来获取stackTrace）。然后从栈顶一个一个找下去，直到找到一个名字为main的方法，这个方法所对应的类就可以认为是主类了。 private Class&lt;?&gt; deduceMainApplicationClass() { try { StackTraceElement[] stackTrace = new RuntimeException().getStackTrace(); for (StackTraceElement stackTraceElement : stackTrace) { if (&quot;main&quot;.equals(stackTraceElement.getMethodName())) { return Class.forName(stackTraceElement.getClassName()); } } } catch (ClassNotFoundException ex) { // Swallow and continue } return null; } 为了验证我也写个了sample public class Test { public static void main(String[] args) { SampleApplication.main(args); } } @SpringBootApplication public class SampleApplication { public static void main(String[] args) { SpringApplication.run(SampleApplication.class, args); } } 发现调用Test.main方法之后，定位出来的mainclass确实是SampleApplication。 run()方法 run()方法做了哪些事情 还是先看这些稍微加了点注释的源码 public ConfigurableApplicationContext run(String... args) { //起了个timer，用于计时，不用管 StopWatch stopWatch = new StopWatch(); stopWatch.start(); ConfigurableApplicationContext context = null; Collection&lt;SpringBootExceptionReporter&gt; exceptionReporters = new ArrayList&lt;&gt;(); //这个就是配置个系统环境变量 java.awt.headless configureHeadlessProperty(); //获取SpringApplicationRunListeners，这个和之前提到的ApplicationListener是两个不同的接口。两者间的联系在于， //在springboot的默认实现中，有一个特殊的类EventPublishingRunListener会继承SpringApplicationRunListeners， //他会在springboot的特殊阶段发布对应的ApplicationEvent，并且被ApplicationListener接收。 SpringApplicationRunListeners listeners = getRunListeners(args); //进入starting阶段，发布ApplicationStartingEvent事件。 listeners.starting(); try { //包装启动参数 ApplicationArguments applicationArguments = new DefaultApplicationArguments(args); //准备environment，确定propertiesSource。发布ApplicationEnvironmentPreparedEvent。 ConfigurableEnvironment environment = prepareEnvironment(listeners, applicationArguments); //配置环境变量spring.beaninfo.ignore configureIgnoreBeanInfo(environment); Banner printedBanner = printBanner(environment); //创建应用上下文 context = createApplicationContext(); //获取SpringBootExceptionReporter exceptionReporters = getSpringFactoriesInstances(SpringBootExceptionReporter.class, new Class[] { ConfigurableApplicationContext.class }, context); //发布ApplicationContextInitializedEvent事件和ApplicationPreparedEvent事件 prepareContext(context, environment, listeners, applicationArguments, printedBanner); /划重点，进行bean的初始化工作 refreshContext(context); //暂时是个空function。 afterRefresh(context, applicationArguments); //关闭计算器，也就是前面创建的那个timer stopWatch.stop(); if (this.logStartupInfo) { new StartupInfoLogger(this.mainApplicationClass).logStarted(getApplicationLog(), stopWatch); } //发布ApplicationStartedEvent事件 listeners.started(context); //运行ApplicationRunner和CommandLineRunner callRunners(context, applicationArguments); } catch (Throwable ex) { handleRunFailure(context, ex, exceptionReporters, listeners); throw new IllegalStateException(ex); } try { //发布ApplicationReadyEvent事件 listeners.running(context); } catch (Throwable ex) { handleRunFailure(context, ex, exceptionReporters, null); throw new IllegalStateException(ex); } return context; } 可以看到这个方法比较复杂，基本上每一行语句都对应着一个函数。为了更加方便地捋清楚整个flow，我们跟着SpringApplicationRunListener，看一下它的每个方法都在什么时候被trigger，两个方法之间都做了什么事情。 首先看一下SpringApplicationRunListener(s)有哪些方法 SpringApplicationRunListener并没有直接被SpringApplication调用，而是作为一个整体SpringApplicationRunListeners被统一调用，它有starting、environmentPrepared、contextPrepared、contextLoaded、started、running、failed等七个方法。由于这一篇我们只讲SpringBoot的启动流程，因此只讨论前六个方法，看他们的调用时机。 public interface SpringApplicationRunListener { /** * Called immediately when the run method has first started. Can be used for very * early initialization. */ default void starting() { } /** * Called once the environment has been prepared, but before the * {@link ApplicationContext} has been created. * @param environment the environment */ default void environmentPrepared(ConfigurableEnvironment environment) { } /** * Called once the {@link ApplicationContext} has been created and prepared, but * before sources have been loaded. * @param context the application context */ default void contextPrepared(ConfigurableApplicationContext context) { } /** * Called once the application context has been loaded but before it has been * refreshed. * @param context the application context */ default void contextLoaded(ConfigurableApplicationContext context) { } /** * The context has been refreshed and the application has started but * {@link CommandLineRunner CommandLineRunners} and {@link ApplicationRunner * ApplicationRunners} have not been called. * @param context the application context. * @since 2.0.0 */ default void started(ConfigurableApplicationContext context) { } /** * Called immediately before the run method finishes, when the application context has * been refreshed and all {@link CommandLineRunner CommandLineRunners} and * {@link ApplicationRunner ApplicationRunners} have been called. * @param context the application context. * @since 2.0.0 */ default void running(ConfigurableApplicationContext context) { } /** * Called when a failure occurs when running the application. * @param context the application context or {@code null} if a failure occurred before * the context was created * @param exception the failure * @since 2.0.0 */ default void failed(ConfigurableApplicationContext context, Throwable exception) { } } 在starting方法之前 springboot在调用run方法后，基本上第一时间就trigger了listener的starting方式，在这之前，基本上没做啥事。 StopWatch stopWatch = new StopWatch(); stopWatch.start(); ConfigurableApplicationContext context = null; Collection&lt;SpringBootExceptionReporter&gt; exceptionReporters = new ArrayList&lt;&gt;(); configureHeadlessProperty(); SpringApplicationRunListeners listeners = getRunListeners(args); 开启计时器 一个是创建了一个timer（StopWatch）用来之后计算启动耗时，下面这段日志就是它功能的体现。 2021-09-22 16:29:38.241 INFO 8044 --- [ main] com.sample.app.SampleApplication : Started SampleApplication in 32.023 seconds (JVM running for 121.055) headless=true 设置无外接设备启动 一个是调用configureHeadlessProperty方法来设置java.awt.headless的系统环境变量，这个默认值是true。 private boolean headless = true; private void configureHeadlessProperty() { System.setProperty(SYSTEM_PROPERTY_JAVA_AWT_HEADLESS, System.getProperty(SYSTEM_PROPERTY_JAVA_AWT_HEADLESS, Boolean.toString(this.headless))); } 获取并启用监听器 再有就是从之前解析好的spring.factories中读取所有的SpringApplicationRunListeners。 private SpringApplicationRunListeners getRunListeners(String[] args) { Class&lt;?&gt;[] types = new Class&lt;?&gt;[] { SpringApplication.class, String[].class }; return new SpringApplicationRunListeners(logger, getSpringFactoriesInstances(SpringApplicationRunListener.class, types, this, args)); } 在starting方法 EventPublishingRunListener发布ApplicationStartingEvent事件 public void starting() { this.initialMulticaster.multicastEvent(new ApplicationStartingEvent(this.application, this.args)); } 订阅了这个事件的ApplicationListener倒是找到了几个，除了LogbackLoggingSystem进行了一些与log相关的简单操作外，都没做啥事。 LiquibaseServiceLocatorApplicationListener DelegatingApplicationListener LogbackLoggingSystem BackgroundPreinitializer 设置应用程序参数 staring之后，environmentPrepared之前 对应的代码在run方法里对应两行，其中第一行是把系统参数包装一下，使得用起来更加方便。第二行则是进行一些环境的准备工作。 ApplicationArguments applicationArguments = new DefaultApplicationArguments(args); ConfigurableEnvironment environment = prepareEnvironment(listeners, applicationArguments); 准备环境变量 我们重点看prepareEnvironment这个方法中做了哪些工作。 private ConfigurableEnvironment prepareEnvironment(SpringApplicationRunListeners listeners, ApplicationArguments applicationArguments) { // Create and configure the environment ConfigurableEnvironment environment = getOrCreateEnvironment(); configureEnvironment(environment, applicationArguments.getSourceArgs()); ConfigurationPropertySources.attach(environment); listeners.environmentPrepared(environment); bindToSpringApplication(environment); if (!this.isCustomEnvironment) { environment = new EnvironmentConverter(getClassLoader()).convertEnvironmentIfNecessary(environment, deduceEnvironmentClass()); } ConfigurationPropertySources.attach(environment); return environment; } 可以清楚的看到，在调用listeners.environmentPrepared(environment);方法之前，只有三行code，分别做了三件事情。 getOrCreateEnvironment根据应用类型来创建ConfigurableEnvironment private ConfigurableEnvironment getOrCreateEnvironment() { if (this.environment != null) { return this.environment; } switch (this.webApplicationType) { case SERVLET: return new StandardServletEnvironment(); case REACTIVE: return new StandardReactiveWebEnvironment(); default: return new StandardEnvironment(); } } configureEnvironment 来配置propertiesSource和profiles 这里面其实做了三件事情，除了上面说的propertiesSource和profiles之外还有对conversionService的配置，但是我还不清楚conversionService这个东西到底是干啥用的，所以先空着吧。先解释剩下的两个吧。 protected void configureEnvironment(ConfigurableEnvironment environment, String[] args) { if (this.addConversionService) { ConversionService conversionService = ApplicationConversionService.getSharedInstance(); environment.setConversionService((ConfigurableConversionService) conversionService); } configurePropertySources(environment, args); configureProfiles(environment, args); } 其中，configurePropertySources是配置environment中的propertySources属性。propertySources是用于记录系统的配置参数可以从哪里读取，各个propertySource在其中的先后顺序体现了他们彼此的优先级。默认的propertySources包括systemProperties和systemEnvironment两个。在configurePropertySources步骤中根据代码或者参数的不同，可能会额外加上defaultProperties和commandLineArgs两个propertySource。 protected void configurePropertySources(ConfigurableEnvironment environment, String[] args) { MutablePropertySources sources = environment.getPropertySources(); if (this.defaultProperties != null &amp;&amp; !this.defaultProperties.isEmpty()) { sources.addLast(new MapPropertySource(&quot;defaultProperties&quot;, this.defaultProperties)); } if (this.addCommandLineProperties &amp;&amp; args.length &gt; 0) { String name = CommandLinePropertySource.COMMAND_LINE_PROPERTY_SOURCE_NAME; if (sources.contains(name)) { PropertySource&lt;?&gt; source = sources.get(name); CompositePropertySource composite = new CompositePropertySource(name); composite.addPropertySource( new SimpleCommandLinePropertySource(&quot;springApplicationCommandLineArgs&quot;, args)); composite.addPropertySource(source); sources.replace(name, composite); } else { sources.addFirst(new SimpleCommandLinePropertySource(args)); } } } configureProfiles则是试图从现有的propertySources中尝试获取spring.profiles.active参数。需要注意的是，此时的propertySources并不包含与你的配置文件（比如application.properties）相对应的propertySource，所以无论你的配置文件怎么配置，对这个阶段都不造成影响。 protected void configureProfiles(ConfigurableEnvironment environment, String[] args) { Set&lt;String&gt; profiles = new LinkedHashSet&lt;&gt;(this.additionalProfiles); profiles.addAll(Arrays.asList(environment.getActiveProfiles())); environment.setActiveProfiles(StringUtils.toStringArray(profiles)); } ConfigurationPropertySources.attach(environment)将自己置于propertySources的第一个的位置 前面也讲了，各个PropertySource在propertySources中的先后顺序决定了优先级，现在把ConfigurationPropertySources放到第一位，也就意味着它的优先级是最高的。细心的人应该也会注意到ConfigurationPropertySources.attach(environment)这一行前后调用了两次，这个也是为了确保ConfigurationPropertySources优先加载的顺序。 public static void attach(Environment environment) { Assert.isInstanceOf(ConfigurableEnvironment.class, environment); MutablePropertySources sources = ((ConfigurableEnvironment) environment).getPropertySources(); PropertySource&lt;?&gt; attached = sources.get(ATTACHED_PROPERTY_SOURCE_NAME); if (attached != null &amp;&amp; attached.getSource() != sources) { sources.remove(ATTACHED_PROPERTY_SOURCE_NAME); attached = null; } if (attached == null) { sources.addFirst(new ConfigurationPropertySourcesPropertySource(ATTACHED_PROPERTY_SOURCE_NAME, new SpringConfigurationPropertySources(sources))); } } 在environmentPrepared方法 EventPublishingRunListener发布ApplicationEnvironmentPreparedEvent事件 public void starting() { this.initialMulticaster.multicastEvent(new ApplicationStartingEvent(this.application, this.args)); } 订阅了这个事件的ApplicationListener也找到了几个，我们先记录下来。 ConfigFileApplicationListener AnsiOutputApplicationListener LoggingApplicationListener ClasspathLoggingApplicationListener BackgroundPreinitializer DelegatingApplicationListener FileEncodingApplicationListener 其中比较有意思的是ConfigFileApplicationListener，因此我们展开讲一下，剩下的等有空了再说。 在接收到ApplicationEnvironmentPreparedEvent之后，做了这件事 private void onApplicationEnvironmentPreparedEvent(ApplicationEnvironmentPreparedEvent event) { // 加载所有EnvironmentPostProcessor并排序 List&lt;EnvironmentPostProcessor&gt; postProcessors = loadPostProcessors(); postProcessors.add(this); AnnotationAwareOrderComparator.sort(postProcessors); //依次调用EnvironmentPostProcessor的postProcessEnvironment方法 for (EnvironmentPostProcessor postProcessor : postProcessors) { postProcessor.postProcessEnvironment(event.getEnvironment(), event.getSpringApplication()); } } 在这块逻辑中，会调用它自身的postProcessEnvironment方法，从而实现配置文件的加载 @Override public void postProcessEnvironment(ConfigurableEnvironment environment, SpringApplication application) { addPropertySources(environment, application.getResourceLoader()); } /** * Add config file property sources to the specified environment. * @param environment the environment to add source to * @param resourceLoader the resource loader * @see #addPostProcessors(ConfigurableApplicationContext) */ protected void addPropertySources(ConfigurableEnvironment environment, ResourceLoader resourceLoader) { RandomValuePropertySource.addToEnvironment(environment); new Loader(environment, resourceLoader).load(); } 其中，loader的load方法我们也记录一下 void load() { FilteredPropertySource.apply(this.environment, DEFAULT_PROPERTIES, LOAD_FILTERED_PROPERTY, (defaultProperties) -&gt; { this.profiles = new LinkedList&lt;&gt;(); this.processedProfiles = new LinkedList&lt;&gt;(); this.activatedProfiles = false; this.loaded = new LinkedHashMap&lt;&gt;(); initializeProfiles(); while (!this.profiles.isEmpty()) { Profile profile = this.profiles.poll(); if (isDefaultProfile(profile)) { addProfileToEnvironment(profile.getName()); } load(profile, this::getPositiveProfileFilter, addToLoaded(MutablePropertySources::addLast, false)); this.processedProfiles.add(profile); } load(null, this::getNegativeProfileFilter, addToLoaded(MutablePropertySources::addFirst, true)); addLoadedPropertySources(); applyActiveProfiles(defaultProperties); }); } 忽略bean信息 private void configureIgnoreBeanInfo(ConfigurableEnvironment environment) { if (System.getProperty( CachedIntrospectionResults.IGNORE_BEANINFO_PROPERTY_NAME) == null) { Boolean ignore = environment.getProperty(&quot;spring.beaninfo.ignore&quot;, Boolean.class, Boolean.TRUE); System.setProperty(CachedIntrospectionResults.IGNORE_BEANINFO_PROPERTY_NAME, ignore.toString()); } } 打印banner信息 显而易见，这个流程就是用来打印控制台那个很大的spring的banner的，就是下面这个东东 那他在哪里打印的呢？他在 SpringBootBanner.java 里面打印的，这个类实现了Banner 接口， 而且banner信息是直接在代码里面写死的； 有些公司喜欢自定义banner信息，如果想要改成自己喜欢的图标该怎么办呢，其实很简单,只需要在resources目录下添加一个 banner.txt 的文件即可，一定要添加到resources目录下，别加错了 创建应用程序上下文 environmentPrepared之后，contextPrepared之前 这部分在SpringApplication对应的代码主要是这么三行 //初始化 context = createApplicationContext(); exceptionReporters = getSpringFactoriesInstances(SpringBootExceptionReporter.class, new Class[] { ConfigurableApplicationContext.class }, context); prepareContext(context, environment, listeners, applicationArguments, printedBanner); 第一行是根据应用类型，创建ApplicationContext的实例 protected ConfigurableApplicationContext createApplicationContext() { Class&lt;?&gt; contextClass = this.applicationContextClass; if (contextClass == null) { try { switch (this.webApplicationType) { case SERVLET: contextClass = Class.forName(DEFAULT_SERVLET_WEB_CONTEXT_CLASS); break; case REACTIVE: contextClass = Class.forName(DEFAULT_REACTIVE_WEB_CONTEXT_CLASS); break; default: contextClass = Class.forName(DEFAULT_CONTEXT_CLASS); } } catch (ClassNotFoundException ex) { throw new IllegalStateException( &quot;Unable create a default ApplicationContext, please specify an ApplicationContextClass&quot;, ex); } } return (ConfigurableApplicationContext) BeanUtils.instantiateClass(contextClass); } 实例化异常报告器 第二行就是获取SpringBootExceptionReporter的实例。 异常报告器是用来捕捉全局异常使用的，当springboot应用程序在发生异常时，异常报告器会将其捕捉并做相应处理，在spring.factories 文件里配置了默认的异常报告器， 需要注意的是，这个异常报告器只会捕获启动过程抛出的异常，如果是在启动完成后，在用户请求时报错，异常报告器不会捕获请求中出现的异常， 了解原理了，接下来我们自己配置一个异常报告器来玩玩； MyExceptionReporter.java 继承 SpringBootExceptionReporter 接口 package com.spring.application; import org.springframework.boot.SpringBootExceptionReporter; import org.springframework.context.ConfigurableApplicationContext; public class MyExceptionReporter implements SpringBootExceptionReporter { private ConfigurableApplicationContext context; // 必须要有一个有参的构造函数，否则启动会报错 MyExceptionReporter(ConfigurableApplicationContext context) { this.context = context; } @Override public boolean reportException(Throwable failure) { System.out.println(&quot;进入异常报告器&quot;); failure.printStackTrace(); // 返回false会打印详细springboot错误信息，返回true则只打印异常信息 return false; } } 在 spring.factories 文件中注册异常报告器 # Error Reporters 异常报告器 org.springframework.boot.SpringBootExceptionReporter=\\ com.spring.application.MyExceptionReporter 接着我们在application.yml 中 把端口号设置为一个很大的值，这样肯定会报错， server: port: 80828888 启动后，控制台打印如下图 准备上下文环境 第三行所对应的方法比较复杂，和我们这个阶段对应的主要是这么几行。 private void prepareContext(ConfigurableApplicationContext context, ConfigurableEnvironment environment, SpringApplicationRunListeners listeners, ApplicationArguments applicationArguments, Banner printedBanner) { context.setEnvironment(environment); postProcessApplicationContext(context); applyInitializers(context); .... } 其中applyInitializers(context)这行就是对我们在new SpringApplication()时找到的ApplicationContextInitializer进行初始化工作。这部分包含两个特殊的ApplicationContextInitializer，他们会增加对应的BeanFactoryPostProcessor。BeanFactoryPostProcessor这个类比较重要，它可以读取并覆盖应用程序上下文中配置的bean属性。 class SharedMetadataReaderFactoryContextInitializer implements ApplicationContextInitializer&lt;ConfigurableApplicationContext&gt;, Ordered { ... @Override public void initialize(ConfigurableApplicationContext applicationContext) { applicationContext.addBeanFactoryPostProcessor(new CachingMetadataReaderFactoryPostProcessor()); } ... } public class ConfigurationWarningsApplicationContextInitializer implements ApplicationContextInitializer&lt;ConfigurableApplicationContext&gt; { ... @Override public void initialize(ConfigurableApplicationContext context) { context.addBeanFactoryPostProcessor(new ConfigurationWarningsPostProcessor(getChecks())); } ... } 在contextPrepared方法 EventPublishingRunListener发布ApplicationContextInitializedEvent事件 public void contextPrepared(ConfigurableApplicationContext context) { this.initialMulticaster .multicastEvent(new ApplicationContextInitializedEvent(this.application, this.args, context)); } 订阅了这个事件的ApplicationListener只找到个两个，我们先记录下来。都没做啥事 BackgroundPreinitializer DelegatingApplicationListener contextPrepared 之后，contextLoaded之前 这个部分code都被包含在prepareContext这个SpringApplication的私有方法中了。 其中比较重要的load(context, sources.toArray(new Object[0]))这一行。这一行会为之后bean的扫描路径做好准备。 在contextLoaded方法 EventPublishingRunListener将listener注册到创建好的ApplicationContext之中，之后发布ApplicationPreparedEvent事件。 public void contextLoaded(ConfigurableApplicationContext context) { for (ApplicationListener&lt;?&gt; listener : this.application.getListeners()) { if (listener instanceof ApplicationContextAware) { ((ApplicationContextAware) listener).setApplicationContext(context); } context.addApplicationListener(listener); } this.initialMulticaster.multicastEvent(new ApplicationPreparedEvent(this.application, this.args, context)); } 订阅了这个事件的ApplicationListener，我们记录下来。其中比较特殊的是ConfigFileApplicationListener，它注册了一个PropertySourceOrderingPostProcessor的BeanFactoryPostProcessor实例到context中。 CloudFoundryVcapEnvironmentPostProcessor ConfigFileApplicationListener LoggingApplicationListener BackgroundPreinitializer DelegatingApplicationListener 刷新上下文 contextLoaded之后，started之前 这个在run方法中只对应了一行代码refreshContext(context);。它会间接的对ApplicationContext的refresh方法进行调用。由于我们现在debug的是一个web项目，因此走进去对应的ApplicationContext的实现类是ServletWebServerApplicationContext。它的refresh方法可以简单的视为对父类refresh方法的调用，它的onRefresh方法则在父类(AbstractApplicationContext)的方法之上创建了WebServer。 @Override public final void refresh() throws BeansException, IllegalStateException { try { super.refresh(); } catch (RuntimeException ex) { WebServer webServer = this.webServer; if (webServer != null) { webServer.stop(); } throw ex; } } @Override protected void onRefresh() { super.onRefresh(); try { createWebServer(); } catch (Throwable ex) { throw new ApplicationContextException(&quot;Unable to start web server&quot;, ex); } } 我们看一下在父类的refresh方法中做了哪些事情 public void refresh() throws BeansException, IllegalStateException { synchronized (this.startupShutdownMonitor) { //这个步骤会清除缓存，检查是否缺少环境参数，保存前期加载的ApplicationListener实例(从applicationListeners属性到earlyApplicationListeners) // Prepare this context for refreshing. prepareRefresh(); // Tell the subclass to refresh the internal bean factory. ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); //这个步骤通过调用ignoreDependencyInterface，registerResolvableDependency，addBeanPostProcessor等方法完成beanFactory的一些配置。其中addBeanPostProcessor方法会被调用至少两次，传参分别为ApplicationContextAwareProcessor和ApplicationListenerDetector // Prepare the bean factory for use in this context. prepareBeanFactory(beanFactory); //这个步骤会清除缓存，检查是否缺少环境参数，保存前期加载的ApplicationListener实例(从applicationListeners属性到earlyApplicationListeners) // Prepare this context for refreshing. prepareRefresh(); // Tell the subclass to refresh the internal bean factory. ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); //这个步骤通过调用ignoreDependencyInterface，registerResolvableDependency，addBeanPostProcessor等方法完成beanFactory的一些配置。其中addBeanPostProcessor方法会被调用至少两次，传参分别为ApplicationContextAwareProcessor和ApplicationListenerDetector // Prepare the bean factory for use in this context. prepareBeanFactory(beanFactory); try { // Allows post-processing of the bean factory in context subclasses. postProcessBeanFactory(beanFactory); //这个步骤会加载bean的definition // Invoke factory processors registered as beans in the context. invokeBeanFactoryPostProcessors(beanFactory); // Register bean processors that intercept bean creation. registerBeanPostProcessors(beanFactory); // Initialize message source for this context. initMessageSource(); // Initialize event multicaster for this context. initApplicationEventMulticaster(); //Web类型的App将在这个步骤创建WebServer，但是WebServer此时并不会立刻启动 // Initialize other special beans in specific context subclasses. onRefresh(); //注册监听 // Check for listener beans and register them. registerListeners(); //这个时候会根据之前获取的的bean的definition，进行bean的初始化 // Instantiate all remaining (non-lazy-init) singletons. finishBeanFactoryInitialization(beanFactory); //Web类型的App将在这个步骤启动之前创建好的WebServer // Last step: publish corresponding event. finishRefresh(); } catch (BeansException ex) { if (logger.isWarnEnabled()) { logger.warn(&quot;Exception encountered during context initialization - &quot; + &quot;cancelling refresh attempt: &quot; + ex); } // Destroy already created singletons to avoid dangling resources. destroyBeans(); // Reset 'active' flag. cancelRefresh(ex); // Propagate exception to caller. throw ex; } finally { // Reset common introspection caches in Spring's core, since we // might not ever need metadata for singleton beans anymore... resetCommonCaches(); } } } 刷新上下文后置处理 afterRefresh 方法是启动后的一些处理，留给用户扩展使用，目前这个方法里面是空的， /** * Called after the context has been refreshed. * @param context the application context * @param args the application arguments */ protected void afterRefresh(ConfigurableApplicationContext context, ApplicationArguments args) { } 结束计时器 发布上下文准备就绪事件 started之后，running之前 这个部分就比较简单了，主要是invoke了ApplicationRunner和CommandLineRunner private void callRunners(ApplicationContext context, ApplicationArguments args) { List&lt;Object&gt; runners = new ArrayList&lt;&gt;(); runners.addAll(context.getBeansOfType(ApplicationRunner.class).values()); runners.addAll(context.getBeansOfType(CommandLineRunner.class).values()); AnnotationAwareOrderComparator.sort(runners); for (Object runner : new LinkedHashSet&lt;&gt;(runners)) { if (runner instanceof ApplicationRunner) { callRunner((ApplicationRunner) runner, args); } if (runner instanceof CommandLineRunner) { callRunner((CommandLineRunner) runner, args); } } } 这是一个扩展功能，callRunners(context, applicationArguments) 可以在启动完成后执行自定义的run方法；有2中方式可以实现： 实现 ApplicationRunner 接口 实现 CommandLineRunner 接口 接下来我们验证一把，为了方便代码可读性，我把这2种方式都放在同一个类里面 package com.spring.init; import org.springframework.boot.ApplicationArguments; import org.springframework.boot.ApplicationRunner; import org.springframework.boot.CommandLineRunner; import org.springframework.stereotype.Component; /** * 自定义run方法的2种方式 */ @Component public class MyRunner implements ApplicationRunner, CommandLineRunner { @Override public void run(ApplicationArguments args) throws Exception { System.out.println(&quot; 我是自定义的run方法1，实现 ApplicationRunner 接口既可运行&quot; ); } @Override public void run(String... args) throws Exception { System.out.println(&quot; 我是自定义的run方法2，实现 CommandLineRunner 接口既可运行&quot; ); } } 启动springboot后就可以看到控制台打印的信息了 ","link":"https://tianxiawuhao.github.io/Tm9S6oyiS/"},{"title":"树形结构查询","content":"树型层级结构查询 数据sql CREATE TABLE `sys_company` ( `id` int(11) NOT NULL AUTO_INCREMENT COMMENT '主键', `code` varchar(36) NOT NULL COMMENT '标识主键', `company_name` varchar(255) NOT NULL COMMENT '组织名称', `pid` int(10) DEFAULT NULL COMMENT '上级组织id', `pname` varchar(255) DEFAULT NULL COMMENT '上级组织名称', `sort` int(4) DEFAULT '0' COMMENT '排序', `state` int(2) DEFAULT '0' COMMENT '是否可用，0可用，1禁用', `create_by` varchar(50) DEFAULT NULL COMMENT '创建者', `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间', `update_by` varchar(50) DEFAULT NULL COMMENT '修改者', `update_time` timestamp NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, PRIMARY KEY (`id`) USING BTREE, KEY `code_index` (`code`) USING BTREE COMMENT '简单索引' ) ENGINE=InnoDB AUTO_INCREMENT=119 DEFAULT CHARSET=utf8mb4 ROW_FORMAT=DYNAMIC COMMENT='组织详情' DTO @Data @EqualsAndHashCode(callSuper = false) @Accessors(chain = true) @AllArgsConstructor @NoArgsConstructor @Builder public class CompanyDTO { @ApiModelProperty(value = &quot;主键&quot;) private Integer id; @ApiModelProperty(value = &quot;标识主键&quot;) private String code; @ApiModelProperty(value = &quot;组织名称&quot;) private String companyName; @ApiModelProperty(value = &quot;上级组织id&quot;) private Integer pid; @ApiModelProperty(value = &quot;上级组织名称&quot;) private String pname; @ApiModelProperty(value = &quot;排序&quot;) private Integer sort; @ApiModelProperty(value = &quot;是否可用，0可用，1禁用&quot;) private Integer state; @ApiModelProperty(value = &quot;创建者&quot;) private String createBy; @ApiModelProperty(value = &quot;创建时间&quot;) private Date createTime; @ApiModelProperty(value = &quot;修改者&quot;) private String updateBy; @ApiModelProperty(value = &quot;修改时间&quot;) private Date updateTime; @ApiModelProperty(value = &quot;子公司组织&quot;) private List&lt;CompanyDTO&gt; children; @ApiModelProperty(value = &quot;编辑删除操作权限标识 true:有 false:无&quot;) private Boolean modAndDelFlag = false; private CompanyDTO CompanyDTO; } service public List&lt;CompanyDTO&gt; getCompanyList(Integer companyId) { CompanyDTO companyDTO = sysCompanyMapper.queryTopData(companyId); while (companyDTO.getCompanyDTO()!=null){ companyDTO = companyDTO.getCompanyDTO(); } List&lt;CompanyDTO&gt; companyDTO = sysCompanyMapper.querySimpleTopData(companyDTO.getId()); } xml &lt;resultMap id=&quot;BaseResultAllMap&quot; type=&quot;com.haier.dtosplat.entity.DTO.company.CompanyDTO&quot;&gt; &lt;id column=&quot;id&quot; property=&quot;id&quot; /&gt; &lt;result column=&quot;code&quot; property=&quot;code&quot; /&gt; &lt;result column=&quot;company_name&quot; property=&quot;companyName&quot; /&gt; &lt;result column=&quot;pid&quot; property=&quot;pid&quot; /&gt; &lt;result column=&quot;pname&quot; property=&quot;pname&quot; /&gt; &lt;result column=&quot;sort&quot; property=&quot;sort&quot; /&gt; &lt;result column=&quot;state&quot; property=&quot;state&quot; /&gt; &lt;result column=&quot;create_by&quot; property=&quot;createBy&quot; /&gt; &lt;result column=&quot;create_time&quot; property=&quot;createTime&quot; /&gt; &lt;result column=&quot;update_by&quot; property=&quot;updateBy&quot; /&gt; &lt;result column=&quot;update_time&quot; property=&quot;updateTime&quot; /&gt; &lt;association property=&quot;companyDTO&quot; column=&quot;{pid = pid}&quot; select=&quot;querySuper&quot;&gt; &lt;/association&gt; &lt;/resultMap&gt; &lt;resultMap id=&quot;BaseResultMap&quot; type=&quot;com.haier.dtosplat.entity.DTO.company.CompanyDTO&quot;&gt; &lt;id column=&quot;id&quot; property=&quot;id&quot; /&gt; &lt;result column=&quot;code&quot; property=&quot;code&quot; /&gt; &lt;result column=&quot;company_name&quot; property=&quot;companyName&quot; /&gt; &lt;result column=&quot;pid&quot; property=&quot;pid&quot; /&gt; &lt;result column=&quot;pname&quot; property=&quot;pname&quot; /&gt; &lt;result column=&quot;sort&quot; property=&quot;sort&quot; /&gt; &lt;result column=&quot;state&quot; property=&quot;state&quot; /&gt; &lt;result column=&quot;create_by&quot; property=&quot;createBy&quot; /&gt; &lt;result column=&quot;create_time&quot; property=&quot;createTime&quot; /&gt; &lt;result column=&quot;update_by&quot; property=&quot;updateBy&quot; /&gt; &lt;result column=&quot;update_time&quot; property=&quot;updateTime&quot; /&gt; &lt;collection property=&quot;children&quot; ofType=&quot;com.haier.dtosplat.entity.DTO.company.CompanyDTO&quot; column=&quot;{id = id}&quot; select=&quot;querySon&quot;&gt; &lt;/collection&gt; &lt;/resultMap&gt; &lt;select id=&quot;queryTopData&quot; resultMap=&quot;BaseResultAllMap&quot;&gt; SELECT * FROM sys_company WHERE id = #{companyId} &lt;/select&gt; &lt;select id=&quot;querySimpleTopData&quot; resultMap=&quot;BaseResultMap&quot;&gt; SELECT * FROM sys_company WHERE id = #{companyId} &lt;/select&gt; &lt;select id=&quot;querySuper&quot; resultMap=&quot;BaseResultMap&quot;&gt; SELECT * FROM sys_company WHERE id = #{pid} &lt;/select&gt; &lt;select id=&quot;querySon&quot; resultMap=&quot;BaseResultMap&quot;&gt; SELECT * FROM sys_company WHERE pid = #{id} &lt;/select&gt; ","link":"https://tianxiawuhao.github.io/0yWpE16Ha/"},{"title":"Spring使用ThreadLocal存储内存泄露风险","content":"Spring使用ThreadLocal存储内存泄露风险 Spring 中有时候我们需要存储一些和 Request 相关联的变量，例如用户的登陆有关信息等，它的生命周期和 Request 相同。 一个容易想到的实现办法是使用ThreadLocal @Slf4j public class CurrentUserHelper { //存储用户信息 private static final InheritableThreadLocal&lt;SysUser&gt; CURRENT_USER = new InheritableThreadLocal&lt;SysUser&gt;(); public static void setCurrentUser(SysUser currentUserInfo) { CURRENT_USER.set(currentUserInfo); } public static SysUser getCurrentUser() { return (SysUser)CURRENT_USER.get(); } public static void clear() { CURRENT_USER.remove(); } } 使用一个自定义的HandlerInterceptor将有关信息注入进去 @Slf4j @Component public class RequestInterceptor implements HandlerInterceptor { @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { try { CurrentUserHelper.setCurrentUser(retrieveUserInfo(request)); } catch (Exception ex) { log.warn(&quot;读取请求信息失败&quot;, ex); } return true; } @Override public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, @Nullable ModelAndView modelAndView) throws Exception { CurrentUserHelper.clear(); } 通过这样，我们就可以在 Controller 中直接使用这个 context，很方便的获取到有关用户的信息 @Slf4j @RestController class Controller { public Result get() { SysUser user = CurrentUserHelper.get(); // ... } } 这个方法也是很多博客中使用的。然而这个方法却存在着一个很隐蔽的坑： HandlerInterceptor 的 postHandle 并不总是会调用。 当Controller中出现Exception @Slf4j @RestController class Controller { public Result get() { SysUser user = CurrentUserHelper.get(); // ... throw new RuntimeException(); } } 或者在HandlerInterceptor的preHandle中出现Exception @Slf4j @Component public class RequestInterceptor implements HandlerInterceptor { @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { try { CurrentUserHelper.setCurrentUser(retrieveUserInfo(request)); } catch (Exception ex) { log.warn(&quot;读取请求信息失败&quot;, ex); } // ... throw new RuntimeException(); //... return true; } } 这些情况下， postHandle 并不会调用。这就导致了 ThreadLocal 变量不能被清理。 在平常的 Java 环境中，ThreadLocal 变量随着 Thread 本身的销毁，是可以被销毁掉的。但 Spring 由于采用了线程池的设计，响应请求的线程可能会一直常驻，这就导致了变量一直不能被 GC 回收。更糟糕的是，这个没有被正确回收的变量，由于线程池对线程的复用，可能会串到别的 Request 当中，进而直接导致代码逻辑的错误。 为了解决这个问题，我们可以使用 Spring 自带的 RequestContextHolder ，它背后的原理也是 ThreadLocal，不过它总会被更底层的 Servlet 的 Filter 清理掉，因此不存在泄露的问题。 下面是一个使用RequestContextHolder重写的例子 public class SecurityContextHolder { private static final String SECURITY_CONTEXT_ATTRIBUTES = &quot;SECURITY_CONTEXT&quot;; public static void setContext(SysUser context) { RequestContextHolder.currentRequestAttributes().setAttribute( SECURITY_CONTEXT_ATTRIBUTES, context, RequestAttributes.SCOPE_REQUEST); } public static SecurityContext get() { return (SysUser)RequestContextHolder.currentRequestAttributes() .getAttribute(SECURITY_CONTEXT_ATTRIBUTES, RequestAttributes.SCOPE_REQUEST); } } 项目中实现WebMvcConfigurer接口或者WebMvcConfigurationSupport接口实现跨域资源访问配置时，要重写addInterceptors方法 @Configuration public class CorsConfig implements WebMvcConfigurer { @Override public void addCorsMappings(CorsRegistry registry) { //配置允许跨域访问的路径 registry.addMapping(&quot;/**&quot;) .allowedOrigins(&quot;*&quot;) // 允许提交请求的方法，*表示全部允许 .allowedMethods(&quot;GET&quot;, &quot;POST&quot;, &quot;DELETE&quot;, &quot;PUT&quot;) .allowedHeaders(&quot;*&quot;) .exposedHeaders(&quot;Access-Control-Allow-Origin&quot;) //是否允许证书 不再默认开启 .allowCredentials(true) .maxAge(3600); } @Override public void addInterceptors(InterceptorRegistry interceptor) { interceptor.addInterceptor(new RequestInterceptor()).addPathPatterns(&quot;/**&quot;); } } 除了使用 RequestContextHolder 还可以使用 Request Scope 的 Bean，或者使用 ThreadLocalTargetSource ，原理上是类似的。 需要时刻注意 ThreadLocal 相当于线程内部的 static 变量，是一个非常容易产生泄露的点，因此使用 ThreadLocal 应该额外小心。 Threadlocal可能会产生内存泄露的问题及原理 刚遇到一个关于threadlocal的内存泄漏问题，刚好总结一下 比较常用的这里先不提，直接提比较重要的部分 为什么会产生内存泄露？ public void set(T value) { Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); } set方法里面，先调用到当前线程thread，每个线程里都会有一个threadlocals成员变量，指向对应的ThreadLocalMap ，然后以new出来的引用作为key，和给定的value一块保存起来。 当外部引用解除以后，对应的ThreadLocal对象由于被内部ThreadLocalMap 引用，不会GC，可能会导致内存泄露。 JVM解决的办法 static class Entry extends WeakReference&lt;ThreadLocal&lt;?&gt;&gt; { /** The value associated with this ThreadLocal. */ Object value; Entry(ThreadLocal&lt;?&gt; k, Object v) { super(k); value = v; } } 继承了一个软引用，在系统进行gc的时候就可以回收 但是回收以后，key变成null，value也无法被访问到，还是可能存在内存泄露。 因此一旦不用了，必须对里面的keyvalue对remove掉，否则就会有内存泄露；而且在threadlocal源码里面，在每次get或者set的时候会清楚里面key为value的记录 ","link":"https://tianxiawuhao.github.io/Y9SH6oPXI/"},{"title":"工具类","content":"工具类 HttpUtil import cn.hutool.json.JSONObject; import cn.hutool.json.JSONUtil; import dt.logincenter.exception.BusinessException; import dt.logincenter.exception.basic.ResponseCode; import lombok.extern.slf4j.Slf4j; import org.springframework.http.*; import org.springframework.stereotype.Component; import org.springframework.web.client.RestTemplate; import javax.annotation.PostConstruct; import javax.annotation.Resource; import java.util.Iterator; import java.util.Map; @Component @Slf4j public class HttpUtil { @Resource private RestTemplate restTemplate; private static HttpUtil httpUtil; @PostConstruct public void init(){ httpUtil = this; httpUtil.restTemplate = this.restTemplate; } /** * 发送请求Get请求 * * @param url 请求接口 * @param param 请求参数 * @param paramType 请求参数类型 json：false String：true */ public JSONObject sendGet(String url, String param, boolean paramType) throws Exception { JSONObject jsonObject = null; try { //设置请求头 HttpHeaders headers = new HttpHeaders(); headers.setContentType(MediaType.APPLICATION_JSON); ResponseEntity&lt;String&gt; resEntity = null; if (!paramType) { HttpEntity httpEntity = new HttpEntity&lt;&gt;(param, headers); resEntity = restTemplate.exchange(url, HttpMethod.GET, httpEntity, String.class); }else { HttpEntity httpEntity = new HttpEntity&lt;&gt;(headers); resEntity = restTemplate.exchange(url, HttpMethod.GET, httpEntity, String.class, param); } jsonObject = JSONUtil.parseObj(resEntity.getBody()); } catch (Exception e) { if (e.toString().contains(&quot;timed out&quot;)) { throw new BusinessException(ResponseCode.CONNECTION_TIMEOUT); } log.error(&quot;接口调用失败{}&quot;,e); throw new BusinessException(ResponseCode.GETDATA_FAIL); } return jsonObject; } /** * 发送请求Get请求 * * @param url 请求接口 * @param params 请求参数 */ public JSONObject sendGet(String url, Map&lt;Object, Object&gt; params){ StringBuffer stringBuffer = new StringBuffer(url); if (params instanceof Map) { Iterator iterator = ((Map) params).entrySet().iterator(); if (iterator.hasNext()) { stringBuffer.append(&quot;?&quot;); Object element; while (iterator.hasNext()) { element = iterator.next(); Map.Entry&lt;String, Object&gt; entry = (Map.Entry) element; if (entry.getValue() != null) { stringBuffer.append(element).append(&quot;&amp;&quot;); } url = stringBuffer.substring(0, stringBuffer.length() - 1); } } } else { throw new BusinessException(ResponseCode.REQUEST_PARAM_ERROR); } JSONObject jsonObject = null; try { //设置请求头 HttpHeaders headers = new HttpHeaders(); headers.setContentType(MediaType.APPLICATION_JSON); ResponseEntity&lt;String&gt; resEntity = null; HttpEntity httpEntity = new HttpEntity&lt;&gt;(headers); resEntity = restTemplate.exchange(url, HttpMethod.GET, httpEntity, String.class); jsonObject = JSONUtil.parseObj(resEntity.getBody()); } catch (Exception e) { if (e.toString().contains(&quot;timed out&quot;)) { throw new BusinessException(ResponseCode.CONNECTION_TIMEOUT); } log.error(&quot;接口调用失败{}&quot;,e); throw new BusinessException(ResponseCode.GETDATA_FAIL); } return jsonObject; } /** * 发送请求Put请求 * * @param url 请求接口 * @param param 请求参数 */ public JSONObject sendPut(String url, JSONObject param) { JSONObject jsonObject = null; try { //设置请求头 HttpHeaders headers = new HttpHeaders(); headers.setContentType(MediaType.APPLICATION_JSON); HttpEntity httpEntity = new HttpEntity&lt;&gt;(param, headers); ResponseEntity&lt;String&gt; resEntity = restTemplate.exchange(url, HttpMethod.PUT, httpEntity, String.class); jsonObject = JSONUtil.parseObj(resEntity.getBody()); } catch (Exception e) { if (e.toString().contains(&quot;timed out&quot;)) { throw new BusinessException(String.valueOf(ResponseCode.CONNECTION_TIMEOUT)); } log.error(&quot;接口调用失败{}&quot;,e); throw new BusinessException(String.valueOf(ResponseCode.GETDATA_FAIL)); } return jsonObject; } /** * 发送请求Post请求 * * @param url 请求接口 * @param param 请求参数 */ public JSONObject sendPost(String url, Object param) { JSONObject jsonObject = null; try { //设置请求头 HttpHeaders headers = new HttpHeaders(); headers.setContentType(MediaType.APPLICATION_JSON); HttpEntity httpEntity = new HttpEntity&lt;&gt;(param, headers); jsonObject = restTemplate.postForObject(url, httpEntity, JSONObject.class); } catch (Exception e) { if (e.toString().contains(&quot;timed out&quot;)) { throw new BusinessException(String.valueOf(ResponseCode.CONNECTION_TIMEOUT)); } log.error(&quot;接口调用失败{}&quot;,e); throw new BusinessException(String.valueOf(ResponseCode.GETDATA_FAIL)); } return jsonObject; } /** * 发送请求Post请求 * * @param url 请求接口 * @param param 请求参数 */ public JSONObject sendPost(String url, Object param,Map&lt;String,String&gt; header) { JSONObject jsonObject = null; try { //设置请求头 HttpHeaders headers = new HttpHeaders(); headers.setContentType(MediaType.APPLICATION_JSON); header.keySet().forEach(item-&gt;{ headers.add(item,header.get(item)); }); HttpEntity httpEntity = new HttpEntity&lt;&gt;(param, headers); jsonObject = restTemplate.postForObject(url, httpEntity, JSONObject.class); } catch (Exception e) { if (e.toString().contains(&quot;timed out&quot;)) { throw new BusinessException(String.valueOf(ResponseCode.CONNECTION_TIMEOUT)); } log.error(&quot;接口调用失败{}&quot;,e); throw new BusinessException(String.valueOf(ResponseCode.GETDATA_FAIL)); } return jsonObject; } /** * 发送请求Get请求获取流文件 * * @param url 请求接口 */ public InputStream sendGet(String url) { try { String path=ResourceUtils.getURL(&quot;classpath:&quot;).getPath(); File file = new File(path); if (!file.exists()) { file.mkdirs(); } //创造文件 path=path+&quot;temp.excel&quot;; File tempFile = new File(path); if (!tempFile.exists()) { tempFile.createNewFile(); } ResponseEntity&lt;byte[]&gt; rsp = restTemplate.getForEntity(url, byte[].class); System.out.println(&quot;文件下载请求结果状态码：&quot; + rsp.getStatusCode()); // 将下载下来的文件内容保存到本地 Files.write(Paths.get(path.substring(1)), Objects.requireNonNull(rsp.getBody(), &quot;未获取到下载文件&quot;)); return new FileInputStream(path); } catch (Exception e) { if (e.toString().contains(&quot;timed out&quot;)) { throw new BusinessException(ResponseCode.CONNECTION_TIMEOUT); } log.error(&quot;接口调用失败{}&quot;,e); throw new BusinessException(ResponseCode.GETDATA_FAIL); } } /** * 发送请求Get请求获取流文件 * * @param url 请求接口 */ public InputStream sendGet(String url) { try { ResponseEntity&lt;byte[]&gt; entity = restTemplate.getForEntity(url, byte[].class); Objects.requireNonNull(entity.getBody()); String path=ResourceUtils.getURL(&quot;classpath:&quot;).getPath()+&quot;temp.excel&quot;; BufferedOutputStream bos; FileOutputStream fos; File file; File dir = new File(path); if (!dir.exists()) {// 判断文件目录是否存在 dir.mkdirs(); } file = new File(path); fos = new FileOutputStream(file); bos = new BufferedOutputStream(fos); bos.write(Objects.requireNonNull(entity.getBody())); bos.close(); fos.close(); return new FileInputStream(file); } catch (Exception e) { if (e.toString().contains(&quot;timed out&quot;)) { throw new BusinessException(ResponseCode.CONNECTION_TIMEOUT); } log.error(&quot;接口调用失败{}&quot;,e); throw new BusinessException(ResponseCode.GETDATA_FAIL); } } } JavaScriptUtil import org.apache.tomcat.util.buf.StringUtils; import javax.script.*; import java.util.Map; import java.util.Set; /** * java执行javaScript代码的工具类 * * @author wuhao */ public class JavaScriptUtil { /** * 单例的JavaScript解析引擎 */ private static final ScriptEngine javaScriptEngine; static { ScriptEngineManager manager = new ScriptEngineManager(); ScriptEngine scriptEngine = manager.getEngineByName(&quot;js&quot;); if (scriptEngine == null) { throw new RuntimeException(&quot;获取JavaScript解析引擎失败&quot;); } javaScriptEngine = scriptEngine; } /** * 执行一段JavaScript代码 * * @param script JavaScript的代码 * @return JavaScript代码运行结果的值 * @throws ScriptException JavaScript代码运行异常 */ public static Object execute(String script) throws ScriptException { return javaScriptEngine.eval(script); } /** * 运行一个JavaScript代码段,并获取指定变量名的值 * * @param script 代码段 * @param attributeName 已知的变量名 * @return 指定变量名对应的值 * @throws ScriptException JavaScript代码运行异常 */ public static Object executeForAttribute(String script, String attributeName) throws ScriptException { javaScriptEngine.eval(script); return javaScriptEngine.getContext().getAttribute(attributeName); } /** * 获取当前语句运行后第一个有值变量的值 * * @param script 代码段 * @return 第一个有值变量的值 * @throws ScriptException JavaScript代码运行异常 */ public static Object executeForFirstAttribute(String script) throws ScriptException { //这里重新获取一个JavaScript解析引擎是为了避免代码中有其他调用工具类的地方的变量干扰 //重新获取后,这个JavaScript解析引擎只执行了这次传入的代码,不会保存其他地方的变量 //全局的解析器中会保存最大200个变量,JavaScript解析引擎本身最大保存100个变量 ScriptEngineManager manager = new ScriptEngineManager(); ScriptEngine scriptEngine = manager.getEngineByName(&quot;js&quot;); if (scriptEngine == null) { throw new RuntimeException(&quot;获取JavaScript解析引擎失败&quot;); } scriptEngine.eval(script); ScriptContext context = scriptEngine.getContext(); if (context == null) { return null; } Bindings bindings = context.getBindings(ScriptContext.ENGINE_SCOPE); if (bindings == null) { return null; } Set&lt;Map.Entry&lt;String, Object&gt;&gt; entrySet = bindings.entrySet(); if (entrySet.isEmpty()) { return null; } for (Map.Entry&lt;String, Object&gt; entry : entrySet) { if (entry.getValue() != null) { return entry.getValue(); } } return null; } /** * 执行一段JavaScript代码，传入参数，获取返回值 * String script = &quot;Math.pow(x, y)&quot;; * Map&lt;String, Object&gt; params = Maps.newHashMap(); * params.put(&quot;x&quot;, 10); * params.put(&quot;y&quot;, 2); * &lt;p&gt; * String script = ”if (company == 1) { * return true; * } else if (company == 2 &amp;&amp; amt &gt; 100) { * return true; * } else if (company == 3 &amp;&amp; amt &gt; 200) { * return true; * } else { * return false; * }“ * &lt;p&gt; * params.put(&quot;company&quot;, 1); * params.put(&quot;amt&quot;, 100); * * @param script * @param params * @return */ public static String execute(String script, Map&lt;String, Object&gt; params) { try { String paramsName = StringUtils.join(params.keySet(), ','); String executeScript = String.format(&quot;function execute(%s) { %s }&quot;, paramsName, script); javaScriptEngine.eval(executeScript); Invocable jsInvoke = (Invocable) javaScriptEngine; Object execute = jsInvoke.invokeFunction(&quot;execute&quot;, params.values().toArray()); return execute.toString(); } catch (ScriptException | NoSuchMethodException e) { System.out.println(&quot;表达式runtime错误:&quot; + e.getMessage()); } return null; } } MinioUtil import io.minio.*; import io.minio.errors.*; import io.minio.http.Method; import io.minio.messages.Item; import io.minio.messages.Upload; import lombok.extern.slf4j.Slf4j; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.stereotype.Component; import org.springframework.util.StringUtils; import org.springframework.web.multipart.MultipartFile; import javax.crypto.SecretKey; import java.io.IOException; import java.io.InputStream; import java.security.InvalidKeyException; import java.security.NoSuchAlgorithmException; import java.util.ArrayList; import java.util.List; @Component @Slf4j public class MinioUtil { @Autowired private MinioClient minioClient; private static final int DEFAULT_EXPIRY_TIME = 7 * 24 * 3600; /** * 检查存储桶是否存在 * * @param bucketName 存储桶名称 * @return * @throws Exception */ public boolean bucketExists(String bucketName) { try { return minioClient.bucketExists(BucketExistsArgs.builder().bucket(bucketName).build()); } catch (Exception e) { log.error(&quot;mino 服务端连接异常&quot;,e); throw new RuntimeException(&quot;mino 服务端连接异常&quot;); } } /** * 创建存储桶 * * @param bucketName 存储桶名称 * @return * @throws Exception */ public boolean makeBucket(String bucketName) throws Exception { boolean flag = bucketExists(bucketName); if (!flag) { minioClient.makeBucket(MakeBucketArgs.builder().bucket(bucketName).build()); return true; } return false; } /** * 文件上传 * * @param bucketName * @param multipartFile */ public void putObject(String bucketName, MultipartFile multipartFile, String filename, SecretKey secretKey) throws Exception { makeBucket(bucketName); PutObjectOptions putObjectOptions = new PutObjectOptions(multipartFile.getSize(), PutObjectOptions.MIN_MULTIPART_SIZE); putObjectOptions.setContentType(multipartFile.getContentType()); if (null != secretKey) { putObjectOptions.setSse(ServerSideEncryption.withCustomerKey(secretKey)); } minioClient.putObject(bucketName, filename, multipartFile.getInputStream(), putObjectOptions); } /** * 普通文件上传 * @param bucketName * @param \\ */ public void putObject(String bucketName, InputStream inputStream, String filename, SecretKey secretKey,long fileSize) throws Exception { PutObjectOptions putObjectOptions = new PutObjectOptions(fileSize, 0); minioClient.putObject(bucketName, filename, inputStream, putObjectOptions); } /** * 分享。 * * @param bucketName 存储桶名称 * @param objectName 存储桶里的对象名称 * @param expires 失效时间（以秒为单位），默认是7天，不得大于七天 * @return */ public String getPresignedObjectUrl(String bucketName, String objectName, Integer expires) throws Exception { boolean flag = bucketExists(bucketName); if (flag) { if (null == expires) { expires = DEFAULT_EXPIRY_TIME; } if (expires &lt; 1 || expires &gt; DEFAULT_EXPIRY_TIME) { throw new RuntimeException(&quot;expires must be in range of 1 to &quot; + DEFAULT_EXPIRY_TIME); } try { return minioClient.getPresignedObjectUrl(Method.GET, bucketName, objectName, expires, null); } catch (Exception e) { e.printStackTrace(); } } return null; } /** * 获取一个临时可以上传的url * @param bucketName * @param objectName * @return * @throws Exception */ public String presignedPutObject(String bucketName, String objectName) { try { return minioClient.presignedPutObject(bucketName, objectName); } catch (Exception e) { throw new RuntimeException(&quot;获取临时文件上传地址异常&quot;); } } /** * 列出某个存储桶中的所有对象 * * @param bucketName * @param objectNamePrefix 前缀 * @return * @throws Exception */ public List&lt;ComposeSource&gt; listObjectsSource(String bucketName, String objectNamePrefix) throws Exception { List&lt;ComposeSource&gt; sources = new ArrayList&lt;&gt;(); Iterable&lt;Result&lt;Item&gt;&gt; results = minioClient.listObjects(bucketName, objectNamePrefix); for (Result&lt;Item&gt; result : results) { Item item = result.get(); sources.add(ComposeSource.builder().bucket(bucketName).object(objectNamePrefix).build()); } // minioClient.listIncompleteUploads() return sources; } /** * 文件合并 * * @param bucketName * @param objectName * @param sources * @param secretKey * @throws Exception */ public void composeObject(String bucketName, String objectName, List&lt;ComposeSource&gt; sources, SecretKey secretKey) throws Exception { List&lt;ComposeSource&gt; sourceObjectList = new ArrayList&lt;&gt;(sources.size()); for (int i=0;i&lt;sources.size();i++){ sourceObjectList.add( ComposeSource.builder() .bucket(bucketName) .object(sources.get(i).object()+&quot;_&quot; + i) .build() ); } ObjectWriteResponse objectWriteResponse = minioClient.composeObject( ComposeObjectArgs.builder() .bucket(bucketName) .object(objectName) .sources(sourceObjectList) .build()); for (ComposeSource source : sourceObjectList) { minioClient.removeObject(source.bucket(), source.object()); } } /** * 查询已经上传成功的文件 * @param bucketName * @param prefix */ public List&lt;Integer&gt; listIncompleteUploads(String bucketName,String prefix){ List&lt;Integer&gt; rslt=new ArrayList&lt;&gt;(); try { Iterable&lt;Result&lt;Item&gt;&gt; results=minioClient.listObjects( ListObjectsArgs.builder() .bucket(bucketName) .prefix(prefix) .maxKeys(1000) .build()); for (Result&lt;Item&gt; result : results) { String obectName = result.get().objectName(); String[] split = StringUtils.split(obectName, &quot;_&quot;); rslt.add(Integer.parseInt(split[split.length-1])); } }catch (Exception e) { log.error(&quot;调用minio查询已经分片上传信息异常&quot;,e); throw new RuntimeException(&quot;调用minio查询已经分片上传信息异常&quot;); } return rslt; } } ","link":"https://tianxiawuhao.github.io/uZBK7g699/"},{"title":"消息推送的7种方案","content":" 什么是消息推送（push） 推送的场景比较多，比如有人关注我的公众号，这时我就会收到一条推送消息，以此来吸引我点击打开应用。 消息推送(push)通常是指网站的运营工作等人员，通过某种工具对用户当前网页或移动设备APP进行的主动消息推送。 消息推送一般又分为web端消息推送和移动端消息推送。 web端消息推送常见的诸如站内信、未读邮件数量、监控报警数量等，应用的也非常广泛。 在具体实现之前，咱们再来分析一下前边的需求，其实功能很简单，只要触发某个事件（主动分享了资源或者后台主动推送消息），web页面的通知小红点就会实时的+1就可以了。 通常在服务端会有若干张消息推送表，用来记录用户触发不同事件所推送不同类型的消息，前端主动查询（拉）或者被动接收（推）用户所有未读的消息数。 消息推送无非是推（push）和拉（pull）两种形式，下边我们逐个了解下。 短轮询 轮询(polling)应该是实现消息推送方案中最简单的一种，这里我们暂且将轮询分为短轮询和长轮询。 短轮询很好理解，指定的时间间隔，由浏览器向服务器发出HTTP请求，服务器实时返回未读消息数据给客户端，浏览器再做渲染显示。 一个简单的JS定时器就可以搞定，每秒钟请求一次未读消息数接口，返回的数据展示即可。 setInterval(() =&gt; { // 方法请求 messageCount().then((res) =&gt; { if (res.code === 200) { this.messageCount = res.data } }) }, 1000); 效果还是可以的，短轮询实现固然简单，缺点也是显而易见，由于推送数据并不会频繁变更，无论后端此时是否有新的消息产生，客户端都会进行请求，势必会对服务端造成很大压力，浪费带宽和服务器资源。 长轮询 长轮询是对上边短轮询的一种改进版本，在尽可能减少对服务器资源浪费的同时，保证消息的相对实时性。长轮询在中间件中应用的很广泛，比如Nacos和apollo配置中心，消息队列kafka、RocketMQ中都有用到长轮询。 [nocas长轮询](nocas长轮询 | tianxia (tinaxiawuhao.github.io))一文中我详细介绍过Nacos长轮询的实现原理，感兴趣的小伙伴可以瞅瞅。 这次我使用apollo配置中心实现长轮询的方式，应用了一个类DeferredResult，它是在servelet3.0后经过Spring封装提供的一种异步请求机制，直意就是延迟结果。 DeferredResult可以允许容器线程快速释放占用的资源，不阻塞请求线程，以此接受更多的请求提升系统的吞吐量，然后启动异步工作线程处理真正的业务逻辑，处理完成调用DeferredResult.setResult(200)提交响应结果。 下边我们用长轮询来实现消息推送。 因为一个ID可能会被多个长轮询请求监听，所以我采用了guava包提供的Multimap结构存放长轮询，一个key可以对应多个value。一旦监听到key发生变化，对应的所有长轮询都会响应。前端得到非请求超时的状态码，知晓数据变更，主动查询未读消息数接口，更新页面数据。 @Controller @RequestMapping(&quot;/polling&quot;) public class PollingController { // 存放监听某个Id的长轮询集合 // 线程同步结构 public static Multimap&lt;String, DeferredResult&lt;String&gt;&gt; watchRequests = Multimaps.synchronizedMultimap(HashMultimap.create()); /** * 设置监听 */ @GetMapping(path = &quot;watch/{id}&quot;) @ResponseBody public DeferredResult&lt;String&gt; watch(@PathVariable String id) { // 延迟对象设置超时时间 DeferredResult&lt;String&gt; deferredResult = new DeferredResult&lt;&gt;(TIME_OUT); // 异步请求完成时移除 key，防止内存溢出 deferredResult.onCompletion(() -&gt; { watchRequests.remove(id, deferredResult); }); // 注册长轮询请求 watchRequests.put(id, deferredResult); return deferredResult; } /** * 变更数据 */ @GetMapping(path = &quot;publish/{id}&quot;) @ResponseBody public String publish(@PathVariable String id) { // 数据变更 取出监听ID的所有长轮询请求，并一一响应处理 if (watchRequests.containsKey(id)) { Collection&lt;DeferredResult&lt;String&gt;&gt; deferredResults = watchRequests.get(id); for (DeferredResult&lt;String&gt; deferredResult : deferredResults) { deferredResult.setResult(&quot;我更新了&quot; + new Date()); } } return &quot;success&quot;; } 当请求超过设置的超时时间，会抛出AsyncRequestTimeoutException异常，这里直接用@ControllerAdvice全局捕获统一返回即可，前端获取约定好的状态码后再次发起长轮询请求，如此往复调用。 @ControllerAdvice public class AsyncRequestTimeoutHandler { @ResponseStatus(HttpStatus.NOT_MODIFIED) @ResponseBody @ExceptionHandler(AsyncRequestTimeoutException.class) public String asyncRequestTimeoutHandler(AsyncRequestTimeoutException e) { System.out.println(&quot;异步请求超时&quot;); return &quot;304&quot;; } } 我们来测试一下，首先页面发起长轮询请求/polling/watch/10086监听消息更变，请求被挂起，不变更数据直至超时，再次发起了长轮询请求；紧接着手动变更数据/polling/publish/10086，长轮询得到响应，前端处理业务逻辑完成后再次发起请求，如此循环往复。 长轮询相比于短轮询在性能上提升了很多，但依然会产生较多的请求，这是它的一点不完美的地方。 iframe流 iframe流就是在页面中插入一个隐藏的&lt;iframe&gt;标签，通过在src中请求消息数量API接口，由此在服务端和客户端之间创建一条长连接，服务端持续向iframe传输数据。 传输的数据通常是HTML、或是内嵌的javascript脚本，来达到实时更新页面的效果。 这种方式实现简单，前端只要一个&lt;iframe&gt;标签搞定了 &lt;iframe src=&quot;/iframe/message&quot; style=&quot;display:none&quot;&gt;&lt;/iframe&gt; 服务端直接组装html、js脚本数据向response写入就行了 @Controller @RequestMapping(&quot;/iframe&quot;) public class IframeController { @GetMapping(path = &quot;message&quot;) public void message(HttpServletResponse response) throws IOException, InterruptedException { while (true) { response.setHeader(&quot;Pragma&quot;, &quot;no-cache&quot;); response.setDateHeader(&quot;Expires&quot;, 0); response.setHeader(&quot;Cache-Control&quot;, &quot;no-cache,no-store&quot;); response.setStatus(HttpServletResponse.SC_OK); response.getWriter().print(&quot; &lt;script type=\\&quot;text/javascript\\&quot;&gt;\\n&quot; + &quot;parent.document.getElementById('clock').innerHTML = \\&quot;&quot; + count.get() + &quot;\\&quot;;&quot; + &quot;parent.document.getElementById('count').innerHTML = \\&quot;&quot; + count.get() + &quot;\\&quot;;&quot; + &quot;&lt;/script&gt;&quot;); } } } 但我个人不推荐，因为它在浏览器上会显示请求未加载完，图标会不停旋转，简直是强迫症杀手。 SSE (我的方式) 很多人可能不知道，服务端向客户端推送消息，其实除了可以用WebSocket这种耳熟能详的机制外，还有一种服务器发送事件(Server-sent events)，简称SSE。 SSE它是基于HTTP协议的，我们知道一般意义上的HTTP协议是无法做到服务端主动向客户端推送消息的，但SSE是个例外，它变换了一种思路。 SSE在服务器和客户端之间打开一个单向通道，服务端响应的不再是一次性的数据包而是text/event-stream类型的数据流信息，在有数据变更时从服务器流式传输到客户端。 整体的实现思路有点类似于在线视频播放，视频流会连续不断的推送到浏览器，你也可以理解成，客户端在完成一次用时很长（网络不畅）的下载。 SSE与WebSocket作用相似，都可以建立服务端与浏览器之间的通信，实现服务端向客户端推送消息，但还是有些许不同： SSE 是基于HTTP协议的，它们不需要特殊的协议或服务器实现即可工作；WebSocket需单独服务器来处理协议。 SSE 单向通信，只能由服务端向客户端单向通信；webSocket全双工通信，即通信的双方可以同时发送和接受信息。 SSE 实现简单开发成本低，无需引入其他组件；WebSocket传输数据需做二次解析，开发门槛高一些。 SSE 默认支持断线重连；WebSocket则需要自己实现。 SSE 只能传送文本消息，二进制数据需要经过编码后传送；WebSocket默认支持传送二进制数据。 SSE 与 WebSocket 该如何选择？ 技术并没有好坏之分，只有哪个更合适 SSE好像一直不被大家所熟知，一部分原因是出现了WebSockets，这个提供了更丰富的协议来执行双向、全双工通信。对于游戏、即时通信以及需要双向近乎实时更新的场景，拥有双向通道更具吸引力。 但是，在某些情况下，不需要从客户端发送数据。而你只需要一些服务器操作的更新。比如：站内信、未读消息数、状态更新、股票行情、监控数量等场景，SEE不管是从实现的难易和成本上都更加有优势。此外，SSE 具有WebSockets在设计上缺乏的多种功能，例如：自动重新连接、事件ID和发送任意事件的能力。 前端只需进行一次HTTP请求，带上唯一ID，打开事件流，监听服务端推送的事件就可以了 &lt;script&gt; let source = null; let userId = 7777 if (window.EventSource) { // 建立连接 source = new EventSource('http://localhost:7777/sse/sub/'+userId); setMessageInnerHTML(&quot;连接用户=&quot; + userId); /** * 连接一旦建立，就会触发open事件 * 另一种写法：source.onopen = function (event) {} */ source.addEventListener('open', function (e) { setMessageInnerHTML(&quot;建立连接。。。&quot;); }, false); /** * 客户端收到服务器发来的数据 * 另一种写法：source.onmessage = function (event) {} */ source.addEventListener('message', function (e) { setMessageInnerHTML(e.data); }); } else { setMessageInnerHTML(&quot;你的浏览器不支持SSE&quot;); } &lt;/script&gt; 服务端的实现更简单，创建一个SseEmitter对象放入sseEmitterMap进行管理 private static Map&lt;String, SseEmitter&gt; sseEmitterMap = new ConcurrentHashMap&lt;&gt;(); /** * 创建连接 * * @date: 2022/7/12 14:51 */ public static SseEmitter connect(String userId) { try { // 设置超时时间，0表示不过期。默认30秒 SseEmitter sseEmitter = new SseEmitter(0L); // 注册回调 sseEmitter.onCompletion(completionCallBack(userId)); sseEmitter.onError(errorCallBack(userId)); sseEmitter.onTimeout(timeoutCallBack(userId)); sseEmitterMap.put(userId, sseEmitter); count.getAndIncrement(); return sseEmitter; } catch (Exception e) { log.info(&quot;创建新的sse连接异常，当前用户：{}&quot;, userId); } return null; } /** * 给指定用户发送消息 * * @date: 2022/7/12 14:51 */ public static void sendMessage(String userId, String message) { if (sseEmitterMap.containsKey(userId)) { try { sseEmitterMap.get(userId).send(message); } catch (IOException e) { log.error(&quot;用户[{}]推送异常:{}&quot;, userId, e.getMessage()); removeUser(userId); } } } 我们模拟服务端推送消息，看下客户端收到了消息，和我们预期的效果一致。 注意： SSE不支持IE浏览器，对其他主流浏览器兼容性做的还不错。 MQTT 什么是 MQTT协议？ MQTT 全称(Message Queue Telemetry Transport)：一种基于发布/订阅（publish/subscribe）模式的轻量级通讯协议，通过订阅相应的主题来获取消息，是物联网（Internet of Thing）中的一个标准传输协议。 该协议将消息的发布者（publisher）与订阅者（subscriber）进行分离，因此可以在不可靠的网络环境中，为远程连接的设备提供可靠的消息服务，使用方式与传统的MQ有点类似。 TCP协议位于传输层，MQTT 协议位于应用层，MQTT 协议构建于TCP/IP协议上，也就是说只要支持TCP/IP协议栈的地方，都可以使用MQTT协议。 为什么要用 MQTT协议？ MQTT协议为什么在物联网（IOT）中如此受偏爱？而不是其它协议，比如我们更为熟悉的 HTTP协议呢？ 首先HTTP协议它是一种同步协议，客户端请求后需要等待服务器的响应。而在物联网（IOT）环境中，设备会很受制于环境的影响，比如带宽低、网络延迟高、网络通信不稳定等，显然异步消息协议更为适合IOT应用程序。 HTTP是单向的，如果要获取消息客户端必须发起连接，而在物联网（IOT）应用程序中，设备或传感器往往都是客户端，这意味着它们无法被动地接收来自网络的命令。 通常需要将一条命令或者消息，发送到网络上的所有设备上。HTTP要实现这样的功能不但很困难，而且成本极高。 Websocket websocket应该是大家都比较熟悉的一种实现消息推送的方式，上边我们在讲SSE的时候也和websocket进行过比较。 WebSocket是一种在TCP连接上进行全双工通信的协议，建立客户端和服务器之间的通信渠道。浏览器和服务器仅需一次握手，两者之间就直接可以创建持久性的连接，并进行双向数据传输。 springboot整合websocket，先引入websocket相关的工具包，和SSE相比额外的开发成本。 &lt;!-- 引入websocket --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-websocket&lt;/artifactId&gt; &lt;/dependency&gt; 服务端使用@ServerEndpoint注解标注当前类为一个websocket服务器，客户端可以通过ws://localhost:7777/webSocket/10086来连接到WebSocket服务器端。 @Component @Slf4j @ServerEndpoint(&quot;/websocket/{userId}&quot;) public class WebSocketServer { //与某个客户端的连接会话，需要通过它来给客户端发送数据 private Session session; private static final CopyOnWriteArraySet&lt;WebSocketServer&gt; webSockets = new CopyOnWriteArraySet&lt;&gt;(); // 用来存在线连接数 private static final Map&lt;String, Session&gt; sessionPool = new HashMap&lt;String, Session&gt;(); /** * 链接成功调用的方法 */ @OnOpen public void onOpen(Session session, @PathParam(value = &quot;userId&quot;) String userId) { try { this.session = session; webSockets.add(this); sessionPool.put(userId, session); log.info(&quot;websocket消息: 有新的连接，总数为:&quot; + webSockets.size()); } catch (Exception e) { } } /** * 收到客户端消息后调用的方法 */ @OnMessage public void onMessage(String message) { log.info(&quot;websocket消息: 收到客户端消息:&quot; + message); } /** * 此为单点消息 */ public void sendOneMessage(String userId, String message) { Session session = sessionPool.get(userId); if (session != null &amp;&amp; session.isOpen()) { try { log.info(&quot;websocket消: 单点消息:&quot; + message); session.getAsyncRemote().sendText(message); } catch (Exception e) { e.printStackTrace(); } } } } 前端初始化打开WebSocket连接，并监听连接状态，接收服务端数据或向服务端发送数据。 &lt;script&gt; var ws = new WebSocket('ws://localhost:7777/webSocket/10086'); // 获取连接状态 console.log('ws连接状态：' + ws.readyState); //监听是否连接成功 ws.onopen = function () { console.log('ws连接状态：' + ws.readyState); //连接成功则发送一个数据 ws.send('test1'); } // 接听服务器发回的信息并处理展示 ws.onmessage = function (data) { console.log('接收到来自服务器的消息：'); console.log(data); //完成通信后关闭WebSocket连接 ws.close(); } // 监听连接关闭事件 ws.onclose = function () { // 监听整个过程中websocket的状态 console.log('ws连接状态：' + ws.readyState); } // 监听并处理error事件 ws.onerror = function (error) { console.log(error); } function sendMessage() { var content = $(&quot;#message&quot;).val(); $.ajax({ url: '/socket/publish?userId=10086&amp;message=' + content, type: 'GET', data: { &quot;id&quot;: &quot;7777&quot;, &quot;content&quot;: content }, success: function (data) { console.log(data) } }) } &lt;/script&gt; 页面初始化建立websocket连接，之后就可以进行双向通信了，效果还不错 自定义推送 上边我们给我出了6种方案的原理和代码实现，但在实际业务开发过程中，不能盲目的直接拿过来用，还是要结合自身系统业务的特点和实际场景来选择合适的方案。 推送最直接的方式就是使用第三推送平台，毕竟钱能解决的需求都不是问题，无需复杂的开发运维，直接可以使用，省时、省力、省心，像goEasy、极光推送都是很不错的三方服务商。 一般大型公司都有自研的消息推送平台，像我们本次实现的web站内信只是平台上的一个触点而已，短信、邮件、微信公众号、小程序凡是可以触达到用户的渠道都可以接入进来。 消息推送系统内部是相当复杂的，诸如消息内容的维护审核、圈定推送人群、触达过滤拦截（推送的规则频次、时段、数量、黑白名单、关键词等等）、推送失败补偿非常多的模块，技术上涉及到大数据量、高并发的场景也很多。所以我们今天的实现方式在这个庞大的系统面前只是小打小闹。 ","link":"https://tianxiawuhao.github.io/GdjsuDFQV/"},{"title":"MQ 消息丢失、重复、积压问题，如何解决","content":"面试官在面试候选人时，如果发现候选人的简历中写了在项目中使用了 MQ 技术（如 Kafka、RabbitMQ、RocketMQ），基本都会抛出一个问题：在使用 MQ 的时候，怎么确保消息 100% 不丢失？ 这个问题在实际工作中很常见，既能考察候选者对于 MQ 中间件技术的掌握程度，又能很好地区分候选人的能力水平。接下来，我们就从这个问题出发，探讨你应该掌握的基础知识和答题思路，以及延伸的面试考点。 案例背景 以京东系统为例，用户在购买商品时，通常会选择用京豆抵扣一部分的金额，在这个过程中，交易服务和京豆服务通过 MQ 消息队列进行通信。在下单时，交易服务发送“扣减账户 X 100 个京豆”的消息给 MQ 消息队列，而京豆服务则在消费端消费这条命令，实现真正的扣减操作。 那在这个过程中你会遇到什么问题呢？ 案例分析 要知道，在互联网面试中，引入 MQ 消息中间件最直接的目的是：做系统解耦合流量控制，追其根源还是为了解决互联网系统的高可用和高性能问题。 系统解耦：用 MQ 消息队列，可以隔离系统上下游环境变化带来的不稳定因素，比如京豆服务的系统需求无论如何变化，交易服务不用做任何改变，即使当京豆服务出现故障，主交易流程也可以将京豆服务降级，实现交易服务和京豆服务的解耦，做到了系统的高可用。 流量控制：遇到秒杀等流量突增的场景，通过 MQ 还可以实现流量的“削峰填谷”的作用，可以根据下游的处理能力自动调节流量。 不过引入 MQ 虽然实现了系统解耦合流量控制，也会带来其他问题。 引入 MQ 消息中间件实现系统解耦，会影响系统之间数据传输的一致性。 在分布式系统中，如果两个节点之间存在数据同步，就会带来数据一致性的问题。同理，在这一讲你要解决的就是：消息生产端和消息消费端的消息数据一致性问题（也就是如何确保消息不丢失）。 而引入 MQ 消息中间件解决流量控制， 会使消费端处理能力不足从而导致消息积压，这也是你要解决的问题。 所以你能发现，问题与问题之间往往是环环相扣的，面试官会借机考察你解决问题思路的连贯性和知识体系的掌握程度。 那面对“在使用 MQ 消息队列时，如何确保消息不丢失”这个问题时，你要怎么回答呢？首先，你要分析其中有几个考点，比如： 如何知道有消息丢失？ 哪些环节可能丢消息？ 如何确保消息不丢失？ 候选人在回答时，要先让面试官知道你的分析思路，然后再提供解决方案：网络中的数据传输不可靠，想要解决如何不丢消息的问题，首先要知道哪些环节可能丢消息，以及我们如何知道消息是否丢失了，最后才是解决方案（而不是上来就直接说自己的解决方案）。 就好比“架构设计”“架构”体现了架构师的思考过程，而“设计”才是最后的解决方案，两者缺一不可。 案例解答 我们首先来看消息丢失的环节，一条消息从生产到消费完成这个过程，可以划分三个阶段，分别为消息生产阶段，消息存储阶段和消息消费阶段。 消息生产阶段： 从消息被生产出来，然后提交给 MQ 的过程中，只要能正常收到 MQ Broker 的 ack 确认响应，就表示发送成功，所以只要处理好返回值和异常，这个阶段是不会出现消息丢失的。 消息存储阶段： 这个阶段一般会直接交给 MQ 消息中间件来保证，但是你要了解它的原理，比如 Broker 会做副本，保证一条消息至少同步两个节点再返回 ack。 消息消费阶段： 消费端从 Broker 上拉取消息，只要消费端在收到消息后，不立即发送消费确认给 Broker，而是等到执行完业务逻辑后，再发送消费确认，也能保证消息的不丢失。 方案看似万无一失，每个阶段都能保证消息的不丢失，但在分布式系统中，故障不可避免，作为消息生产端，你并不能保证 MQ 是不是弄丢了你的消息，消费者是否消费了你的消息，所以，本着 Design for Failure 的设计原则，你还是需要一种机制，来 Check 消息是否丢失了。 紧接着，你还可以向面试官阐述怎么进行消息检测？ 总体方案解决思路为：在消息生产端，给每个发出的消息都指定一个全局唯一 ID，或者附加一个连续递增的版本号，然后在消费端做对应的版本校验。 具体怎么落地实现呢？你可以利用拦截器机制。 在生产端发送消息之前，通过拦截器将消息版本号注入消息中（版本号可以采用连续递增的 ID 生成，也可以通过分布式全局唯一 ID生成）。然后在消费端收到消息后，再通过拦截器检测版本号的连续性或消费状态，这样实现的好处是消息检测的代码不会侵入到业务代码中，可以通过单独的任务来定位丢失的消息，做进一步的排查。 这里需要你注意：如果同时存在多个消息生产端和消息消费端，通过版本号递增的方式就很难实现了，因为不能保证版本号的唯一性，此时只能通过全局唯一 ID 的方案来进行消息检测，具体的实现原理和版本号递增的方式一致。 现在，你已经知道了哪些环节（消息存储阶段、消息消费阶段）可能会出问题，并有了如何检测消息丢失的方案，然后就要给出解决防止消息丢失的设计方案。 回答完“如何确保消息不会丢失？” 之后，面试官通常会追问“怎么解决消息被重复消费的问题？ ” 比如：在消息消费的过程中，如果出现失败的情况，通过补偿的机制发送方会执行重试，重试的过程就有可能产生重复的消息，那么如何解决这个问题？ 这个问题其实可以换一种说法，就是如何解决消费端幂等性问题（幂等性，就是一条命令，任意多次执行所产生的影响均与一次执行的影响相同），只要消费端具备了幂等性，那么重复消费消息的问题也就解决了。 我们还是来看扣减京豆的例子，将账户 X 的金豆个数扣减 100 个，在这个例子中，我们可以通过改造业务逻辑，让它具备幂等性。 最简单的实现方案，就是在数据库中建一张消息日志表， 这个表有两个字段：消息 ID 和消息执行状态。这样，我们消费消息的逻辑可以变为：在消息日志表中增加一条消息记录，然后再根据消息记录，异步操作更新用户京豆余额。 因为我们每次都会在插入之前检查是否消息已存在，所以就不会出现一条消息被执行多次的情况，这样就实现了一个幂等的操作。当然，基于这个思路，不仅可以使用关系型数据库，也可以通过 Redis 来代替数据库实现唯一约束的方案。 在这里我多说一句，想要解决“消息丢失”和“消息重复消费”的问题，有一个前提条件就是要实现一个全局唯一 ID 生成的技术方案。这也是面试官喜欢考察的问题，你也要掌握。 在分布式系统中，全局唯一 ID 生成的实现方法有数据库自增主键、UUID、Redis，Twitter-Snowflake 算法，我总结了几种方案的特点，你可以参考下。 我提醒你注意，无论哪种方法，如果你想同时满足简单、高可用和高性能，就要有取舍，所以你要站在实际的业务中，说明你的选型所考虑的平衡点是什么。 我个人在业务中比较倾向于选择 Snowflake 算法，在项目中也进行了一定的改造，主要是让算法中的 ID 生成规则更加符合业务特点，以及优化诸如时钟回拨等问题。 当然，除了“怎么解决消息被重复消费的问题？”之外，面试官还会问到你“消息积压”。 原因在于消息积压反映的是性能问题，解决消息积压问题，可以说明候选者有能力处理高并发场景下的消费能力问题。 你在解答这个问题时，依旧要传递给面试官一个这样的思考过程： 如果出现积压，那一定是性能问题，想要解决消息从生产到消费上的性能问题，就首先要知道哪些环节可能出现消息积压，然后在考虑如何解决。 因为消息发送之后才会出现积压的问题，所以和消息生产端没有关系，又因为绝大部分的消息队列单节点都能达到每秒钟几万的处理能力，相对于业务逻辑来说，性能不会出现在中间件的消息存储上面。毫无疑问，出问题的肯定是消息消费阶段，那么从消费端入手，如何回答呢？ 如果是线上突发问题，要临时扩容，增加消费端的数量，与此同时，降级一些非核心的业务。通过扩容和降级承担流量，这是为了表明你对应急问题的处理能力。 其次，才是排查解决异常问题，如通过监控，日志等手段分析是否消费端的业务逻辑代码出现了问题，优化消费端的业务处理逻辑。 最后，如果是消费端的处理能力不足，可以通过水平扩容来提供消费端的并发处理能力，但这里有一个考点需要特别注意， 那就是在扩容消费者的实例数的同时，必须同步扩容主题 Topic 的分区数量，确保消费者的实例数和分区数相等。如果消费者的实例数超过了分区数，由于分区是单线程消费，所以这样的扩容就没有效果。 比如在 Kafka 中，一个 Topic 可以配置多个 Partition（分区），数据会被写入到多个分区中，但在消费的时候，Kafka 约定一个分区只能被一个消费者消费，Topic 的分区数量决定了消费的能力，所以，可以通过增加分区来提高消费者的处理能力。 总结 至此，我们讲解了 MQ 消息队列的热门问题的解决方案，无论是初中级还是高级研发工程师，本篇文章的内容都是你需要掌握的，你都可以从这几点出发，与面试官进行友好的交流。我来总结一下今天的重点内容。 如何确保消息不会丢失？ 你要知道一条消息从发送到消费的每个阶段，是否存在丢消息，以及如何监控消息是否丢失，最后才是如何解决问题，方案可以基于“ MQ 的可靠消息投递 ”的方式。 如何保证消息不被重复消费？ 在进行消息补偿的时候，一定会存在重复消息的情况，那么如何实现消费端的幂等性就这道题的考点。 如何处理消息积压问题？ 这道题的考点就是如何通过 MQ 实现真正的高性能，回答的思路是，本着解决线上异常为最高优先级，然后通过监控和日志进行排查并优化业务逻辑，最后是扩容消费端和分片的数量。 在回答问题的时候，你需要特别注意的是，让面试官了解到你的思维过程，这种解决问题的能力是面试官更为看中的，比你直接回答一道面试题更有价值。 另外，如果你应聘的部门是基础架构部，那么除了要掌握本讲中的常见问题的主线知识以外，还要掌握消息中间件的其他知识体系，如： 如何选型消息中间件？ 消息中间件中的队列模型与发布订阅模型的区别？ 为什么消息队列能实现高吞吐？ 序列化、传输协议，以及内存管理等问题 ","link":"https://tianxiawuhao.github.io/-GeRQ4Ed-/"},{"title":"消息服务：MQ使用场景与选型对比","content":"MQ的使用场景，引入MQ后的注意事项以及MQ的选型对比。 MQ的使用场景 MQ的英文全称是Message Queue，翻译成中文就是消息队列，队列实现了先进先出（FIFO）的消息模型。通过消息队列，我们可以实现多个进程之间的通信，例如，可以实现多个微服务之间的消息通信。MQ的最简模型就是生产者生产消息，将消息发送到MQ，消息消费者订阅MQ，消费消息。 MQ的使用场景通常包含：异步解耦、流量削峰。 异步解耦 关于异步的场景，我们这里举一个用户下单成功后，向用户发送通知消息，为用户增加积分和优惠券的场景。 同步耦合场景分析 如果是同步调用的场景，则具体业务为：当用户提交订单成功后，订单系统会调用通知系统为用户发送消息通知，告知用户下单成功，订单系统调用积分系统为用户增加积分，订单系统调用优惠券系统为用户增加优惠券。整个调用流程如下所示。 通过上图的分析，可以看到，用户调用订单系统下单时，总共会经过8个步骤。并且每个步骤都是紧密耦合在一起串行执行，如下所示。 此时，订单系统、通知系统、积分系统和优惠券系统是紧密耦合在一起的，订单系统下单、通知系统发通知、积分系统发积分和优惠券系统发优惠券，四个任务全部完成后，才会给用户返回提交订单的结果信息。 用户提交订单花费的总时间为调用订单系统下单的时间+订单系统调用通知系统发送通知的时间+订单系统调用积分系统发放积分的时间+订单系统调用优惠券系统发放优惠券的时间。 注意：这里为了更好的说明系统之间串行执行的问题，忽略了网络的延迟时间。 这种串行化的系统执行方式，在高并发、大流量场景下是不可取的。另外，如果其中一个系统异常或者宕机，势必会影响到订单系统的可用性。在系统维护上，只要任意一个系统的接口发生变动，订单系统的逻辑也要跟着发生变动。 异步解耦场景分析 既然在高并发、大流量场景下使用订单系统直接串行调用通知系统、积分系统和优惠券系统的方式不可取。那我们是否能够使用异步解耦的方式呢。 其实，在用户提交订单的场景中，用户最关心的是自己的订单是否提交成功，由于下单时，订单系统会直接返回是否下单成功的提示。 对于通知、积分和优惠券可以以异步的方式延后一小段时间执行。并且通知系统、积分系统和优惠券系统之间不存在必然的业务关联逻辑，它们之间可以以并行的方式执行。 所以，可以使用MQ将订单系统与通知系统、积分系统和优惠券系统进行解耦，用户调用订单系统的接口下单时，订单系统向数据库写入订单数据后，向MQ写入消息，就可以直接返回给用户下单成功的提示，此时通知系统、积分系统和优惠券系统都订阅MQ中的消息，收到消息后各自执行自身的业务逻辑即可。 当引入MQ进行异步解耦之后，用户调用订单系统的接口下单，订单系统执行完业务逻辑将订单数据入口，会向MQ发送一条消息，随后便直接返回用户下单成功的提示。通知系统、积分系统和优惠券系统会同时订阅MQ中的消息，当收到消息时，它们各自会执行自身的业务逻辑，并且它们是以并行的方式执行各自的业务逻辑。 从执行的时间线上可以看出，当引入MQ进行异步解耦之后，通知系统、积分系统、优惠券系统和订单系统回复响应都是并行执行的，大大提高系统的执行性能。 并且解耦后，任意一个系统异常或者宕机，都不会影响到订单系统的可用性。只要订单系统与其他系统提前约定好发送的消息格式和消息内容，后续任意一个系统的业务逻辑变动，几乎都不会影响到订单系统的逻辑。 流量削峰 MQ在高并发、大流量的场景下可以用作削峰填谷的利器，例如，12306的春运抢票场景、高并发秒杀场景、双十一和618的大促场景等。 在高并发、大流量业务场景下，瞬间会有大量用户的请求涌入系统，如果不对这些流量做处理的话，直接让这些流量进入下游系统，则很可能由于下游系统无法支撑如此高的并发而导致系统崩溃或宕机。为了解决这些问题，可以引入MQ进行流量的削峰填谷。 将流量发送到MQ中后，下游系统根据自身的处理能力进行消费即可。保证了下游系统的高可用性。 引入MQ后的注意事项 引入MQ最大的优点就是异步解耦和流量削峰，但是引入MQ后也有很多需要注意的事项和问题，主要包括：系统的整体可用性降低、系统的复杂度变高、引入了消息一致性的问题。 系统的整体可用性降低 在对一个系统进行架构设计时，引入的外部依赖越多，系统的稳定性和可用性就会降低。系统中引入了MQ，部分业务就会出现强依赖MQ的现象，此时，如果MQ宕机，则部分业务就会变得不可用。所以，引入MQ时，我们就要考虑如何实现MQ的高可用。 系统的复杂度变高 引入MQ后，会使之前的同步接口调用变成通过MQ的异步调用，在实际的开发过程中，异步调用会比同步调用复杂的多。并且异步调用出现问题后，重现问题，定位问题和解决问题都会比同步调用复杂的多。 并且引入MQ后，还要考虑如何保证消息的顺序等问题。 消息一致性问题 引入MQ后，不得不考虑的一个问题就是消息的一致性问题。这期间就要考虑如何保证消息不丢失，消息幂等和消息数据处理的幂等性问题。 MQ选型对比 目前，在行业内使用的比较多的MQ包含RabbitMQ、Kafka和RocketMQ。这里，我将三者的对比简单整理了个表格，如下所示。 消息中间件(MQ) 优点 缺点 使用场景 RabbitMQ 功能全面、消息的可靠性比较高 吞吐量低，消息大量积累会影响性能，使用的开发语言是erlang，不好定制功能。 规模不大的场景 Kafka 吞吐量最高，性能最好，集群模式下高可用 功能上比较单一，会丢失部分数据 日志分析，大数据场景 RocketMQ 吞吐量高，性能高，可用性高，功能全面。使用Java语言开发，容易定制功能。 开源版不如阿里云上版，文档比较简单。 几乎支持所有场景，包含大数据场景和业务场景。 ","link":"https://tianxiawuhao.github.io/8FPjAXq14/"},{"title":"搭建并整合Seata","content":"搭建并整合Seata 接下来，我们就正式在项目中整合Seata来实现分布式事务。这里，我们主要整合Seata的AT模式。 搭建Seata基础环境 （1）到https://github.com/seata/seata/releases/tag/v1.4.2链接下载Seata的安装包和源码，这里，下载的是1.4.2版本，如下所示。 这里我下载的都是zip压缩文件。 （2）进入Nacos，选择的命名空间，如下所示。 点击新建命名空间，并填写Seata相关的信息，如下所示。 可以看到，这里我填写的信息如下所示。 命名空间ID：seata_namespace_001，如果不填的话Nacos会自动生成命名空间的ID。 命名空间名：seata。 描述：seata的命名空间。 「这里，需要记录下命名空间的ID：seata_namespace_001，在后面的配置中会使用到。」 点击确定后如下所示。 可以看到，这里为Seata在Nacos中创建了命名空间。 （3）解压Seata安装文件，进入解压后的seata/seata-server-1.4.2/conf目录，修改registry.conf注册文件，修改后的部分文件内容如下所示。 registry { # file 、nacos 、eureka、redis、zk、consul、etcd3、sofa type = &quot;nacos&quot; nacos { application = &quot;seata-server&quot; serverAddr = &quot;127.0.0.1:8848&quot; group = &quot;SEATA_GROUP&quot; namespace = &quot;seata_namespace_001&quot; cluster = &quot;default&quot; username = &quot;nacos&quot; password = &quot;nacos&quot; } } config { # file、nacos 、apollo、zk、consul、etcd3 type = &quot;nacos&quot; nacos { serverAddr = &quot;127.0.0.1:8848&quot; namespace = &quot;seata_namespace_001&quot; group = &quot;SEATA_GROUP&quot; username = &quot;nacos&quot; password = &quot;nacos&quot; dataId = &quot;seataServer.properties&quot; } } 其中，namespace的值就是在Nacos中配置的Seata的命名空间ID：seata_namespace_001。 「注意：这里只列出了修改的部分内容，完整的registry.conf文件可以到项目的doc/nacos/config/chapter25目录下获取。」 （4）修改Seata安装文件的seata/seata-server-1.4.2/conf目录下的file.conf文件，修改后的部分配置如下所示。 store { mode = &quot;db&quot; publicKey = &quot;&quot; db { datasource = &quot;druid&quot; dbType = &quot;mysql&quot; driverClassName = &quot;com.mysql.jdbc.Driver&quot; url = &quot;jdbc:mysql://127.0.0.1:3306/seata?useSSL=false&amp;useUnicode=true&amp;characterEncoding=utf-8&amp;allowMultiQueries=true&amp;serverTimezone=Asia/Shanghai&quot; user = &quot;root&quot; password = &quot;root&quot; minConn = 5 maxConn = 100 globalTable = &quot;global_table&quot; branchTable = &quot;branch_table&quot; lockTable = &quot;lock_table&quot; queryLimit = 100 maxWait = 5000 } } 「注意：这里只列出了修改的部分内容，完整的file.conf文件可以到项目的doc/nacos/config/chapter25目录下获取。」 （5）在下载的Seata源码的seata-1.4.2/script/config-center目录下找到config.txt文件，如下所示。 将其复制到Seata安装包解压的根目录下，如下所示。 接下来，修改Seata安装包解压的根目录下的config.txt文件，这里还是只列出修改的部分，如下所示。 service.vgroupMapping.server-order-tx_group=default service.vgroupMapping.server-product-tx_group=default store.mode=db store.publicKey=&quot;&quot; store.db.datasource=druid store.db.dbType=mysql store.db.driverClassName=com.mysql.jdbc.Driver store.db.url=jdbc:mysql://127.0.0.1:3306/seata?useSSL=false&amp;useUnicode=true&amp;characterEncoding=utf-8&amp;allowMultiQueries=true&amp;serverTimezone=Asia/Shanghai store.db.user=root store.db.password=root store.redis.sentinel.masterName=&quot;&quot; store.redis.sentinel.sentinelHosts=&quot;&quot; store.redis.password=&quot;&quot; 「注意：在config.txt中，部分配置的等号“=”后面为空，需要在等号“=“后面添加空字符串&quot;&quot;。同样的，小伙伴们可以到项目的doc/nacos/config/chapter25目录下获取完整的config.txt文件。」 （6）在下载的Seata源码的seata-1.4.2/script/config-center/nacos目录下找到nacos-config.sh文件，如下所示。 将nacos-config.sh文件复制到Seata安装文件解压目录的seata/seata-server-1.4.2/scripts目录下，其中scripts目录需要手动创建，如下所示。 （7）.sh文件是Linux操作系统上的脚本文件，如果想在Windows操作系统上运行.sh文件，可以在Windows操作系统上安装Git后在运行.sh文件。 接下来，在Git的Bash命令行进入Seata安装文件中nacos-config.sh文件所在的目录，执行如下命令。 sh nacos-config.sh -h 127.0.0.1 -p 8848 -g SEATA_GROUP -t seata_namespace_001 -u nacos -w nacos 其中，命令中的每个参数含义如下所示。 -h：Nacos所在的IP地址。 -p：Nacos的端口号。 -g：分组。 -t：命名空间的ID，这里我们填写在Nacos中创建的命名空间的ID：seata_namespace_001。如果不填，默认是public命名空间。 -u：Nacos的用户名。 -w：Nacos的密码。 执行命令后的结果信息如下所示。 ========================================================================= Complete initialization parameters, total-count:89 , failure-count:0 ========================================================================= Init nacos config finished, please start seata-server. 可以看到，整个配置执行成功。 （8）打开Nacos的配置管理-配置列表界面，切换到seata命名空间，可以看到有关Seata的配置都注册到Nacos中了，如下所示。 （9）在MySQL数据库中创建seata数据库，如下所示。 create database if not exists seata; 接下来，在seata数据库中执行Seata源码包seata-1.4.2/script/server/db目录下的mysql.sql脚本文件，mysql.sql脚本的内容如下所示。 -- -------------------------------- The script used when storeMode is 'db' -------------------------------- -- the table to store GlobalSession data CREATE TABLE IF NOT EXISTS `global_table` ( `xid` VARCHAR(128) NOT NULL, `transaction_id` BIGINT, `status` TINYINT NOT NULL, `application_id` VARCHAR(32), `transaction_service_group` VARCHAR(32), `transaction_name` VARCHAR(128), `timeout` INT, `begin_time` BIGINT, `application_data` VARCHAR(2000), `gmt_create` DATETIME, `gmt_modified` DATETIME, PRIMARY KEY (`xid`), KEY `idx_gmt_modified_status` (`gmt_modified`, `status`), KEY `idx_transaction_id` (`transaction_id`) ) ENGINE = InnoDB DEFAULT CHARSET = utf8; -- the table to store BranchSession data CREATE TABLE IF NOT EXISTS `branch_table` ( `branch_id` BIGINT NOT NULL, `xid` VARCHAR(128) NOT NULL, `transaction_id` BIGINT, `resource_group_id` VARCHAR(32), `resource_id` VARCHAR(256), `branch_type` VARCHAR(8), `status` TINYINT, `client_id` VARCHAR(64), `application_data` VARCHAR(2000), `gmt_create` DATETIME(6), `gmt_modified` DATETIME(6), PRIMARY KEY (`branch_id`), KEY `idx_xid` (`xid`) ) ENGINE = InnoDB DEFAULT CHARSET = utf8; -- the table to store lock data CREATE TABLE IF NOT EXISTS `lock_table` ( `row_key` VARCHAR(128) NOT NULL, `xid` VARCHAR(128), `transaction_id` BIGINT, `branch_id` BIGINT NOT NULL, `resource_id` VARCHAR(256), `table_name` VARCHAR(32), `pk` VARCHAR(36), `gmt_create` DATETIME, `gmt_modified` DATETIME, PRIMARY KEY (`row_key`), KEY `idx_branch_id` (`branch_id`) ) ENGINE = InnoDB DEFAULT CHARSET = utf8; 这里，也将mysql.sql文件放在了项目的doc/nacos/config/chapter25目录下。 （10）启动Seata服务，进入在命令行进入Seata安装文件的seata/seata-server-1.4.2/bin目录，执行如下命令。 seata-server.bat -p 8091 -h 127.0.0.1 -m db 可以看到，在启动Seata的命令行输出了如下信息。 i.s.core.rpc.netty.NettyServerBootstrap : Server started, listen port: 8091 说明Seata已经启动成功。 至此，Seata的基础环境搭建完毕。 项目整合Seata 在我们开发的微服务程序中，订单微服务下单成功后会调用库存微服务扣减商品的库存信息，而用户微服务只提供了查询用户信息的接口。这里，我们在商品微服务和订单微服务中整合Seata。 导入unlog表 我们使用的是Seata的AT模式，需要我们在涉及到使用Seata解决分布式事务问题的每个业务库中创建一个Seata的undo_log数据表，Seata中本身提供了创建数据表的SQL文件，这些SQL文件位于Seata源码包下的seata-1.4.2/script/client/at/db目录中，如下所示。 这里，我们使用mysql.sql脚本。mysql.sql脚本的内容如下所示。 -- for AT mode you must to init this sql for you business database. the seata server not need it. CREATE TABLE IF NOT EXISTS `undo_log` ( `branch_id` BIGINT NOT NULL COMMENT 'branch transaction id', `xid` VARCHAR(128) NOT NULL COMMENT 'global transaction id', `context` VARCHAR(128) NOT NULL COMMENT 'undo_log context,such as serialization', `rollback_info` LONGBLOB NOT NULL COMMENT 'rollback info', `log_status` INT(11) NOT NULL COMMENT '0:normal status,1:defense status', `log_created` DATETIME(6) NOT NULL COMMENT 'create datetime', `log_modified` DATETIME(6) NOT NULL COMMENT 'modify datetime', UNIQUE KEY `ux_undo_log` (`xid`, `branch_id`) ) ENGINE = InnoDB AUTO_INCREMENT = 1 DEFAULT CHARSET = utf8 COMMENT ='AT transaction mode undo table'; 注意，这里要在shop数据库中执行mysql.sql脚本，同样的，我会将这里的mysql.sql文件放到项目的doc/nacos/config/chapter25目录下，并重命名为mysql_client.sql。 商品微服务整合Seata （1）在商品微服务shop-product的pom.xml文件中引入Seata依赖，如下所示。 &lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-seata&lt;/artifactId&gt; &lt;/dependency&gt; （2）修改商品微服务shop-product的bootstrap.yml，修改后的文件如下所示。 spring: application: name: server-product cloud: nacos: config: server-addr: 127.0.0.1:8848 file-extension: yaml group: product_group shared-configs[0]: data_id: server-all.yaml group: all_group refresh: true discovery: server-addr: 127.0.0.1:8848 alibaba: seata: tx-service-group: ${spring.application.name}-tx_group profiles: active: dev seata: application-id: ${spring.application.name} service: vgroup-mapping: server-product-tx_group: default registry: nacos: server-addr: ${spring.cloud.nacos.discovery.server-addr} username: nacos password: nacos group: SEATA_GROUP namespace: seata_namespace_001 application: seata-server config: type: nacos nacos: server-addr: ${spring.cloud.nacos.discovery.server-addr} username: nacos password: nacos group: SEATA_GROUP namespace: seata_namespace_001 其中，配置的Nacos的namespace与group与registry.conf文件中的一致。 订单微服务整合Seata （1）在订单微服务shop-product的pom.xml文件中引入Seata依赖，如下所示。 &lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-seata&lt;/artifactId&gt; &lt;/dependency&gt; （2）修改订单微服务shop-order的bootstrap.yml，修改后的文件如下所示。 spring: application: name: server-order cloud: nacos: config: server-addr: 127.0.0.1:8848 file-extension: yaml group: order_group shared-configs[0]: data_id: server-all.yaml group: all_group refresh: true discovery: server-addr: 127.0.0.1:8848 alibaba: seata: tx-service-group: ${spring.application.name}-tx_group profiles: active: dev seata: application-id: ${spring.application.name} service: vgroup-mapping: server-order-tx_group: default registry: nacos: server-addr: ${spring.cloud.nacos.discovery.server-addr} username: nacos password: nacos group: SEATA_GROUP namespace: seata_namespace_001 application: seata-server config: type: nacos nacos: server-addr: ${spring.cloud.nacos.discovery.server-addr} username: nacos password: nacos group: SEATA_GROUP namespace: seata_namespace_001 （3）修改订单微服务的io.binghe.shop.order.service.impl.OrderServiceV8Impl类的saveOrder()方法，在saveOrder()方法上添加Seata的@GlobalTransactional注解，如下所示。 @Override @GlobalTransactional public void saveOrder(OrderParams orderParams) { //省略具体方法代码 } 至此，搭建并整合Seata完毕，就是这么简单。 验证Seata事务 重置数据库数据 这里，首先将商品数据表t_product中id为1001的数据的库存信息重置为100，如下所示。 update t_product set t_pro_stock = 100 where id = 1001; 查询数据表数据 （1）打开cmd终端，进入MySQL命令行，并进入shop商城数据库，如下所示。 C:\\Users\\binghe&gt;mysql -uroot -p Enter password: **** Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 15 Server version: 5.7.35 MySQL Community Server (GPL) Copyright (c) 2000, 2021, Oracle and/or its affiliates. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. mysql&gt; use shop; Database changed （2）查看商品数据表，如下所示。 mysql&gt; select * from t_product; +------+------------+-------------+-------------+ | id | t_pro_name | t_pro_price | t_pro_stock | +------+------------+-------------+-------------+ | 1001 | 华为 | 2399.00 | 100 | | 1002 | 小米 | 1999.00 | 100 | | 1003 | iphone | 4999.00 | 100 | +------+------------+-------------+-------------+ 3 rows in set (0.00 sec) 这里，我们以id为1001的商品为例，此时发现商品的库存为100。 （3）查询订单数据表，如下所示。 mysql&gt; select * from t_order; Empty set (0.00 sec) 可以发现订单数据表为空。 （4）查询订单条目数据表，如下所示。 mysql&gt; select * from t_order_item; Empty set (0.00 sec) 可以看到，订单条目数据表为空。 验证Seata事务 （1）分别启动Nacos、Sentinel、ZinKin、RocketMQ，Seata，并启动用户微服务，商品微服务，订单微服务和服务网关。打开浏览器访问http://localhost:10001/server-order/order/submit_order?userId=1001&amp;productId=1001&amp;count=1，如下所示。 返回的原始数据如下所示。 {&quot;code&quot;:500,&quot;codeMsg&quot;:&quot;执行失败&quot;,&quot;data&quot;:&quot;/ by zero&quot;} （2）查看各个微服务和网关输出的日志信息，分别如下所示。 用户微服务输出的日志如下所示。 获取到的用户信息为：{&quot;address&quot;:&quot;北京&quot;,&quot;id&quot;:1001,&quot;password&quot;:&quot;c26be8aaf53b15054896983b43eb6a65&quot;,&quot;phone&quot;:&quot;13212345678&quot;,&quot;username&quot;:&quot;binghe&quot;} 说明用户微服务无异常信息。 商品微服务输出的日志如下所示。 获取到的商品信息为：{&quot;id&quot;:1001,&quot;proName&quot;:&quot;华为&quot;,&quot;proPrice&quot;:2399.00,&quot;proStock&quot;:100} 更新商品库存传递的参数为: 商品id:1001, 购买数量:1 说明商品微服务无异常信息。 值得注意的是，整合Seata后，商品微服务同时输出了如下日志。 rm handle branch rollback process:xid=192.168.0.111:8091:6638572304823066625,branchId=6638572304823066634,branchType=AT,resourceId=jdbc:mysql://localhost:3306/shop,applicationData=null Branch Rollbacking: 192.168.0.111:8091:6638572304823066625 6638572304823066634 jdbc:mysql://localhost:3306/shop xid 192.168.0.111:8091:6638572304823066625 branch 6638572304823066634, undo_log deleted with GlobalFinished Branch Rollbacked result: PhaseTwo_Rollbacked 看上去应该是有事务回滚了。 订单微服务输出的日志如下所示。 提交订单时传递的参数:{&quot;count&quot;:1,&quot;empty&quot;:false,&quot;productId&quot;:1001,&quot;userId&quot;:1001} 库存扣减成功 服务器抛出了异常：{} java.lang.ArithmeticException: / by zero 说明订单微服务抛出了ArithmeticException异常。 同时，商品微服务会输出如下日志。 Branch Rollbacked result: PhaseTwo_Rollbacked [192.168.0.111:8091:6638572304823066625] rollback status: Rollbacked 看上去应该是有事务回滚了。 网关服务输出的日志如下所示。 执行前置过滤器逻辑 执行后置过滤器逻辑 访问接口主机: localhost 访问接口端口: 10001 访问接口URL: /server-order/order/submit_order 访问接口URL参数: userId=1001&amp;productId=1001&amp;count=1 访问接口时长: 1632ms 可以看到，网关服务无异常信息。 通过微服务打印出的日志信息，可以看到，有事务回滚了。 查询数据表数据 （1）打开cmd终端，进入MySQL命令行，并进入shop商城数据库，如下所示。 C:\\Users\\binghe&gt;mysql -uroot -p Enter password: **** Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 15 Server version: 5.7.35 MySQL Community Server (GPL) Copyright (c) 2000, 2021, Oracle and/or its affiliates. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. mysql&gt; use shop; Database changed （2）查看商品数据表，如下所示。 mysql&gt; select * from t_product; +------+------------+-------------+-------------+ | id | t_pro_name | t_pro_price | t_pro_stock | +------+------------+-------------+-------------+ | 1001 | 华为 | 2399.00 | 100 | | 1002 | 小米 | 1999.00 | 100 | | 1003 | iphone | 4999.00 | 100 | +------+------------+-------------+-------------+ 3 rows in set (0.00 sec) 可以看到，此时商品数据表中，id为1001的商品库存数量仍然为100。 （3）查看订单数据表，如下所示。 mysql&gt; select * from t_order; Empty set (0.00 sec) 可以看到，订单数据表为空。 （4）查看订单条目数据表，如下所示。 mysql&gt; select * from t_order_item; Empty set (0.00 sec) 可以看到，订单条目数据表为空。 至此，我们成功在项目中整合了Seata解决了分布式事务的问题 ","link":"https://tianxiawuhao.github.io/IMU3BaL0b/"},{"title":"Seata","content":"Seata 是什么? Seata 是一款开源的分布式事务解决方案，致力于提供高性能和简单易用的分布式事务服务。Seata 将为用户提供了 AT、TCC、SAGA 和 XA 事务模式，为用户打造一站式的分布式解决方案。 AT 模式 前提 基于支持本地 ACID 事务的关系型数据库。 Java 应用，通过 JDBC 访问数据库。 整体机制 两阶段提交协议的演变： 一阶段：业务数据和回滚日志记录在同一个本地事务中提交，释放本地锁和连接资源。 二阶段： 提交异步化，非常快速地完成。 回滚通过一阶段的回滚日志进行反向补偿。 写隔离 一阶段本地事务提交前，需要确保先拿到 「全局锁」 。 拿不到 「全局锁」 ，不能提交本地事务。 拿 「全局锁」 的尝试被限制在一定范围内，超出范围将放弃，并回滚本地事务，释放本地锁。 以一个示例来说明： 两个全局事务 tx1 和 tx2，分别对 a 表的 m 字段进行更新操作，m 的初始值 1000。 tx1 先开始，开启本地事务，拿到本地锁，更新操作 m = 1000 - 100 = 900。本地事务提交前，先拿到该记录的 「全局锁」 ，本地提交释放本地锁。tx2 后开始，开启本地事务，拿到本地锁，更新操作 m = 900 - 100 = 800。本地事务提交前，尝试拿该记录的 「全局锁」 ，tx1 全局提交前，该记录的全局锁被 tx1 持有，tx2 需要重试等待 「全局锁」 。 tx1 二阶段全局提交，释放 「全局锁」 。tx2 拿到 「全局锁」 提交本地事务。 如果 tx1 的二阶段全局回滚，则 tx1 需要重新获取该数据的本地锁，进行反向补偿的更新操作，实现分支的回滚。 此时，如果 tx2 仍在等待该数据的 「全局锁」，同时持有本地锁，则 tx1 的分支回滚会失败。分支的回滚会一直重试，直到 tx2 的 「全局锁」 等锁超时，放弃 「全局锁」 并回滚本地事务释放本地锁，tx1 的分支回滚最终成功。 因为整个过程 「全局锁」 在 tx1 结束前一直是被 tx1 持有的，所以不会发生 「脏写」 的问题。 读隔离 在数据库本地事务隔离级别 「读已提交（Read Committed）」 或以上的基础上，Seata（AT 模式）的默认全局隔离级别是 「读未提交（Read Uncommitted）」 。 如果应用在特定场景下，必需要求全局的 「读已提交」 ，目前 Seata 的方式是通过 SELECT FOR UPDATE 语句的代理。 SELECT FOR UPDATE 语句的执行会申请 「全局锁」 ，如果 「全局锁」 被其他事务持有，则释放本地锁（回滚 SELECT FOR UPDATE 语句的本地执行）并重试。这个过程中，查询是被 block 住的，直到 「全局锁」 拿到，即读取的相关数据是 「已提交」 的，才返回。 出于总体性能上的考虑，Seata 目前的方案并没有对所有 SELECT 语句都进行代理，仅针对 FOR UPDATE 的 SELECT 语句。 工作机制 以一个示例来说明整个 AT 分支的工作过程。 业务表：product Field Type Key id bigint(20) PRI name varchar(100) since varchar(100) AT 分支事务的业务逻辑： update product set name = 'GTS' where name = 'TXC'; 「一阶段」 过程： 解析 SQL：得到 SQL 的类型（UPDATE），表（product），条件（where name = 'TXC'）等相关的信息。 查询前镜像：根据解析得到的条件信息，生成查询语句，定位数据。 select id, name, since from product where name = 'TXC'; 得到前镜像： id name since 1 TXC 2014 执行业务 SQL：更新这条记录的 name 为 'GTS'。 查询后镜像：根据前镜像的结果，通过 「主键」 定位数据。 select id, name, since from product where id = 1; 得到后镜像： id name since 1 GTS 2014 插入回滚日志：把前后镜像数据以及业务 SQL 相关的信息组成一条回滚日志记录，插入到 UNDO_LOG 表中。 { &quot;branchId&quot;: 641789253, &quot;undoItems&quot;: [{ &quot;afterImage&quot;: { &quot;rows&quot;: [{ &quot;fields&quot;: [{ &quot;name&quot;: &quot;id&quot;, &quot;type&quot;: 4, &quot;value&quot;: 1 }, { &quot;name&quot;: &quot;name&quot;, &quot;type&quot;: 12, &quot;value&quot;: &quot;GTS&quot; }, { &quot;name&quot;: &quot;since&quot;, &quot;type&quot;: 12, &quot;value&quot;: &quot;2014&quot; }] }], &quot;tableName&quot;: &quot;product&quot; }, &quot;beforeImage&quot;: { &quot;rows&quot;: [{ &quot;fields&quot;: [{ &quot;name&quot;: &quot;id&quot;, &quot;type&quot;: 4, &quot;value&quot;: 1 }, { &quot;name&quot;: &quot;name&quot;, &quot;type&quot;: 12, &quot;value&quot;: &quot;TXC&quot; }, { &quot;name&quot;: &quot;since&quot;, &quot;type&quot;: 12, &quot;value&quot;: &quot;2014&quot; }] }], &quot;tableName&quot;: &quot;product&quot; }, &quot;sqlType&quot;: &quot;UPDATE&quot; }], &quot;xid&quot;: &quot;xid:xxx&quot; } 提交前，向 TC 注册分支：申请 product 表中，主键值等于 1 的记录的 「全局锁」 。 本地事务提交：业务数据的更新和前面步骤中生成的 UNDO LOG 一并提交。 将本地事务提交的结果上报给 TC。 「二阶段-回滚」 收到 TC 的分支回滚请求，开启一个本地事务，执行如下操作。 通过 XID 和 Branch ID 查找到相应的 UNDO LOG 记录。 数据校验：拿 UNDO LOG 中的后镜与当前数据进行比较，如果有不同，说明数据被当前全局事务之外的动作做了修改。这种情况，需要根据配置策略来做处理，详细的说明在另外的文档中介绍。 根据 UNDO LOG 中的前镜像和业务 SQL 的相关信息生成并执行回滚的语句： update product set name = 'TXC' where id = 1; 提交本地事务。并把本地事务的执行结果（即分支事务回滚的结果）上报给 TC。 「二阶段-提交」 收到 TC 的分支提交请求，把请求放入一个异步任务的队列中，马上返回提交成功的结果给 TC。 异步任务阶段的分支提交请求将异步和批量地删除相应 UNDO LOG 记录。 附录 「回滚日志表」 UNDO_LOG Table：不同数据库在类型上会略有差别。 以 MySQL 为例： Field Type branch_id bigint PK xid varchar(100) context varchar(128) rollback_info longblob log_status tinyint log_created datetime log_modified datetime -- 注意此处0.7.0+ 增加字段 context CREATE TABLE `undo_log` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `branch_id` bigint(20) NOT NULL, `xid` varchar(100) NOT NULL, `context` varchar(128) NOT NULL, `rollback_info` longblob NOT NULL, `log_status` int(11) NOT NULL, `log_created` datetime NOT NULL, `log_modified` datetime NOT NULL, PRIMARY KEY (`id`), UNIQUE KEY `ux_undo_log` (`xid`,`branch_id`) ) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8; TCC 模式 回顾总览中的描述：一个分布式的全局事务，整体是 「两阶段提交」 的模型。全局事务是由若干分支事务组成的，分支事务要满足 「两阶段提交」 的模型要求，即需要每个分支事务都具备自己的： 一阶段 prepare 行为 二阶段 commit 或 rollback 行为 根据两阶段行为模式的不同，我们将分支事务划分为 「Automatic (Branch) Transaction Mode」 和 「Manual (Branch) Transaction Mode」. AT 模式（参考链接 TBD）基于 「支持本地 ACID 事务」 的 「关系型数据库」： 一阶段 prepare 行为：在本地事务中，一并提交业务数据更新和相应回滚日志记录。 二阶段 commit 行为：马上成功结束，「自动」 异步批量清理回滚日志。 二阶段 rollback 行为：通过回滚日志，「自动」 生成补偿操作，完成数据回滚。 相应的，TCC 模式，不依赖于底层数据资源的事务支持： 一阶段 prepare 行为：调用 「自定义」 的 prepare 逻辑。 二阶段 commit 行为：调用 「自定义」 的 commit 逻辑。 二阶段 rollback 行为：调用 「自定义」 的 rollback 逻辑。 所谓 TCC 模式，是指支持把 「自定义」 的分支事务纳入到全局事务的管理中。 Saga 模式 Saga模式是SEATA提供的长事务解决方案，在Saga模式中，业务流程中每个参与者都提交本地事务，当出现某一个参与者失败则补偿前面已经成功的参与者，一阶段正向服务和二阶段补偿服务都由业务开发实现。 理论基础：Hector &amp; Kenneth 发表论⽂ Sagas （1987） 适用场景 业务流程长、业务流程多 参与者包含其它公司或遗留系统服务，无法提供 TCC 模式要求的三个接口 优势 一阶段提交本地事务，无锁，高性能 事件驱动架构，参与者可异步执行，高吞吐 补偿服务易于实现 缺点 不保证隔离性 ","link":"https://tianxiawuhao.github.io/XFKzbOO8H/"},{"title":"服务容错：服务雪崩与容错方案","content":"本文主要内容如下所示。 并发对系统的影响 当一个系统的架构设计采用微服务架构模式时，会将庞大而复杂的业务拆分成一个个小的微服务，各个微服务之间以接口或者RPC的形式进行互相调用。在调用的过程中，就会涉及到网路的问题，再加上微服务自身的原因，例如很难做到100%的高可用等。如果众多微服务当中的某个或某些微服务出现问题，不可用或者宕机了，那么其他微服务调用这些微服务的接口时就会出现延迟。如果此时有大量请求进入系统，就会造成请求任务的大量堆积，甚至会造成整体服务的瘫痪。 压测说明 为了更加直观的说明当系统没有容错能力时，高并发、大流量场景对于系统的影响，我们在这里模拟一个并发的场景。在订单微服务shop-order的io.binghe.shop.order.controller.OrderController类中新增一个concurrentRequest()方法，源码如下所示。 @GetMapping(value = &quot;/concurrent_request&quot;) public String concurrentRequest(){ log.info(&quot;测试业务在高并发场景下是否存在问题&quot;); return &quot;binghe&quot;; } 接下来，为了更好的演示效果，我们限制下Tomcat处理请求的最大并发数，在订单微服务shop-order的resources目录下的application.yml文件中添加如下配置。 server: port: 8080 tomcat: max-threads: 20 限制Tomcat一次最多只能处理20个请求。接下来，我们就使用JMeter对 http://localhost:8080/order/submit_order 接口进行压测，由于订单微服务中没有做任何的容错处理，当对 http://localhost:8080/order/submit_order 接口的请求压力过大时，我们再访问http://localhost:8080/order/concurrent_request 接口时，会发现http://localhost:8080/order/concurrent_request 接口会受到并发请求的影响，访问很慢甚至根本访问不到。 压测实战 使用JMeter对 http://localhost:8080/order/submit_order 接口进行压测，JMeter的配置过程如下所示。 （1）打开JMeter的主界面，如下所示。 （2）在JMeter中右键测试计划添加线程组，如下所示。 （3）在JMeter中的线程组中配置并发线程数，如下所示。 如上图所示，将线程数配置成50，Ramp-Up时间配置成0，循环次数为100。表示JMeter每次会在同一时刻向系统发送50个请求，发送100次为止。 （4）在JMeter中右键线程组添加HTTP请求，如下所示。 （5）在JMeter中配置HTTP请求，如下所示。 具体配置如下所示。 协议：http 服务器名称或IP：localhost 端口号：8080 方法：GET 路径：/order/submit_order?userId=1001&amp;productId=1001&amp;count=1 内容编码：UTF-8 （6）配置好JMeter后，点击JMeter上的绿色小三角开始压测，如下所示。 点击后会弹出需要保存JMeter脚本的弹出框，根据实际需要点击保存即可。 点击保存后，开始对 http://localhost:8080/order/submit_order 接口进行压测，在压测的过程中会发现订单微服务打印日志时，会比较卡顿，同时在浏览器或其他工具中访问http://localhost:8080/order/concurrent_request 接口会卡顿，甚至根本访问不到。 说明订单微服务中的某个接口一旦访问的并发量过高，其他接口也会受到影响，进而导致订单微服务整体不可用。为了说明这个问题，我们再来看看服务雪崩是个什么鬼。 服务雪崩 系统采用分布式或微服务的架构模式后，由于网络或者服务自身的问题，一般服务是很难做到100%高可用的。如果一个服务出现问题，就可能会导致其他的服务级联出现问题，这种故障性问题会在整个系统中不断扩散，进而导致服务不可用，甚至宕机，最终会对整个系统造成灾难性后果。 为了最大程度的预防服务雪崩，组成整体系统的各个微服务需要支持服务容错的功能。 服务容错方案 服务容错在一定程度上就是尽最大努力来兼容错误情况的发生，因为在分布式和微服务环境中，不可避免的会出现一些异常情况，我们在设计分布式和微服务系统时，就要考虑到这些异常情况的发生，使得系统具备服务容错能力。 常见的服务错误方案包含：服务限流、服务隔离、服务超时、服务熔断和服务降级等。 服务限流 服务限流就是限制进入系统的流量，以防止进入系统的流量过大而压垮系统。其主要的作用就是保护服务节点或者集群后面的数据节点，防止瞬时流量过大使服务和数据崩溃（如前端缓存大量实效），造成不可用；还可用于平滑请求。 限流算法有两种，一种就是简单的请求总量计数，一种就是时间窗口限流（一般为1s），如令牌桶算法和漏牌桶算法就是时间窗口的限流算法。 服务隔离 服务隔离有点类似于系统的垂直拆分，就按照一定的规则将系统划分成多个服务模块，并且每个服务模块之间是互相独立的，不会存在强依赖的关系。如果某个拆分后的服务发生故障后，能够将故障产生的影响限制在某个具体的服务内，不会向其他服务扩散，自然也就不会对整体服务产生致命的影响。 互联网行业常用的服务隔离方式有：线程池隔离和信号量隔离。 服务超时 整个系统采用分布式和微服务架构后，系统被拆分成一个个小服务，就会存在服务与服务之间互相调用的现象，从而形成一个个调用链。形成调用链关系的两个服务中，主动调用其他服务接口的服务处于调用链的上游，提供接口供其他服务调用的服务处于调用链的下游。 服务超时就是在上游服务调用下游服务时，设置一个最大响应时间，如果超过这个最大响应时间下游服务还未返回结果，则断开上游服务与下游服务之间的请求连接，释放资源。 服务熔断 在分布式与微服务系统中，如果下游服务因为访问压力过大导致响应很慢或者一直调用失败时，上游服务为了保证系统的整体可用性，会暂时断开与下游服务的调用连接。这种方式就是熔断。 服务熔断一般情况下会有三种状态：关闭、开启和半熔断。 关闭状态：服务一切正常，没有故障时，上游服务调用下游服务时，不会有任何限制。 开启状态：上游服务不再调用下游服务的接口，会直接返回上游服务中预定的方法。 半熔断状态：处于开启状态时，上游服务会根据一定的规则，尝试恢复对下游服务的调用。此时，上游服务会以有限的流量来调用下游服务，同时，会监控调用的成功率。如果成功率达到预期，则进入关闭状态。如果未达到预期，会重新进入开启状态。 服务降级 服务降级，说白了就是一种服务托底方案，如果服务无法完成正常的调用流程，就使用默认的托底方案来返回数据。例如，在商品详情页一般都会展示商品的介绍信息，一旦商品详情页系统出现故障无法调用时，会直接获取缓存中的商品介绍信息返回给前端页面。 ","link":"https://tianxiawuhao.github.io/WLM-Ra1r4/"},{"title":"nocas长轮询","content":"前言 传统的静态配置方式想要修改某个配置时，必须重新启动一次应用，如果是数据库连接串的变更，那可能还容易接受一些，但如果变更的是一些运行时实时感知的配置，如某个功能项的开关，重启应用就显得有点大动干戈了。 配置中心正是为了解决此类问题应运而生的，特别是在微服务架构体系中，更倾向于使用配置中心来统一管理配置。 配置中心最核心的能力就是配置的动态推送，常见的配置中心如 Nacos、Apollo 等都实现了这样的能力。 在早期接触配置中心时，我就很好奇，配置中心是如何做到服务端感知配置变化实时推送给客户端的？ 在没有研究过配置中心的实现原理之前，我一度认为配置中心是通过长连接来做到配置推送的。 事实上，目前比较流行的两款配置中心：Nacos 和 Apollo 恰恰都没有使用长连接，而是使用的长轮询。 本文便是介绍一下长轮询这种听起来好像已经是上个世纪的技术，老戏新唱，看看能不能品出别样的韵味。 文中会有代码示例，呈现一个简易的配置监听流程。 数据交互模式 众所周知，数据交互有两种模式：Push（推模式）和 Pull（拉模式）。 推模式指的是客户端与服务端建立好网络长连接，服务方有相关数据，直接通过长连接通道推送到客户端。 其优点是及时，一旦有数据变更，客户端立马能感知到；另外对客户端来说逻辑简单，不需要关心有无数据这些逻辑处理。缺点是不知道客户端的数据消费能力，可能导致数据积压在客户端，来不及处理。 拉模式指的是客户端主动向服务端发出请求，拉取相关数据。 其优点是此过程由客户端发起请求，故不存在推模式中数据积压的问题。缺点是可能不够及时，对客户端来说需要考虑数据拉取相关逻辑，何时去拉，拉的频率怎么控制等等。 长轮询与轮询 在开头，重点介绍一下长轮询（Long Polling）和轮询（Polling）的区别，两者都是拉模式的实现。 「轮询」是指不管服务端数据有无更新，客户端每隔定长时间请求拉取一次数据，可能有更新数据返回，也可能什么都没有。 配置中心如果使用「轮询」实现动态推送，会有以下问题： 推送延迟。客户端每隔 5s 拉取一次配置，若配置变更发生在第 6s，则配置推送的延迟会达到 4s。 服务端压力。配置一般不会发生变化，频繁的轮询会给服务端造成很大的压力。 推送延迟和服务端压力无法中和。降低轮询的间隔，延迟降低，压力增加；增加轮询的间隔，压力降低，延迟增高。 「长轮询」则不存在上述的问题。 客户端发起长轮询，如果服务端的数据没有发生变更，会 hold 住请求，直到服务端的数据发生变化，或者等待一定时间超时才会返回。返回后，客户端又会立即再次发起下一次长轮询。 配置中心使用「长轮询」如何解决「轮询」遇到的问题也就显而易见了： 推送延迟。服务端数据发生变更后，长轮询结束，立刻返回响应给客户端。 服务端压力。长轮询的间隔期一般很长，例如 30s、60s，并且服务端 hold 住连接不会消耗太多服务端资源。 以 Nacos 为例的长轮询流程如下： 可能有人会有疑问，为什么一次长轮询需要等待一定时间超时，超时后又发起长轮询，为什么不让服务端一直 hold 住？ 主要有两个层面的考虑，一是连接稳定性的考虑，长轮询在传输层本质上还是走的 TCP 协议，如果服务端假死、fullgc 等异常问题，或者是重启等常规操作，长轮询没有应用层的心跳机制，仅仅依靠 TCP 层的心跳保活很难确保可用性，所以一次长轮询设置一定的超时时间也是在确保可用性。 除此之外，在配置中心场景，还有一定的业务需求需要这么设计。 在配置中心的使用过程中，用户可能随时新增配置监听，而在此之前，长轮询可能已经发出，新增的配置监听无法包含在旧的长轮询中，所以在配置中心的设计中，一般会在一次长轮询结束后，将新增的配置监听给捎带上，而如果长轮询没有超时时间，只要配置一直不发生变化，响应就无法返回，新增的配置也就没法设置监听了。 配置中心长轮询设计 上文的图中，介绍了长轮询的流程，本节会详解配置中心长轮询的设计细节。 客户端发起长轮询 客户端发起一个 HTTP 请求，请求信息包含配置中心的地址，以及监听的 dataId（本文出于简化说明的考虑，认为 dataId 是定位配置的唯一键）。若配置没有发生变化，客户端与服务端之间一直处于连接状态。 服务端监听数据变化 服务端会维护 dataId 和长轮询的映射关系，如果配置发生变化，服务端会找到对应的连接，为响应写入更新后的配置内容。如果超时内配置未发生变化，服务端找到对应的超时长轮询连接，写入 304 响应。 304 在 HTTP 响应码中代表“未改变”，并不代表错误。比较契合长轮询时，配置未发生变更的场景。 客户端接收长轮询响应 首先查看响应码是 200 还是 304，以判断配置是否变更，做出相应的回调。之后再次发起下一次长轮询。 服务端设置配置写入的接入点 主要用配置控制台和 client 发布配置，触发配置变更 这几点便是配置中心实现长轮询的核心步骤，也是指导下面章节代码实现的关键。但在编码之前，仍有一些其他的注意点需要实现阐明。 配置中心往往是为分布式的集群提供服务的，而每个机器上部署的应用，又会有多个 dataId 需要监听，实例级别 * 配置数是一个不小的数字，配置中心服务端维护这些 dataId 的长轮询连接显然不能用线程一一对应，否则会导致服务端线程数爆炸式增长。 一个 Tomcat 默认也就 200 个线程，长轮询也不应该阻塞 Tomcat 的业务线程，所以需要配置中心在实现长轮询时，往往采用异步响应的方式来实现。 而比较方便实现异步 HTTP 的常见手段便是 Servlet3.0 提供的 AsyncContext 机制。 Servlet3.0 并不是一个特别新的规范，它跟 Java 6 是同一时期的产物。例如 SpringBoot 内嵌的 Tomcat 很早就支持了 Servlet3.0，你无需担心 AsyncContext 机制不起作用。 SpringMVC 实现了 DeferredResult 和 Servlet3.0 提供的 AsyncContext 其实没有多大区别，我并没有深入研究过两个实现背后的源码，但从使用层面上来看，AsyncContext 更加的灵活，例如其可以自定义响应码，而 DeferredResult 在上层做了封装，可以快速的帮助开发者实现一个异步响应，但没法细粒度地控制响应。 所以下文的示例中，我选择了 AsyncContext。 配置中心长轮询实现 &lt;!--注解工具包--&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt; &lt;artifactId&gt;httpclient&lt;/artifactId&gt; &lt;version&gt;4.5.2&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/com.google.guava/guava --&gt; &lt;dependency&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;version&gt;22.0&lt;/version&gt; &lt;/dependency&gt; 客户端实现 @Slf4j public class ConfigClient { private CloseableHttpClient httpClient; private RequestConfig requestConfig; public ConfigClient() { this.httpClient = HttpClientBuilder.create().build(); // ① httpClient 客户端超时时间要大于长轮询约定的超时时间 this.requestConfig = RequestConfig.custom().setSocketTimeout(40000).build(); } @SneakyThrows public void longPolling(String url, String dataId) { String endpoint = url + &quot;?dataId=&quot; + dataId; HttpGet request = new HttpGet(endpoint); CloseableHttpResponse response = httpClient.execute(request); switch (response.getStatusLine().getStatusCode()) { case 200: { BufferedReader rd = new BufferedReader(new InputStreamReader(response.getEntity() .getContent())); StringBuilder result = new StringBuilder(); String line; while ((line = rd.readLine()) != null) { result.append(line); } response.close(); String configInfo = result.toString(); log.info(&quot;dataId: [{}] changed, receive configInfo: {}&quot;, dataId, configInfo); longPolling(url, dataId); break; } // ② 304 响应码标记配置未变更 case 304: { log.info(&quot;longPolling dataId: [{}] once finished, configInfo is unchanged, longPolling again&quot;, dataId); longPolling(url, dataId); break; } default: { throw new RuntimeException(&quot;unExcepted HTTP status code&quot;); } } } public static void main(String[] args) { // httpClient 会打印很多 debug 日志，关闭掉 Logger logger = (Logger)LoggerFactory.getLogger(&quot;org.apache.http&quot;); logger.setLevel(Level.INFO); logger.setAdditive(false); ConfigClient configClient = new ConfigClient(); // ③ 对 dataId: user 进行配置监听 configClient.longPolling(&quot;http://127.0.0.1:8080/listener&quot;, &quot;user&quot;); } } 主要有三个注意点： RequestConfig.custom().setSocketTimeout(40000).build() 。httpClient 客户端超时时间要大于长轮询约定的超时时间。很好理解，不然还没等服务端返回，客户端会自行断开 HTTP 连接。 response.getStatusLine().getStatusCode() == 304 。前文介绍过，约定使用 304 响应码来标识配置未发生变更，客户端继续发起长轮询。 configClient.longPolling(&quot;http://127.0.0.1:8080/listener&quot;, &quot;user&quot;)。在示例中，我们处于简单考虑，仅仅启动一个客户端，对单一的 dataId：user 进行监听（注意，需要先启动 server 端）。 服务端实现 @RestController @Slf4j @SpringBootApplication public class ConfigServer { @Data private static class AsyncTask { // 长轮询请求的上下文，包含请求和响应体 private AsyncContext asyncContext; // 超时标记 private boolean timeout; public AsyncTask(AsyncContext asyncContext, boolean timeout) { this.asyncContext = asyncContext; this.timeout = timeout; } } // guava 提供的多值 Map，一个 key 可以对应多个 value private Multimap&lt;String, AsyncTask&gt; dataIdContext = Multimaps.synchronizedSetMultimap(HashMultimap.create()); private ThreadFactory threadFactory = new ThreadFactoryBuilder().setNameFormat(&quot;longPolling-timeout-checker-%d&quot;) .build(); private ScheduledExecutorService timeoutChecker = new ScheduledThreadPoolExecutor(1, threadFactory); // 配置监听接入点 @RequestMapping(&quot;/listener&quot;) public void addListener(HttpServletRequest request, HttpServletResponse response) { String dataId = request.getParameter(&quot;dataId&quot;); // 开启异步 AsyncContext asyncContext = request.startAsync(request, response); AsyncTask asyncTask = new AsyncTask(asyncContext, true); // 维护 dataId 和异步请求上下文的关联 dataIdContext.put(dataId, asyncTask); // 启动定时器，30s 后写入 304 响应 timeoutChecker.schedule(() -&gt; { if (asyncTask.isTimeout()) { dataIdContext.remove(dataId, asyncTask); response.setStatus(HttpServletResponse.SC_NOT_MODIFIED); asyncContext.complete(); } }, 30000, TimeUnit.MILLISECONDS); } // 配置发布接入点 @RequestMapping(&quot;/publishConfig&quot;) @SneakyThrows public String publishConfig(String dataId, String configInfo) { log.info(&quot;publish configInfo dataId: [{}], configInfo: {}&quot;, dataId, configInfo); Collection&lt;AsyncTask&gt; asyncTasks = dataIdContext.removeAll(dataId); for (AsyncTask asyncTask : asyncTasks) { asyncTask.setTimeout(false); HttpServletResponse response = (HttpServletResponse)asyncTask.getAsyncContext().getResponse(); response.setStatus(HttpServletResponse.SC_OK); response.getWriter().println(configInfo); asyncTask.getAsyncContext().complete(); } return &quot;success&quot;; } public static void main(String[] args) { SpringApplication.run(ConfigServer.class, args); } } 对上述实现的一些说明： @RequestMapping(&quot;/listener&quot;) ，配置监听接入点，也是长轮询的入口。在获取 dataId 之后，使用 request.startAsync 将请求设置为异步，这样在方法结束后，不会占用 Tomcat 的线程池。 接着 dataIdContext.put(dataId, asyncTask) 会将 dataId 和异步请求上下文给关联起来，方便配置发布时，拿到对应的上下文。 注意这里使用了一个 guava 提供的数据结构 Multimap&lt;String, AsyncTask&gt; dataIdContext ，它是一个多值 Map，一个 key 可以对应多个 value，你也可以理解为 Map&lt;String,List&gt; ，但使用 Multimap 维护起来可以更方便地处理一些并发逻辑。 至于为什么会有多值，很好理解，因为配置中心的 Server 端会接受来自多个客户端对同一个 dataId 的监听。 timeoutChecker.schedule() 启动定时器，30s 后写入 304 响应。再结合之前客户端的逻辑，接收到 304 之后，会重新发起长轮询，形成一个循环。 @RequestMapping(&quot;/publishConfig&quot;) ，配置发布的入口。配置变更后，根据 dataId 一次拿出所有的长轮询，为之写入变更的响应，同时不要忘记取消定时任务。至此，完成了一个配置变更后推送的流程。 启动配置监听 先启动 ConfigServer，再启动 ConfigClient。客户端打印长轮询的日志如下： 22:18:09.185 [main] INFO moe.cnkirito.demo.ConfigClient - longPolling dataId: [user] once finished, configInfo is unchanged, longPolling again 22:18:39.197 [main] INFO moe.cnkirito.demo.ConfigClient - longPolling dataId: [user] once finished, configInfo is unchanged, longPolling again 发布一条配置，curl -X GET &quot;localhost:8080/publishConfig?dataId=user&amp;configInfo=helloworld&quot; 服务端打印日志如下： 22:18:50.801 INFO 73301 --- [nio-8080-exec-6] moe.cnkirito.demo.ConfigServer : publish configInfo dataId: [user], configInfo: helloworld 客户端接受配置推送： 22:18:50.806 [main] INFO moe.cnkirito.demo.ConfigClient - dataId: [user] changed, receive configInfo: helloworld 实现细节思考 为什么需要定时器返回 304 上述的实现中，服务端采用了一个定时器，在配置未发生变更时，定时返回 304，客户端接收到 304 之后，重新发起长轮询。 在前文，已经解释过了为什么需要超时后重新发起长轮询，而不是由服务端一直 hold，直到配置变更再返回。 但可能有读者还会有疑问，为什么不由客户端控制超时，服务端去除掉定时器，这样客户端超时后重新发起下一次长轮询，这样的设计不是更简单吗？ 无论是 Nacos 还是 Apollo 都有这样的定时器，而不是靠客户端控制超时，这样做主要有两点考虑： 和真正的客户端超时区分开。 仅仅使用异常（Exception）来表达异常流，而不应该用异常来表达正常的业务流。304 不是超时异常，而是长轮询中配置未变更的一种正常流程，不应该使用超时异常来表达。 客户端超时需要单独配置，且需要比服务端长轮询的超时要长。 正如上述的 demo 中客户端超时设置的是 40s，服务端判断一次长轮询超时是 30s。 这两个值在 Nacos 中默认是 30s 和 29.5s，在 Apollo 中默认是是 90s 和 60s。 长轮询包含多组 dataId 在上述的 demo 中，一个 dataId 会发起一次长轮询，在实际配置中心的设计中肯定不能这样设计，一般的优化方式是，一批 dataId 组成一个组批量包含在一个长轮询任务中。 在 Nacos 中，按照 3000 个 dataId 为一组包装成一个长轮询任务。 长轮询和长连接 讲完实现细节，本文最核心的部分已经介绍完了。 再回到最前面提到的数据交互模式上提到的推模型和拉模型，其实在写这篇文章时，我曾经问过交流群中的小伙伴们“配置中心实现动态推送的原理”，他们中绝大多数人认为是长连接的推模型。 然而事实上，主流的配置中心几乎都是使用了本文介绍的长轮询方案，这又是为什么呢？ 我也翻阅了不少博客，显然他们给出的理由并不能说服我，我尝试着从自己的角度分析了一下这个既定的事实。 长轮询实现起来比较容易，完全依赖于 HTTP 便可以实现全部逻辑，而 HTTP 是最能够被大众接受的通信方式。 长轮询使用 HTTP，便于多语言客户端的编写，大多数语言都有 HTTP 的客户端。 那么长连接是不是真的就不适合用于配置中心场景呢？ 有人可能会认为维护一条长连接会消耗大量资源，而长轮询可以提升系统的吞吐量，而在配置中心场景，这一假设并没有实际的压测数据能够论证，benchmark everything！please~ 另外，翻阅了一下 Nacos 2.0 的 milestone，我发现了一个有意思的规划，Nacos 的注册中心（目前是短轮询 + udp 推送）和配置中心（目前是长轮询）都有计划改造为长连接模式。 再回过头来看，长轮询实现已经将配置中心这个组件支撑的足够好了，替换成长连接，一定需要找到合适的理由才行。 总结 本文介绍了长轮询、轮询、长连接这几种数据交互模型的差异性。 分析了 Nacos 和 Apollo 等主流配置中心均是通过长轮询的方式实现配置的实时推送的。实时感知建立在客户端拉的基础上，因为本质上还是通过 HTTP 进行的数据交互，之所以有“推”的感觉，是因为服务端 hold 住了客户端的响应体，并且在配置变更后主动写入了返回 response 对象再进行返回。 通过一个简单的 demo，实现了长轮询配置实时推送的过程演示 ","link":"https://tianxiawuhao.github.io/t9Kci8xf6/"},{"title":"ZipKin核心架构","content":"ZipKin核心架构 Zipkin 是 Twitter 的一个开源项目，它基于Google Dapper论文实现，可以收集微服务运行过程中的实时链路数据，并进行展示。 ZipKin概述 Zipkin是一种分布式链路跟踪系统，能够收集微服务运行过程中的实时调用链路信息，并能够将这些调用链路信息展示到Web界面上供开发人员分析，开发人员能够从ZipKin中分析出调用链路中的性能瓶颈，识别出存在问题的应用程序，进而定位问题和解决问题。 ZipKin核心架构 ZipKin的核心架构图如下所示。 注：图片来源：zipkin.io/pages/architecture.html 其中，ZipKin核心组件的功能如下所示。 Reporter：ZipKin中上报链路数据的模块，主要配置在具体的微服务应用中。 Transport：ZipKin中传输链路数据的模块，此模块可以配置为Kafka，RocketMQ、RabbitMQ等。 Collector：ZipKin中收集并消费链路数据的模块，默认是通过http协议收集，可以配置为Kafka消费。 Storage：ZipKin中存储链路数据的模块，此模块的具体可以配置为ElasticSearch、Cassandra或者MySQL，目前ZipKin支持这三种数据持久化方式。 API：ZipKin中的API 组件，主要用来提供外部访问接口。比如给客户端展示跟踪信息，或是开放给外部系统实现监控等。 UI：ZipKin中的UI 组件，基于API组件实现的上层应用。通过UI组件用户可以方便并且很直观地查询和分析跟踪信息。 Zipkin在总体上会分为两个端，一个是Zipkin服务端，一个是Zipkin客户端，客户端主要是配置在微服务应用中，收集微服务中的调用链路信息，将数据发送给ZipKin服务端。 项目整合ZipKin Zipkin总体上分为服务端和客户端，我们需要下载并启动ZipKin服务端的Jar包，在微服务中集成ZipKin的客户端。 下载安装ZipKin服务端 （1）下载ZipKin服务端Jar文件，可以直接在浏览器中输入如下链接进行下载。 https://search.maven.org/remote_content?g=io.zipkin.java&amp;a=zipkin-server&amp;v=LATEST&amp;c=exec 如果大家使用的是Linux操作系统，也可以在命令行输入如下命令进行下载。 wget https://search.maven.org/remote_content?g=io.zipkin.java&amp;a=zipkin-server&amp;v=LATEST&amp;c=exec 这里，我通过浏览器下载的ZipKin服务端Jar文件为：zipkin-server-2.12.9-exec.jar。 （2）在命令行输入如下命令启动ZipKin服务端。 java -jar zipkin-server-2.12.9-exec.jar （3）由于ZipKin服务端启动时，默认监听的端口号为9411，所以，在浏览器中输入http://localhost:9411链接就可以打开ZipKin的界面，如下所示。 在浏览器中输入http://localhost:9411链接能够打开上述页面就说明ZipKin服务端已经准备好啦。 项目整合ZipKin客户端 （1）在每个微服务（用户微服务shop-user，商品微服务shop-product，订单微服务shop-order，网关服务shop-gateway）中添加ZipKin依赖，如下所示。 &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-zipkin&lt;/artifactId&gt; &lt;/dependency&gt; （2）在网关服务shop-gateway的application.yml文件中添加如下配置。 spring: sleuth: sampler: probability: 1.0 zipkin: base-url: http://127.0.0.1:9411 discovery-client-enabled: false 其中各配置的说明如下所示。 spring.sleuth.sampler.probability：表示Sleuth的采样百分比。 spring.zipkin.base-url：ZipKin服务端的地址。 spring.zipkin.discovery-client-enabled：配置成false，使Nacos将其当成一个URL，不要按服务名处理。 （3）分别启动用户微服务，商品微服务，订单微服务和服务网关，在浏览器中访问链接http://localhost:10001/server-order/order/submit_order?userId=1001&amp;productId=1001&amp;count=1，如下所示。 （4）点击Zipkin界面上的查找按钮，如下所示。 点击后的界面如下所示。 可以看到，点击查找按钮后，会出现一个请求链路，包含：网关服务server-gateway耗时63.190毫秒，订单微服务server-order耗时53.101毫秒，用户微服务server-user耗时14.640毫秒，商品微服务server-product耗时10.941毫秒。 （5）点开ZipKin界面上显示的调用链路，如下所示。 点开后的界面如下所示。 可以非常清晰的看到整个调用的访问链路。 我们还可以点击具体的节点来查看具体的调用信息。 例如我们点击网关微服务查看网关的具体链路，如下所示。 点开后的效果如下所示。 接下来，查看下订单微服务的调用链路具体信息，如下所示。 点开后的效果如下所示。 可以看到，通过ZipKin能够查看服务的调用链路，并且能够查看具体微服务的调用情况。我们可以基于ZipKin来分析系统的调用链路情况，找出系统的瓶颈点，进而进行针对性的优化。 另外，ZipKin中也支持下载系统调用链路的Json数据，如下所示。 点击JSON按钮后，效果如下所示。 其中，显示的Json数据如下所示。 [ [ { &quot;traceId&quot;: &quot;9d244edbc1668d92&quot;, &quot;parentId&quot;: &quot;3f01ba499fac4ce9&quot;, &quot;id&quot;: &quot;5f0932b5d06fe757&quot;, &quot;kind&quot;: &quot;SERVER&quot;, &quot;name&quot;: &quot;get /get/{pid}&quot;, &quot;timestamp&quot;: 1652413758790051, &quot;duration&quot;: 10941, &quot;localEndpoint&quot;: { &quot;serviceName&quot;: &quot;server-product&quot;, &quot;ipv4&quot;: &quot;192.168.0.111&quot; }, &quot;remoteEndpoint&quot;: { &quot;ipv4&quot;: &quot;192.168.0.111&quot;, &quot;port&quot;: 54140 }, &quot;tags&quot;: { &quot;http.method&quot;: &quot;GET&quot;, &quot;http.path&quot;: &quot;/product/get/1001&quot;, &quot;mvc.controller.class&quot;: &quot;ProductController&quot;, &quot;mvc.controller.method&quot;: &quot;getProduct&quot; }, &quot;shared&quot;: true }, { &quot;traceId&quot;: &quot;9d244edbc1668d92&quot;, &quot;parentId&quot;: &quot;3f01ba499fac4ce9&quot;, &quot;id&quot;: &quot;c020c7f6e0fa1604&quot;, &quot;kind&quot;: &quot;SERVER&quot;, &quot;name&quot;: &quot;get /update_count/{pid}/{count}&quot;, &quot;timestamp&quot;: 1652413758808052, &quot;duration&quot;: 5614, &quot;localEndpoint&quot;: { &quot;serviceName&quot;: &quot;server-product&quot;, &quot;ipv4&quot;: &quot;192.168.0.111&quot; }, &quot;remoteEndpoint&quot;: { &quot;ipv4&quot;: &quot;192.168.0.111&quot;, &quot;port&quot;: 54140 }, &quot;tags&quot;: { &quot;http.method&quot;: &quot;GET&quot;, &quot;http.path&quot;: &quot;/product/update_count/1001/1&quot;, &quot;mvc.controller.class&quot;: &quot;ProductController&quot;, &quot;mvc.controller.method&quot;: &quot;updateCount&quot; }, &quot;shared&quot;: true }, { &quot;traceId&quot;: &quot;9d244edbc1668d92&quot;, &quot;parentId&quot;: &quot;9d244edbc1668d92&quot;, &quot;id&quot;: &quot;3f01ba499fac4ce9&quot;, &quot;kind&quot;: &quot;CLIENT&quot;, &quot;name&quot;: &quot;get&quot;, &quot;timestamp&quot;: 1652413758763816, &quot;duration&quot;: 54556, &quot;localEndpoint&quot;: { &quot;serviceName&quot;: &quot;server-gateway&quot;, &quot;ipv4&quot;: &quot;192.168.0.111&quot; }, &quot;remoteEndpoint&quot;: { &quot;ipv4&quot;: &quot;192.168.0.111&quot;, &quot;port&quot;: 8080 }, &quot;tags&quot;: { &quot;http.method&quot;: &quot;GET&quot;, &quot;http.path&quot;: &quot;/order/submit_order&quot; } }, { &quot;traceId&quot;: &quot;9d244edbc1668d92&quot;, &quot;parentId&quot;: &quot;9d244edbc1668d92&quot;, &quot;id&quot;: &quot;475ff483fb0973b1&quot;, &quot;kind&quot;: &quot;CLIENT&quot;, &quot;name&quot;: &quot;get&quot;, &quot;timestamp&quot;: 1652413758759023, &quot;duration&quot;: 59621, &quot;localEndpoint&quot;: { &quot;serviceName&quot;: &quot;server-gateway&quot;, &quot;ipv4&quot;: &quot;192.168.0.111&quot; }, &quot;tags&quot;: { &quot;http.method&quot;: &quot;GET&quot;, &quot;http.path&quot;: &quot;/order/submit_order&quot; } }, { &quot;traceId&quot;: &quot;9d244edbc1668d92&quot;, &quot;id&quot;: &quot;9d244edbc1668d92&quot;, &quot;kind&quot;: &quot;SERVER&quot;, &quot;name&quot;: &quot;get&quot;, &quot;timestamp&quot;: 1652413758757034, &quot;duration&quot;: 63190, &quot;localEndpoint&quot;: { &quot;serviceName&quot;: &quot;server-gateway&quot;, &quot;ipv4&quot;: &quot;192.168.0.111&quot; }, &quot;remoteEndpoint&quot;: { &quot;ipv4&quot;: &quot;127.0.0.1&quot;, &quot;port&quot;: 54137 }, &quot;tags&quot;: { &quot;http.method&quot;: &quot;GET&quot;, &quot;http.path&quot;: &quot;/server-order/order/submit_order&quot; } }, { &quot;traceId&quot;: &quot;9d244edbc1668d92&quot;, &quot;parentId&quot;: &quot;3f01ba499fac4ce9&quot;, &quot;id&quot;: &quot;a048eda8d5fd3dc9&quot;, &quot;kind&quot;: &quot;CLIENT&quot;, &quot;name&quot;: &quot;get&quot;, &quot;timestamp&quot;: 1652413758774201, &quot;duration&quot;: 12054, &quot;localEndpoint&quot;: { &quot;serviceName&quot;: &quot;server-order&quot;, &quot;ipv4&quot;: &quot;192.168.0.111&quot; }, &quot;tags&quot;: { &quot;http.method&quot;: &quot;GET&quot;, &quot;http.path&quot;: &quot;/user/get/1001&quot; } }, { &quot;traceId&quot;: &quot;9d244edbc1668d92&quot;, &quot;parentId&quot;: &quot;3f01ba499fac4ce9&quot;, &quot;id&quot;: &quot;5f0932b5d06fe757&quot;, &quot;kind&quot;: &quot;CLIENT&quot;, &quot;name&quot;: &quot;get&quot;, &quot;timestamp&quot;: 1652413758787924, &quot;duration&quot;: 12557, &quot;localEndpoint&quot;: { &quot;serviceName&quot;: &quot;server-order&quot;, &quot;ipv4&quot;: &quot;192.168.0.111&quot; }, &quot;tags&quot;: { &quot;http.method&quot;: &quot;GET&quot;, &quot;http.path&quot;: &quot;/product/get/1001&quot; } }, { &quot;traceId&quot;: &quot;9d244edbc1668d92&quot;, &quot;parentId&quot;: &quot;3f01ba499fac4ce9&quot;, &quot;id&quot;: &quot;c020c7f6e0fa1604&quot;, &quot;kind&quot;: &quot;CLIENT&quot;, &quot;name&quot;: &quot;get&quot;, &quot;timestamp&quot;: 1652413758805787, &quot;duration&quot;: 7031, &quot;localEndpoint&quot;: { &quot;serviceName&quot;: &quot;server-order&quot;, &quot;ipv4&quot;: &quot;192.168.0.111&quot; }, &quot;tags&quot;: { &quot;http.method&quot;: &quot;GET&quot;, &quot;http.path&quot;: &quot;/product/update_count/1001/1&quot; } }, { &quot;traceId&quot;: &quot;9d244edbc1668d92&quot;, &quot;parentId&quot;: &quot;9d244edbc1668d92&quot;, &quot;id&quot;: &quot;3f01ba499fac4ce9&quot;, &quot;kind&quot;: &quot;SERVER&quot;, &quot;name&quot;: &quot;get /submit_order&quot;, &quot;timestamp&quot;: 1652413758765048, &quot;duration&quot;: 53101, &quot;localEndpoint&quot;: { &quot;serviceName&quot;: &quot;server-order&quot;, &quot;ipv4&quot;: &quot;192.168.0.111&quot; }, &quot;remoteEndpoint&quot;: { &quot;ipv4&quot;: &quot;127.0.0.1&quot; }, &quot;tags&quot;: { &quot;http.method&quot;: &quot;GET&quot;, &quot;http.path&quot;: &quot;/order/submit_order&quot;, &quot;mvc.controller.class&quot;: &quot;OrderController&quot;, &quot;mvc.controller.method&quot;: &quot;submitOrder&quot; }, &quot;shared&quot;: true }, { &quot;traceId&quot;: &quot;9d244edbc1668d92&quot;, &quot;parentId&quot;: &quot;3f01ba499fac4ce9&quot;, &quot;id&quot;: &quot;a048eda8d5fd3dc9&quot;, &quot;kind&quot;: &quot;SERVER&quot;, &quot;name&quot;: &quot;get /get/{uid}&quot;, &quot;timestamp&quot;: 1652413758777073, &quot;duration&quot;: 14640, &quot;localEndpoint&quot;: { &quot;serviceName&quot;: &quot;server-user&quot;, &quot;ipv4&quot;: &quot;192.168.0.111&quot; }, &quot;remoteEndpoint&quot;: { &quot;ipv4&quot;: &quot;192.168.0.111&quot;, &quot;port&quot;: 54139 }, &quot;tags&quot;: { &quot;http.method&quot;: &quot;GET&quot;, &quot;http.path&quot;: &quot;/user/get/1001&quot;, &quot;mvc.controller.class&quot;: &quot;UserController&quot;, &quot;mvc.controller.method&quot;: &quot;getUser&quot; }, &quot;shared&quot;: true } ] ] 小伙伴们也可以根据Json数据分析下系统的调用链路。 ZipKin数据持久化 我们实现了在项目中集成ZipKin，但是此时我们集成ZipKin后，ZipKin中的数据是保存在系统内存中的，如果我们重启了ZipKin，则保存在系统内存中的数据就会丢失，那我如何避免数据丢失呢？ZipKin支持将数据进行持久化来防止数据丢失，可以将数据保存到ElasticSearch、Cassandra或者MySQL中。这里，我们重点介绍下如何将数据保存到MySQL和ElasticSearch中。 ZipKin数据持久化到MySQL （1）将Zipkin数据持久化到MySQL，我们需要知道MySQL的数据表结构，好在ZipKin提供了MySQL脚本，小伙伴们可以在链接：https://github.com/openzipkin/zipkin/tree/master/zipkin-storage里面下载。 当然，我将下载后的MySQL脚本放到了网关服务shop-gateway的resources目录下的scripts目录下。 （2）在MySQL数据库中新建zipkin数据库，如下所示。 create database if not exists zipkin; （3）在新建的数据库zipkin中运行mysql.sql脚本，运行脚本后的效果如下所示。 可以看到，在zipkin数据库中新建了zipkin_annotations、zipkin_dependencies和zipkin_spans三张数据表。 （4）启动ZipKin时指定MySQL数据源，如下所示。 java -jar zipkin-server-2.12.9-exec.jar --STORAGE_TYPE=mysql --MYSQL_HOST=127.0.0.1 --MYSQL_TCP_PORT=3306 --MYSQL_DB=zipkin --MYSQL_USER=root --MYSQL_PASS=root （5）启动ZipKin后，在浏览器中访问链接http://localhost:10001/server-order/order/submit_order?userId=1001&amp;productId=1001&amp;count=1，如下所示。 （6）查看zipkin数据库中的数据，发现zipkin_annotations数据表与zipkin_spans数据表已经存在系统的调用链路数据。 zipkin_annotations数据表部分数据如下所示。 zipkin_spans数据表部分数据如下所示。 可以看到，ZipKin已经将数据持久化到MySQL中，重启ZipKin后就会从MySQL中读取数据，数据也不会丢失了。 ZipKin数据持久化到ElasticSearch （1）到ElasticSearch官网下载ElasticSearch，链接为： https://www.elastic.co/cn/downloads/elasticsearch。 这里下载的安装包是：elasticsearch-8.2.0-windows-x86_64.zip。 （2）解压elasticsearch-8.2.0-windows-x86_64.zip，在解压后的bin目录下找到elasticsearch.bat脚本，双击运行ElasticSearch。 （3）启动ZipKin服务端时，指定ElasticSearch，如下所示。 java -jar zipkin-server-2.12.9-exec.jar --STORAGE_TYPE=elasticsearch --ESHOST=localhost:9200 （4）启动ZipKin服务端后，在浏览器中访问链接http://localhost:10001/server-order/order/submit_order?userId=1001&amp;productId=1001&amp;count=1，如下所示。 ZipKin就会将请求的链路信息保存到ElasticSearch中进行持久化。 ","link":"https://tianxiawuhao.github.io/OTKk-Bytc/"},{"title":"Sleuth概述","content":"Sleuth概述 Sleuth是SpringCloud中提供的一个分布式链路追踪组件，在设计上大量参考并借用了Google Dapper的设计。 Span简介 Span在Sleuth中代表一组基本的工作单元，当请求到达各个微服务时，Sleuth会通过一个唯一的标识，也就是SpanId来标记开始通过这个微服务，在当前微服务中执行的具体过程和执行结束。 此时，通过SpanId标记的开始时间戳和结束时间戳，就能够统计到当前Span的调用时间，也就是当前微服务的执行时间。另外，也可以用过Span获取到事件的名称，请求的信息等数据。 总结：远程调用和Span是一对一的关系，是分布式链路追踪中最基本的工作单元，每次发送一个远程调用服务就会产生一个 Span。Span Id 是一个 64 位的唯一 ID，通过计算 Span 的开始和结束时间，就可以统计每个服务调用所耗费的时间。 Trace简介 Trace的粒度比Span的粒度大，Trace主要是由具有一组相同的Trace ID的Span组成的，从请求进入分布式系统入口经过调用各个微服务直到返回的整个过程，都是一个Trace。 也就是说，当请求到达分布式系统的入口时，Sleuth会为请求创建一个唯一标识，这个唯一标识就是Trace Id，不管这个请求在分布式系统中如何流转，也不管这个请求在分布式系统中调用了多少个微服务，这个Trace Id都是不变的，直到整个请求返回。 总结：一个 Trace 可以对应多个 Span，Trace和Span是一对多的关系。Trace Id是一个64 位的唯一ID。Trace Id可以将进入分布式系统入口经过调用各个微服务，再到返回的整个过程的请求串联起来，内部每通过一次微服务时，都会生成一个新的SpanId。Trace串联了整个请求链路，而Span在请求链路中区分了每个微服务。 Annotation简介 Annotation记录了一段时间内的事件，内部使用的重要注解如下所示。 cs（Client Send）客户端发出请求，标记整个请求的开始时间。 sr（Server Received）服务端收到请求开始进行处理，通过sr与cs可以计算网络的延迟时间，例如：sr－ cs = 网络延迟（服务调用的时间）。 ss（Server Send）服务端处理完毕准备将结果返回给客户端， 通过ss与sr可以计算服务器上的请求处理时间，例如：ss - sr = 服务器上的请求处理时间。 cr（Client Reveived）客户端收到服务端的响应，请求结束。通过cr与cs可以计算请求的总时间，例如：cr - cs = 请求的总时间。 总结：链路追踪系统内部定义了少量核心注解，用来定义一个请求的开始和结束，通过这些注解，我们可以计算出请求的每个阶段的时间。需要注解的是，这里说的请求，是在系统内部流转的请求，而不是从浏览器、APP、H5、小程序等发出的请求。 项目整合Sleuth Sleuth提供了分布式链路追踪能力，如果需要使用Sleuth的链路追踪功能，需要在项目中集成Sleuth。 最简使用 （1）在每个微服务（用户微服务shop-user、商品微服务shop-product、订单微服务shop-order、网关服务shop-gateway）下的pom.xml文件中添加如下Sleuth的依赖。 &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-sleuth&lt;/artifactId&gt; &lt;/dependency&gt; （2）将项目的application.yml文件备份成application-pre-filter.yml，并将application.yml文件的内容替换为application-sentinel.yml文件的内容，这一步是为了让整个项目集成Sentinel、SpringCloud Gateway和Nacos。application.yml替换后的文件内容如下所示。 server: port: 10002 spring: application: name: server-gateway cloud: nacos: discovery: server-addr: 127.0.0.1:8848 sentinel: transport: port: 7777 dashboard: 127.0.0.1:8888 web-context-unify: false eager: true gateway: globalcors: cors-configurations: '[/**]': allowedOrigins: &quot;*&quot; allowedMethods: &quot;*&quot; allowCredentials: true allowedHeaders: &quot;*&quot; discovery: locator: enabled: true route-id-prefix: gateway- （3）分别启动Nacos、Sentinel、用户微服务shop-user，商品微服务shop-product，订单微服务shop-order和网关服务shop-gateway，在浏览器中输入链接localhost:10001/server-order/order/submit_order?userId=1001&amp;productId=1001&amp;count=1，如下所示。 （4）分别查看用户微服务shop-user，商品微服务shop-product，订单微服务shop-order和网关服务shop-gateway的控制台输出，每个服务的控制台都输出了如下格式所示的信息。 [微服务名称,TraceID,SpanID,是否将结果输出到第三方平台] 具体如下所示。 用户微服务shop-user [server-user,03fef3d312450828,76b298dba54ec579,true] 商品微服务shop-product [server-product,03fef3d312450828,41ac8836d2df4eec,true] [server-product,03fef3d312450828,6b7b3662d63372bf,true] 订单微服务shop-order [server-order,03fef3d312450828,cbd935d57cae84f9,true] 网关服务shop-gateway [server-gateway,03fef3d312450828,03fef3d312450828,true] 可以看到，每个服务都打印出了链路追踪的日志信息，说明引入Sleuth的依赖后，就可以在命令行查看链路追踪情况。 抽样采集数据 Sleuth支持抽样采集数据。尤其是在高并发场景下，如果采集所有的数据，那么采集的数据量就太大了，非常耗费系统的性能。通常的做法是可以减少一部分数据量，特别是对于采用Http方式去发送采集数据，能够提升很大的性能。 Sleuth可以采用如下方式配置抽样采集数据。 spring: sleuth: sampler: probability: 1.0 追踪自定义线程池 Sleuth支持对异步任务的链路追踪，在项目中使用@Async注解开启一个异步任务后，Sleuth会为异步任务重新生成一个Span。但是如果使用了自定义的异步任务线程池，则会导致Sleuth无法新创建一个Span，而是会重新生成Trace和Span。此时，需要使用Sleuth提供的LazyTraceExecutor类来包装下异步任务线程池，才能在异步任务调用链路中重新创建Span。 在服务中开启异步线程池任务，需要使用@EnableAsync。所以，在演示示例前，先在用户微服务shop-user的io.binghe.shop.UserStarter启动类上添加@EnableAsync注解，如下所示。 /** * @author binghe * @version 1.0.0 * @description 启动用户服的类 */ @SpringBootApplication @EnableTransactionManagement(proxyTargetClass = true) @MapperScan(value = { &quot;io.binghe.shop.user.mapper&quot; }) @EnableDiscoveryClient @EnableAsync public class UserStarter { public static void main(String[] args){ SpringApplication.run(UserStarter.class, args); } } 演示使用@Async注解开启任务 （1）在用户微服务shop-user的io.binghe.shop.user.service.UserService接口中定义一个asyncMethod()方法，如下所示。 void asyncMethod(); （2）在用户微服务shop-user的io.binghe.shop.user.service.impl.UserServiceImpl类中实现asyncMethod()方法，并在asyncMethod()方法上添加@Async注解，如下所示。 @Async @Override public void asyncMethod() { log.info(&quot;执行了异步任务...&quot;); } （3）在用户微服务shop-user的io.binghe.shop.user.controller.UserController类中新增asyncApi()方法，如下所示。 @GetMapping(value = &quot;/async/api&quot;) public String asyncApi() { log.info(&quot;执行异步任务开始...&quot;); userService.asyncMethod(); log.info(&quot;异步任务执行结束...&quot;); return &quot;asyncApi&quot;; } （4）分别启动用户微服务和网关服务，在浏览器中输入链接http://localhost:10001/server-user/user/async/api （5）查看用户微服务与网关服务的控制台日志，分别存在如下日志。 用户微服务 [server-user,499d6c7128399ed0,a81bd920de0b07de,true]执行异步任务开始... [server-user,499d6c7128399ed0,a81bd920de0b07de,true]异步任务执行结束... [server-user,499d6c7128399ed0,e2f297d512f40bb8,true]执行了异步任务... 网关服务 [server-gateway,499d6c7128399ed0,499d6c7128399ed0,true] 可以看到Sleuth为异步任务重新生成了Span。 演示自定义任务线程池 在演示使用@Async注解开启任务的基础上继续演示自定义任务线程池，验证Sleuth是否为自定义线程池新创建了Span。 （1）在用户微服务shop-user中新建io.binghe.shop.user.config包，在包下创建ThreadPoolTaskExecutorConfig类，继承org.springframework.scheduling.annotation.AsyncConfigurerSupport类，用来自定义异步任务线程池，代码如下所示。 /** * @author binghe * @version 1.0.0 * @description Sleuth异步线程池配置 */ @Configuration @EnableAutoConfiguration public class ThreadPoolTaskExecutorConfig extends AsyncConfigurerSupport { @Override public Executor getAsyncExecutor() { ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor(); executor.setCorePoolSize(2); executor.setMaxPoolSize(5); executor.setQueueCapacity(10); executor.setThreadNamePrefix(&quot;trace-thread-&quot;); executor.initialize(); return executor; } } （2）以debug的形式启动用户微服务和网关服务，并在io.binghe.shop.user.config.ThreadPoolTaskExecutorConfig#getAsyncExecutor()方法中打上断点，如下所示。 可以看到，项目启动后并没有进入io.binghe.shop.user.config.ThreadPoolTaskExecutorConfig#getAsyncExecutor()方法，说明项目启动时，并不会创建异步任务线程池。 （3）在浏览器中输入链接http://localhost:10001/server-user/user/async/api，此时可以看到程序已经执行到io.binghe.shop.user.config.ThreadPoolTaskExecutorConfig#getAsyncExecutor()方法的断点位置。 说明异步任务线程池是在调用了异步任务的时候创建的。 接下来，按F8跳过断点继续运行程序，可以看到浏览器上的显示结果如下。 （4）查看用户微服务与网关服务的控制台日志，分别存在如下日志。 用户微服务 [server-user,f89f2355ec3f9df1,4d679555674e96a4,true]执行异步任务开始... [server-user,f89f2355ec3f9df1,4d679555674e96a4,true]异步任务执行结束... [server-user,0ee48d47e58e2a42,0ee48d47e58e2a42,true]执行了异步任务... 网关服务 [server-gateway,f89f2355ec3f9df1,f89f2355ec3f9df1,true] 可以看到，使用自定义异步任务线程池时，在用户微服务中在执行异步任务时，重新生成了Trace和Span。 注意对比用户微服务中输出的三条日志信息，最后一条日志信息的TraceID和SpanID与前两条日志都不同。 演示包装自定义线程池 在自定义任务线程池的基础上继续演示包装自定义线程池，验证Sleuth是否为包装后的自定义线程池新创建了Span。 （1）在用户微服务shop-user的io.binghe.shop.user.config.ThreadPoolTaskExecutorConfig类中注入BeanFactory，并在getAsyncExecutor()方法中使用org.springframework.cloud.sleuth.instrument.async.LazyTraceExecutor()来包装返回的异步任务线程池，修改后的io.binghe.shop.user.config.ThreadPoolTaskExecutorConfig类的代码如下所示。 /** * @author binghe * @version 1.0.0 * @description Sleuth异步线程池配置 */ @Configuration @EnableAutoConfiguration public class ThreadPoolTaskExecutorConfig extends AsyncConfigurerSupport { @Autowired private BeanFactory beanFactory; @Override public Executor getAsyncExecutor() { ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor(); executor.setCorePoolSize(2); executor.setMaxPoolSize(5); executor.setQueueCapacity(10); executor.setThreadNamePrefix(&quot;trace-thread-&quot;); executor.initialize(); return new LazyTraceExecutor(this.beanFactory, executor); } } （2）分别启动用户微服务和网关服务，在浏览器中输入链接http://localhost:10001/server-user/user/async/api （3）查看用户微服务与网关服务的控制台日志，分别存在如下日志。 用户微服务 [server-user,157891cb90fddb65,0a278842776b1f01,true]执行异步任务开始... [server-user,157891cb90fddb65,0a278842776b1f01,true]异步任务执行结束... [server-user,157891cb90fddb65,1ba55fd3432b77ae,true]执行了异步任务... 网关服务 [server-gateway,157891cb90fddb65,157891cb90fddb65,true] 可以看到Sleuth为异步任务重新生成了Span。 综上说明：Sleuth支持对异步任务的链路追踪，在项目中使用@Async注解开启一个异步任务后，Sleuth会为异步任务重新生成一个Span。但是如果使用了自定义的异步任务线程池，则会导致Sleuth无法新创建一个Span，而是会重新生成Trace和Span。此时，需要使用Sleuth提供的LazyTraceExecutor类来包装下异步任务线程池，才能在异步任务调用链路中重新创建Span。 自定义链路过滤器 在Sleuth中存在链路过滤器，并且还支持自定义链路过滤器。 自定义链路过滤器概述 TracingFilter是Sleuth中负责处理请求和响应的组件，可以通过注册自定义的TracingFilter实例来实现一些扩展性的需求。 演示自定义链路过滤器 本案例演示通过过滤器验证只有HTTP或者HTTPS请求才能访问接口，并且在访问的链接不是静态文件时，将traceId放入HttpRequest中在服务端获取，并在响应结果中添加自定义Header，名称为SLEUTH-HEADER，值为traceId。 （1）在用户微服务shop-user中新建io.binghe.shop.user.filter包，并创建MyGenericFilter类，继承org.springframework.web.filter.GenericFilterBean类，代码如下所示。 /** * @author binghe * @version 1.0.0 * @description 链路过滤器 */ @Component @Order( Ordered.HIGHEST_PRECEDENCE + 6) public class MyGenericFilter extends GenericFilterBean{ private Pattern skipPattern = Pattern.compile(SleuthWebProperties.DEFAULT_SKIP_PATTERN); private final Tracer tracer; public MyGenericFilter(Tracer tracer){ this.tracer = tracer; } @Override public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException { if (!(request instanceof HttpServletRequest) || !(response instanceof HttpServletResponse)){ throw new ServletException(&quot;只支持HTTP访问&quot;); } Span currentSpan = this.tracer.currentSpan(); if (currentSpan == null) { chain.doFilter(request, response); return; } HttpServletRequest httpServletRequest = (HttpServletRequest) request; HttpServletResponse httpServletResponse = ((HttpServletResponse) response); boolean skipFlag = skipPattern.matcher(httpServletRequest.getRequestURI()).matches(); if (!skipFlag){ String traceId = currentSpan.context().traceIdString(); httpServletRequest.setAttribute(&quot;traceId&quot;, traceId); httpServletResponse.addHeader(&quot;SLEUTH-HEADER&quot;, traceId); } chain.doFilter(httpServletRequest, httpServletResponse); } } （2）在用户微服务shop-user的io.binghe.shop.user.controller.UserController类中新建sleuthFilter()方法，在sleuthFilter()方法中获取并打印traceId，如下所示。 @GetMapping(value = &quot;/sleuth/filter/api&quot;) public String sleuthFilter(HttpServletRequest request) { Object traceIdObj = request.getAttribute(&quot;traceId&quot;); String traceId = traceIdObj == null ? &quot;&quot; : traceIdObj.toString(); log.info(&quot;获取到的traceId为: &quot; + traceId); return &quot;sleuthFilter&quot;; } （3）分别启动用户微服务和网关服务，在浏览器中输入http://localhost:10001/server-user/user/sleuth/filter/api，如下所示。 查看用户微服务的控制台会输出如下信息。 获取到的traceId为: f63ae7702f6f4bba 查看浏览器的控制台，看到在响应的结果信息中新增了一个名称为SLEUTH-HEADER，值为f63ae7702f6f4bba的Header，如下所示。 说明使用Sleuth的过滤器可以处理请求和响应信息，并且可以在Sleuth的过滤器中获取到TraceID。 ","link":"https://tianxiawuhao.github.io/ZbHW6UCYN/"},{"title":"分布式链路追踪","content":"随着互联网的不断发展，企业的业务系统变得越来越复杂，原本单一的单体应用系统已经无法满足企业业务发展的需要。于是，很多企业开始了对项目的分布式与微服务改造，新项目也在开始的时候就会采用分布式与微服务的架构模式。 一个系统采用分布式与微服务架构后，会被拆分成许多服务模块，这些服务模块之间的调用关系错综复杂，对于客户端请求的分析与处理就会显得异常复杂。此时，就需要一种技术来解决这些问题，而这种技术就是分布式链路追踪技术。 分布式链路追踪 随着互联网业务快速扩展，企业的业务系统变得越来越复杂，不少企业开始向分布式、微服务方向发展，将原本的单体应用拆分成分布式、微服务。这也使得当客户端请求系统的接口时，原本在同一个系统内部的请求逻辑变成了需要在多个微服务之间流转的请求。 单体架构中可以使用AOP在调用具体的业务逻辑前后分别打印一下时间即可计算出整体的调用时间，使用 AOP捕获异常也可知道是哪里的调用导致的异常。 但是在分布式微服务场景下，使用AOP技术是无法追踪到各个微服务的调用情况的，也就无法知道系统中处理一次请求的整体调用链路。 另外，在分布式与微服务场景下，我们需要解决如下问题： 如何快速发现并定位到分布式系统中的问题。 如何尽可能精确的判断故障对系统的影响范围与影响程度。 如何尽可能精确的梳理出服务之间的依赖关系，并判断出服务之间的依赖关系是否合理。 如何尽可能精确的分析整个系统调用链路的性能与瓶颈点。 如何尽可能精确的分析系统的存储瓶颈与容量规划。 如何实时观测系统的整体调用链路情况。 上述问题就是分布式链路追踪技术要解决的问题。所谓的分布式链路追踪，就是将对分布式系统的一次请求转化成一个完整的调用链路。这个完整的调用链路从请求进入分布式系统的入口开始，直到整个请求返回为止。并在请求调用微服务的过程中，记录相应的调用日志，监控系统调用的性能，并且可以按照某种方式显示请求调用的情况。 在分布式链路追踪中，可以统计调用每个微服务的耗时，请求会经过哪些微服务的流转，每个微服务的运行状况等信息。 核心原理 假定三个微服务调用的链路如下图所示：Service 1 调用 Service 2，Service 2 调用 Service 3 和 Service 4。 那么链路追踪会在每个服务调用的时候加上 Trace ID 和 Span ID。如下图所示： 小伙伴注意上面的颜色，相同颜色的代表是同一个 Span ID，说明是链路追踪中的一个节点。 第一步：用户端调用 Service 1，生成一个 Request，Trace ID 和 Span ID 为空，那个时候请求还没有到 Service 1。 第二步：请求到达 Service 1，记录了 Trace ID = X，Span ID 等于 A。 第三步：Service 1 发送请求给 Service 2，Span ID 等于 B，被称作 Client Sent，即用户端发送一个请求。 第四步：请求到达 Service 2，Span ID 等于 B，Trace ID 不会改变，被称作 Server Received，即服务端取得请求并准备开始解决它。 第五步：Service 2 开始解决这个请求，解决完之后，Trace ID 不变，Span ID = C。 第六步：Service 2 开始发送这个请求给 Service 3，Trace ID 不变，Span ID = D，被称作 Client Sent，即用户端发送一个请求。 第七步：Service 3 接收到这个请求，Span ID = D，被称作 Server Received。 第八步：Service 3 开始解决这个请求，解决完之后，Span ID = E。 第九步：Service 3 开始发送响应给 Service 2，Span ID = D，被称作 Server Sent，即服务端发送响应。 第十步：Service 3 收到 Service 2 的响应，Span ID = D，被称作 Client Received，即用户端接收响应。 第十一步：Service 2 开始返回 响应给 Service 1，Span ID = B，和第三步的 Span ID 相同，被称作 Client Received，即用户端接收响应。 第十二步：Service 1 解决完响应，Span ID = A，和第二步的 Span ID 相同。 第十三步：Service 1 开始向用户端返回响应，Span ID = A、 Service 3 向 Service 4 发送请求和 Service 3 相似，对应的 Span ID 是 F 和 G。可以参照上面前面的第六步到第十步。 把以上的相同颜色的步骤简化为下面的链路追踪图： 第一个节点：Span ID = A，Parent ID = null，Service 1 接收到请求。 第二个节点：Span ID = B，Parent ID= A，Service 1 发送请求到 Service 2 返回响应给Service 1 的过程。 第三个节点：Span ID = C，Parent ID= B，Service 2 的 中间解决过程。 第四个节点：Span ID = D，Parent ID= C，Service 2 发送请求到 Service 3 返回响应给Service 2 的过程。 第五个节点：Span ID = E，Parent ID= D，Service 3 的中间解决过程。 第六个节点：Span ID = F，Parent ID= C，Service 3 发送请求到 Service 4 返回响应给 Service 3 的过程。 第七个节点：Span ID = G，Parent ID= F，Service 4 的中间解决过程。 通过 Parent ID 就可找到父节点，整个链路即可以进行跟踪追溯了。 备注：核心原理部分内容来源：cnblogs.com/jackson0714/p/sleuth_zipkin.html 解决方案 目前，行业内比较成熟的分布式链路追踪技术解决方案如下所示。 技术 说明 Cat 由大众点评开源，基于Java开发的实时应用监控平台，包括实时应用监控，业务监控 。集成方案是通过代码埋点的方式来实现监控，比如：拦截器，过滤器等。对代码的侵入性很大，集成成本较高。风险较大。 ZipKin 由Twitter公司开源，开放源代码分布式的跟踪系统，用于收集服务的定时数据，以解决微服务架构中的延迟问题，包括：数据的收集、存储、查找和展现。结合spring-cloud-sleuth使用较为简单， 集成方便， 但是功能较简单。 Pinpoint Pinpoint是一款开源的基于字节码注入的调用链分析，以及应用监控分析工具。特点是支持多种插件， UI功能强大，接入端无代码侵入。 Skywalking SkyWalking是国人开源的基于字节码注入的调用链分析，以及应用监控分析工具。特点是支持多种插件， UI功能较强，接入端无代码侵入。 Sleuth Sleuth是SpringCloud中的一个组件，为Spring Cloud实现了分布式跟踪解决方案。 注意：我们后续会使用 Sleuth+ZipKin的方案实现分布式链路追踪。 ","link":"https://tianxiawuhao.github.io/Lq63Gr95h/"},{"title":"网关断言","content":"网关断言 断言的英文是Predicate，也可以翻译成谓词。主要的作用就是进行条件判断，可以在网关中实现多种条件判断，只有所有的判断结果都通过时，也就是所有的条件判断都返回true，才会真正的执行路由功能。 SpringCloud Gateway内置断言 SpringCloud Gateway包括许多内置的断言工厂，所有这些断言都与HTTP请求的不同属性匹配。 基于日期时间类型的断言 基于日期时间类型的断言根据时间做判断，主要有三个： AfterRoutePredicateFactory：接收一个日期时间参数，判断当前请求的日期时间是否晚于指定的日期时间。 BeforeRoutePredicateFactory：接收一个日期时间参数，判断当前请求的日期时间是否早于指定的日期时间。 BetweenRoutePredicateFactory：接收两个日期时间参数，判断当前请求的日期时间是否在指定的时间时间段内。 使用示例 - After=2022-05-10T23:59:59.256+08:00[Asia/Shanghai] 基于远程地址的断言 RemoteAddrRoutePredicateFactory：接收一个IP地址段，判断发出请求的客户端的IP地址是否在指定的IP地址段内。 使用示例 - RemoteAddr=192.168.0.1/24 基于Cookie的断言 CookieRoutePredicateFactory：接收两个参数， Cookie的名称和一个正则表达式。判断请求的Cookie是否具有给定名称且值与正则表达式匹配。 使用示例 - Cookie=name, binghe. 基于Header的断言 HeaderRoutePredicateFactory：接收两个参数，请求Header的名称和正则表达式。判断请求Header中是否具有给定的名称且值与正则表达式匹配。 使用示例 - Header=X-Request-Id, \\d+ 基于Host的断言 HostRoutePredicateFactory：接收一个参数，这个参数通常是主机名或者域名的模式，例如**.binghe.com这种格式。判断发出请求的主机是否满足匹配规则。 使用示例 - Host=**.binghe.com 基于Method请求方法的断言 MethodRoutePredicateFactory：接收一个参数，判断请求的类型是否跟指定的类型匹配，通常指的是请求方式。例如，POST、GET、PUT等请求方式。 使用示例 - Method=GET 基于Path请求路径的断言 PathRoutePredicateFactory：接收一个参数，判断请求的链接地址是否满足路径规则，通常指的是请求的URI部分。 使用示例 - Path=/binghe/{segment} 基于Query请求参数的断言 QueryRoutePredicateFactory ：接收两个参数，请求参数和正则表达式， 判断请求的参数是否具有给定的名称并且参数值是否与正则表达式匹配。 使用示例 - Query=name, binghe. 基于路由权重的断言 WeightRoutePredicateFactory：接收一个[组名,权重]格式的数组，然后对于同一个组内的路由按照权重转发。 使用示例 - id: weight1 uri: http://localhost:8080 predicates: - Path=/api/** - Weight=group1,2 filters: - StripPrefix=1 - id: weight2 uri: http://localhost:8081 predicates: - Path=/api/** - Weight=group1,8 filters: - StripPrefix=1 演示内置断言 在演示的示例中，我们基于Path请求路径的断言判断请求路径是否符合规则，基于远程地址的断言判断请求主机地址是否在地址段中，并且限制请求的方式为GET方式。整个演示的过程以访问用户微服务的接口为例。 （1）由于在开发项目时，所有的服务都是在我本地启动的，首先查看下我本机的IP地址，如下所示。 可以看到，我本机的IP地址为192.168.0.27，属于192.168.0.1/24网段。 （2）在服务网关模块shop-gateway中，将application.yml文件备份成application-sentinel.yml文件，并将application.yml文件中的内容修改成application-simple.yml文件中的内容。接下来，在application.yml文件中的spring.cloud.gateway.routes节点下的- id: user-gateway下面进行断言配置，配置后的结果如下所示。 spring: cloud: gateway: routes: - id: user-gateway uri: http://localhost:8060 order: 1 predicates: - Path=/server-user/** - RemoteAddr=192.168.0.1/24 - Method=GET filters: - StripPrefix=1 注意：完整的配置参见案例完整源代码。 （3）配置完成后启动用户微服务和网关服务，通过网关服务访问用户微服务，在浏览器中输入http://localhost:10001/server-user/user/get/1001，如下所示。 可以看到通过http://localhost:10001/server-user/user/get/1001链接不能正确访问到用户信息。 接下来，在浏览器中输入http://192.168.0.27:10001/server-user/user/get/1001，能够正确获取到用户的信息。 （4）停止网关微服务，将基于远程地址的断言配置成- RemoteAddr=192.168.1.1/24，也就是将基于远程地址的断言配置成与我本机IP地址不在同一个网段，这样就能演示请求主机地址不在地址段中的情况，修改后的基于远程地址的断言配置如下所示。 - RemoteAddr=192.168.1.1/24 （5）重启网关服务，再次在浏览器中输入http://localhost:10001/server-user/user/get/1001，如下所示。 可以看到通过http://localhost:10001/server-user/user/get/1001链接不能正确访问到用户信息。 接下来，在浏览器中输入http://192.168.0.27:10001/server-user/user/get/1001，也不能正确获取到用户的信息了。 自定义断言 SpringCloud Gateway支持自定义断言功能，我们可以在具体业务中，基于SpringCloud Gateway自定义特定的断言功能。 自定义断言概述 SpringCloud Gateway虽然提供了多种内置的断言功能，但是在某些场景下无法满足业务的需要，此时，我们就可以基于SpringCloud Gateway自定义断言功能，以此来满足我们的业务场景。 实现自定义断言 这里，我们基于SpringCloud Gateway实现断言功能，实现后的效果是在服务网关的application.yml文件中的spring.cloud.gateway.routes节点下的- id: user-gateway下面进行如下配置。 spring: cloud: gateway: routes: - id: user-gateway uri: http://localhost:8060 order: 1 predicates: - Path=/server-user/** - Name=binghe filters: - StripPrefix=1 通过服务网关访问用户微服务时，只有在访问的链接后面添加?name=binghe参数时才能正确访问用户微服务。 （1）在网关服务shop-gateway中新建io.binghe.shop.predicate包，在包下新建NameRoutePredicateConfig类，主要定义一个Spring类型的name成员变量，用来接收配置文件中的参数，源码如下所示。 /** * @author binghe * @version 1.0.0 * @description 接收配置文件中的参数 */ @Data public class NameRoutePredicateConfig implements Serializable { private static final long serialVersionUID = -3289515863427972825L; private String name; } （2）实现自定义断言时，需要新建类继承org.springframework.cloud.gateway.handler.predicate.AbstractRoutePredicateFactory类，在io.binghe.shop.predicate包下新建NameRoutePredicateFactory类，继承org.springframework.cloud.gateway.handler.predicate.AbstractRoutePredicateFactory类，并覆写相关的方法，源码如下所示。 /** * @author binghe * @version 1.0.0 * @description 自定义断言功能 */ @Component public class NameRoutePredicateFactory extends AbstractRoutePredicateFactory&lt;NameRoutePredicateConfig&gt; { public NameRoutePredicateFactory() { super(NameRoutePredicateConfig.class); } @Override public Predicate&lt;ServerWebExchange&gt; apply(NameRoutePredicateConfig config) { return (serverWebExchange)-&gt;{ String name = serverWebExchange.getRequest().getQueryParams().getFirst(&quot;name&quot;); if (StringUtils.isEmpty(name)){ name = &quot;&quot;; } return name.equals(config.getName()); }; } @Override public List&lt;String&gt; shortcutFieldOrder() { return Arrays.asList(&quot;name&quot;); } } （3）在服务网关的application.yml文件中的spring.cloud.gateway.routes节点下的- id: user-gateway下面进行如下配置。 spring: cloud: gateway: routes: - id: user-gateway uri: http://localhost:8060 order: 1 predicates: - Path=/server-user/** - Name=binghe filters: - StripPrefix=1 （4）分别启动用户微服务与网关服务，在浏览器中输入http://localhost:10001/server-user/user/get/1001，如下所示。 可以看到，在浏览器中输入http://localhost:10001/server-user/user/get/1001，无法获取到用户信息。 （5）在浏览器中输入http://localhost:10001/server-user/user/get/1001?name=binghe，如下所示。 可以看到，在访问链接后添加?name=binghe参数后，能够正确获取到用户信息。 至此，我们实现了自定义断言功能。 网关过滤器 过滤器可以在请求过程中，修改请求的参数和响应的结果等信息。在生命周期的角度总体上可以分为前置过滤器（Pre）和后置过滤器(Post)。在实现的过滤范围角度可以分为局部过滤器（GatewayFilter）和全局过滤器（GlobalFilter）。局部过滤器作用的范围是某一个路由，全局过滤器作用的范围是全部路由。 Pre前置过滤器：在请求被网关路由之前调用，可以利用这种过滤器实现认证、鉴权、路由等功能，也可以记录访问时间等信息。 Post后置过滤器：在请求被网关路由到微服务之后执行。可以利用这种过滤器修改HTTP的响应Header信息，修改返回的结果数据（例如对于一些敏感的数据，可以在此过滤器中统一处理后返回），收集一些统计信息等。 局部过滤器（GatewayFilter）：也可以称为网关过滤器，这种过滤器主要是作用于单一路由或者某个路由分组。 全局过滤器（GlobalFilter）：这种过滤器主要作用于所有的路由。 局部过滤器 局部过滤器又称为网关过滤器，这种过滤器主要是作用于单一路由或者某个路由分组。 局部过滤器概述 在SpringCloud Gateway中内置了很多不同类型的局部过滤器，主要如下所示。 演示内部过滤器 演示内部过滤器时，我们为原始请求添加一个名称为IP的Header，值为localhost，并添加一个名称为name的参数，参数值为binghe。同时修改响应的结果状态，将结果状态修改为1001。 （1）在服务网关的application.yml文件中的spring.cloud.gateway.routes节点下的- id: user-gateway下面进行如下配置。 spring: cloud: gateway: routes: - id: user-gateway uri: http://localhost:8060 order: 1 predicates: - Path=/server-user/** filters: - StripPrefix=1 - AddRequestHeader=IP,localhost - AddRequestParameter=name,binghe - SetStatus=1001 （2）在用户微服务的io.binghe.shop.user.controller.UserController类中新增apiFilter1()方法，如下所示。 @GetMapping(value = &quot;/api/filter1&quot;) public String apiFilter1(HttpServletRequest request, HttpServletResponse response){ log.info(&quot;访问了apiFilter1接口&quot;); String ip = request.getHeader(&quot;IP&quot;); String name = request.getParameter(&quot;name&quot;); log.info(&quot;ip = &quot; + ip + &quot;, name = &quot; + name); return &quot;apiFilter1&quot;; } 可以看到，在新增加的apiFilter1()方法中，获取到新增加的Header与参数，并将获取出来的参数与Header打印出来。并且方法返回的是字符串apiFilter1。 （3）分别启动用户微服务与网关服务，在浏览器中输入http://localhost:10001/server-user/user/api/filter1，如下所示。 此时，查看浏览器中的响应状态码，如下所示。 可以看到，此时的状态码已经被修改为1001。 接下来，查看下用户微服务的控制台输出的信息，发现在输出的信息中存在如下数据。 访问了apiFilter1接口 ip = localhost, name = binghe 说明使用SpringCloud Gateway的内置过滤器成功为原始请求添加了一个名称为IP的Header，值为localhost，并添加了一个名称为name的参数，参数值为binghe。同时修改了响应的结果状态，将结果状态修改为1001，符合预期效果。 自定义局部过滤器 这里，我们基于SpringCloud Gateway自定义局部过滤器实现是否开启灰度发布的功能，整个实现过程如下所示。 （1）在服务网关的application.yml文件中的spring.cloud.gateway.routes节点下的- id: user-gateway下面进行如下配置。 spring: cloud: gateway: routes: - id: user-gateway uri: http://localhost:8060 order: 1 predicates: - Path=/server-user/** filters: - StripPrefix=1 - Grayscale=true （2）在网关服务模块shop-gateway中新建io.binghe.shop.filter包，在包下新建GrayscaleGatewayFilterConfig类，用于接收配置中的参数，如下所示。 /** * @author binghe * @version 1.0.0 * @description 接收配置参数 */ @Data public class GrayscaleGatewayFilterConfig implements Serializable { private static final long serialVersionUID = 983019309000445082L; private boolean grayscale; } （3）在io.binghe.shop.filter包下GrayscaleGatewayFilterFactory类，继承org.springframework.cloud.gateway.filter.factory.AbstractGatewayFilterFactory类，主要是实现自定义过滤器，模拟实现灰度发布。代码如下所示。 /** * @author binghe * @version 1.0.0 * @description 自定义过滤器模拟实现灰度发布 */ @Component public class GrayscaleGatewayFilterFactory extends AbstractGatewayFilterFactory&lt;GrayscaleGatewayFilterConfig&gt; { public GrayscaleGatewayFilterFactory(){ super(GrayscaleGatewayFilterConfig.class); } @Override public GatewayFilter apply(GrayscaleGatewayFilterConfig config) { return (exchange, chain) -&gt; { if (config.isGrayscale()){ System.out.println(&quot;开启了灰度发布功能...&quot;); }else{ System.out.println(&quot;关闭了灰度发布功能...&quot;); } return chain.filter(exchange); }; } @Override public List&lt;String&gt; shortcutFieldOrder() { return Arrays.asList(&quot;grayscale&quot;); } } （4）分别启动用户微服务和服务网关，在浏览器中输入http://localhost:10001/server-user/user/get/1001，如下所示。 可以看到，通过服务网关正确访问到了用户微服务，并正确获取到了用户信息。 接下来，查看下服务网关的终端，发现已经成功输出了如下信息。 开启了灰度发布功能... 说明正确实现了自定义的局部过滤器。 全局过滤器 全局过滤器是一系列特殊的过滤器，会根据条件应用到所有路由中。 全局过滤器概述 在SpringCloud Gateway中内置了多种不同的全局过滤器，如下所示。 演示全局过滤器 （1）在服务网关模块shop-gateway模块下的io.binghe.shop.config包下新建GatewayFilterConfig类，并在类中配置几个全局过滤器，如下所示。 /** * @author binghe * @version 1.0.0 * @description 网关过滤器配置 */ @Configuration @Slf4j public class GatewayFilterConfig { @Bean @Order(-1) public GlobalFilter globalFilter() { return (exchange, chain) -&gt; { log.info(&quot;执行前置过滤器逻辑&quot;); return chain.filter(exchange).then(Mono.fromRunnable(() -&gt; { log.info(&quot;执行后置过滤器逻辑&quot;); })); }; } } 注意：@Order注解中的数字越小，执行的优先级越高。 （2）启动用户微服务与服务网关，在浏览器中访问http://localhost:10001/server-user/user/get/1001，如下所示。 在服务网关终端输出如下信息。 执行前置过滤器逻辑 执行后置过滤器逻辑 说明我们演示的全局过滤器生效了。 自定义全局过滤器 SpringCloud Gateway内置了很多全局过滤器，一般情况下能够满足实际开发需要，但是对于某些特殊的业务场景，还是需要我们自己实现自定义全局过滤器。 这里，我们就模拟实现一个获取客户端访问信息，并统计访问接口时长的全局过滤器。 （1）在网关服务模块shop-order的io.binghe.shop.filter包下，新建GlobalGatewayLogFilter类，实现org.springframework.cloud.gateway.filter.GlobalFilter接口和org.springframework.core.Ordered接口，代码如下所示。 /** * @author binghe * @version 1.0.0 * @description 自定义全局过滤器，模拟实现获取客户端信息并统计接口访问时长 */ @Slf4j @Component public class GlobalGatewayLogFilter implements GlobalFilter, Ordered { /** * 开始访问时间 */ private static final String BEGIN_VISIT_TIME = &quot;begin_visit_time&quot;; @Override public Mono&lt;Void&gt; filter(ServerWebExchange exchange, GatewayFilterChain chain) { //先记录下访问接口的开始时间 exchange.getAttributes().put(BEGIN_VISIT_TIME, System.currentTimeMillis()); return chain.filter(exchange).then(Mono.fromRunnable(()-&gt;{ Long beginVisitTime = exchange.getAttribute(BEGIN_VISIT_TIME); if (beginVisitTime != null){ log.info(&quot;访问接口主机: &quot; + exchange.getRequest().getURI().getHost()); log.info(&quot;访问接口端口: &quot; + exchange.getRequest().getURI().getPort()); log.info(&quot;访问接口URL: &quot; + exchange.getRequest().getURI().getPath()); log.info(&quot;访问接口URL参数: &quot; + exchange.getRequest().getURI().getRawQuery()); log.info(&quot;访问接口时长: &quot; + (System.currentTimeMillis() - beginVisitTime) + &quot;ms&quot;); } })); } @Override public int getOrder() { return 0; } } 上述代码的实现逻辑还是比较简单的，这里就不再赘述了。 （2）启动用户微服务与网关服务，在浏览器中输入http://localhost:10001/server-user/user/api/filter1?name=binghe，如下所示。 接下来，查看服务网关的终端日志，可以发现已经输出了如下信息。 访问接口主机: localhost 访问接口端口: 10001 访问接口URL: /server-user/user/api/filter1 访问接口URL参数: name=binghe 访问接口时长: 126ms 说明我们自定义的全局过滤器生效了。 ","link":"https://tianxiawuhao.github.io/ipsD-XbrM/"},{"title":"gateway项目整合网关","content":"项目整合网关 我们需要在项目中增加一个服务网关模块shop-gateway，在服务网关模块中实现网关的能力。此时，我们的项目中就会有用户微服务、商品微服务、订单微服务和服务网关。 新建网关模块 在项目中新建shop-gateway模块，新增网关模块后项目的结构如下图所示。 初步整合SpringCloud Gateway （1）在服务网关shop-gateway模块的pom.xml文件中添加如下依赖。 &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-gateway&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; （2）在服务网关shop-gateway模块的resources目录下新建application.yml文件，并在文件中添加如下配置信息。 server: port: 10001 spring: application: name: server-gateway cloud: gateway: globalcors: cors-configurations: '[/**]': allowedOrigins: &quot;*&quot; allowedMethods: &quot;*&quot; allowCredentials: true allowedHeaders: &quot;*&quot; routes: - id: user-gateway uri: http://localhost:8060 order: 1 predicates: - Path=/server-user/** filters: - StripPrefix=1 - id: product-gateway uri: http://localhost:8070 order: 1 predicates: - Path=/server-product/** filters: - StripPrefix=1 - id: order-gateway uri: http://localhost:8080 order: 1 predicates: - Path=/server-order/** filters: - StripPrefix=1 我们重点来看下 spring.cloud.gateway 节点下的配置。 globalcors：此节点下的配置是为了解决SpringCloud Gateway跨域的问题。 routes：表示一个路由数组，可以在此节点下配置多个路由信息。 id：当前路由的唯一标识。 order：路由的优先级，数字越小表示优先级越高。 predicates：网关断言，也就是路由转发的条件，也是一个数组，可以配置多个路由转发条件。 Path：当客户端请求的路径满足Path的规则时，进行路由转发操作。 filters：网关过滤器，在过滤器中可以修改请求的参数和header信息，以及响应的结果和header信息，网关过滤器也是一个数组，可以配置多个过滤规则。 StripPrefix：网关在进行路由转发之前，会去掉1层访问路径。 （3）在服务网关shop-gateway模块的io.binghe.shop包下新建GatewayStarter类，表示服务网关的启动类，源码如下所示。 /** * @version 1.0.0 * @description 服务网关启动类 */ @SpringBootApplication public class GatewayStarter { public static void main(String[] args){ SpringApplication.run(GatewayStarter.class, args); } } （4）由于之前项目中整合了Nacos和Sentinel，所以，在启动项目前，要分别启动Nacos和Sentinel。 进入到Nacos的bin目录下，输入如下命令启动Nacos。 startup.cmd -m standalone 进入Sentinel Jar包所在的目录，输入如下命令启动Sentinel。 java -Dserver.port=8888 -Dcsp.sentinel.dashboard.server=localhost:8888 -Dproject.name=sentinel-dashboard -jar sentinel-dashboard-1.8.4.jar （5）分别启动用户微服务、商品微服务、订单微服务和服务网关。 （6）通过服务网关访问用户微服务，在浏览器中输入http://localhost:10001/server-user/user/get/1001，如下所示。 用户微服务返回的原始数据如下所示。 { &quot;id&quot;: 1001, &quot;username&quot;: &quot;binghe&quot;, &quot;password&quot;: &quot;c26be8aaf53b15054896983b43eb6a65&quot;, &quot;phone&quot;: &quot;13212345678&quot;, &quot;address&quot;: &quot;北京&quot; } 可以看到，通过服务网关能够正确访问到用户微服务。 （7）通过服务网关访问商品微服务，在浏览器中输入http://localhost:10001/server-product/product/get/1001，如下所示。 商品微服务返回的原始数据如下所示。 { &quot;id&quot;: 1001, &quot;proName&quot;: &quot;华为&quot;, &quot;proPrice&quot;: 2399, &quot;proStock&quot;: 100 } 可以看到，通过服务网关能够正确访问到商品微服务。 （8）通过服务网关访问订单微服务，在浏览器中输入http://localhost:10001/server-order/order/test_sentinel，如下所示。 可以看到，通过服务网关能够正确访问到订单微服务。 网关整合Nacos 在初步整合SpringCloud Gateway中，我们在服务网关模块的application.yml文件中硬编码配置了服务转发的地址，如下所示。 硬编码用户微服务地址 uri: http://localhost:8060 硬编码商品微服务地址 uri: http://localhost:8070 硬编码订单微服务地址 uri: http://localhost:8080 这里，我们将网关整合Nacos实现从Nacos注册中心获取转发的服务地址。 （1）在服务网关shop-gateway模块的pom.xml文件中继续添加如下依赖。 &lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt; &lt;/dependency&gt; （2）在服务网关shop-gateway模块的启动类io.binghe.shop.GatewayStarter上添加@EnableDiscoveryClient注解，如下所示。 /** * @version 1.0.0 * @description 服务网关启动类 */ @SpringBootApplication @EnableDiscoveryClient public class GatewayStarter { public static void main(String[] args){ SpringApplication.run(GatewayStarter.class, args); } } （3）将application.yml备份一份，命名为application-simple.yml，并修改application.yml配置文件，修改后的文件如下所示。 server: port: 10001 spring: application: name: server-gateway cloud: nacos: discovery: server-addr: 127.0.0.1:8848 gateway: globalcors: cors-configurations: '[/**]': allowedOrigins: &quot;*&quot; allowedMethods: &quot;*&quot; allowCredentials: true allowedHeaders: &quot;*&quot; discovery: locator: enabled: true routes: - id: user-gateway uri: lb://server-user order: 1 predicates: - Path=/server-user/** filters: - StripPrefix=1 - id: product-gateway uri: lb://server-product order: 1 predicates: - Path=/server-product/** filters: - StripPrefix=1 - id: order-gateway uri: lb://server-order order: 1 predicates: - Path=/server-order/** filters: - StripPrefix=1 上述配置中增加了Nacos相关的配置，如下所示。 spring: cloud: nacos: discovery: server-addr: 127.0.0.1:8848 新增了让SpringCloud Gateway可以发现Nacos中的服务配置，如下所示。 Spring: cloud: gateway: discovery: locator: enabled: true 另外，将硬编码的服务转发地址修改成从Nacos中按照名称获取微服务地址，并按照负载均衡策略分发。 从Nacos中获取用户微服务 uri: lb://server-user 从Nacos中获取商品微服务 uri: lb://server-product 从Nacos中获取订单微服务 uri: lb://server-order 其中，lb指的是从Nacos中按照微服务的名称获取微服务地址，并按照负载均衡的策略分发。使用lb从Nacos中获取微服务时，遵循如下的格式。 lb://微服务名称 微服务的名称就是各个微服务在application.yml文件中配置的服务名称。 spring: application: name: 服务名称 （4）分别启动用户微服务、商品微服务、订单微服务和服务网关。 （5）通过服务网关访问用户微服务，在浏览器中输入http://localhost:10001/server-user/user/get/1001，如下所示。 用户微服务返回的原始数据如下所示。 { &quot;id&quot;: 1001, &quot;username&quot;: &quot;binghe&quot;, &quot;password&quot;: &quot;c26be8aaf53b15054896983b43eb6a65&quot;, &quot;phone&quot;: &quot;13212345678&quot;, &quot;address&quot;: &quot;北京&quot; } 可以看到，通过服务网关能够正确访问到用户微服务。 （6）通过服务网关访问商品微服务，在浏览器中输入http://localhost:10001/server-product/product/get/1001，如下所示。 商品微服务返回的原始数据如下所示。 { &quot;id&quot;: 1001, &quot;proName&quot;: &quot;华为&quot;, &quot;proPrice&quot;: 2399, &quot;proStock&quot;: 100 } 可以看到，通过服务网关能够正确访问到商品微服务。 （7）通过服务网关访问订单微服务，在浏览器中输入http://localhost:10001/server-order/order/test_sentinel，如下所示。 可以看到，通过服务网关能够正确访问到订单微服务。 网关整合Nacos最简配置 SpringCloud Gateway整合Nacos后，可以不用手动指定其他微服务的名称来从Nacos中获取微服务的地址。接下来，我们就来实现SpringCloud Gateway网关整合Nacos的最简配置。 （1）将application.yml备份一份，命名为application-nacos.yml，并修改application.yml配置文件，修改后的文件如下所示。 server: port: 10001 spring: application: name: server-gateway cloud: nacos: discovery: server-addr: 127.0.0.1:8848 gateway: globalcors: cors-configurations: '[/**]': allowedOrigins: &quot;*&quot; allowedMethods: &quot;*&quot; allowCredentials: true allowedHeaders: &quot;*&quot; discovery: locator: enabled: true 可以看到，在application.yml文件中，去掉了spring.cloud.gateway.routes 节点及其下面的所有配置。 （2）分别启动用户微服务、商品微服务、订单微服务和服务网关。 （3）通过服务网关访问用户微服务，在浏览器中输入http://localhost:10001/server-user/user/get/1001，如下所示。 用户微服务返回的原始数据如下所示。 { &quot;id&quot;: 1001, &quot;username&quot;: &quot;binghe&quot;, &quot;password&quot;: &quot;c26be8aaf53b15054896983b43eb6a65&quot;, &quot;phone&quot;: &quot;13212345678&quot;, &quot;address&quot;: &quot;北京&quot; } 可以看到，通过服务网关能够正确访问到用户微服务。 （4）通过服务网关访问商品微服务，在浏览器中输入http://localhost:10001/server-product/product/get/1001，如下所示。 商品微服务返回的原始数据如下所示。 { &quot;id&quot;: 1001, &quot;proName&quot;: &quot;华为&quot;, &quot;proPrice&quot;: 2399, &quot;proStock&quot;: 100 } 可以看到，通过服务网关能够正确访问到商品微服务。 （5）通过服务网关访问订单微服务，在浏览器中输入http://localhost:10001/server-order/order/test_sentinel，如下所示。 可以看到，通过服务网关能够正确访问到订单微服务。 注意：SpringCloud Gateway整合Nacos最简配置时，通过网关访问微服务的格式如下所示。 http(s)://网关IP:网关端口/访问的目标微服务名称/接口地址 网关整合Sentinel限流 Sentinel从1.6.0版本开始，提供了SpringCloud Gateway的适配模块，并且可以提供两种资源维度的限流，一种是route维度；另一种是自定义API分组维度。 route维度：对application.yml文件中配置的spring.cloud.gateway.routes.id限流，并且资源名为spring.cloud.gateway.routes.id对应的值。 自定义API分组维度：利用Sentinel提供的API接口来自定义API分组，并且对这些API分组进行限流。 实现route维度限流 （1）在服务网关shop-gateway模块的pom.xml文件中添加如下依赖。 &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-sentinel&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-alibaba-sentinel-gateway&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-gateway&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba.csp&lt;/groupId&gt; &lt;artifactId&gt;sentinel-spring-cloud-gateway-adapter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; （2）在服务网关shop-gateway模块中新建io.binghe.shop.config包，并在包下新建GatewayConfig类。基于Sentinel 的Gateway限流是通过其提供的Filter来完成的，使用时只需注入对应的SentinelGatewayFilter实例以及 SentinelGatewayBlockExceptionHandler 实例即可。 GatewayConfig类的源代码如下所示。 /** * @version 1.0.0 * @description 网关配置类 */ @Configuration public class GatewayConfig { private final List&lt;ViewResolver&gt; viewResolvers; private final ServerCodecConfigurer serverCodecConfigurer; @Value(&quot;${spring.cloud.gateway.discovery.locator.route-id-prefix}&quot;) private String routeIdPrefix; public GatewayConfig(ObjectProvider&lt;List&lt;ViewResolver&gt;&gt; viewResolversProvider, ServerCodecConfigurer serverCodecConfigurer) { this.viewResolvers = viewResolversProvider.getIfAvailable(Collections::emptyList); this.serverCodecConfigurer = serverCodecConfigurer; } /** * 初始化一个限流的过滤器 */ @Bean @Order(Ordered.HIGHEST_PRECEDENCE) public GlobalFilter sentinelGatewayFilter() { return new SentinelGatewayFilter(); } @PostConstruct public void init() { this.initGatewayRules(); this.initBlockHandlers(); } /** * 配置初始化的限流参数 */ private void initGatewayRules() { Set&lt;GatewayFlowRule&gt; rules = new HashSet&lt;&gt;(); /** * Sentinel整合SpringCloud Gateway使用的API类型为Route ID类型，也就是基于route维度时， * 由于Sentinel为SpringCloud Gateway网关生成的API名称规则如下： * 生成的规则为：${spring.cloud.gateway.discovery.locator.route-id-prefix}后面直接加上目标微服务的名称，如下所示。 * ${spring.cloud.gateway.discovery.locator.route-id-prefix}目标微服务的名称 * 其中，${spring.cloud.gateway.discovery.locator.route-id-prefix}是在yml文件中配置的访问前缀 * * 为了让通过服务网关访问目标微服务链接后，请求链路中生成的API名称与流控规则中生成的API名称一致，以达到启动项目即可实现访问链接的限流效果， * 而无需登录Setinel管理界面手动配置限流规则，可以将 * resource参数设置为${spring.cloud.gateway.discovery.locator.route-id-prefix}目标微服务的名称 * * 当然，如果不按照上述配置，也可以在项目启动后，通过服务网关访问目标微服务链接后，在Sentinel管理界面的请求链路中找到对应的API名称所代表的请求链路， * 然后手动配置限流规则。 **/ // //用户微服务网关 // rules.add(this.getGatewayFlowRule(&quot;user-gateway&quot;)); // //商品微服务网关 // rules.add(this.getGatewayFlowRule(&quot;product-gateway&quot;)); // //订单微服务网关 // rules.add(this.getGatewayFlowRule(&quot;order-gateway&quot;)); //用户微服务网关 rules.add(this.getGatewayFlowRule(getResource(&quot;server-user&quot;))); //商品微服务网关 rules.add(this.getGatewayFlowRule(getResource(&quot;server-product&quot;))); //订单微服务网关 rules.add(this.getGatewayFlowRule(getResource(&quot;server-order&quot;))); //加载规则 GatewayRuleManager.loadRules(rules); } private String getResource(String targetServiceName){ if (routeIdPrefix == null){ routeIdPrefix = &quot;&quot;; } return routeIdPrefix.concat(targetServiceName); } private GatewayFlowRule getGatewayFlowRule(String resource){ //传入资源名称生成GatewayFlowRule GatewayFlowRule gatewayFlowRule = new GatewayFlowRule(resource); //限流阈值 gatewayFlowRule.setCount(1); //统计的时间窗口，单位为 gatewayFlowRule.setIntervalSec(1); return gatewayFlowRule; } /** * 配置限流的异常处理器 */ @Bean @Order(Ordered.HIGHEST_PRECEDENCE) public SentinelGatewayBlockExceptionHandler sentinelGatewayBlockExceptionHandler() { return new SentinelGatewayBlockExceptionHandler(viewResolvers, serverCodecConfigurer); } /** * 自定义限流异常页面 */ private void initBlockHandlers() { BlockRequestHandler blockRequestHandler = new BlockRequestHandler() { @Override public Mono&lt;ServerResponse&gt; handleRequest(ServerWebExchange serverWebExchange, Throwable throwable) { Map map = new HashMap&lt;&gt;(); map.put(&quot;code&quot;, 1001); map.put(&quot;codeMsg&quot;, &quot;接口被限流了&quot;); return ServerResponse.status(HttpStatus.OK). contentType(MediaType.APPLICATION_JSON_UTF8). body(BodyInserters.fromObject(map)); } }; GatewayCallbackManager.setBlockHandler(blockRequestHandler); } } GatewayConfig类的源代码看上去比较多，但是都是一些非常简单的方法，冰河在这里就不再赘述了。 这里有个需要特别注意的地方： Sentinel1.8.4整合SpringCloud Gateway使用的API类型为Route ID类型时，也就是基于route维度时，由于Sentinel为SpringCloud Gateway网关生成的API名称规则如下： 生成的规则为：****${spring.cloud.gateway.discovery.locator.route-id-prefix}后面直接加上目标微服务的名称，如下所示。 {spring.cloud.gateway.discovery.locator.route-id-prefix}目标微服务的名称。其中，${spring.cloud.gateway.discovery.locator.route-id-prefix}是在yml文件中配置的访问前缀。 为了让通过服务网关访问目标微服务链接后，请求链路中生成的API名称与流控规则中生成的API名称一致，以达到启动项目即可实现访问链接的限流效果，而无需登录Setinel管理界面手动配置限流规则，可以将生成GatewayFlowRule对象的resource参数设置为${spring.cloud.gateway.discovery.locator.route-id-prefix}目标微服务的名称 当然，如果不按照上述配置，也可以在项目启动后，通过服务网关访问目标微服务链接后，在Sentinel管理界面的请求链路中找到对应的API名称所代表的请求链路，然后手动配置限流规则。 （3）将服务网关shop-gateway模块的application.yml文件备份一份名称为application-nacos-simple.yml的文件，并将application.yml文件的内容修改成如下所示。 server: port: 10001 spring: application: name: server-gateway main: allow-bean-definition-overriding: true cloud: nacos: discovery: server-addr: 127.0.0.1:8848 sentinel: transport: port: 7777 dashboard: 127.0.0.1:8888 web-context-unify: false eager: true gateway: globalcors: cors-configurations: '[/**]': allowedOrigins: &quot;*&quot; allowedMethods: &quot;*&quot; allowCredentials: true allowedHeaders: &quot;*&quot; discovery: locator: enabled: true route-id-prefix: gateway- 其中： spring.cloud.sentinel.eager表示程序启动时，流控规则是否立即注册到Sentinel，配置为true表示立即注册到Sentinel。 spring.cloud.gateway.discovery.locator.route-id-prefix：生成流控规则API名称的前缀。 （4）在IDEA中配置启动服务网关shop-gateway模块的参数-Dcsp.sentinel.app.type=1，如下所示。 如果是在命令行启动网关服务的Jar包，则可以使用如下命令。 java -Dcsp.sentinel.app.type=1 shop-gateway.jar 或者在启动类io.binghe.shop.GatewayStarter的main()方法中添加一行System.setProperty(&quot;csp.sentinel.app.type&quot;, &quot;1&quot;);代码，如下所示。 /** * @version 1.0.0 * @description 服务网关启动类 */ @SpringBootApplication @EnableDiscoveryClient public class GatewayStarter { public static void main(String[] args){ System.setProperty(&quot;csp.sentinel.app.type&quot;, &quot;1&quot;); SpringApplication.run(GatewayStarter.class, args); } } （5）分别启动用户微服务、商品微服务、订单微服务和服务网关，启动后会在Sentinel管理界面左侧菜单栏中看到server-gateway菜单，如下所示。 在server-gateway菜单下的流控规则子菜单中可以看到网关的流控规则已经注册到Sentinel，如下所示。 （6）通过服务网关访问用户微服务，在浏览器中输入http://localhost:10001/server-user/user/get/1001，不断刷新页面，如下所示。 用户微服务返回的原始数据如下所示。 { &quot;code&quot;: 1001, &quot;codeMsg&quot;: &quot;接口被限流了&quot; } 可以看到，通过服务网关不断刷新用户微服务时，触发了服务限流，并返回了自定义的限流结果数据。 （7）通过服务网关访问商品微服务，在浏览器中输入http://localhost:10001/server-product/product/get/1001，不断刷新页面，如下所示。 商品微服务返回的原始数据如下所示。 { &quot;code&quot;: 1001, &quot;codeMsg&quot;: &quot;接口被限流了&quot; } 可以看到，通过服务网关不断刷新商品微服务时，触发了服务限流，并返回了自定义的限流结果数据。 （8）通过服务网关访问订单微服务，在浏览器中输入http://localhost:10001/server-order/order/test_sentinel，不断刷新页面，如下所示。 可以看到，通过服务网关不断刷新订单微服务时，触发了服务限流，并返回了自定义的限流结果数据。 实现自定义API分组维度限流 前面，我们实现了route维度的限流，接下来，我们再基于Sentinel与SpringCloud gateway实现自定义API分组维度的限流。 （1）在服务网关shop-gateway模块的io.binghe.shop.config.GatewayConfig配置类中新增initCustomizedApis()方法，初始化API管理的信息，源码如下所示。 private void initCustomizedApis() { Set&lt;ApiDefinition&gt; definitions = new HashSet&lt;&gt;(); ApiDefinition api1 = new ApiDefinition(&quot;user_api1&quot;) .setPredicateItems(new HashSet&lt;ApiPredicateItem&gt;() {{ // 以/server-user/user/api1 开头的请求 add(new ApiPathPredicateItem().setPattern(&quot;/server-user/user/api1/**&quot;). setMatchStrategy(SentinelGatewayConstants.URL_MATCH_STRATEGY_PREFIX)); }}); ApiDefinition api2 = new ApiDefinition(&quot;user_api2&quot;) .setPredicateItems(new HashSet&lt;ApiPredicateItem&gt;() {{ // 以/server-user/user/api2/demo1 完成的url路径匹配 add(new ApiPathPredicateItem().setPattern(&quot;/server-user/user/api2/demo1&quot;)); }}); definitions.add(api1); definitions.add(api2); GatewayApiDefinitionManager.loadApiDefinitions(definitions); } 上述代码中，配置了两个API分组，每个API分组的规则如下。 user_api1分组：匹配以/product-serv/product/api1开头的所有请求。 user_api2分组：精确匹配/server-user/user/api2/demo1。 （2）在服务网关shop-gateway模块的io.binghe.shop.config.GatewayConfig配置类中init()方法中调用initCustomizedApis()方法，为了避免route维度的限流对自定义API分组维度的限流产生影响，这里，同时在init()方法中注释掉调用initGatewayRules()方法，修改后的init()方法的代码如下所示。 @PostConstruct public void init() { //this.initGatewayRules(); this.initBlockHandlers(); this.initCustomizedApis(); } （3）在用户微服务shop-user的io.binghe.shop.user.controller.UserController类中新增四个测试接口，源码如下所示。 @GetMapping(value = &quot;/api1/demo1&quot;) public String api1Demo1(){ log.info(&quot;访问了api1Demo1接口&quot;); return &quot;api1Demo1&quot;; } @GetMapping(value = &quot;/api1/demo2&quot;) public String api1Demo2(){ log.info(&quot;访问了api1Demo2接口&quot;); return &quot;api1Demo2&quot;; } @GetMapping(value = &quot;/api2/demo1&quot;) public String api2Demo1(){ log.info(&quot;访问了api2Demo1接口&quot;); return &quot;api2Demo1&quot;; } @GetMapping(value = &quot;/api2/demo2&quot;) public String api2Demo2(){ log.info(&quot;访问了api2Demo2接口&quot;); return &quot;api2Demo2&quot;; } （4）分别启动用户微服务、商品微服务、订单微服务和服务网关，启动后会在Sentinel管理界面左侧菜单栏中看到server-gateway菜单，如下所示。 此时，由于我们注释了调用以route维度限流的方法，所以，在流控规则里的限流规则为空，如下所示。 在API管理里面会发现我们定义的API分组已经自动注册到Sentinel中了，如下所示。 （5）在Sentinel管理界面的流控规则中，新增网关流控规则，如下所示。 点击新增网关流控规则后，会弹出新增网关流控规则配置框，按照如下方式为user_api1分组配置限流规则。 点击新增按钮后，按照同样的方式为user_api2分组配置限流规则。 配置完毕后，在流控规则中的限流规则如下所示。 （6）预期的测试结果如下。 当频繁访问http://localhost:10001/server-user/user/api1/demo1时会被限流。 当频繁访问http://localhost:10001/server-user/user/api1/demo2时会被限流。 当频繁访问http://localhost:10001/server-user/user/api2/demo1时会被限流。 当频繁访问http://localhost:10001/server-user/user/api2/demo2时不会被限流。 注意：只有最后一个不会被限流。 （7）在浏览器上频繁访问http://localhost:10001/server-user/user/api1/demo1，如下所示。 返回的原始数据如下所示。 { &quot;code&quot;: 1001, &quot;codeMsg&quot;: &quot;接口被限流了&quot; } 说明触发了服务限流，并返回了自定义的限流结果数据。 （8）在浏览器上频繁访问http://localhost:10001/server-user/user/api1/demo2，如下所示。 返回的原始数据如下所示。 { &quot;code&quot;: 1001, &quot;codeMsg&quot;: &quot;接口被限流了&quot; } 说明触发了服务限流，并返回了自定义的限流结果数据。 （9）在浏览器上频繁访问http://localhost:10001/server-user/user/api2/demo1，如下所示。 返回的原始数据如下所示。 { &quot;code&quot;: 1001, &quot;codeMsg&quot;: &quot;接口被限流了&quot; } 说明触发了服务限流，并返回了自定义的限流结果数据。 （10）在浏览器上频繁访问http://localhost:10001/server-user/user/api2/demo2，如下所示。 可以看到，访问http://localhost:10001/server-user/user/api2/demo2时，无论访问的频率多频繁，都不会触发Sentinel限流。 至此，我们就成功在项目中整合了SpringCloud Gateway网关，并通过Sentinel整合SpringCloud Gateway实现了网关的限流操作。 ","link":"https://tianxiawuhao.github.io/PN2nvPeue/"},{"title":"gateway网关概述","content":"网关概述 当采用分布式、微服务的架构模式开发系统中，服务网关是整个系统中必不可少的一部分。 没有网关的弊端 当一个系统使用分布式、微服务架构后，系统会被拆分为一个个小的微服务，每个微服务专注一个小的业务。那么，客户端如何调用这么多微服务的接口呢？如果不做任何处理，没有服务网关，就只能在客户端记录下每个微服务的每个接口地址，然后根据实际需要去调用相应的接口。 这种直接使用客户端记录并管理每个微服务的每个接口的方式，存在着太多的问题。比如，这里我列举几个常见的问题。 由客户端记录并管理所有的接口缺乏安全性。 由客户端直接请求不同的微服务，会增加客户端程序编写的复杂性。 涉及到服务认证与鉴权规则时，需要在每个微服务中实现这些逻辑，增加了代码的冗余性。 客户端调用多个微服务，由于每个微服务可能部署的服务器和域名不同，存在跨域的风险。 当客户端比较多时，每个客户端上都管理和配置所有的接口，维护起来相对比较复杂。 引入API网关 API网关，其实就是整个系统的统一入口。网关会封装微服务的内部结构，为客户端提供统一的入口服务，同时，一些与具体业务逻辑无关的通用逻辑可以在网关中实现，比如认证、授权、路由转发、限流、监控等。引入API网关后，如下所示。 可以看到，引入API网关后，客户端只需要连接API网关，由API网关根据实际情况进行路由转发，将请求转发到具体的微服务，同时，API网关会提供认证、授权、限流和监控等功能。 主流的API网关 当系统采用分布式、微服务的架构模式后，API网关就成了整个系统不可分割的一部分。业界通过不断的探索与创新，实现了多种API网关的解决方案。目前，比较主流的API网关有：Nginx+Lua、Kong官网、Zuul网关、Apache Shenyu网关、SpringCloud Gateway网关。 Nginx+Lua Nginx的一些插件本身就实现了限流、缓存、黑白名单和灰度发布，再加上Nginx的反向代理和负载均衡，能够实现对服务接口的负载均衡和高可用。而Lua语言可以实现一些简单的业务逻辑，Nginx又支持Lua语言。所以，可以基于Nginx+Lua脚本实现网关。 Kong网关 Kong网关基于Nginx与Lua脚本开发，性能高，比较稳定，提供多个限流、鉴权等插件，这些插件支持热插拔，开箱即用。Kong网关提供了管理页面，但是，目前基于Kong网关二次开发比较困难。 Zuul网关 Zuul网关是Netflix开源的网关，功能比较丰富，主要基于Java语言开发，便于在Zuul网关的基础上进行二次开发。但是Zuul网关无法实现动态配置规则，依赖的组件相对来说也比较多，在性能上不如Nginx。 Apache Shenyu网关 Dromara社区开发的网关框架，ShenYu 的前名是 soul，最近正式加入了 Apache 的孵化器，因此改名为 ShenYu。其是一个异步的，高性能的，跨语言的，响应式的API网关，并在此基础上提供了非常丰富的扩展功能： 支持各种语言(http协议)，支持Dubbo, Spring-Cloud, Grpc, Motan, Sofa, Tars等协议。 插件化设计思想，插件热插拔，易扩展。 灵活的流量筛选，能满足各种流量控制。 内置丰富的插件支持，鉴权，限流，熔断，防火墙等等。 流量配置动态化，性能极高。 支持集群部署，支持 A/B Test，蓝绿发布。 SpringCloud Gateway网关 Spring为了替换Zuul而开发的网关，SpringCloud Alibaba技术栈中，并没有单独实现网关的组件。在后续的案例实现中，我们会使用SpringCloud Gateway实现网关功能。 SpringCloud Gateway网关 Spring Cloud Gateway是Spring公司基于Spring 5.0， Spring Boot 2.0 和 Project Reactor 等技术开发的网关，它旨在为微服务架构提供一种简单有效的统一的 API 路由管理方式。 它的目标是替代Netflix Zuul，其不仅提供统一的路由方式，并且基于 Filter 链的方式提供了网关基本的功能，例如：安全，监控和限流、重试等。 SpringCloud Gateway概述 Spring Cloud Gateway是Spring Cloud的一个全新项目，基于Spring 5.0 + Spring Boot 2.0和Project Reactor等技术开发的网关，它旨在为微服务架构提供一种简单有效的统一的API路由管理方式。 Spring Cloud Geteway作为Spring Cloud生态系统中的网关，目标是替代Zuul，在Spring Cloud2.0以上版本中，没有对新版本的Zuul 2.0以上最新高性能版本进行集成，仍然还是使用的Zuul 1.x非Reactor模式的老版本。 为了提升网关性能，Spring Cloud Gateway是基于WebFlux框架实现的，而WebFlux框架底层则使用了高性能的Reactor模式通信框架Netty。 Spring Cloud Gateway的目标提供统一的路由方式且基于Filter链的方式提供了网关基本的功能，例如：安全，监控/指标，和限流。 总结一句话：Spring Cloud Gateway使用的Webflux中的reactor-netty响应式编程组件，底层使用Netty通讯框架。 SpringCloud Gateway核心架构 客户端请求到 Gateway 网关，会先经过 Gateway Handler Mapping 进行请求和路由匹配。匹配成功后再发送到 Gateway Web Handler 处理，然后会经过特定的过滤器链，经过所有前置过滤后，会发送代理请求。请求结果返回后，最后会执行所有的后置过滤器。 由上图可以看出，SpringCloud Gateway的主要流程为：客户端请求会先打到Gateway，具体的讲应该是DispacherHandler（因为Gateway引入了WebFlux，作用可以类比MVC的DispacherServlet）。 Gateway根据用户的请求找到相应的HandlerMapping，请求和具体的handler之间有一个映射关系，网关会对请求进行路由，handler会匹配到RoutePredicateHandlerMapping，匹配请求对应的Route，然后到达Web处理器。 WebHandler代理了一系列网关过滤器和全局过滤器的实例，这些过滤器可以对请求和响应进行修改，最后由代理服务完成用户请求，并将结果返回。 ","link":"https://tianxiawuhao.github.io/8Wk_K0cjl/"},{"title":"Sentinel源码拓展之——限流的各种实现方式","content":"Sentinel源码拓展之——限流的各种实现方式 一 常见的限流功能实现有以下三种方式 滑动时间窗口、令牌桶、漏桶，这三种实现方式，有各自擅长的应用场景，而在 Sentinel 中这三种限流实现都有被用到，只不过使用在不同的限流场景下： **滑动时间窗口：**普通QPS限流下的快速失败、Warmup预热，使用场景最多；—— Sentinel的实现，是使用“环形时间窗口”来表示无边无际的时间； **令牌桶：**热点参数限流，需要为每一个请求参数，创建一个令牌桶；—— Sentinel的实现，实际不是真的创建很多个桶； **漏桶（流量整形）：**普通QPS限流下的排队等待，将请求暂存在一个队列中，按时间间隔拉取并处理；—— Sentinel的实现，实际不是真的放入一个队列中； 1 时间窗口（滑动时间窗口） 固定时间窗口，由于时间窗口粒度太粗，而时间本身其实是没有边界的，所以无法保证任意单位时间窗口中的QPS都不超限（如下图粉红色区域）； 为了解决这种问题，可以将我们设置的时间窗口做N等分，划分为更小粒度的窗口，这就是滑动时间窗口： 假设滑动时间跨度Internel为1秒，我们可以做N=2等分，那么最小时间窗口其实就是500ms； 在限流统计QPS时，窗口范围就是从( currentTime – interval) 之后的第一个时区开始，到 currentTime 所在时间窗口。 随着N的值越大，限流的精度就会越高。 2 令牌桶 以固定的速率生成令牌，存在令牌桶中，如果令牌桶满了以后，多余的令牌将会被丢弃； 请求进入后，必须先尝试从令牌桶中获取令牌，获取到令牌的请求才会被处理，没有获取到令牌的请求将会被拒绝，或者等待。 3 漏桶 将每一个“请求”视作“水滴”，放入“漏桶”中存储； “漏桶”以固定速率向外“漏出”请求进行处理，如果“漏桶”空了则代表没有待处理的请求； 如果“漏桶”满了，则多余的“请求”将被快速拒绝； 以上都是三种实现方案的理论知识，而Sentinel在实际实现时，是做了一定的优化的！ 二 Sentinel中三种限流方案的实现源码剖析之——滑动时间窗口算法 滑动时间窗口 —— QPS快速失败 + WarmUp预热： Sentinel中的时间窗口，其实使用的是如下图所示的环形时间窗口： 因为时间是没有边界的， 而且我们一般也只会关注一个Internel之内的时间，如果一个Internel被分成了N个小窗口，那么我们也只会关注N个小的时间窗口； 所以使用环形时间窗口，既能满足使用需求，又能解决内存空间。 通过上一篇关于 ProcessorSlotChain 插槽链的介绍，我们已经清楚了： QPS 的统计工作将会由 StatisticSlot 插槽完成； 限流判断的逻辑将会由 FlowSlot 插槽来完成； 而所有的 PrcessorSlot 的处理逻辑，都在他们的 entry() 方法中； 1 时间窗口请求量统计：StatisticSlot public class StatisticSlot extends AbstractLinkedProcessorSlot&lt;DefaultNode&gt; { @Override public void entry(Context context, ResourceWrapper resourceWrapper, DefaultNode node, int count, boolean prioritized, Object... args) throws Throwable { try { // 先去执行后面插槽和请求体的任务 fireEntry(context, resourceWrapper, node, count, prioritized, args); // 处理完之后才会回来做统计（如果后面失败了，肯定就不用统计了） node.increaseThreadNum(); // 线程数统计（服务隔离） node.addPassRequest(count); // QPS请求量统计（服务限流） ...... } catch (Exception e){ ...... } } } // com.alibaba.csp.sentinel.node.DefaultNode#addPassRequest public void addPassRequest(int count) { super.addPassRequest(count); // 为DefaultNode做统计 this.clusterNode.addPassRequest(count); // 为ClusterNode做统计 } 我们知道DefaultNode 和 ClusterNode 都是 StatisticNode 的子类，所以这里都会调用到StatisticNode的addPassRequest()方法： public class StatisticNode implements Node { // 秒级统计（看变量名就知道是一个环形计数器） private transient volatile Metric rollingCounterInSecond = new ArrayMetric(SampleCountProperty.SAMPLE_COUNT, IntervalProperty.INTERVAL); // 分钟级统计（看变量名就知道是一个环形计数器） private transient Metric rollingCounterInMinute = new ArrayMetric(60, 60 * 1000, false); // 统计QPS方法 public void addPassRequest(int count) { rollingCounterInSecond.addPass(count); // 秒级统计 rollingCounterInMinute.addPass(count); // 分钟级统计 } } // intervalInMs：滑动窗口的时间间隔，Sentinel默认为 1000ms // sampleCount：时间窗口的分割数量，Sentinel默认为 2 // 所以最小的时间窗口就为 500ms public class ArrayMetric implements Metric { private final LeapArray&lt;MetricBucket&gt; data; public ArrayMetric(int sampleCount, int intervalInMs) { this.data = new OccupiableBucketLeapArray(sampleCount, intervalInMs); } } 时间线铺平的效果如下图： 获取当前请求所在时间窗口，并增加计数 public class ArrayMetric implements Metric { private final LeapArray&lt;MetricBucket&gt; data; public ArrayMetric(int sampleCount, int intervalInMs) { this.data = new OccupiableBucketLeapArray(sampleCount, intervalInMs); } @Override public void addPass(int count) { // 获取当前请求所在时间窗口 WindowWrap&lt;MetricBucket&gt; wrap = data.currentWindow(); // 计数器 + count wrap.value().addPass(count); } } ArrayMetric 又是如何获取当前所在时间窗口的呢？ 首先我们得要弄清楚LeapArray保存了哪些数据？ public abstract class LeapArray&lt;T&gt; { // 小时间窗口的时间长度，默认为 500ms protected int windowLengthInMs; // 一个Interval时间被划分的数量，秒级统计默认为2，分钟级默认为60 protected int sampleCount; // 滑动时间窗口时间间隔，默认为 1000ms protected int intervalInMs; // 每一个Interval时间窗口内的小窗口数组，这里就是2 protected final AtomicReferenceArray&lt;WindowWrap&lt;T&gt;&gt; array; } 再次上图，LeapArray是一个环形数组： 我们直接看data.currentWindow() 方法： // com.alibaba.csp.sentinel.slots.statistic.base.LeapArray#currentWindow(当前时间戳) public WindowWrap&lt;T&gt; currentWindow(long timeMillis) { if (timeMillis &lt; 0) { return null; } // 计算当前时间对应的数组角标 = (当前时间/500ms)%16 int idx = calculateTimeIdx(timeMillis); // 计算当前时间所在窗口的开始时间 = 当前时间 - 当前时间 % 500ms long windowStart = calculateWindowStart(timeMillis); /* * 先根据角标获取数组中保存的 oldWindow 对象，可能是旧数据，需要判断. * * (1) oldWindow 不存在, 说明是第一次，创建新 window并存入，然后返回即可 * (2) oldWindow的 starTime = 本次请求的 windowStar, 说明正是要找的窗口，直接返回. * (3) oldWindow的 starTime &lt; 本次请求的 windowStar, 说明是旧数据，需要被覆盖，创建 * 新窗口，覆盖旧窗口 */ while (true) { WindowWrap&lt;T&gt; old = array.get(idx); if (old == null) { // 创建新 window WindowWrap&lt;T&gt; window = new WindowWrap&lt;T&gt;(windowLengthInMs, windowStart, newEmptyBucket(timeMillis)); // 基于CAS写入数组，避免线程安全问题 if (array.compareAndSet(idx, null, window)) { // 写入成功，返回新的 window return window; } else { // 写入失败，说明有并发更新，等待其它人更新完成即可 Thread.yield(); } } else if (windowStart == old.windowStart()) { return old; } else if (windowStart &gt; old.windowStart()) { if (updateLock.tryLock()) { try { // 获取并发锁，覆盖旧窗口并返回 return resetWindowTo(old, windowStart); } finally { updateLock.unlock(); } } else { // 获取锁失败，等待其它线程处理就可以了 Thread.yield(); } } else if (windowStart &lt; old.windowStart()) { // 这种情况不应该存在，写这里只是以防万一。 return new WindowWrap&lt;T&gt;(windowLengthInMs, windowStart, newEmptyBucket(timeMillis)); } } } 获取到时间窗口后，增加计数就比较简单了，使用“自旋 + CAS”的方式完成计数器安全增加即可。 2 滑动窗口QPS计算，并判断限流逻辑：FlowSlot public class FlowSlot extends AbstractLinkedProcessorSlot&lt;DefaultNode&gt; { @Override public void entry(Context context, ResourceWrapper resourceWrapper, DefaultNode node, int count, boolean prioritized, Object... args) throws Throwable { // 先做限流判断 checkFlow(resourceWrapper, context, node, count, prioritized); // 判断通过后，才会放行到下一个插槽 fireEntry(context, resourceWrapper, node, count, prioritized, args); } } FlowSlot的限流判断最终都由TrafficShapingController接口中的canPass方法来实现。该接口有三个实现类： DefaultController：快速失败，默认的方式，基于滑动时间窗口算法； WarmUpController：预热模式，基于滑动时间窗口算法，只不过阈值是动态的； RateLimiterController：排队等待模式，基于漏桶算法； 这里我们就以DefaultController.canPass() 方法为例： com.alibaba.csp.sentinel.slots.block.flow.controller.DefaultController#canPass() public boolean canPass(Node node, int acquireCount, boolean prioritized) { // 重点：计算目前为止滑动窗口内已经存在的请求量 int curCount = avgUsedTokens(node); // 判断：已使用请求量 + 需要的请求量（1） 是否大于 窗口的请求阈值 if (curCount + acquireCount &gt; count) { // 大于，说明超出阈值，返回false if (prioritized &amp;&amp; grade == RuleConstant.FLOW_GRADE_QPS) { long currentTime; long waitInMs; currentTime = TimeUtil.currentTimeMillis(); waitInMs = node.tryOccupyNext(currentTime, acquireCount, count); if (waitInMs &lt; OccupyTimeoutProperty.getOccupyTimeout()) { node.addWaitingRequest(currentTime + waitInMs, acquireCount); node.addOccupiedPass(acquireCount); sleep(waitInMs); // PriorityWaitException indicates that the request will pass after waiting for {@link @waitInMs}. throw new PriorityWaitException(waitInMs); } } return false; } // 小于等于，说明在阈值范围内，返回true return true; } 很显然，关键点就在于，如何计算滑动窗口内已经用掉的请求量 private int avgUsedTokens(Node node) { if (node == null) { return DEFAULT_AVG_USED_TOKENS; } return grade == RuleConstant.FLOW_GRADE_THREAD ? node.curThreadNum() : (int)(node.passQps()); } // 我们这里肯定是QPS限流 // com.alibaba.csp.sentinel.node.StatisticNode#passQps public double passQps() { // 请求量 ÷ 滑动窗口时间间隔(默认1秒) ，得到的就是QPS return rollingCounterInSecond.pass() / rollingCounterInSecond.getWindowIntervalInSec(); } 那么rollingCounterInSecond.pass() 又是如何计算当前时间窗口内的请求量的呢？ 以秒级统计为例，其实环形数组的长度只为2，只需要记录当前小时间窗口和前一个小时间窗口即可，一个都不多记录，节约内存！ // com.alibaba.csp.sentinel.slots.statistic.metric.ArrayMetric#pass public long pass() { // 获取当前窗口 data.currentWindow(); long pass = 0; // 获取 当前时间的 滑动窗口范围内 的所有小窗口 List&lt;MetricBucket&gt; list = data.values(); // 遍历 for (MetricBucket window : list) { // 累加求和 pass += window.pass(); } // 返回 return pass; } | | // data.values()如何获取 滑动窗口范围内 的所有小窗口的 // com.alibaba.csp.sentinel.slots.statistic.base.LeapArray#values(long) public List&lt;T&gt; values(long timeMillis) { if (timeMillis &lt; 0) { return new ArrayList&lt;T&gt;(); } // 创建空集合，大小等于 LeapArray长度（2） int size = array.length(); List&lt;T&gt; result = new ArrayList&lt;T&gt;(size); // 遍历 LeapArray for (int i = 0; i &lt; size; i++) { // 获取每一个小窗口 WindowWrap&lt;T&gt; windowWrap = array.get(i); // 判断这个小窗口是否在 滑动窗口时间范围内（1秒内） if (windowWrap == null || isWindowDeprecated(timeMillis, windowWrap)) { // 不在范围内，则跳过 continue; } // 在范围内，则添加到集合中 result.add(windowWrap.value()); } // 返回集合 return result; } | | // isWindowDeprecated(timeMillis, windowWrap)又是如何判断窗口是否符合要求呢？ public boolean isWindowDeprecated(long time, WindowWrap&lt;T&gt; windowWrap) { // 当前时间 - 窗口开始时间 是否大于 滑动窗口的最大间隔（1秒） // 也就是说，我们要统计的是 距离当前时间1秒内的 小窗口的 count之和 return time - windowWrap.windowStart() &gt; intervalInMs; } 到这里，我们就可以理清了： StatisticSlot会帮助我们记录每一次的request请求，统计每个小时间窗口内的请求数； 秒级统计的时间窗口环只有 2 格； 分钟级统计的时间窗口环有 60格； FlowSlot在需要的时候，会去除当前时间窗口内包含的所有小窗口，然后累加他们的请求量； 最后判断是否溢出限流阈值，允许通过，或直接拒绝！ 三 Sentinel中三种限流方案的实现源码剖析之——令牌桶算法 “热点参数”限流策略，不适合使用 StatisticSlot 中常规的 “滑动时间窗口算法”，因为StatisticSlot中统计的维度是Node级别； 很显然，“热点参数”并不适合使用上面的“环形时间窗口算法”来实现； 相比下来，“令牌桶算法”最适合用在“热点参数限流”场景下，只需要为每个不同的参数值创建一个令牌桶即可。 Controller中的方法资源是不可以进行热点参数限流的：通过Sentinel添加的springmvc拦截器实现，创建Entry时候没有传入params参数； 其它的我们通过@SentinelResource添加的资源才艺进行热点参数限流：通过AOP切面编程实现，创建Entry的时候，也将Params一并传入了； public class ParamFlowSlot extends AbstractLinkedProcessorSlot&lt;DefaultNode&gt; { public void entry(Context context, ResourceWrapper resourceWrapper, DefaultNode node, int count, boolean prioritized, Object... args) throws Throwable { if (!ParamFlowRuleManager.hasRules(resourceWrapper.getName())) { this.fireEntry(context, resourceWrapper, node, count, prioritized, args); } else { // 校验参数限流 this.checkFlow(resourceWrapper, count, args); // 放行到下一个插槽 this.fireEntry(context, resourceWrapper, node, count, prioritized, args); } } void checkFlow(ResourceWrapper resourceWrapper, int count, Object... args) throws BlockException { // args == null 情况有二：1、确实没有参数； 2、Controller方法无法进行参数限流 if (args != null) { if (ParamFlowRuleManager.hasRules(resourceWrapper.getName())) { List&lt;ParamFlowRule&gt; rules = ParamFlowRuleManager.getRulesOfResource(resourceWrapper.getName()); Iterator var5 = rules.iterator(); ParamFlowRule rule; // do-while循环，对每一条rule规则做判断 do { if (!var5.hasNext()) { return; } rule = (ParamFlowRule)var5.next(); this.applyRealParamIdx(rule, args.length); // 初始化“令牌桶”—— 加“”号，并非真的令牌桶 ParameterMetricStorage.initParamMetricsFor(resourceWrapper, rule); } while(ParamFlowChecker.passCheck(resourceWrapper, rule, count, args)); String triggeredParam = &quot;&quot;; if (args.length &gt; rule.getParamIdx()) { Object value = args[rule.getParamIdx()]; triggeredParam = String.valueOf(value); } throw new ParamFlowException(resourceWrapper.getName(), triggeredParam, rule); } } } } 1ParameterMetricStorage.initParamMetricsFor() 令牌桶初始化方法 public final class ParameterMetricStorage { // 以资源名区分不同资源下的令牌桶容器（因为这还不是真正的令牌桶） private static final Map&lt;String, ParameterMetric&gt; metricsMap = new ConcurrentHashMap(); private static final Object LOCK = new Object(); public static void initParamMetricsFor(ResourceWrapper resourceWrapper, ParamFlowRule rule) { if (resourceWrapper != null &amp;&amp; resourceWrapper.getName() != null) { String resourceName = resourceWrapper.getName(); ParameterMetric metric; if ((metric = (ParameterMetric)metricsMap.get(resourceName)) == null) { synchronized(LOCK) { // 如果当前资源还没创建过自己的令牌桶容器，那就创建一个 if ((metric = (ParameterMetric)metricsMap.get(resourceName)) == null) { metric = new ParameterMetric(); metricsMap.put(resourceWrapper.getName(), metric); RecordLog.info(&quot;[ParameterMetricStorage] Creating parameter metric for: &quot; + resourceWrapper.getName(), new Object[0]); } } } // 对令牌桶容器进行初始化 metric.initialize(rule); } } } 2 初始化令牌桶容器，真正的令牌桶即将出现 Sentinel中的令牌桶，其实是维护2个双层Map： 容器中剩余令牌数的Map：&lt;ParamFlowRule, &lt;参数值, 属于这个参数值得桶中的剩余可用令牌数&gt;&gt;：此时第二层Map还是空Map； 记录最近一次通过的请求时间戳的Map：&lt;ParamFlowRule, &lt;参数值, 最近一次请求的时间戳&gt;&gt;：此时第二层Map还是空Map； public class ParameterMetric { private static final int THREAD_COUNT_MAX_CAPACITY = 4000; private static final int BASE_PARAM_MAX_CAPACITY = 4000; private static final int TOTAL_MAX_CAPACITY = 200000; private final Object lock = new Object(); // 令牌桶实现之一：双层Map：&lt;ParamFlowRule, &lt;参数值, 上次请求的时间戳&gt;&gt; private final Map&lt;ParamFlowRule, CacheMap&lt;Object, AtomicLong&gt;&gt; ruleTimeCounters = new HashMap(); // 令牌桶之二：桶中剩余的令牌数 private final Map&lt;ParamFlowRule, CacheMap&lt;Object, AtomicLong&gt;&gt; ruleTokenCounter = new HashMap(); private final Map&lt;Integer, CacheMap&lt;Object, AtomicInteger&gt;&gt; threadCountMap = new HashMap(); // 初始化令牌桶容器 public void initialize(ParamFlowRule rule) { long size; if (!this.ruleTimeCounters.containsKey(rule)) { synchronized(this.lock) { // 初始化ParamRule对应的“最近请求时间戳Map” if (this.ruleTimeCounters.get(rule) == null) { size = Math.min(4000L * rule.getDurationInSec(), 200000L); this.ruleTimeCounters.put(rule, new ConcurrentLinkedHashMapWrapper(size)); } } } if (!this.ruleTokenCounter.containsKey(rule)) { synchronized(this.lock) { // 初始化ParamRule对应的“桶中可用令牌Map” if (this.ruleTokenCounter.get(rule) == null) { size = Math.min(4000L * rule.getDurationInSec(), 200000L); this.ruleTokenCounter.put(rule, new ConcurrentLinkedHashMapWrapper(size)); } } } if (!this.threadCountMap.containsKey(rule.getParamIdx())) { synchronized(this.lock) { if (this.threadCountMap.get(rule.getParamIdx()) == null) { this.threadCountMap.put(rule.getParamIdx(), new ConcurrentLinkedHashMapWrapper(4000L)); } } } } } 以上，只算是将令牌桶容器初始化好了，但是还没开始正式使用！ 3 正式使用上面创建的令牌桶容器，一步步调用至“热点参数限流”判断逻辑核心方法 ParamFlowChecker.passCheck(resourceWrapper, rule, count, args) com.alibaba.csp.sentinel.slots.block.flow.param.ParamFlowChecker#passLocalCheck com.alibaba.csp.sentinel.slots.block.flow.param.ParamFlowChecker#passSingleValueCheck com.alibaba.csp.sentinel.slots.block.flow.param.ParamFlowChecker#passDefaultLocalCheck 4ParamFlowChecker.passDefaultLocalCheck() 即为“热点限流逻辑”的核心方法 // com.alibaba.csp.sentinel.slots.block.flow.param.ParamFlowChecker#passDefaultLocalCheck static boolean passDefaultLocalCheck(ResourceWrapper resourceWrapper, ParamFlowRule rule, int acquireCount, Object value) { ParameterMetric metric = getParameterMetric(resourceWrapper); // 根据rule从上面初始化好的令牌桶容器中去除第二层Map // &lt;参数值, 上次请求的时间戳&gt; // &lt;参数值, 属于这个参数值得桶中的剩余可用令牌数&gt; CacheMap&lt;Object, AtomicLong&gt; tokenCounters = metric == null ? null : metric.getRuleTokenCounter(rule); CacheMap&lt;Object, AtomicLong&gt; timeCounters = metric == null ? null : metric.getRuleTimeCounter(rule); if (tokenCounters != null &amp;&amp; timeCounters != null) { Set&lt;Object&gt; exclusionItems = rule.getParsedHotItems().keySet(); long tokenCount = (long)rule.getCount(); if (exclusionItems.contains(value)) { tokenCount = (long)(Integer)rule.getParsedHotItems().get(value); } if (tokenCount == 0L) { return false; } else { // 允许的最大请求数，也是桶的最大值，也是我们rule中设置的单机阈值 // 后者是突发阈值，一般不配置，为0 long maxCount = tokenCount + (long)rule.getBurstCount(); if ((long)acquireCount &gt; maxCount) { return false; } else { while(true) { long currentTime = TimeUtil.currentTimeMillis(); // 从timeMap中获取最近一次请求通过的时间戳 AtomicLong lastAddTokenTime = (AtomicLong)timeCounters.putIfAbsent(value, new AtomicLong(currentTime)); if (lastAddTokenTime == null) { // 相同参数第一次，直接放行，并更新tokenMap = maxCount - 1 tokenCounters.putIfAbsent(value, new AtomicLong(maxCount - (long)acquireCount)); return true; } // 距离上一次请求通过的时间的差值 long passTime = currentTime - lastAddTokenTime.get(); AtomicLong oldQps; long restQps; // 距最近一次时间差 &gt; 一次统计窗口 if (passTime &gt; rule.getDurationInSec() * 1000L) { oldQps = (AtomicLong)tokenCounters.putIfAbsent(value, new AtomicLong(maxCount - (long)acquireCount)); if (oldQps == null) { lastAddTokenTime.set(currentTime); return true; } // tokenMap中剩余的令牌数 restQps = oldQps.get(); // 在距离上次请求的这段时间内，应该补充生成多少新的令牌 long toAddCount = passTime * tokenCount / (rule.getDurationInSec() * 1000L); // 上面2者相加，与maxCount允许的最大令牌数对比，取min值，并减去本次需要的令牌数 long newQps = toAddCount + restQps &gt; maxCount ? maxCount - (long)acquireCount : restQps + toAddCount - (long)acquireCount; if (newQps &lt; 0L) { return false; } // 通过CAS替换原来的tokenMap，并修改最新通过的请求时间戳 if (oldQps.compareAndSet(restQps, newQps)) { lastAddTokenTime.set(currentTime); return true; } Thread.yield(); } else { // 距最近一次时间差 &lt; 一次统计窗口 oldQps = (AtomicLong)tokenCounters.get(value); if (oldQps != null) { restQps = oldQps.get(); // tokenMap中剩余令牌不足，直接拒绝 if (restQps - (long)acquireCount &lt; 0L) { return false; } // 剩余令牌充足，CAS从tokenMap中减去当前需要的令牌数 if (oldQps.compareAndSet(restQps, restQps - (long)acquireCount)) { return true; } } Thread.yield(); } } } } } else { return true; } } 总结：对于热点参数限流： Sentinel 会为每个资源，维护两个双层数组： 容器中剩余令牌数的tokenMap：&lt;ParamFlowRule, &lt;参数值, 属于这个参数值得桶中的剩余可用令牌数&gt;&gt; 记录最近一次通过的请求时间戳的timeMap：&lt;ParamFlowRule, &lt;参数值, 最近一次请求的时间戳&gt;&gt; 以参数 x=100，限流为5为例： 第一次到达时，肯定不会超限，放行，同时往tokenMap中put入&lt;100, 4&gt; 过一会儿，x = 100 的请求再次到达，则判断，本次请求，距离上一次请求时间，是否超过一个时间统计窗口： 在同一个时间窗口，则在前一次的基础上继续减少token值，tokenMap中值变为 &lt;100, 3&gt; 如果不在一个时间窗口，那么计算距离上次请求的这段时间内，应该新生成的token数 + 现在桶中剩余的token数，与maxToken数做对比，取小值就相当于是此时桶中应该有的令牌数，然后减去自己本次需要的令牌数，之后再更新tokenMap； 四、Sentinel中三种限流方案的实现源码剖析之——漏桶算法 漏桶算法的核心思想是：将请求放在漏桶中，漏桶会按照固定时间间隔，向外“漏出”请求，进行处理，这样很明显的好处就是“流量整形”，当瞬间流量过大时，也可以先放在漏桶中，慢慢处理！ 漏桶算法的入口也是在FlowSlot中，上面有讲过： FlowSlot的限流判断最终都由TrafficShapingController接口中的canPass方法来实现。该接口有三个实现类： DefaultController：快速失败，默认的方式，基于滑动时间窗口算法； WarmUpController：预热模式，基于滑动时间窗口算法，只不过阈值是动态的； RateLimiterController：排队等待模式，基于漏桶算法； 直接进入RateLimiterController.canPass()方法逻辑： // com.alibaba.csp.sentinel.slots.block.flow.controller.RateLimiterController#canPass() // 最新一次的请求执行时间（不一定是已经通过的，而是队列中排在最尾端的请求的预期执行时间） private final AtomicLong latestPassedTime = new AtomicLong(-1); @Override public boolean canPass(Node node, int acquireCount, boolean prioritized) { // Pass when acquire count is less or equal than 0. if (acquireCount &lt;= 0) { return true; } // 阈值小于等于 0 ，阻止请求，不太可能 if (count &lt;= 0) { return false; } // 获取当前时间 long currentTime = TimeUtil.currentTimeMillis(); // 计算两次请求之间允许的最小时间间隔 // 正常时候acquireCount=1，那么costTime = 200ms long costTime = Math.round(1.0 * (acquireCount) / count * 1000); // 计算本次请求 允许执行的时间点 = 上一次请求的可能执行时间 + 两次请求的最小间隔 long expectedTime = costTime + latestPassedTime.get(); // 如果允许执行的时间点小于当前时间，说明可以立即执行 if (expectedTime &lt;= currentTime) { // 更新上一次的请求的执行时间 latestPassedTime.set(currentTime); // 这种情况说明该执行了，立即执行 return true; } else { // 不能立即执行，需要计算 预期等待时长 // 预期等待时长 = 两次请求的最小间隔 + 最近一次请求的可执行时间 - 当前时间 long waitTime = costTime + latestPassedTime.get() - TimeUtil.currentTimeMillis(); // 如果预期等待时间超出阈值，则拒绝请求 if (waitTime &gt; maxQueueingTimeMs) { return false; } else { // 预期等待时间小于阈值，更新最近一次请求的可执行时间，加上costTime long oldTime = latestPassedTime.addAndGet(costTime); try { // 保险起见，再判断一次预期等待时间，是否超过阈值 waitTime = oldTime - TimeUtil.currentTimeMillis(); if (waitTime &gt; maxQueueingTimeMs) { // 如果超过，则把刚才 加 的时间再 减回来 latestPassedTime.addAndGet(-costTime); // 拒绝 return false; } // in race condition waitTime may &lt;= 0 if (waitTime &gt; 0) { // 预期等待时间在阈值范围内，休眠要等待的时间，醒来后继续执行 Thread.sleep(waitTime); } return true; } catch (InterruptedException e) { } } } return false; } 总结： Sentinel的漏桶算法，不是真的维护了一个队列，而是通过计算各个请求的预计执行时间； 如果预计执行时间 &gt; 最大等待时间，那么久不用等了，直接拒绝； 如果预计执行时间 &lt; 最大等待时间，那么就等待吧，自己通过 Thread.sleep(waitTime) 实现等待！ ","link":"https://tianxiawuhao.github.io/xUda2KT1A/"},{"title":"Sentinel核心源码——插槽机制（责任链模式）","content":"Sentinel核心源码——插槽机制（责任链模式） Sentinel的工作原理：https://github.com/alibaba/Sentinel/wiki Sentinel会为所有的资源，以资源名为区分，创建各自的DefaultProcessorSlotChain，放在缓存中； DefaultProcessorSlotChain的9个ProcessorSlot插槽都是通过SPI机制从 META/services/ 目录下加载的； 每一个ProcessorSlot 其实是一个 AbstractLinkedProcessorSlot 抽象链表处理器插槽， 有一个next属性，指向下一个Slot，当某一个Slot执行完后，会调用fireEntry()方法， 将请求转到下一个Slot继续执行。 最终完成责任链上所有ProcessorSlot的逻辑！ —————————————————————————————————————————————— Context链路上下文为request请求级别的，放在ThreadLocal中，请求结束即释放； entranceNode是应用级别的，创建完成后，会缓存起来（key为contextName），下一个请求可以继续使用； processorSlotChain也是应用级别的，创建完成后，会缓存起来（key为resourceName），下一个请求可以继续使用； 一 ProcessorSlotChain处理器插槽链和Node节点的引入 首先，根据官方Wiki中的图，我们可以很形象地看到，整个请求处理过程就像一个链条一样，一步步地向后执行，这是一种典型地“责任链模式”； 责任链模式 —— 为请求创建一个接收者对象的链，链上的每一个节点服务处理各自的业务逻辑，实现解耦，每一个处理者节点记录着下一个节点的引用，请求将沿着这条链被传递下去，以此处理对应的逻辑。 1 ProcessorSlotChain处理器插槽链的引入 ProcessorSlotChain是上图整个链的骨架，基于“责任链模式”设计，将“统计、授权、限流、降级等”处理逻辑封装成一个个的Slot插槽，串联起来。 处理链中的Slot插槽可粗分为上下两大类：数据统计部分 + 规则判断部分 数据统计： **NodeSelectorSlot：**负责构建簇点链路中的各个节点（DefaultNode），形成NodeTree **ClusterBuilderSlot：**负责构建某个资源的ClusterNode（具体的DefaultNode和ClusterNode的区别见下文） **StatisticSlot：**负责实时统计请求的各种调用信息，如来源信息、请求次数、运行信息等； 规则判断： AuthoritySlot：授权规则判断（来源控制） SystemSlot：系统保护规则判断，当系统资源使用量达到一定程度后，拒绝新的请求进入等； ParamFlowSlot：热点参数限流规则判断 FlowSolt：普通限流规则判断 DegradeSlot：降级规则判断 2 为什么要存在NodeSelectorSlot和ClusterBuilderSlot两个插槽？DefaultNode和ClusterNode有什么区别？ **DefaultNode： 同一份资源，经过不同的链路调用，会创建不同的DefaultNode，记录不同链路访问当前资源的统计元数据 **，因为整个Sentinel是支持“根据链路限流”的，所以肯定要分开统计； ClusterNode： 同一份资源，在整个系统中只会创建一个ClusterNode，记录所有入口访问当前资源的统计元数据，因为很多时候，我们只需要统计该资源的整体使用情况。 注意这里的用词，DefaultNode和ClusterNode都只是负责记录统计元数据，真正的统计工作由之后的StatisticSlot进行，另外ParmFlowSlot会负责热点参数限流这种特殊场景下的数据统计。（热点参数限流的统计为什么要单独出来，后面做限流算法实现的讲解时就清楚了）。 3 如何自定义一个Sentinel资源？@SentinelResource注解？ 我们知道，在实际使用过程中，当我们要自定义sentinel资源时，只需要使用@SentinelResource注解定义即可，很方便。 而且Sentinel默认就已经将 springmvc 的 controller 中的方法注册为sentinel资源了，但是这些方法并没有添加 @SentinelResource 注解呀！ 其实@SentinelResource底层也就是通过AOP + Entry 的方式来手动注册 Sentinel资源的： // 资源名可使用任意有业务语义的字符串，比如方法名、接口名或其它可唯一标识的字符串。 try (Entry entry = SphU.entry(&quot;resourceName&quot;)) { // 被保护的业务逻辑 // do something here... } catch (BlockException ex) { // 资源访问阻止，被限流或被降级 // 在此处进行相应的处理操作 } SentinelResourceAspect切面类： @Aspect public class SentinelResourceAspect extends AbstractSentinelAspectSupport { @Pointcut(&quot;@annotation(com.alibaba.csp.sentinel.annotation.SentinelResource)&quot;) public void sentinelResourceAnnotationPointcut() { } // 经典AOP实现 @Around(&quot;sentinelResourceAnnotationPointcut()&quot;) public Object invokeResourceWithSentinel(ProceedingJoinPoint pjp) throws Throwable { Method originMethod = this.resolveMethod(pjp); SentinelResource annotation = (SentinelResource)originMethod.getAnnotation(SentinelResource.class); if (annotation == null) { throw new IllegalStateException(&quot;Wrong state for SentinelResource annotation&quot;); } else { String resourceName = this.getResourceName(annotation.value(), originMethod); EntryType entryType = annotation.entryType(); int resourceType = annotation.resourceType(); Entry entry = null; try { Object var18; try { // 注册对应的资源 entry = SphU.entry(resourceName, resourceType, entryType, pjp.getArgs()); // 执行具体的业务逻辑 return pjp.proceed(); } catch (Exception e) { ...... } } finally { ...... } } } } 所以，通过Entry手动注册资源 和 通过@SentinelResource 注解自动注入资源，原理上时一样的，都是通过 SphU.entry(…) 方法实现。 4 链路上下文Context public class Context { private final String name; private DefaultNode entranceNode; private Entry curEntry; private String origin = &quot;&quot;; private final boolean async; } Context代表调用链路的上下文，贯穿一次链路调用中的所有资源（Entry），基于ThreadLocal实现； Context维护者入口节点（entranceNode）、当前资源节点（curEntry —&gt;curNode）、调用来源origin等信息； 后续所有的Slot插槽都可以通过context拿到DefaultNode 和 ClusterNode，从而完成统计或判断逻辑； Context创建过程中，会创建EntranceNode，contextName 就是entranceNode的名称； // 创建context，包含两个参数：context名称、 来源名称 ContextUtil.enter(&quot;contextName&quot;, &quot;originName&quot;); 默认情况下，Sentinel的entranceNode是sentinel_default_context，如果我们要想做链路限流，就必须关闭“统一入口配置”，从而让每一个Controller方法为Context的入口。 public final static String CONTEXT_DEFAULT_NAME = &quot;sentinel_default_context&quot;; spring: cloud: sentinel: web-context-unify: false # 关闭context统一入口配置 二 Sentinel源码剖析——Context的初始化 1 spring-cloud-starter-alibaba-sentinel 的 spring.factory 中有两个相关的自动装配类 由于，Context的初始化，涉及到了将Controller中的方法定义为entranceNode的过程，所以肯定是看 SentinelWebAutoConfiguration 这个自动装配类！ 2 向 springmvc 处理链中添加一个Sentinel的拦截器 @Configuration( proxyBeanMethods = false // Lite模式，关闭Full模式 ) @ConditionalOnWebApplication( type = Type.SERVLET ) @ConditionalOnProperty( name = {&quot;spring.cloud.sentinel.enabled&quot;}, matchIfMissing = true ) @ConditionalOnClass({SentinelWebInterceptor.class}) @EnableConfigurationProperties({SentinelProperties.class}) public class SentinelWebAutoConfiguration implements WebMvcConfigurer { // 通过实现 WebMvcConfigurer 接口，允许了手动向springmvc中添加拦截器 Interceptor ...... // 注入本类下文定义的SentinelWebInterceptor @Autowired private Optional&lt;SentinelWebInterceptor&gt; sentinelWebInterceptorOptional; public SentinelWebAutoConfiguration() { } // 添加 public void addInterceptors(InterceptorRegistry registry) { if (this.sentinelWebInterceptorOptional.isPresent()) { Filter filterConfig = this.properties.getFilter(); registry.addInterceptor((HandlerInterceptor)this.sentinelWebInterceptorOptional.get()) .order(filterConfig.getOrder()).addPathPatterns(filterConfig.getUrlPatterns()); } } // 向 IOC 容器中注入一个 SentinelWebInterceptor 拦截器 @Bean @ConditionalOnProperty( name = {&quot;spring.cloud.sentinel.filter.enabled&quot;}, matchIfMissing = true ) public SentinelWebInterceptor sentinelWebInterceptor(SentinelWebMvcConfig sentinelWebMvcConfig) { return new SentinelWebInterceptor(sentinelWebMvcConfig); } } 3 SentinelWebInterceptor拦截器的核心方法 SentinelWebInterceptor 中会对父类 AbstractSentinelInterceptor 中的抽象方法做实现（模板方法模式）： public class SentinelWebInterceptor extends AbstractSentinelInterceptor { // 获取resourceName： // controller中请求方法的路径（资源）：/order/{orderId} protected String getResourceName(HttpServletRequest request) { Object resourceNameObject = request.getAttribute(HandlerMapping.BEST_MATCHING_PATTERN_ATTRIBUTE); if (resourceNameObject != null &amp;&amp; resourceNameObject instanceof String) { String resourceName = (String)resourceNameObject; UrlCleaner urlCleaner = this.config.getUrlCleaner(); if (urlCleaner != null) { resourceName = urlCleaner.clean(resourceName); } if (StringUtil.isNotEmpty(resourceName) &amp;&amp; this.config.isHttpMethodSpecify()) { resourceName = request.getMethod().toUpperCase() + &quot;:&quot; + resourceName; } return resourceName; } else { return null; } } // 获取contextName： // 如果开启了统一入口配置，则contextName就是默认的统一入口：sentinel_spring_web_context // 如果关闭了统一入口配置，则contextName就是当前资源的名称； protected String getContextName(HttpServletRequest request) { return this.config.isWebContextUnify() ? super.getContextName(request) : this.getResourceName(request); } } 而作为一个拦截器，最重要的逻辑，肯定是在 prehandler() 中： public abstract class AbstractSentinelInterceptor implements HandlerInterceptor { public static final String SENTINEL_SPRING_WEB_CONTEXT_NAME = &quot;sentinel_spring_web_context&quot;; private static final String EMPTY_ORIGIN = &quot;&quot;; private final BaseWebMvcConfig baseWebMvcConfig; // 前置拦截的核心逻辑 public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { try { // 获取资源名称，一般是controller方法的@RequestMapping路径，例如/order/{orderId} String resourceName = this.getResourceName(request); if (StringUtil.isEmpty(resourceName)) { return true; } else if (this.increaseReferece(request, this.baseWebMvcConfig.getRequestRefName(), 1) != 1) { return true; } else { // 从request中获取请求来源，将来做 授权规则（来源控制） 判断时会用 String origin = this.parseOrigin(request); // 获取 contextName，默认是sentinel_spring_web_context； // 如果关闭统一入口，那就是当前resourceName String contextName = this.getContextName(request); // 创建Context核心方法 ContextUtil.enter(contextName, origin); // 构建ProcessorSlotChain处理器插槽链的核心逻辑 Entry entry = SphU.entry(resourceName, 1, EntryType.IN); request.setAttribute(this.baseWebMvcConfig.getRequestAttributeName(), entry); return true; } } catch (BlockException var12) { BlockException e = var12; try { this.handleBlockException(request, response, e); } finally { ContextUtil.exit(); } return false; } } // 当请求体业务处理完成后，关闭所有的资源 public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception { if (this.increaseReferece(request, this.baseWebMvcConfig.getRequestRefName(), -1) == 0) { Entry entry = this.getEntryInRequest(request, this.baseWebMvcConfig.getRequestAttributeName()); if (entry == null) { ...Log... } else { this.traceExceptionAndExit(entry, ex); // entry.exit()退出 this.removeEntryInRequest(request); ContextUtil.exit(); // contextHolder.set(null); } } } } // private static ThreadLocal&lt;Context&gt; contextHolder = new ThreadLocal&lt;&gt;(); // 保存context的threadLocal 4、ContextUtil.enter(contextName, origin) 创建Context核心方法： // com.alibaba.csp.sentinel.context.ContextUtil#enter public static Context enter(String name, String origin) { // &quot;sentinel_default_context&quot;是不允许被创建的 if (Constants.CONTEXT_DEFAULT_NAME.equals(name)) { throw new ContextNameDefineException( &quot;The &quot; + Constants.CONTEXT_DEFAULT_NAME + &quot; can't be permit to defined!&quot;); } return trueEnter(name, origin); } | | // com.alibaba.csp.sentinel.context.ContextUtil#trueEnter protected static Context trueEnter(String name, String origin) { // 尝试获取context，一般一个新的请求到达后，获取context肯定为null Context context = contextHolder.get(); // 判空 if (context == null) { // 如果为空，开始初始化 Map&lt;String, DefaultNode&gt; localCacheNameMap = contextNameNodeMap; // 尝试获取入口节点 DefaultNode node = localCacheNameMap.get(name); if (node == null) { LOCK.lock(); try { node = contextNameNodeMap.get(name); if (node == null) { // 双重检测锁 // 入口节点为空，初始化入口节点 EntranceNode node = new EntranceNode(new StringResourceWrapper(name, EntryType.IN), null); // 添加入口节点到 ROOT，所有的节点共用一个ROOT根节点 Constants.ROOT.addChild(node); // 将入口节点放入缓存（下次请求时候，根据contextName获取，可直接使用） Map&lt;String, DefaultNode&gt; newMap = new HashMap&lt;&gt;(contextNameNodeMap.size() + 1); newMap.putAll(contextNameNodeMap); newMap.put(name, node); contextNameNodeMap = newMap; // CopyOnWrite } } finally { LOCK.unlock(); } } // 创建Context，参数为：入口节点 和 contextName context = new Context(node, name); // 设置请求来源 origin context.setOrigin(origin); // 将context放入ThreadLocal contextHolder.set(context); } // 返回 return context; } 由此我们可以得出重要结论： 在每一个请求到达时，Sentinel的拦截器都会为本次请求封装一个“链路上下文context”，然后放入到ThreadLocal中，便于请求在后面的处理过程中取用； 默认情况下，“统一入口配置开启”，“链路上下文context”以sentinel-spring-web-context 命名； 如果关闭了“统一入口配置”，“链路上下文context”将以本次请求对应的controller方法的 @RequestMapping() 的值命名，如“/order/{orderId}”； 由于context是放在Thread中的，所以当本次请求结束后，context就会被释放，下次请求需要重新创建；（context生命周期为request） 但是入口 entranceNode 却是放在缓存HashMap中的，所以下一次新的请求到达时，就没有必要再重新创建了；(entranceNode生命周期为应用级) 创建入口方法 entranceNode 时，使用了 双重检测锁 + CopyOnWrite ，因为存在多个请求线程并发情况； 创建 context 过程不需要考虑多线程安全 ，原因也是因为 context时线程内的，单线程 。 三 Sentinel核心源码之ProcessorSlotChain的构建 1 入口方法，正式上文拦截器中创建Context之后的方法 Entry entry = SphU.entry(resourceName, 1, EntryType.IN); 该方法，将“一脉单传”调用到以下方法 entryWithPriority() ： private Entry entryWithPriority(ResourceWrapper resourceWrapper, int count, boolean prioritized, Object... args) throws BlockException { // 获取 Context Context context = ContextUtil.getContext(); if (context == null) { // Using default context. context = InternalContextUtil.internalEnter(Constants.CONTEXT_DEFAULT_NAME); } // 获取 Slot执行链，同一个资源（如：/order/{orderId}），会创建一个执行链，放入缓存 ProcessorSlot&lt;Object&gt; chain = lookProcessChain(resourceWrapper); // 创建 Entry，并将 resource、chain、context 记录在 Entry中 Entry e = new CtEntry(resourceWrapper, chain, context); try { // 执行 slotChain chain.entry(context, resourceWrapper, null, count, prioritized, args); } catch (BlockException e1) { // 如果执行 slotChain 过程中发生异常，也直接将对应的资源释放 e.exit(count, args); ...... } return e; } 2 lookProcessChain() 创建或获取资源对应的ProcessorSlotChain的方法 // com.alibaba.csp.sentinel.CtSph#lookProcessChain ProcessorSlot&lt;Object&gt; lookProcessChain(ResourceWrapper resourceWrapper) { // 从缓存chainMap中获取 ProcessorSlotChain chain = chainMap.get(resourceWrapper); if (chain == null) { synchronized (LOCK) { chain = chainMap.get(resourceWrapper); if (chain == null) { // 又是双重检测锁 // Entry size limit. if (chainMap.size() &gt;= Constants.MAX_SLOT_CHAIN_SIZE) { return null; } // 入口本资源对应的chain不存在，则创建一个新的 chain = SlotChainProvider.newSlotChain(); Map&lt;ResourceWrapper, ProcessorSlotChain&gt; newMap = new HashMap&lt;ResourceWrapper, ProcessorSlotChain&gt;( chainMap.size() + 1); newMap.putAll(chainMap); newMap.put(resourceWrapper, chain); chainMap = newMap; // 又是CopyOnWrite } } } return chain; } 虽然每一次请求的ResourceWrapper都是新new的，但是由于它的hashCode() 和 equals() 方法，只会对比 name; public abstract class ResourceWrapper { protected final String name; protected final EntryType entryType; protected final int resourceType; @Override public int hashCode() { return getName().hashCode(); } @Override public boolean equals(Object obj) { if (obj instanceof ResourceWrapper) { ResourceWrapper rw = (ResourceWrapper)obj; return rw.getName().equals(getName()); } return false; } } 所以，得出结论： Sentinel会为所有的资源，以资源名为区分，创建对应的ProcessorSlotChain ，并缓存到chainMap中； ProcessorSlotChain应用级有效，创建后，下次相同名称的Resource请求进入时，将不需要再次创建chain； 3 SlotChainProvider.newSlotChain() 处理器插槽链的构建过程 // com.alibaba.csp.sentinel.slotchain.SlotChainProvider#newSlotChain public static ProcessorSlotChain newSlotChain() { if (slotChainBuilder != null) { return slotChainBuilder.build(); } // 默认肯定是得到一个 DefaultSlotChainBuilder slotChainBuilder = SpiLoader.loadFirstInstanceOrDefault(SlotChainBuilder.class, DefaultSlotChainBuilder.class); if (slotChainBuilder == null) { slotChainBuilder = new DefaultSlotChainBuilder(); } else { ...... } return slotChainBuilder.build(); } public class DefaultSlotChainBuilder implements SlotChainBuilder { @Override public ProcessorSlotChain build() { // 创建一个 DefaultProcessorSlotChain ProcessorSlotChain chain = new DefaultProcessorSlotChain(); // 该方法会通过spi机制从 \\META-INF\\services\\目录下，加载所有的ProcessorSlot类 List&lt;ProcessorSlot&gt; sortedSlotList = SpiLoader.loadPrototypeInstanceListSorted(ProcessorSlot.class); for (ProcessorSlot slot : sortedSlotList) { if (!(slot instanceof AbstractLinkedProcessorSlot)) { continue; } // 最终创建的 chain.addLast((AbstractLinkedProcessorSlot&lt;?&gt;) slot); } return chain; } } 4 SpiLoader.loadPrototypeInstanceListSorted(ProcessorSlot.class)通过SPI机制加载所有的ProcessorSlot插槽类 // com.alibaba.csp.sentinel.util.SpiLoader#loadPrototypeInstanceListSorted public static &lt;T&gt; List&lt;T&gt; loadPrototypeInstanceListSorted(Class&lt;T&gt; clazz) { try { // Not use SERVICE_LOADER_MAP, to make sure the instances loaded are different. ServiceLoader&lt;T&gt; serviceLoader = ServiceLoaderUtil.getServiceLoader(clazz); List&lt;SpiOrderWrapper&lt;T&gt;&gt; orderWrappers = new ArrayList&lt;&gt;(); // SPI机制会从本地的 META-INF/services/ 目录下加载 ProcessorSlot 列表； for (T spi : serviceLoader) { int order = SpiOrderResolver.resolveOrder(spi); // Since SPI is lazy initialized in ServiceLoader, we use online sort algorithm here. SpiOrderResolver.insertSorted(orderWrappers, spi, order); } List&lt;T&gt; list = new ArrayList&lt;&gt;(orderWrappers.size()); for (int i = 0; i &lt; orderWrappers.size(); i++) { list.add(orderWrappers.get(i).spi); } return list; } catch (Throwable t) { t.printStackTrace(); return new ArrayList&lt;&gt;(); } } 本地 META/services/ 目录下的 ProcessorSlot文件定义了9个插槽！ 5 最终构建成的ProcessorSlotChain的结构 首先，所有的9大ProcessorSlot都继承于一个AbstractLinkedProcessorSlot类： public abstract class AbstractLinkedProcessorSlot&lt;T&gt; implements ProcessorSlot&lt;T&gt; { private AbstractLinkedProcessorSlot&lt;?&gt; next = null; // fireEntry的作用主要就是让请求流转到下一个ProcessorSlot(如果存在的话) @Override public void fireEntry(Context context, ResourceWrapper resourceWrapper, Object obj, int count, boolean prioritized, Object... args) throws Throwable { if (next != null) { next.transformEntry(context, resourceWrapper, obj, count, prioritized, args); } } // 所有ProcessorSlot的入口方法，其中会通过模板方法模式，调用各自的entry处理逻辑 // 而再所有的处理逻辑的最后，都会再调一次 fireEntry() 方法 void transformEntry(Context context, ResourceWrapper resourceWrapper, Object o, int count, boolean prioritized, Object... args) throws Throwable { T t = (T)o; entry(context, resourceWrapper, t, count, prioritized, args); } } 经过 next 指向，最终构建出来的 DefaultProcessorSlotChain 如下： 综上总结： Sentinel会 为所有的资源，以资源名为区分，创建各自的DefaultProcessorSlotChain，放在缓存 中； DefaultProcessorSlotChain的 每一个ProcessorSlot插槽都是通过SPI机制从 META/services/ 目录下加载的 ； 每一个ProcessorSlot 其实是一个 AbstractLinkedProcessorSlot 抽象链表处理器插槽，有一个next属性，指向下一个Slot ，当某一个Slot执行完后，会调用 fireEntry() 方法，将请求转到下一个Slot继续执行。 最终完成责任链上所有ProcessorSlot的逻辑！ 四 九大ProcessorSlot处理器插槽的工作原理 LogSlot插槽是一个边缘插槽，做一些日志记录，所以不算重要，排除在外后，就剩8大插槽，也就是&lt;第一章节&gt;列出的八大插槽： 数据统计部分 + 规则判断部分 数据统计： NodeSelectorSlot：负责构建簇点链路中的各个节点（DefaultNode），形成NodeTree ClusterBuilderSlot：负责构建某个资源的ClusterNode（具体的DefaultNode和ClusterNode的区别见下文） StatisticSlot：负责实时统计请求的各种调用信息，如来源信息、请求次数、运行信息等； 规则判断： AuthoritySlot：授权规则判断（来源控制） SystemSlot：系统保护规则判断，当系统资源使用量达到一定程度后，拒绝新的请求进入等； ParamFlowSlot：热点参数限流规则判断 FlowSolt：普通限流规则判断 DegradeSlot：降级规则判断 其实，当Sentinel的整体架构，和调用逻辑梳理清楚后，每一个责任链节点的处理逻辑就很简单了 ","link":"https://tianxiawuhao.github.io/xdN8U8BuX/"},{"title":"Sentinel限流熔断降级——知识点总结","content":" Sentinei官网地址：https://sentinelguard.io/zh-cn/docs/quick-start.html Sentinel Github地址：https://github.com/alibaba/Sentinel/wiki Sentinel生产级使用：https://github.com/alibaba/Sentinel/wiki/在生产环境中使用-Sentinel 一 Sentinel是什么,为什么要引入Sentinel 1 什么是“服务雪崩”？ 在微服务架构中，存在着大量的服务间调用，有时候，由于某个服务发生了故障，而导致调用它的服务的资源得不到正常释放，从而也发生故障，故障不断传播，最终导致整个微服务无法对外提供服务，这就发生了“服务雪崩”！ 2 “服务雪崩”如何解决？ 解决“服务雪崩”的核心点有2个：尽量控制流量，不要让服务因为过载还出现故障；当一个服务已经出现故障时，不要让故障在微服务整个调用链路中进行传播！ 针对上面两点，“服务雪崩”就有了常见的2大类解决方案： 流量控制，避免出现故障： 流量控制：在微服务调用链路的各个核心资源点，进行流量控制，超出的流量直接不要放行，以对目标资源进行直接保护！ 出现故障时，防止故障传播： 超时处理：设置超时时间，超过设定时间就返回报错信息，做对应处理，防止“无休止”等待！ 服务隔离：限定每个业务能使用的线程数，避免耗尽整个服务的所有线程资源，保护其他业务！ 熔断降级：通过断路器统计服务调用的“异常数”或“异常比例”，如果超出设定阈值，则直接熔断，后续一定时间内的请求到达时，直接返回降级处理的结果，而不真正去调用目标服务！ 而上面所说的这一系列解决方案，Sentinel都为我们提供了实现，所以我们选择Sentinel！ 3 阿里的Sentinel和网飞的Hystrix的对比？ 其实很明显，Sentinel比Hystrix强大很多，而且Hystrix已经进入维护期不再更新了，以后的微服务项目中肯定是选择Sentinel！ 二 Sentinel的简单使用和 首先虽然Sentinel是作为SpringCloud Alibaba的重要组件而出名，但是并不是强依赖于SpringCloud Alibaba！ 独立使用Sentinel文档：https://sentinelguard.io/zh-cn/docs/quick-start.html 在spring-cloud-alibaba框架中使用：https://github.com/alibaba/spring-cloud-alibaba/wiki/Sentinel 1 在spring-cloud-alibaba框架中的简单使用Sentinel 引入依赖starter： &lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-sentinel&lt;/artifactId&gt; &lt;/dependency&gt; 启动Sentinel Dashboard控制台： https://sentinelguard.io/zh-cn/docs/dashboard.html # Sentinel的启动非常简单，java -jar sentinel-dashboard.jar 运行即可： java -Dserver.port=8080 -Dcsp.sentinel.dashboard.server=localhost:8080 -Dproject.name=sentinel-dashboard -jar sentinel-dashboard.jar 在我们的项目文件中配置sentinel-dashboard控制台的地址： spring: cloud: sentinel: transport: dashboard: localhost:8080 2 开启Sentinel对Feign调用的支持 除了上面的依赖和配置外，我们还需要增加一下配置： 如果我们引入的Feign是通过 spring-cloud-starter-openfeign 引入Feign的，那么 Sentinel starter 中的自动配置类就会生效！ 此时，我们只需要在 application.yml 配置文件中开启 Sentinel 对 Feign 的支持即可： feign: sentinel: enabled: true # 开启feign对sentinel的支持 3 Sentinel Dashboard 的配置持久化到 Nacos 配置中心 https://github.com/alibaba/Sentinel/wiki/在生产环境中使用-Sentinel Sentinel Dashboard中的所有配置在默认情况下，刷新后就会丢失，那么实际生产中肯定是可允许的，所以我们必须要想办法将Sentinel的配置持久化； 而正好Nacos就提供了配置的“持久化”与“监听机制”，所以我们就选择通过 Nacos 完成 Sentinel 的配置持久化！（push模式，也是生产中最常用的） 在项目的pom.xml中增加 sentinel-datasource-nacos 的依赖，开启对nacos的配置中心的监听： &lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-sentinel&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba.csp&lt;/groupId&gt; &lt;artifactId&gt;sentinel-datasource-nacos&lt;/artifactId&gt; &lt;/dependency&gt; 在 application.yml 中增加 sentinel 的数据源配置： spring: application: name: orderservice cloud: sentinel: transport: dashboard: localhost:8080 # sentinel控制台地址 web-context-unify: false # 关闭context整合 datasource: flow: nacos: server-addr: localhost:8848 # nacos地址 dataId: orderservice-flow-rules groupId: SENTINEL_GROUP rule-type: flow # 还可以是：degrade、authority、param-flow feign: sentinel: enabled: true # 开启feign对sentinel的支持 4 Sentinel的三种规则配置管理模式 原始模式：如果我们简单使用Dashboard，不做任何修改，就是采用的这种模式： ​ 客户端在启动时，会同时启动一个ServerSocket，默认端口为：8719（如果被占用，尝试3次后+1，继续尝试），告诉Dashboard，当Dashboard中的配置有变化时，通过API接口告诉我们的服务中的Sentinel客户端，Sentinel将这些配置存到内存中，服务重启即丢失，该模式只能用于简单测试，一定不能用于生产环境！ 可从Env类一路向下研究，就可以知道逻辑： public class Env { public static final Sph sph = new CtSph(); static { // If init fails, the process will exit. InitExecutor.doInit(); } } Pull模式：Sentinel客户端（我们的应用服务）向远端的配置管理中心主动定期轮询拉取规则，更新到内存缓存，同时写入到本地磁盘文件，这样的话也可以实现持久化，但是数据一致性、实时性不太好，而且大量的轮询对服务性能又有影响！ // 需要在客户端注册数据源： // —将对应的读数据源注册至对应的 RuleManager， // ——将写数据源注册至 transport 的 WritableDataSourceRegistry 中。 public class FileDataSourceInit implements InitFunc { @Override public void init() throws Exception { String flowRulePath = &quot;xxx&quot;; ReadableDataSource&lt;String, List&lt;FlowRule&gt;&gt; ds = new FileRefreshableDataSource&lt;&gt;( flowRulePath, source -&gt; JSON.parseObject(source, new TypeReference&lt;List&lt;FlowRule&gt;&gt;() {}) ); // 将可读数据源注册至 FlowRuleManager. FlowRuleManager.register2Property(ds.getProperty()); WritableDataSource&lt;List&lt;FlowRule&gt;&gt; wds = new FileWritableDataSource&lt;&gt;(flowRulePath, this::encodeJson); // 将可写数据源注册至 transport 模块的 WritableDataSourceRegistry 中. // 这样收到控制台推送的规则时，Sentinel 会先更新到内存，然后将规则写入到文件中. WritableDataSourceRegistry.registerFlowDataSource(wds); } private &lt;T&gt; String encodeJson(T t) { return JSON.toJSONString(t); } } Push模式：生产环境最推荐的一种模式，需要依赖于外部的注册中心，nacos、zookeeper等 我们在Sentinel Dashboard中修改的规则配置，首先会先持久化到我们的配置中心中（配置中心已经实现了持久化）； sentinel客户端（我们的应用程序）是从配置中心获取数据。 具体的Push模式（nacos）的实现已经在“第3段”有讲述； 但是暂时默认开源的Sentinel Dashboard中并没有直接提供对nacos等注册中心的直接支持，得自己改造，或者直接去Nacos中盲配！ 三 Sentinel Dashboard中得各种规则配置与解读 首先，当我们第一次打开Sentinel Dashboard时，我们会发现控制台空空如也，这是由于Sentinel使用的是懒加载，所以，我们需要先调用一次服务的接口，然后才可以看到“簇点链路”： 之后我们就可以开始愉快的配置啦！ 本章节，不做具体配置和调试，主要是对所有的配置进行解释，加深印象！ 1 流控规则：见名知意，流量控制 2种阈值类型： QPS：对单位时间内的请求数量进行统计，控制流量； 线程数：属于服务隔离（线程隔离），Sentinel默认使用的是“信号量隔离”，而Hystrix默认采用的是“线程池隔离” 信号量隔离：通过“信号量计数器”实现，开销小，但是隔离性一般；适合“扇出”大的场景，如gateway就是扇出比较大的场景； 线程池隔离：基于线程池实现，额外开销大，但是隔离性更强；适合于“扇出”小的场景； 3种流控模式： 直接：流量统计对象 和 限流对象 都是当前资源本身； 关联：流量统计的对象是其它资源，限流的对象却是自己； 链路：只统计从指定链路访问本资源的请求，触发阈值时，也只对指定的链路进行限流； 注意，默认情况下，所有的请求的入口链路都是默认的： sentinel_default_context ，链路模式是不好使用的，所以，如果要使用链路模式，需要关闭“context统一入口配置” spring: application: name: orderservice cloud: sentinel: transport: dashboard: localhost:8080 # sentinel控制台地址 web-context-unify: false # 关闭context统一入口 3种流控效果： 快速失败：达到限流阈值后，直接抛出FlowException异常； WarmUp预热：与“快速失败”类似，达到阈值后，也是直接抛出FlowException异常，但是该模式下的阈值是变化的，默认从一个最大值的1/3，逐渐增加到最大值；常用于服务的预热启动阶段，防止服务流量一下子打到最大，导致一些非常态问题发生； 排队等待（流量整形）：请求到达后，放入一个队列中，按照 1/QPS 的速度进行消费处理，在队列中的请求等待时间，最大不可以超过“设定的超时时间”！ 2 热点规则（热点限流规则）—— 特殊的流控规则 由于Sentinel默认知乎将Springmvc的注解如@RequestMapping等注册为资源，所以当我们需要定义其它资源时，需要手动使用@SentinelResource注解去定义： @SentinelResource(&quot;hot&quot;) public Order queryOrderById(Long orderId) { // 1.查询订单 Order order = orderMapper.findById(orderId); // 2.用Feign远程调用 User user = userClient.findById(order.getUserId()); // 3.封装user到Order order.setUser(user); // 4.返回 return order; } 之后，当我们调用过一次后，就可以看到“hot”这个资源了，而后对它进行流控配置： 上图的热点流控规则解读为：对于hot这个资源，对于它的第0个请求参数，允许它1秒内相同值的最大请求数为5，同时，包含一个特殊情况，对于value=101，允许的QPS阈值为10。 3 降级规则（熔断降级） 熔断降级通常配置 Feign 服务间调用使用，我们通常会定义两个类 UserClient代理接口 + UserClientFallbackFactory降级工厂类： UserClient： @FeignClient(value = &quot;userservice&quot;, fallbackFactory = UserClientFallbackFactory.class) public interface UserClient { @GetMapping(&quot;/user/{id}&quot;) User findById(@PathVariable(&quot;id&quot;) Long id); } UserClientFallbackFactory： @Slf4j @Component public class UserClientFallbackFactory implements FallbackFactory&lt;UserClient&gt; { @Override public UserClient create(Throwable throwable) { return new UserClient() { @Override public User findById(Long id) { log.info(&quot;请求用户数据失败&quot;); return new User().setId(100L).setUsername(&quot;default&quot;).setAddress(&quot;defaultAddress&quot;); } }; } } 上图的熔断降级规则解读为： 当ResponseTime时间超过400ms时为慢调用，统计最近10000ms内的请求，如果请求总数超过10次，且慢调用的比例达到0.6，则触发熔断OPEN； 熔断时常为5秒； 5秒后，进入HALF-OPEN状态，放行一次请求做测试，如果OK，则断路器重新CLOSE闭合，开始正常工作！ 4、授权规则（对请求的来源做控制） 有时候，我们担心由于服务暴露，导致有些请求会越过Gateway，而直接访问我们的服务，这时候，我们就可以通过Sentinel授权只允许从Gateway过来的请求访问我们的服务。 在gateway中通过filter为经过的请求增加一个header值 origin = gateway： spring: application: name: gateway cloud: nacos: server-addr: localhost:8848 # nacos地址 gateway: routes: - id: user-service # 路由标示，必须唯一 uri: lb://userservice # 路由的目标地址 predicates: # 路由断言，判断请求是否符合规则 - Path=/user/** # 路径断言，判断路径是否是以/user开头，如果是则符合 - id: order-service uri: lb://orderservice predicates: - Path=/order/** default-filters: - AddRequestHeader=origin,gateway 在项目中注入一个RequestOriginParser： @Component public class HeaderOriginParser implements RequestOriginParser { @Override public String parseOrigin(HttpServletRequest request) { // 1.获取请求头 String origin = request.getHeader(&quot;origin&quot;); // 2.非空判断 if (StringUtils.isEmpty(origin)) { origin = &quot;blank&quot;; } return origin; } } 最后，配置一条授权规则白名单： 上图授权规则解读：只允许请求头中，origin = gateway 的请求通过！ 四 补充其他知识点 1 默认情况下，发生限流、授权拦截时，会直接抛出异常到调用方，很不友好，最好要自定义异常返回结果 需要为 BlockExceptionHandler 写一个实现： @Component public class SentinelExceptionHandler implements BlockExceptionHandler { @Override public void handle(HttpServletRequest request, HttpServletResponse response, BlockException e) throws Exception { String msg = &quot;未知异常&quot;; int status = 429; if (e instanceof FlowException) { msg = &quot;请求被限流了&quot;; } else if (e instanceof ParamFlowException) { msg = &quot;请求被热点参数限流&quot;; } else if (e instanceof DegradeException) { msg = &quot;请求被降级了&quot;; } else if (e instanceof AuthorityException) { msg = &quot;没有权限访问&quot;; status = 401; } response.setContentType(&quot;application/json;charset=utf-8&quot;); response.setStatus(status); response.getWriter().println(&quot;{\\&quot;msg\\&quot;: &quot; + msg + &quot;, \\&quot;status\\&quot;: &quot; + status + &quot;}&quot;); } } 2 不同的流控规则，使用的技术实现方案不一样 对于限流的需求，常见的实现方案有多种，滑动时间窗口、令牌桶、漏桶，这三种有各自擅长的业务场景，而Sentinel支持的业务场景很多，在不同的场景下，就选择了不同的限流实现。 快速失败、WarmUp预热：滑动时间窗口 热点限流：令牌桶 排队等待：漏桶 具体的限流实现，请参阅另一篇文章：Sentinel源码拓展之——限流的各种实现方式 ","link":"https://tianxiawuhao.github.io/XPUt_T_-A/"},{"title":"openfeign-ribbon核心源码剖析","content":"一、总结前置： 1 ribbon,feign,openfeign三者的对比 我们现在工作中现在几乎都是直接使用openfeign，而我们很有必要了解一下，ribbon、feign、openfeign三者之间的关系！ ribbon：ribbon是netflix开源的客户端负载均衡组件，可以调用注册中心获取服务列表，并通过负载均衡算法挑选一台Server； feign：feign也是netflix开源的，是对ribbon的一层封装，通过接口的方式使用，更加优雅，不需要每次都去写restTemplate，只需要定义一个接口来使用；但是Feign不支持springmvc的注解； openfeign：是对feign的进一步封装，使其能够支持springmvc注解，如@GetMapping等，使用起来更加方便和优雅！ 2 工作中都是如何使用openfeign // 想要使用openfeign，必须要在启动类增加@EnableFeignClient注解，否则启动报错 @FeignClient(&quot;feign-provider&quot;) public interface OrderService { @GetMapping(&quot;/order/get/{id}&quot;) public String getById(@PathVariable(&quot;id&quot;) String id); } 3 openfeign与ribbon如何分工 openfeign通过动态代理的方式，对feignclient注解修饰的类进行动态代理，拼接成临时URL：http://feign-provider/order/get/100，交给ribbon ribbon通过自己的拦截器，截取出serviceName在服务注册表中找到对应的serverList，并通过负载均衡策略挑选一台Server，拼接成最终的URL：http://10.206.73.156:1111/order/get/100，交还给feign； 最后，feign通过自己封装的Client对目标地址发起调用，并获得返回结果；（Client是对原生 java.net 中的URL类的封装，实现远程调用） 二 核心代码——Openfeign生成的动态代理类是啥 1 启动类必须增加@EnableFeignClient，那我们就从这里入口 @EnableFeignClients @Import(FeignClientsRegistrar.class) // 遇到import(Registrar)，我们就去看它的registerBeanDefinitions()方法： public void registerBeanDefinitions(AnnotationMetadata metadata, BeanDefinitionRegistry registry) { registerDefaultConfiguration(metadata, registry); // 注册FeignClient，其实FeignClient就是我们需要的动态代理类 registerFeignClients(metadata, registry); } public void registerFeignClients(AnnotationMetadata metadata,BeanDefinitionRegistry registry) { ...... // 定义过滤器，把加了@FeignClient注解的接口都过滤出来 AnnotationTypeFilter annotationTypeFilter = new AnnotationTypeFilter(FeignClient.class); ...过滤逻辑... for (String basePackage : basePackages) { Set&lt;BeanDefinition&gt; candidateComponents = scanner .findCandidateComponents(basePackage); for (BeanDefinition candidateComponent : candidateComponents) { if (candidateComponent instanceof AnnotatedBeanDefinition) { // verify annotated class is an interface AnnotatedBeanDefinition beanDefinition = (AnnotatedBeanDefinition) candidateComponent; AnnotationMetadata annotationMetadata = beanDefinition.getMetadata(); Assert.isTrue(annotationMetadata.isInterface(), &quot;@FeignClient can only be specified on an interface&quot;); Map&lt;String, Object&gt; attributes = annotationMetadata .getAnnotationAttributes( FeignClient.class.getCanonicalName()); String name = getClientName(attributes); registerClientConfiguration(registry, name, attributes.get(&quot;configuration&quot;)); // 将为每一个过滤出来的接口，注册FeignClient registerFeignClient(registry, annotationMetadata, attributes); } } } } private void registerFeignClient(BeanDefinitionRegistry registry, AnnotationMetadata annotationMetadata, Map&lt;String, Object&gt; attributes) { String className = annotationMetadata.getClassName(); BeanDefinitionBuilder definition = BeanDefinitionBuilder .genericBeanDefinition(FeignClientFactoryBean.class); ...目标FactoryBean为FeignClientFactoryBean... ...FactoryBean一般用于定制化一些特殊的Bean，spring会调用它的getObject接口... BeanDefinitionHolder holder = new BeanDefinitionHolder(beanDefinition, className, new String[] { alias }); BeanDefinitionReaderUtils.registerBeanDefinition(holder, registry); } 2 见名知意FeignClient的工厂bean：FeignClientFactoryBean.getObject()方法 class FeignClientFactoryBean{ @Override public Object getObject() throws Exception { return getTarget(); } &lt;T&gt; T getTarget() { FeignContext context = this.applicationContext.getBean(FeignContext.class); Feign.Builder builder = feign(context); // 当我们不为@FeignClient()注解配置url属性时 // 这里同时会根据注解，拼接出url前缀，如：http://feign-provider if (!StringUtils.hasText(this.url)) { if (!this.name.startsWith(&quot;http&quot;)) { this.url = &quot;http://&quot; + this.name; } else { this.url = this.name; } this.url += cleanPath(); return (T) loadBalance(builder, context, new HardCodedTarget&lt;&gt;(this.type, this.name, this.url)); } ...如果我们为@FeignClient()注解配置url属性之后的逻辑... } } protected &lt;T&gt; T loadBalance(Feign.Builder builder, FeignContext context, HardCodedTarget&lt;T&gt; target) { // 从容器中获取一个类型为Client的bean，那么IOC容器中肯定有一个这样的Bean Client client = getOptional(context, Client.class); if (client != null) { builder.client(client); Targeter targeter = get(context, Targeter.class); return targeter.target(this, builder, context, target); } // 如果找不到类型为Client的Bean就是有问题的啦！ throw new IllegalStateException( &quot;No Feign Client for loadBalancing defined. Did you forget to include spring-cloud-starter-netflix-ribbon?&quot;); } 3 容器中类型为Client的Bean是谁 此时还是启动阶段，我们去看看有哪些bean会被注入，在 spring.factories 中，我们会找到两个自动配置类： 这两个类，很关键，都会往容器中注入负载均衡的Client Bean，但是只会有一个成功， FeignRibbonClientAutoCOnfiguration类： @ConditionalOnClass({ ILoadBalancer.class, Feign.class }) @ConditionalOnProperty(value = &quot;spring.cloud.loadbalancer.ribbon.enabled&quot;,matchIfMissing = true) @Configuration(proxyBeanMethods = false) // 重点1：在FeignAutoConfiguration之前装载配置 @AutoConfigureBefore(FeignAutoConfiguration.class) @EnableConfigurationProperties({ FeignHttpClientProperties.class }) @Import({ HttpClientFeignLoadBalancedConfiguration.class, OkHttpFeignLoadBalancedConfiguration.class, DefaultFeignLoadBalancedConfiguration.class }) // 重点2： public class FeignRibbonClientAutoConfiguration { ...非重点... } @Configuration(proxyBeanMethods = false) class DefaultFeignLoadBalancedConfiguration { @Bean @ConditionalOnMissingBean // 如果Client类型的bean已经存在，则不执行 public Client feignClient(CachingSpringLoadBalancerFactory cachingFactory, SpringClientFactory clientFactory) { return new LoadBalancerFeignClient(new Client.Default(null, null), cachingFactory, clientFactory); } } FeignLoadBalancerAutoConfiguration类： @ConditionalOnClass(Feign.class) @ConditionalOnBean(BlockingLoadBalancerClient.class) @AutoConfigureBefore(FeignAutoConfiguration.class) // 重点1：在FeignRibbonClientAutoConfiguration之后装载配置 @AutoConfigureAfter(FeignRibbonClientAutoConfiguration.class) @EnableConfigurationProperties(FeignHttpClientProperties.class) @Configuration(proxyBeanMethods = false) @Import({ HttpClientFeignLoadBalancerConfiguration.class, OkHttpFeignLoadBalancerConfiguration.class, DefaultFeignLoadBalancerConfiguration.class }) // 重点2： public class FeignLoadBalancerAutoConfiguration { ...非重点... } @Configuration(proxyBeanMethods = false) class DefaultFeignLoadBalancerConfiguration { @Bean @ConditionalOnMissingBean // 如果Client类型的bean已经存在，则不执行 public Client feignClient(BlockingLoadBalancerClient loadBalancerClient) { return new FeignBlockingLoadBalancerClient(new Client.Default(null, null), loadBalancerClient); } } 到这里就非常清晰了： 通过spring.factories注入了两个配置类，并通过 @AutoConfigureBefore 和 @AutoConfigureAfter强制了两个配置类的装载顺序！ 这两个配置类各import 了 另一个配置类：DefaultFeignLoadBalancedConfiguration（注入：LoadBalancerFeignClient） 和 DefaultFeignLoadBalancerConfiguration（注入：FeignBlockingLoadBalancerClient），但是由于@ConditionalOnMissingBean注解，只有一个前面那个会注入成功！ 所以，结论就是：IOC容器中的“Client”类型的 Bean 为 “LoadBalancerFeignClient” 所以，最终的结论就是：Openfeign通过的动态代理，为每个“@FeignClient”注解修饰的接口生成一个类型为 “LoadBalancerFeignClient”的代理类。 三 FeignClient代理类执行时，是如何使用Ribbon的 1 当执行LoadBalancerFeignClient.execute()方法时： // org.springframework.cloud.openfeign.ribbon.LoadBalancerFeignClient#execute public Response execute(Request request, Request.Options options) throws IOException { try { URI asUri = URI.create(request.url()); String clientName = asUri.getHost(); URI uriWithoutHost = cleanUrl(request.url(), clientName); FeignLoadBalancer.RibbonRequest ribbonRequest = new FeignLoadBalancer.RibbonRequest( this.delegate, request, uriWithoutHost); // 可以获取到nacos的一些信息 IClientConfig requestConfig = getClientConfig(options, clientName); // lbClient(clientName)：可以获得具体的代理类，每个@FeignClient修饰的接口代理类时独立的 return lbClient(clientName) .executeWithLoadBalancer(ribbonRequest, requestConfig).toResponse(); } catch (ClientException e) { ...... } } 我们可以很清楚的看到，这里肯定会要用到Ribbon； 2 通过负载均衡器，执行调用，并返回结果 // com.netflix.client.AbstractLoadBalancerAwareClient#executeWithLoadBalancer(...) public T executeWithLoadBalancer(final S request, final IClientConfig requestConfig) throws ClientException { LoadBalancerCommand&lt;T&gt; command = buildLoadBalancerCommand(request, requestConfig); try { return command.submit( // 这步会返回具体的，负载均衡选定好的Server new ServerOperation&lt;T&gt;() { @Override public Observable&lt;T&gt; call(Server server) { // submit选好的Server作为入参 URI finalUri = reconstructURIWithServer(server, request.getUri()); S requestForServer = (S) request.replaceUri(finalUri); try { return Observable.just(AbstractLoadBalancerAwareClient.this.execute(requestForServer, requestConfig)); } catch (Exception e) { return Observable.error(e); } } }) .toBlocking() .single(); } catch (Exception e) { ...... } } // com.netflix.loadbalancer.reactive.LoadBalancerCommand#submit public Observable&lt;T&gt; submit(final ServerOperation&lt;T&gt; operation) { final ExecutionInfoContext context = new ExecutionInfoContext(); ...... final int maxRetrysSame = retryHandler.getMaxRetriesOnSameServer(); final int maxRetrysNext = retryHandler.getMaxRetriesOnNextServer(); // selectServer()方法会通过负载均衡选择一台Server Observable&lt;T&gt; o = (server == null ? selectServer() : Observable.just(server)) .concatMap(new Func1&lt;Server, Observable&lt;T&gt;&gt;() { ...... }); } 3 selectServer()具体方法 // com.netflix.loadbalancer.reactive.LoadBalancerCommand#selectServer private Observable&lt;Server&gt; selectServer() { return Observable.create(new OnSubscribe&lt;Server&gt;() { @Override public void call(Subscriber&lt;? super Server&gt; next) { try { Server server = loadBalancerContext.getServerFromLoadBalancer(loadBalancerURI, loadBalancerKey); next.onNext(server); next.onCompleted(); } catch (Exception e) { next.onError(e); } } }); } 4 这里将使用到非常重要的一个ZoneAwareLoadBalancer // com.netflix.loadbalancer.LoadBalancerContext#getServerFromLoadBalancer public Server getServerFromLoadBalancer(@Nullable URI original, @Nullable Object loadBalancerKey) throws ClientException { String host = null; int port = -1; if (original != null) { host = original.getHost(); } if (original != null) { Pair&lt;String, Integer&gt; schemeAndPort = deriveSchemeAndPortFromPartialUri(original); port = schemeAndPort.second(); } // 关键一步，获得了一个ILoadBalancer ILoadBalancer lb = getLoadBalancer(); if (host == null) { // 重要：这里其实获得的是ZoneAwareLoadBalancer类的Bean if (lb != null){ Server svc = lb.chooseServer(loadBalancerKey); if (svc == null){ throw new ClientException(ClientException.ErrorType.GENERAL, &quot;Load balancer does not have available server for client: &quot; + clientName); } host = svc.getHost(); return svc; } else { ...... } } else { ...... } return new Server(host, port); } // com.netflix.loadbalancer.BaseLoadBalancer#chooseServer public Server chooseServer(Object key) { if (!ENABLED.get() || getLoadBalancerStats().getAvailableZones().size() &lt;= 1) { logger.debug(&quot;Zone aware logic disabled or there is only one zone&quot;); return super.chooseServer(key); } ...其它分支在国内一般不会走，没有zone的概念... } // com.netflix.loadbalancer.BaseLoadBalancer#chooseServer public Server chooseServer(Object key) { if (counter == null) { counter = createCounter(); } counter.increment(); if (rule == null) { return null; } else { try { return rule.choose(key); // PredicateBasedRule } catch (Exception e) { return null; } } } // com.netflix.loadbalancer.PredicateBasedRule#choose public Server choose(Object key) { ILoadBalancer lb = getLoadBalancer(); Optional&lt;Server&gt; server = getPredicate().chooseRoundRobinAfterFiltering(lb.getAllServers(), key); if (server.isPresent()) { return server.get(); } else { return null; } } 5 可以看到 lb.getAllServer()，正是ZoneAwareLoadBalancer.getAllServers()方法 // com.netflix.loadbalancer.BaseLoadBalancer#getAllServers public class BaseLoadBalancer extends AbstractLoadBalancer { protected volatile List&lt;Server&gt; allServerList = Collections.synchronizedList(new ArrayList&lt;Server&gt;()); public List&lt;Server&gt; getAllServers() { return Collections.unmodifiableList(allServerList); } } 所以，我们现在用弄清楚的有两点： 为什么重要的ILoadBalancer的实现就是ZoneAwareLoadBalancer； ZoneAwareLoadBalancer中的allServerList属性是何时被赋值的； 四 ZoneAwareLoadBalancer是何时注入的 1 在spring-cloud-netfliex-ribbon包的spring.factories中有RibbonAutoConfiguration类 我们看到这个类的构造方法，创建了一个SpringClientFactory工厂类： @Configuration @Conditional(RibbonAutoConfiguration.RibbonClassesConditions.class) @RibbonClients @AutoConfigureAfter(name = &quot;org.springframework.cloud.netflix.eureka.EurekaClientAutoConfiguration&quot;) @AutoConfigureBefore({ LoadBalancerAutoConfiguration.class, AsyncLoadBalancerAutoConfiguration.class }) @EnableConfigurationProperties({ RibbonEagerLoadProperties.class, ServerIntrospectorProperties.class }) public class RibbonAutoConfiguration { @Autowired(required = false) private List&lt;RibbonClientSpecification&gt; configurations = new ArrayList&lt;&gt;(); @Autowired private RibbonEagerLoadProperties ribbonEagerLoadProperties; @Bean public HasFeatures ribbonFeature() { return HasFeatures.namedFeature(&quot;Ribbon&quot;, Ribbon.class); } @Bean public SpringClientFactory springClientFactory() { // SpringClientFactory这个Client工厂类很关键 SpringClientFactory factory = new SpringClientFactory(); factory.setConfigurations(this.configurations); return factory; } } 我们再看看这个工厂类的构造方法做了什么事： public class SpringClientFactory extends NamedContextFactory&lt;RibbonClientSpecification&gt; { static final String NAMESPACE = &quot;ribbon&quot;; public SpringClientFactory() { super(RibbonClientConfiguration.class, NAMESPACE, &quot;ribbon.client.name&quot;); } } public abstract class NamedContextFactory&lt;C extends NamedContextFactory.Specification&gt; implements DisposableBean, ApplicationContextAware { private final String propertySourceName; private final String propertyName; private Map&lt;String, AnnotationConfigApplicationContext&gt; contexts = new ConcurrentHashMap(); private Map&lt;String, C&gt; configurations = new ConcurrentHashMap(); private ApplicationContext parent; private Class&lt;?&gt; defaultConfigType; public NamedContextFactory(Class&lt;?&gt; defaultConfigType, String propertySourceName, String propertyName) { this.defaultConfigType = defaultConfigType; this.propertySourceName = propertySourceName; this.propertyName = propertyName; } } 显然，构造了一个上下工厂“NamedContextFactory”类，这个工厂类的默认配置类是“RibbonClientConfiguraion”，这个在之后Feign的调用过程中非常关键； 2 LoadBalancerFeignClient这个代理类的execute()方法中 // org.springframework.cloud.openfeign.ribbon.LoadBalancerFeignClient#execute public Response execute(Request request, Request.Options options) throws IOException { try { URI asUri = URI.create(request.url()); String clientName = asUri.getHost(); URI uriWithoutHost = cleanUrl(request.url(), clientName); FeignLoadBalancer.RibbonRequest ribbonRequest = new FeignLoadBalancer.RibbonRequest( this.delegate, request, uriWithoutHost); // 这个方法不用讲，得到的IclientConfig肯定是RibbonClientConfiguraion IClientConfig requestConfig = getClientConfig(options, clientName); return lbClient(clientName) .executeWithLoadBalancer(ribbonRequest, requestConfig).toResponse(); } catch (ClientException e) { ...... } } // org.springframework.cloud.openfeign.ribbon.LoadBalancerFeignClient#getClientConfig IClientConfig getClientConfig(Request.Options options, String clientName) { ...... requestConfig = this.clientFactory.getClientConfig(clientName); ...... } // org.springframework.cloud.netflix.ribbon.SpringClientFactory#getClientConfig public IClientConfig getClientConfig(String name) { return getInstance(name, IClientConfig.class); } // org.springframework.cloud.netflix.ribbon.SpringClientFactory#getInstance public &lt;C&gt; C getInstance(String name, Class&lt;C&gt; type) { // SpringClientFactory的super父类就是NamedContextFactory C instance = super.getInstance(name, type); if (instance != null) { return instance; } IClientConfig config = getInstance(name, IClientConfig.class); return instantiateWithConfig(getContext(name), type, config); } // org.springframework.cloud.context.named.NamedContextFactory#getInstance public &lt;T&gt; T getInstance(String name, Class&lt;T&gt; type) { AnnotationConfigApplicationContext context = this.getContext(name); return BeanFactoryUtils.beanNamesForTypeIncludingAncestors(context, type).length &gt; 0 ? context.getBean(type) : null; } 找到了**NamedContextFactory ，就和第一点串起来了，它的 默认 配置类正是RibbonClientConfiguration **； 3 RibbonClientConfiguration又是如何注入我们需要的ZoneAwareLoadBalancer的 @Configuration(proxyBeanMethods = false) @EnableConfigurationProperties // Order is important here, last should be the default, first should be optional // see // https://github.com/spring-cloud/spring-cloud-netflix/issues/2086#issuecomment-316281653 @Import({ HttpClientConfiguration.class, OkHttpRibbonConfiguration.class, RestClientRibbonConfiguration.class, HttpClientRibbonConfiguration.class }) public class RibbonClientConfiguration { @Bean @ConditionalOnMissingBean public ILoadBalancer ribbonLoadBalancer(IClientConfig config, ServerList&lt;Server&gt; serverList, ServerListFilter&lt;Server&gt; serverListFilter, IRule rule, IPing ping, ServerListUpdater serverListUpdater) { if (this.propertiesFactory.isSet(ILoadBalancer.class, name)) { return this.propertiesFactory.get(ILoadBalancer.class, config, name); } // 默认注入的类正是ZoneAwareLoadBalancer return new ZoneAwareLoadBalancer&lt;&gt;(config, rule, ping, serverList, serverListFilter, serverListUpdater); } } 所以，这就解释了，为什么在后面的 ILoadBalancer lb = getLoadBalancer(); 获得到的就是ZoneAwareLoadBalancer！ 同时，这个类是在动态代理被执行execute()的时候被调起的，所以Ribbon也是在被使用时才从Nacos获取注册表的，并不是在容器启动时！ 五 Ribbon又是何时从Nacos服务端获取allServerList的 1 我们需要跟踪ZoneAwareLoadBalancer对应的Bean的构造过程 // com.netflix.loadbalancer.ZoneAwareLoadBalancer#ZoneAwareLoadBalancer public ZoneAwareLoadBalancer(IClientConfig clientConfig, IRule rule, IPing ping, ServerList&lt;T&gt; serverList, ServerListFilter&lt;T&gt; filter, ServerListUpdater serverListUpdater) { super(clientConfig, rule, ping, serverList, filter, serverListUpdater); } // com.netflix.loadbalancer.DynamicServerListLoadBalancer#DynamicServerListLoadBalancer public DynamicServerListLoadBalancer(IClientConfig clientConfig, IRule rule, IPing ping, ServerList&lt;T&gt; serverList, ServerListFilter&lt;T&gt; filter, ServerListUpdater serverListUpdater) { super(clientConfig, rule, ping); this.serverListImpl = serverList; this.filter = filter; this.serverListUpdater = serverListUpdater; if (filter instanceof AbstractServerListFilter) { ((AbstractServerListFilter) filter).setLoadBalancerStats(getLoadBalancerStats()); } //执行远程调用，并初始化BaseLoadBalancer中的allServerList restOfInit(clientConfig); } 2 远程调用Nacos并初始化restOfInit()方法的逻辑 void restOfInit(IClientConfig clientConfig) { boolean primeConnection = this.isEnablePrimingConnections(); // turn this off to avoid duplicated asynchronous priming done in BaseLoadBalancer.setServerList() this.setEnablePrimingConnections(false); // 获取服务列表功能 enableAndInitLearnNewServersFeature(); updateListOfServers(); if (primeConnection &amp;&amp; this.getPrimeConnections() != null) { this.getPrimeConnections() .primeConnections(getReachableServers()); } this.setEnablePrimingConnections(primeConnection); } public void enableAndInitLearnNewServersFeature() { serverListUpdater.start(updateAction); } 这个updateAction变量是可执行的： protected final ServerListUpdater.UpdateAction updateAction = new ServerListUpdater.UpdateAction() { @Override public void doUpdate() { updateListOfServers(); // 更新Servers列表 } }; 3、updateListOfServer()方法如何从Nacos服务端获取服务列表： // com.netflix.loadbalancer.DynamicServerListLoadBalancer#updateListOfServers public void updateListOfServers() { List&lt;T&gt; servers = new ArrayList&lt;T&gt;(); if (serverListImpl != null) { // 其中的serverListImpl有三个实现，其中之一就是Nacos servers = serverListImpl.getUpdatedListOfServers(); LOGGER.debug(&quot;List of Servers for {} obtained from Discovery client: {}&quot;, getIdentifier(), servers); if (filter != null) { servers = filter.getFilteredListOfServers(servers); LOGGER.debug(&quot;Filtered List of Servers for {} obtained from Discovery client: {}&quot;, getIdentifier(), servers); } } // 用获得到的服务列表更新BaseLoadBalancer中的allServerList updateAllServerList(servers); } 不用想了，肯定是NacosServerList，这个简单追踪就不赘述了！ NacosServerList.getUpdatedListOfServers()最终会调用nacos客户端的核心类 NacosNamingServer.selectInstances() 方法，该方法只会返回健康实例列表； 4 根据从nacos获得到的Servers，更新本地的allServerList属性 // com.netflix.loadbalancer.DynamicServerListLoadBalancer#updateAllServerList protected void updateAllServerList(List&lt;T&gt; ls) { // other threads might be doing this - in which case, we pass if (serverListUpdateInProgress.compareAndSet(false, true)) { try { for (T s : ls) { s.setAlive(true); // set so that clients can start using these // servers right away instead // of having to wait out the ping cycle. } setServersList(ls); super.forceQuickPing(); } finally { serverListUpdateInProgress.set(false); } } } public void setServersList(List lsrv) { super.setServersList(lsrv); // 核心 List&lt;T&gt; serverList = (List&lt;T&gt;) lsrv; Map&lt;String, List&lt;Server&gt;&gt; serversInZones = new HashMap&lt;String, List&lt;Server&gt;&gt;(); for (Server server : serverList) { // make sure ServerStats is created to avoid creating them on hot // path getLoadBalancerStats().getSingleServerStat(server); String zone = server.getZone(); if (zone != null) { zone = zone.toLowerCase(); List&lt;Server&gt; servers = serversInZones.get(zone); if (servers == null) { servers = new ArrayList&lt;Server&gt;(); serversInZones.put(zone, servers); } servers.add(server); } } setServerListForZones(serversInZones); } // com.netflix.loadbalancer.BaseLoadBalancer#setServersList public void setServersList(List lsrv) { Lock writeLock = allServerLock.writeLock(); logger.debug(&quot;LoadBalancer [{}]: clearing server list (SET op)&quot;, name); ArrayList&lt;Server&gt; newServers = new ArrayList&lt;Server&gt;(); writeLock.lock(); try { ArrayList&lt;Server&gt; allServers = new ArrayList&lt;Server&gt;(); ...对每一个Server进行下包装... allServerList = allServers; //本地的allServerList赋值 if (canSkipPing()) { for (Server s : allServerList) { s.setAlive(true); } upServerList = allServerList; } else if (listChanged) { forceQuickPing(); } } finally { writeLock.unlock(); } } 到这里，总算和 二.5 章节中的allServerList进行呼应了！ 至此，整个从FeignClient注解生成动态代理类LoadBalancerFeignClient； ——&gt; Client执行时会生成ribbon下的ZoneAwareLoadBalancer类，调用nacos服务端获取服务列表； ——&gt; 在通过ZoneAwareLoadBalancer的父类的BaseLoadBalancer.chooseServer()方法根据负载均衡算法挑选一台服务器； ——&gt; 最后交给Feign的的Client通过URL类完成调用。 六 Feign和Ribbon的重试机制 1 首先我们做两个事情 服务提供者feign-provider： @GetMapping(&quot;/get/{id}&quot;) public String getById(@PathVariable(&quot;id&quot;) String id){ System.out.println(&quot;被调用，时间：&quot; + System.currentTimeMillis()); try { TimeUnit.SECONDS.sleep(100); } catch (InterruptedException e) { e.printStackTrace(); } return &quot;feign-provider:com.jiguiquan.springcloud.controller.OrderController#getById=&quot; + id; } 服务消费者feign-consumer： @GetMapping(&quot;test&quot;) public String test(){ return orderService.getById(&quot;100&quot;); } 2 不配置任何重试机制，使用默认值 调用接口后，看服务提供者的日志（重试了2次，时间间隔为1秒）： 3 我们为系统注入Feign的默认Retryer重试机制 @Bean public Retryer retryer(){ return new Retryer.Default(); } 再次调用，再看日志（重试了10次，）： 4 Feign的默认重试器的核心代码（默认是不开启的） public static class Default implements Retryer { private final int maxAttempts; private final long period; private final long maxPeriod; int attempt; long sleptForMillis; public Default() { // 默认的重试间隔时间100ms，最大间隔时间为1s，最大重试次数为5次 this(100L, TimeUnit.SECONDS.toMillis(1L), 5); } // 默认下次重试间隔时间 100 * 1.5^(尝试轮次-1) 次方 // 第一次尝试：100 * 1.5(2 -1) = 150ms; 以此类推 long nextMaxInterval() { long interval = (long)((double)this.period * Math.pow(1.5D, (double)(this.attempt - 1))); return interval &gt; this.maxPeriod ? this.maxPeriod : interval; } } 5 Ribbon和Feign的重试器的源码就不过度追溯了，直接上结论 Ribbon的重试机制默认是开启的，重试1次，共调用两次； Feign的Retryer重试器默认是关闭的 NEVER_RETRY; Feign如果想开启默认重试器，直接在Spring容器中注入 Retryer.Default 即可，默认重试5次； 6 修改Ribbon的默认重试机制 # Ribbon 配置 ribbon: # 单节点最大重试次数(不包含默认调用的1次)，达到最大值时，切换到下一个示例 MaxAutoRetries: 0 # 0 相当于关闭ribbon重试 # 更换下一个重试节点的最大次数，可以设置为服务提供者副本数（副本数 = 总机器数 - 1），也是就每个副本都查询一次 MaxAutoRetriesNextServer: 0 # 是否对所有请求进行重试，默认fasle，则只会对GET请求进行重试，建议配置为false，不然添加数据接口，会造成多条重复，也就是幂等性问题。 OkToRetryOnAllOperations: false 7 自定义Feign重试机制（直接给 Retryer.Default 构造方法传参即可） @Bean public Retryer retryer(){ return new Retryer.Default(100, 1000, 3); // 调用3次（包含原本的一次调用） } 当然，如果对 Retryer.Dafault 的默认的方法逻辑不认可，可以直接实现一个自己的CustomRetryer注入到spring容器中即可： @Bean public Retryer customerRetryer(){ return new Retryer() { @Override public void continueOrPropagate(RetryableException e) { } @Override public Retryer clone() { return null; } }; } ","link":"https://tianxiawuhao.github.io/Y45PUcltI/"},{"title":"Nacos核心源码剖析——配置中心","content":"Nacos官方文档：https://nacos.io/zh-cn/docs/quick-start.html 服务端对外暴露的API：https://nacos.io/zh-cn/docs/open-api.html Nacos的Server端其实就是一个Web服务，对外提供了Http服务接口，所有的客户端与服务端的通讯都通过Http调用完成（短链接）。 Nacos注册服务核心类：NacosNamingService Nacos配置中心核心类：NacosConfigService Nacos配置中心的nameSpace/Group和注册中心类似，但是没有集群Cluster的概念！ 配置文件的核心主键是DataId（与注册中心不一样，注册中心为ServiceName） spring: application: name: nacos-config-client cloud: nacos: config: server-addr: 10.206.73.156:8848 namespace: haier-iot group: dev file-extension: yaml # 上面的配置，拼装成的最高优先级配置文件为 haier-iot/dev/ nacos-config-client-dev.yaml Nacos还支持扩展配置：extension-configs，和共享配置：shared-configs，以支持各种复杂的应用场景！ 官方github wiki地址：https://github.com/alibaba/spring-cloud-alibaba/wiki/Nacos-config Client客户端核心知识点： 当需要获取配置时，先尝试从本地配置文件获取，获取不到时，再去远端Server获取；从远端获取成功后，保存到本地配置文件，后面通过ClientWorker中的长轮询完成配置的实时更新! ClientWorker中有两个线程executor和executorService： 单线程executor定时任务：每10ms执行checkConfigInfo()方法，看看本地配置信息是否有变化，以3000个为一组，判断是否要增加新的LongPollingRunnable长轮询任务； 多线程executorService执行LongPollingRunnable长轮询任务核心逻辑：根据本地配置项的dataId，group，Md5值，tenant拼接字符串，调用服务端“监听配置——长轮询接口”看看这批次的配置是否有变化，有变化的话，就遍历调用“获取配置详情接口”获取最新配置值，没变化的不用动，任务最后，再次执行this，循环往复执行此长轮询任务！ 当本地的配置文件发生改变时，会回调注册在这些配置上的监听器的回调方法，从而完成应用程序的配置更新！（refresh(context)后完成Nacos监听器的注册） 服务端核心知识点： 服务端即使配置了mysql，每次请求也不是直接去查询mysql的，而是借助 本地内存缓存中的元数据 + 本地磁盘中的配置文件； Mysql主要用于集群节点启动时的数据加载（全量加载、增量加载） 和 数据变动时的刷新同步； 某节点处理配置发布请求时，不是着急更新自己的状态，而是会先写入mysql数据库，之后通过ConfigDataChangeEvent事件实现异步处理，通知所有节点更新自己的内存缓存和本地磁盘文件；（包括自己） AP集群，存在数据的短时间不一致，但是可以保证最终一致性，对客户端的影响也就是可能配置更新慢那么一点。 一 nacos客户端加载配置的核心逻辑 1 nacos核心配置NacosPropertySourceLocator类的定位： 如果要弄清楚Nacos配置文件加载到Spring容器中的流程，还需要熟悉Springcloud的源码流程，了解Springcloud中的配置，还有重要的Bean是如果装载到Spring容器中的； 这里只能大概聊一下： Springboot项目启动时，在prepareEnvironment()阶段，会通过spring.factories文件中的BootstrapConfiguration类找到NacosConfigBootstrapConfiguration配置类，并完成注入： NacosConfigBootstrapConfiguration配置类，会向Spring容器中注入几个Nacos配置读取重要的类 public class NacosConfigBootstrapConfiguration { public NacosConfigBootstrapConfiguration() { } @Bean @ConditionalOnMissingBean public NacosConfigProperties nacosConfigProperties() { return new NacosConfigProperties(); } @Bean @ConditionalOnMissingBean public NacosConfigManager nacosConfigManager(NacosConfigProperties nacosConfigProperties) { return new NacosConfigManager(nacosConfigProperties); } // NacosPropertySourceLocator能够一步步地把把Nacos的配置文件都找到 @Bean public NacosPropertySourceLocator nacosPropertySourceLocator(NacosConfigManager nacosConfigManager) { return new NacosPropertySourceLocator(nacosConfigManager); } } 然后在prepareContext()上下文的时候，会通过之前从spring.factories中读取到的ApplicationInitializer初始化器，这里遍历执行时，就会执行到springcloud的PropertySpurceBootstrapConfiguration类的initialize()方法： 可以看到PropertySpurceBootstrapConfiguration.initialize()方法中需要用到PropertySourceLocator接口的实现类，而我们配置的Nacos正好为这个接口提供了实现类NacosPropertySourceLocator 2 通过NacosPropertySourceLocator类，理清Nacos各中配置文件的优先级： // com.alibaba.cloud.nacos.client.NacosPropertySourceLocator#locate public PropertySource&lt;?&gt; locate(Environment env) { this.nacosConfigProperties.setEnvironment(env); ConfigService configService = this.nacosConfigManager.getConfigService(); if (null == configService) { log.warn(&quot;no instance of config service found, can't load config from nacos&quot;); return null; } else { long timeout = (long)this.nacosConfigProperties.getTimeout(); this.nacosPropertySourceBuilder = new NacosPropertySourceBuilder(configService, timeout); String name = this.nacosConfigProperties.getName(); String dataIdPrefix = this.nacosConfigProperties.getPrefix(); if (StringUtils.isEmpty(dataIdPrefix)) { dataIdPrefix = name; } if (StringUtils.isEmpty(dataIdPrefix)) { dataIdPrefix = env.getProperty(&quot;spring.application.name&quot;); } CompositePropertySource composite = new CompositePropertySource(&quot;NACOS&quot;); // 1. 先加载共享配置文件 this.loadSharedConfiguration(composite); // 2. 再加载扩展配置文件 this.loadExtConfiguration(composite); // 3. 最后才加载本应用自己的配置文件 this.loadApplicationConfiguration(composite, dataIdPrefix, this.nacosConfigProperties, env); return composite; } } 本应用自己的配置文件也是可以存在多个的，也是有优先级的： // 入参中的dataIdPrefix，理解为就是spring.application.name private void loadApplicationConfiguration(CompositePropertySource compositePropertySource, String dataIdPrefix, NacosConfigProperties properties, Environment environment) { String fileExtension = properties.getFileExtension(); String nacosGroup = properties.getGroup(); // 1. 先尝试加载：纯微服务名称对应的配置文件，如：order-config this.loadNacosDataIfPresent(compositePropertySource, dataIdPrefix, nacosGroup, fileExtension, true); // 2. 再尝试加载：微服务名称 + &quot;.&quot; + 文件扩展名的文件，如：order-config.yaml this.loadNacosDataIfPresent(compositePropertySource, dataIdPrefix + &quot;.&quot; + fileExtension, nacosGroup, fileExtension, true); String[] var7 = environment.getActiveProfiles(); int var8 = var7.length; // 3. 最后再尝试加载：微服务名称 + &quot;-&quot; + profile + &quot;.&quot; + 文件扩展名的文件，如：order-config-dev.yaml for(int var9 = 0; var9 &lt; var8; ++var9) { String profile = var7[var9]; String dataId = dataIdPrefix + &quot;-&quot; + profile + &quot;.&quot; + fileExtension; this.loadNacosDataIfPresent(compositePropertySource, dataId, nacosGroup, fileExtension, true); } } 根据后加载的覆盖先加载的原则，最后我们很容易就可以知道整个服务的配置文件的优先级为： order-config-dev.yaml &gt; order-config.yaml &gt; order-config &gt; extension-configs &gt; shared-configs 二 Nacos配置中心的核心类NacosConfigService的引入 1 客户端是何时从远程配置Nacos服务端拉取配置的 // 还记得刚开始时候，通过spring.factories注入了一个配置类NacosConfigBootstrapConfiguration // 该配置类，会向Spring容器中注入一个Bean：NacosConfigManager @Bean @ConditionalOnMissingBean public NacosConfigManager nacosConfigManager(NacosConfigProperties nacosConfigProperties) { return new NacosConfigManager(nacosConfigProperties); } //而NacosConfigManager的构造方法中，就会“写死”为我们create一个ConfigService，赋值给静态变量service private static ConfigService service = createConfigService(nacosConfigProperties); | public static ConfigService createConfigService(Properties properties) throws NacosException { try { Class&lt;?&gt; driverImplClass = Class.forName(&quot;com.alibaba.nacos.client.config.NacosConfigService&quot;); Constructor constructor = driverImplClass.getConstructor(Properties.class); ConfigService vendorImpl = (ConfigService)constructor.newInstance(properties); return vendorImpl; } catch (Throwable var4) { throw new NacosException(-400, var4); } } 此时，NacosConfigService闪亮登场！ 2 NacosConfigService类的构造方法中会创建重要的两个属性agent和worker： public NacosConfigService(Properties properties) throws NacosException { ValidatorUtils.checkInitParam(properties); String encodeTmp = properties.getProperty(PropertyKeyConst.ENCODE); if (StringUtils.isBlank(encodeTmp)) { this.encode = Constants.ENCODE; } else { this.encode = encodeTmp.trim(); } initNamespace(properties); // agent是一个http代理，如果需要登录验证等操作，ServerHttpAgent构造时会完成验证 this.agent = new MetricsHttpAgent(new ServerHttpAgent(properties)); this.agent.start(); // 客户端的实际工作者ClientWorker，其中的agent也就是上面创建的agent this.worker = new ClientWorker(this.agent, this.configFilterChainManager, properties); } 3 NacosConfigService中的核心获取配置方法getConfig() ——&gt; getConfigInner()： 客户端需要使用配置文件时，不是直接去调用远端Server获取，而是先尝试从本地Failover文件获取； 如果本地文件不存在，则才会从远端Server获取； 从远端Server成功获取配置后，会向本地文件保存快照，以备后用；（本地文件的更新由后面的长轮询完成） private String getConfigInner(String tenant, String dataId, String group, long timeoutMs) throws NacosException { group = null2defaultGroup(group); ParamUtils.checkKeyParam(dataId, group); ConfigResponse cr = new ConfigResponse(); cr.setDataId(dataId); cr.setTenant(tenant); cr.setGroup(group); // 优先使用本地配置 String content = LocalConfigInfoProcessor.getFailover(agent.getName(), dataId, group, tenant); if (content != null) { cr.setContent(content); configFilterChainManager.doFilter(null, cr); content = cr.getContent(); return content; } try { // 本地没有后，就尝试从远端获取配置 String[] ct = worker.getServerConfig(dataId, group, tenant, timeoutMs); cr.setContent(ct[0]); configFilterChainManager.doFilter(null, cr); content = cr.getContent(); return content; } catch (NacosException ioe) { ...... } ...... return content; } 所以Nacos配置中心与注册中心类似，都是先尝试从本地获取配置，只不过注册中心比配置中心多了一份内存注册表！ 注册中心：本地Failover故障转移文件 ——&gt; 本地内存注册表 ——&gt; 远端请求服务列表 配置中心：本地Failover故障转移文件（也就是本地配置快照文件）——&gt; 远端请求配置文件 worker.getServerConfig()远端配置请求成功后，还会往本地配置文件存一份Snapshot： // worker.getServerConfig() public String[] getServerConfig(String dataId, String group, String tenant, long readTimeout){ // 从远端Server获取配置，这里的agent就是NacosConfigService构造方法中创建的agent代理 result = agent.httpGet(Constants.CONFIG_CONTROLLER_PATH, null, params, agent.getEncode(), readTimeout); switch (result.getCode()) { case HttpURLConnection.HTTP_OK: // 往本地配置快照文件存一份 LocalConfigInfoProcessor.saveSnapshot(agent.getName(), dataId, group, tenant, result.getData()); ct[0] = result.getData(); if (result.getHeader().getValue(CONFIG_TYPE) != null) { ct[1] = result.getHeader().getValue(CONFIG_TYPE); } else { ct[1] = ConfigType.TEXT.getType(); } return ct; case HttpURLConnection.HTTP_NOT_FOUND: LocalConfigInfoProcessor.saveSnapshot(agent.getName(), dataId, group, tenant, null); return ct; case HttpURLConnection.HTTP_CONFLICT: { ... } case HttpURLConnection.HTTP_FORBIDDEN: { ... } default: { ... } } } 4 Client第一次获取到Server端配置后，之后如何进行定时更新？ // ClientWorker构造时，会创建2个线程池 // 1. executor(单线程) ：定时每10毫秒执行checkConfigInfo()方法 // 2. executorService(1~核数/2): 具体的执行长轮询的线程 public ClientWorker(final HttpAgent agent, final ConfigFilterChainManager configFilterChainManager,final Properties properties) { this.agent = agent; this.configFilterChainManager = configFilterChainManager; // Initialize the timeout parameter init(properties); this.executor = Executors.newScheduledThreadPool(1, new ThreadFactory() { @Override public Thread newThread(Runnable r) { Thread t = new Thread(r); t.setName(&quot;com.alibaba.nacos.client.Worker.&quot; + agent.getName()); t.setDaemon(true); return t; } }); this.executorService = Executors .newScheduledThreadPool(Runtime.getRuntime().availableProcessors(), new ThreadFactory() { @Override public Thread newThread(Runnable r) { Thread t = new Thread(r); t.setName(&quot;com.alibaba.nacos.client.Worker.longPolling.&quot; + agent.getName()); t.setDaemon(true); return t; } }); this.executor.scheduleWithFixedDelay(new Runnable() { @Override public void run() { try { checkConfigInfo(); } catch (Throwable e) { LOGGER.error(&quot;[&quot; + agent.getName() + &quot;] [sub-check] rotate check error&quot;, e); } } }, 1L, 10L, TimeUnit.MILLISECONDS); } public void checkConfigInfo() { // 将总的需要监听的配置数，以3000个为一组，创建长轮询LongPollingRunnable任务，监听配置更新！ int listenerSize = cacheMap.get().size(); int longingTaskCount = (int) Math.ceil(listenerSize / ParamUtil.getPerTaskConfigSize()); if (longingTaskCount &gt; currentLongingTaskCount) { for (int i = (int) currentLongingTaskCount; i &lt; longingTaskCount; i++) { // 实际的干活线程，从远端拉取最新的config，与本地的config对比MD5值，看是否发生变化 executorService.execute(new LongPollingRunnable(i)); } currentLongingTaskCount = longingTaskCount; } } 总结：ClientWorker实例化时，会创建两个线程：executor和executorService executor（单线程）：每隔10ms检查本地的配置数是否发生改变，以3000为一批次创建长轮询任务，不足的话，不另外创建长轮询任务； executorService（多线程1~核数/2）:具体执行长轮询LongPolling任务的工作线程； 5 Nacos客户端长轮询LongPolling任务的核心逻辑（md5比对）： 监听配置的长轮询接口API：https://nacos.io/zh-cn/docs/open-api.html // LongPollingRunnable.run() public void run() { List&lt;CacheData&gt; cacheDatas = new ArrayList&lt;CacheData&gt;(); List&lt;String&gt; inInitializingCacheList = new ArrayList&lt;String&gt;(); try { // 检查本地的配置文件 for (CacheData cacheData : cacheMap.get().values()) { if (cacheData.getTaskId() == taskId) { cacheDatas.add(cacheData); try { checkLocalConfig(cacheData); if (cacheData.isUseLocalConfigInfo()) { cacheData.checkListenerMd5(); } } catch (Exception e) { LOGGER.error(&quot;get local config info error&quot;, e); } } } // 会根据上面得到的cacheDatas，组装参数，调用服务端的监听配置的长轮询接口/nacos/v1/cs/configs/listener // 长轮询的返回值是dataId^2group^2tenant^1，空串代表无变化 List&lt;String&gt; changedGroupKeys = checkUpdateDataIds(cacheDatas, inInitializingCacheList); if (!CollectionUtils.isEmpty(changedGroupKeys)) { LOGGER.info(&quot;get changedGroupKeys:&quot; + changedGroupKeys); } for (String groupKey : changedGroupKeys) { String[] key = GroupKey.parseKey(groupKey); String dataId = key[0]; String group = key[1]; String tenant = null; if (key.length == 3) { tenant = key[2]; } try { // 会根据返回值中有变化的dataId配置项，单独去获取最新的配置值回本地 String[] ct = getServerConfig(dataId, group, tenant, 3000L); CacheData cache = cacheMap.get().get(GroupKey.getKeyTenant(dataId, group, tenant)); cache.setContent(ct[0]); if (null != ct[1]) { cache.setType(ct[1]); } LOGGER.info(&quot;[{}] [data-received] dataId={}, group={}, tenant={}, md5={}, content={}, type={}&quot;, agent.getName(), dataId, group, tenant, cache.getMd5(), ContentUtils.truncateContent(ct[0]), ct[1]); } catch (NacosException ioe) { String message = String .format(&quot;[%s] [get-update] get changed config exception. dataId=%s, group=%s, tenant=%s&quot;, agent.getName(), dataId, group, tenant); LOGGER.error(message, ioe); } } for (CacheData cacheData : cacheDatas) { if (!cacheData.isInitializing() || inInitializingCacheList .contains(GroupKey.getKeyTenant(cacheData.dataId, cacheData.group, cacheData.tenant))) { cacheData.checkListenerMd5(); cacheData.setInitializing(false); } } inInitializingCacheList.clear(); // 再次执行该方法，不断循环，不停监听配置文件的更新 executorService.execute(this); } catch (Throwable e) { // If the rotation training task is abnormal, the next execution time of the task will be punished LOGGER.error(&quot;longPolling error : &quot;, e); executorService.schedule(this, taskPenaltyTime, TimeUnit.MILLISECONDS); } } checkUpdateDataIds(cacheDatas, inInitializingCacheList)核心逻辑： // 拼接本地所有的dataId为一个长字符串 List&lt;String&gt; checkUpdateDataIds(List&lt;CacheData&gt; cacheDatas, List&lt;String&gt; inInitializingCacheList) throws Exception { StringBuilder sb = new StringBuilder(); for (CacheData cacheData : cacheDatas) { if (!cacheData.isUseLocalConfigInfo()) { sb.append(cacheData.dataId).append(WORD_SEPARATOR); sb.append(cacheData.group).append(WORD_SEPARATOR); if (StringUtils.isBlank(cacheData.tenant)) { sb.append(cacheData.getMd5()).append(LINE_SEPARATOR); } else { sb.append(cacheData.getMd5()).append(WORD_SEPARATOR); sb.append(cacheData.getTenant()).append(LINE_SEPARATOR); } if (cacheData.isInitializing()) { // It updates when cacheData occours in cacheMap by first time. inInitializingCacheList .add(GroupKey.getKeyTenant(cacheData.dataId, cacheData.group, cacheData.tenant)); } } } boolean isInitializingCacheList = !inInitializingCacheList.isEmpty(); return checkUpdateConfigStr(sb.toString(), isInitializingCacheList); } // 向服务端发起长轮询的查询逻辑，超时为30秒，不要挂起我 List&lt;String&gt; checkUpdateConfigStr(String probeUpdateString, boolean isInitializingCacheList) { Map&lt;String, String&gt; params = new HashMap&lt;String, String&gt;(2); params.put(Constants.PROBE_MODIFY_REQUEST, probeUpdateString); Map&lt;String, String&gt; headers = new HashMap&lt;String, String&gt;(2); headers.put(&quot;Long-Pulling-Timeout&quot;, &quot;&quot; + timeout); if (isInitializingCacheList) { headers.put(&quot;Long-Pulling-Timeout-No-Hangup&quot;, &quot;true&quot;); } if (StringUtils.isBlank(probeUpdateString)) { return Collections.emptyList(); } try { // In order to prevent the server from handling the delay of the client's long task, // increase the client's read timeout to avoid this problem. long readTimeoutMs = timeout + (long) Math.round(timeout &gt;&gt; 1); HttpRestResult&lt;String&gt; result = agent .httpPost(Constants.CONFIG_CONTROLLER_PATH + &quot;/listener&quot;, headers, params, agent.getEncode(), readTimeoutMs); if (result.ok()) { setHealthServer(true); return parseUpdateDataIdResponse(result.getData()); } else { setHealthServer(false); LOGGER.error(&quot;[{}] [check-update] get changed dataId error, code: {}&quot;, agent.getName(), result.getCode()); } } catch (Exception e) { setHealthServer(false); LOGGER.error(&quot;[&quot; + agent.getName() + &quot;] [check-update] get changed dataId exception&quot;, e); throw e; } return Collections.emptyList(); } 总结：长轮询LongPollingRunnable长轮询任务的核心逻辑就是： 先查询本地配置文件Failover文件（本地配置快照Snapshot文件）； 根据本地配置文件，得到所有配置项，拼接请求参数（dataId/group/Md5/tenant），向服务端提供的长轮询接口（监听配置）：POST：/nacos/v1/cs/configs/listener 发起长轮询调用； 如果本批次的配置项有变动，服务端就会返回有变动的配置项字符串数组，如果没有变动，就返回空串； 如果返回不为空，说明有变动，就遍历这些变动项，然后通过具体的获取配置接口：GET：/nacos/v1/cs/configs 获取该配置项的最新配置； 最后，再次执行本次的 LongPollingRunnable 任务，循环往复，完成配置的实时监听更新！ 6 经过长轮询后，如果本地配置文件更新后，该如何通知到我们的应用程序（注册监听器）？ 在Springboot项目完成refresh(context)后，会调用 listeners.running(context) 方法，这个方法会向系统发出ApplicationReadyEvent事件，其它的监听了这个事件的Listener就可以做对应的工作了，而NacosContextRefresher就是其中之一！ public class NacosContextRefresher implements ApplicationListener&lt;ApplicationReadyEvent&gt;, ApplicationContextAware { // 监听到ApplicationReadyEvent事件后，开始注册Nacos的监听器 public void onApplicationEvent(ApplicationReadyEvent event) { if (this.ready.compareAndSet(false, true)) { this.registerNacosListenersForApplications(); } } private void registerNacosListenersForApplications() { if (this.isRefreshEnabled()) { Iterator var1 = NacosPropertySourceRepository.getAll().iterator(); while(var1.hasNext()) { NacosPropertySource propertySource = (NacosPropertySource)var1.next(); if (propertySource.isRefreshable()) { // 默认使开启的 String dataId = propertySource.getDataId(); this.registerNacosListener(propertySource.getGroup(), dataId); } } } } } 有了这一系列的监听器，当客户端知道配置发生改变时，就会回调对应的监听器的回调方法，通知应用程序更新对应的Bean（从IOC容器中删除旧Bean，放入新的Bean）。 三 AP模式nacos集群，服务端如何工作 首先一定要清楚，即使集群情况下，配置了mysql，当客户端查询配置时，也不是直接从mysql获取的，而是从每个节点的本地文件读取； 各节点本地文件：具体的配置； 各节点内存缓存：存储配置的元信息，如Md5值等； Mysql：具体的配置的最新值和历史记录，方便新节点启动时加载，以及节点之间的数据同步； 1 配置文件的Dump加载：DumpService DumpService由两个实现类：EmbeddedDumpService（derby） 和 ExternalDumpService（mysql）； 当新节点启动时，需要从mysql中加载配置数据，如果最后心跳时间&gt;6h，则从mysql加载全量数据；如果最后心跳时间&lt;6h，则从mysql加载增量数据； 全量加载：删除本地的配置文件，全部从mysql加载配置数据；（每次捞取1000条） 增量加载：捞取最近6小时的新增配置，更新本地和内存元数据后，与mysql数据库中的配置进行比对，如果不一致，则再同步一次！ 2 新的配置被发布后，如何在集群间进行同步？ AP集群，每个节点地位平等，发布配置后，根据轮询机制，会由某一台Server节点处理本地请求，该节点首先会将配置写入到Mysql数据库中； 该节点会发布一个ConfigDataChangeEvent事件，该事件会被自己的监听器处理，处理时，会通过HTTP调用集群的所有节点，告知配置发生改变（包括自己，因为上面只是写mysql，自己的本地文件和内存文件也都没有被修改） 所有节点接收到本地配置修改的通知后，会到Mysql中同步最新的配置，刷新内存缓存 和 本地磁盘文件； ","link":"https://tianxiawuhao.github.io/bp8DT_bHg/"},{"title":"Nacos核心源码剖析（CP架构）——注册中心","content":"一 Nacos CP集群架构的基础知识 1 Nacos集群部署后，可以同时支持AP和CP（注意，不是同时支持CAP） AP架构：临时实例 CP架构：持久化实例 在注册服务时，如果我们让我们的节点注册为：持久化实例，即自动会走CP架构！ spring: application: name: nacos-config-client cloud: nacos: discovery: server-addr: 10.206.73.156:8848 namespace: haier-iot group: dev cluster-name: BJ ephemeral: true # 持久化实例走CP架构 2 Nacos CP架构使用的分布式一致性协议？（简化版的Raft） Raft分布式一致性协议 和 Zookeeper使用的ZAB原子广播协议非常相似； 都是一个Leader带领多个Follower，区别在于，Leader选举时的投票机制： ZAB投票时：所有候选节点都会发起投票，然后进行选票PK，决定谁胜出； Raft投票时：会让所有节点随机睡眠，先睡醒的节点发起投票，投自己，并将选票发到其它节点等待结果； 但是，对结果的判断，ZAB 和 Raft 都遵循“半数机制”。 二 CP架构下，持久化实例的注册逻辑 1 注册实例的API接口不变：/nacos/v1/ns/instance 这里当然时选择RaftConsistencyServiceImpl的实现： public void put(String key, Record value) throws NacosException { checkIsStopWork(); try { raftCore.signalPublish(key, value); } catch (Exception e) { ...... } } 2 整个CP架构下的主节点注册逻辑都在signalPublish()方法中 public void signalPublish(String key, Record value) throws Exception { if (stopWork) { throw new IllegalStateException(&quot;old raft protocol already stop work&quot;); } // 判断自己是不是Leader if (!isLeader()) { ObjectNode params = JacksonUtils.createEmptyJsonNode(); params.put(&quot;key&quot;, key); params.replace(&quot;value&quot;, JacksonUtils.transferToJsonNode(value)); Map&lt;String, String&gt; parameters = new HashMap&lt;&gt;(1); parameters.put(&quot;key&quot;, key); final RaftPeer leader = getLeader(); // 如果本节点不是Leader，就把请求转发给Leader节点处理 raftProxy.proxyPostLarge(leader.ip, API_PUB, params.toString(), parameters); return; } OPERATE_LOCK.lock(); try { ...... // 核心方法，写本地数据，写内存缓存，发布事件——更新内存服务注册表 onPublish(datum, peers.local()); final String content = json.toString(); // 通过CountDownLatch实现半数ack的统计，如果获得到半数以上的ack，则Countdownlatch逻辑才可以继续向下走！ final CountDownLatch latch = new CountDownLatch(peers.majorityCount()); for (final String server : peers.allServersIncludeMyself()) { if (isLeader(server)) { // 首先自己的钥匙先用上 latch.countDown(); continue; } final String url = buildUrl(server, API_ON_PUB); // /v1/ns/raft/datum/commit HttpClient.asyncHttpPostLarge(url, Arrays.asList(&quot;key&quot;, key), content, new Callback&lt;String&gt;() { @Override public void onReceive(RestResult&lt;String&gt; result) { latch.countDown(); // Http调用正常后，则当作一次ack } @Override public void onError(Throwable throwable) { } @Override public void onCancel() { } }); } } finally { OPERATE_LOCK.unlock(); } 其实Nacos实现的简单Raft协议，逻辑有点不太严谨，就是：即使本次同步不成功，但是主节点的本地磁盘文件 + 内存文件 已经都修改过了，不像Zookeeper的两阶段提交； 后期Nacos的一致性协议会修改为JRaft，这点肯定会解决！ 3、Leader本节点保存数据的逻辑：onPublish(datum, peers.local()) public void onPublish(Datum datum, RaftPeer source) throws Exception { ...... // 逻辑能走到这，这个if正常都为true if (KeyBuilder.matchPersistentKey(datum.key)) { // 核心，将数据写到本地磁盘 raftStore.write(datum); } // 往内存中保存一些注册表信息 datums.put(datum.key, datum); if (isLeader()) { local.term.addAndGet(PUBLISH_TERM_INCREASE_COUNT); } else { if (local.term.get() + PUBLISH_TERM_INCREASE_COUNT &gt; source.term.get()) { //set leader term: getLeader().term.set(source.term.get()); local.term.set(getLeader().term.get()); } else { local.term.addAndGet(PUBLISH_TERM_INCREASE_COUNT); } } raftStore.updateTerm(local.term.get()); // 发布一个ValueChangeEvent事件，PersistentNotifier.onEvent(ValueChangeEvent)会去处理这个事件 NotifyCenter.publishEvent(ValueChangeEvent.builder().key(datum.key).action(DataOperation.CHANGE).build()); Loggers.RAFT.info(&quot;data added/updated, key={}, term={}&quot;, datum.key, local.term); } Leader保存本节点数据，分为三个过程： 1. 保存数据到磁盘文件 2. 保存部分信息到缓存datums 3. 保存文件到内存中的服务注册表（双层ConcurrentHashMap）—— 但不是同步保存，而是通过事件发布，实现异步保存 需要保存到内存中的服务注册表时，会发布一个ValueChangeEvent事件，该事件会被PersisNotifier.onEvent(ValueChangeEvent)捕捉到，并进行处理： // com.alibaba.nacos.naming.consistency.persistent.PersistentNotifier#onEvent public void onEvent(ValueChangeEvent event) { notify(event.getKey(), event.getAction(), find.apply(event.getKey())); } public &lt;T extends Record&gt; void notify(final String key, final DataOperation action, final T value) { ...... for (RecordListener listener : listenerMap.get(key)) { try { if (action == DataOperation.CHANGE) { listener.onChange(key, value); //执行updateIps更新内存注册表 continue; } if (action == DataOperation.DELETE) { listener.onDelete(key); } } catch (Throwable e) { ...... } } } public void onChange(String key, Instances value) throws Exception { ...... // 更新注册表（这个方法，在AP架构师，重点介绍过） updateIPs(value.getInstanceList(), KeyBuilder.matchEphemeralInstanceListKey(key)); recalculateChecksum(); } 至此，整个Leader节点的持久化节点注册逻辑就完成了！ 同步数据给其它Follower节点，就是HTTP调用“raft/datum/commit”接口，处理逻辑比主节点简单！ 三 Nacos CP集群选举过程 1 集群节点启动时，会执行RaftCore.init()核心方法 这个Init()核心方法，会启动2个核心定时任务（每500ms执行一次）： 选举任务：new MasterElection() 心跳任务：new HeartBeat() @Component public class RaftCore implements Closeable { @PostConstruct public void init() throws Exception { // 启动CP集群节点 raftStore.loadDatums(notifier, datums); //从本地磁盘文件加载数据 // 如果Leader周期不存在，则置为0 setTerm(NumberUtils.toLong(raftStore.loadMeta().getProperty(&quot;term&quot;), 0L)); initialized = true; // 每500ms做一次选举任务 masterTask = GlobalExecutor.registerMasterElection(new MasterElection()); // 每500ms做一次心跳任务 heartbeatTask = GlobalExecutor.registerHeartbeat(new HeartBeat()); versionJudgement.registerObserver(isAllNewVersion -&gt; { stopWork = isAllNewVersion; if (stopWork) { try { shutdown(); raftListener.removeOldRaftMetadata(); } catch (NacosException e) { throw new NacosRuntimeException(NacosException.SERVER_ERROR, e); } } }, 100); // 注册PersistentNotifier监听器，用来监听处理 ValueChangeEvent 事件，保存内存中服务注册表时用的 NotifyCenter.registerSubscriber(notifier); } } 2 选举任务的核心run()方法 public class MasterElection implements Runnable { @Override public void run() { try { RaftPeer local = peers.local(); local.leaderDueMs -= GlobalExecutor.TICK_PERIOD_MS; if (local.leaderDueMs &gt; 0) { return; } // Raft选举前的随机休眠阶段（15s到20s之间的随机值） local.resetLeaderDue(); // 重新心跳时间为5s local.resetHeartbeatDue(); // 率先跳出休眠的节点，发起投票 sendVote(); } catch (Exception e) { } } private void sendVote() { RaftPeer local = peers.get(NetUtils.localServer()); // 重置集群节点投票 peers.reset(); // 选举周期+1 local.term.incrementAndGet(); // 默认投给自己 local.voteFor = local.ip; // 把自己的状态改为 “候选者” local.state = RaftPeer.State.CANDIDATE; Map&lt;String, String&gt; params = new HashMap&lt;&gt;(1); params.put(&quot;vote&quot;, JacksonUtils.toJson(local)); for (final String server : peers.allServersWithoutMySelf()) { // 其它节点的API接口为：/raft/vote final String url = buildUrl(server, API_VOTE); try { // 向其它节点发出选票 HttpClient.asyncHttpPost(url, null, params, new Callback&lt;String&gt;() { @Override //其它节点给本节点的响应 public void onReceive(RestResult&lt;String&gt; result) { if (!result.ok()) { Loggers.RAFT.error(&quot;NACOS-RAFT vote failed: {}, url: {}&quot;, result.getCode(), url); return; } RaftPeer peer = JacksonUtils.toObj(result.getData(), RaftPeer.class); // 判断是否达到半数选票，成为Leader peers.decideLeader(peer); } @Override public void onError(Throwable throwable) { } @Override public void onCancel() { } }); } catch (Exception e) { } } } } 3 其它节点收到投票后的处理逻辑 如果收到的候选节点的term小于自己本地节点的term，则voteFor自己；（我更适合做Leader，这一票我投给自己） 否则，重置自己的election timeout，设置voteFor为收到的候选节点，更新集群周期term为候选节点的term；（我同意收到的节点做Leader） 给Http的调用方返回response； 四 Nacos CP集群的心跳任务 1 心跳任务由Leader节点发出，有2个作用 确定Follower节点在线； 帮助Follower节点判断数据是否一致；（因为服务注册或变更时，Leader节点自己修改了，且收到了“过半”以上节点的ack，但是不排除有些节点没有执行成功，所以通过心跳任务，进行纠错容错） 2 心跳任务的核心run()方法(只有Leader节点才可以向其它节点发送心跳包) public class HeartBeat implements Runnable { @Override public void run() { try { if (stopWork) { return; } if (!peers.isReady()) { return; } RaftPeer local = peers.local(); // 任务每0.5s执行一次，每次减0.5s，总共5s减完后，就可以开始sendBeat() local.heartbeatDueMs -= GlobalExecutor.TICK_PERIOD_MS; if (local.heartbeatDueMs &gt; 0) { return; } // 重置心跳间隔时间为5s local.resetHeartbeatDue(); sendBeat(); } catch (Exception e) { Loggers.RAFT.warn(&quot;[RAFT] error while sending beat {}&quot;, e); } } private void sendBeat() throws IOException, InterruptedException { RaftPeer local = peers.local(); // 如果当前是单机模式，或者本节点不是Leader节点，则无权发送心跳，直接跳过 if (EnvUtil.getStandaloneMode() || local.state != RaftPeer.State.LEADER) { return; } local.resetLeaderDue(); // build data ObjectNode packet = JacksonUtils.createEmptyJsonNode(); packet.replace(&quot;peer&quot;, JacksonUtils.transferToJsonNode(local)); ArrayNode array = JacksonUtils.createEmptyArrayNode(); if (switchDomain.isSendBeatOnly()) { Loggers.RAFT.info(&quot;[SEND-BEAT-ONLY] {}&quot;, switchDomain.isSendBeatOnly()); } // 封装心跳包，从内存中获取Leader节点注册表缓存中抽取数据的key和timestamp值 if (!switchDomain.isSendBeatOnly()) { for (Datum datum : datums.values()) { ObjectNode element = JacksonUtils.createEmptyJsonNode(); if (KeyBuilder.matchServiceMetaKey(datum.key)) { element.put(&quot;key&quot;, KeyBuilder.briefServiceMetaKey(datum.key)); } else if (KeyBuilder.matchInstanceListKey(datum.key)) { element.put(&quot;key&quot;, KeyBuilder.briefInstanceListkey(datum.key)); } element.put(&quot;timestamp&quot;, datum.timestamp.get()); array.add(element); } } packet.replace(&quot;datums&quot;, array); // broadcast Map&lt;String, String&gt; params = new HashMap&lt;String, String&gt;(1); params.put(&quot;beat&quot;, JacksonUtils.toJson(packet)); String content = JacksonUtils.toJson(params); // 对心跳包做gzip压缩 ByteArrayOutputStream out = new ByteArrayOutputStream(); GZIPOutputStream gzip = new GZIPOutputStream(out); gzip.write(content.getBytes(StandardCharsets.UTF_8)); gzip.close(); byte[] compressedBytes = out.toByteArray(); String compressedContent = new String(compressedBytes, StandardCharsets.UTF_8); // 把压缩后的心跳包，发送给除自己外的其他所有节点 for (final String server : peers.allServersWithoutMySelf()) { try { // 心跳包的API为：/raft/beat final String url = buildUrl(server, API_BEAT); HttpClient.asyncHttpPostLarge(url, null, compressedBytes, new Callback&lt;String&gt;() { @Override public void onReceive(RestResult&lt;String&gt; result) { peers.update(JacksonUtils.toObj(result.getData(), RaftPeer.class)); } @Override public void onError(Throwable throwable) { } @Override public void onCancel() { } }); } catch (Exception e) { } } } } 3 Follower节点收到心跳包后的处理逻辑 Leader发出的心跳包中，包含了数据中的所有key和timestamp，Follower节点通过遍历对比，可以排查自己数据是否为最新最全数据； 如果数据不是最新或最全的，则批量从Leader节点获取不一致的数据的最新值；（Leader节点新增或修改的数据） 同时要删除掉自己比Leader多出来的数据；（Leader节点删除掉的数据） public RaftPeer receivedBeat(JsonNode beat) throws Exception { ...从心跳包中解析数据... // 设置Leader为发送心跳包给我的机器，因为只有Leader才可以发送心跳包 peers.makeLeader(remote); if (!switchDomain.isSendBeatOnly()) { // receivedKeysMap 的作用是判断出本节点 比 Leader节点多出来的数据（见方法的最后） Map&lt;String, Integer&gt; receivedKeysMap = new HashMap&lt;&gt;(datums.size()); for (Map.Entry&lt;String, Datum&gt; entry : datums.entrySet()) { // 如果这个Map中的数据为0，则代表是本地自己的数据； // 接收到的主节点数据时，把对应的值改为1； // 那么直到处理最后，这个Map中还有0，说明这条数据在主节点并没有，只有一种可能，这条数据在主节点中被删除掉了！（妙） // 最后，可以把这些数据，在本地清除掉 receivedKeysMap.put(entry.getKey(), 0); } // batch用来收集本节点没有的数据，或者不是最新的数据 List&lt;String&gt; batch = new ArrayList&lt;&gt;(); int processedCount = 0; // 已处理的数据条数 for (Object object : beatDatums) { processedCount = processedCount + 1; ...... receivedKeysMap.put(datumKey, 1); try { // 包含，且我自己缓存中这条key对应的数据的时间戳&gt;=收到的心跳中的数据，则代表这条数据我有，就可以跳出本轮 if (datums.containsKey(datumKey) &amp;&amp; datums.get(datumKey).timestamp.get() &gt;= timestamp &amp;&amp; processedCount &lt; beatDatums.size()) { continue; } // 取反，不满足上面的条件，则说明这条数据我没有，或者不是最新的，则收集到batch中 // 因为节点变化的时候，虽然Leader节点收到了半数以上的ack，但是毕竟还有可能有些节点没有收到，或者处理不成功，所以这里通过心跳包进行数据同步的容错处理 if (!(datums.containsKey(datumKey) &amp;&amp; datums.get(datumKey).timestamp.get() &gt;= timestamp)) { batch.add(datumKey); } // 当batch的数据量&gt;=50或者数据已经全部处理完，则就可以继续下面向Leader节点发起批量请求数据的逻辑； // 反过来，如果batch&lt;50，且数据还没有处理完，那么这里先跳过，不要向Leader节点发起批量获取数据的请求 if (batch.size() &lt; 50 &amp;&amp; processedCount &lt; beatDatums.size()) { continue; } String keys = StringUtils.join(batch, &quot;,&quot;); // 如果batch为空，当然也不用发请求 if (batch.size() &lt;= 0) { continue; } // update datum entry String url = buildUrl(remote.ip, API_GET); Map&lt;String, String&gt; queryParam = new HashMap&lt;&gt;(1); queryParam.put(&quot;keys&quot;, URLEncoder.encode(keys, &quot;UTF-8&quot;)); // 从Leader批量获取本节点缺少的或过时的数据 HttpClient.asyncHttpGet(url, null, queryParam, new Callback&lt;String&gt;() { @Override public void onReceive(RestResult&lt;String&gt; result) { ...获取缺少的或过时的数据成功后... for (JsonNode datumJson : datumList) { Datum newDatum = null; OPERATE_LOCK.lock(); try { ...... // 和上面Leader节点新增数据时候逻辑相同，写内存注册表 raftStore.write(newDatum); datums.put(newDatum.key, newDatum); notifier.notify(newDatum.key, DataOperation.CHANGE, newDatum.value); ...... } catch (Throwable e) { } finally { OPERATE_LOCK.unlock(); } } return; } @Override public void onError(Throwable throwable) { } @Override public void onCancel() { } }); batch.clear(); } catch (Exception e) { } } // 如果最后receivedKeysMap中还有value为0的数据，说明这些数据在主节点已经被删除了，那我们从节点也主动删除一下 List&lt;String&gt; deadKeys = new ArrayList&lt;&gt;(); for (Map.Entry&lt;String, Integer&gt; entry : receivedKeysMap.entrySet()) { if (entry.getValue() == 0) { deadKeys.add(entry.getKey()); } } for (String deadKey : deadKeys) { try { deleteDatum(deadKey); //删除本节点多出来的数据逻辑 } catch (Exception e) { } } } return local; } ","link":"https://tianxiawuhao.github.io/zolXTKRvU/"},{"title":"Nacos核心源码剖析（AP架构）——注册中心","content":"Nacos官方文档：https://nacos.io/zh-cn/docs/quick-start.html 服务端对外暴露的API：https://nacos.io/zh-cn/docs/open-api.html Nacos的Server端其实就是一个Web服务，对外提供了Http服务接口，所有的客户端与服务端的通讯都通过Http调用完成（短链接）。 Nacos注册服务核心类：NacosNamingService Nacos配置中心核心类：NacosConfigService 一 微服务中常用的注册中心对比 Zookeeper（Apache）：典型的CP架构，有Leader节点，在选举Leader的过程中，整个集群对外不可用，为了强一致性，牺牲高可用性！（Client与Server之间为心跳维持的TCP长连接） Eureka（Netflix）：AP架构，为了高可用性，牺牲强一致性；服务提供者新节点注册后，消费者需要一定的时间后才能拿到最新服务列表，最长可达60s； Nacos（阿里）：参考了Zookeeper+Eureka，同时支持AP/CP架构，集群默认为AP架构，也可以通过配置切换为CP架构（Raft）；服务列表变动后，消费者获取最新列表最然会有一点延迟，但是比Eureka好很多，而且还可以通过udp实时通知，虽然UDP可靠性无法保证！（Client与Server之间为短链接Http调用） 二 NACOS的服务架构图 服务注册+服务心跳：首先无论是“服务提供者”还是“服务消费者”都会将自己注册到nacos，并维持心跳。（每5秒发送一次心跳包） 服务健康检查：服务端再启动后，会以Service为单位，开启ClientBeatCheckTask心跳检查任务。（每5秒检查一次，如果某个客户端最后一次心跳超过15秒，标记为不健康，超过30秒踢除） 服务发现：“服务消费者”会根据需要自己的自己所需的目标服务的namespace/group/serviceName/cluster只根据需要查询对应的服务注册表，保存在本地。（定时每10s去服务端更新一次）！ 服务同步：服务端集群之间会同步服务注册表，用来保证服务信息的一致性！（注意AP架构的集群中，即使配置了mysql，也不是用来存放注册表） 三 Nacos的核心注册表结构（双层ConcurrentHashMap） 1 Nacos和Eureka的注册表底层都是双层ConcurrentHashMap // 本篇只介绍Nacos public class ServiceManager implements RecordListener&lt;Service&gt; { // Nacos服务注册表的实际存储结构（双层Map） Map&lt;String, Map&lt;String, Service&gt;&gt; serviceMap = new ConcurrentHashMap&lt;&gt;() //Map&lt;nameSpaceId, Map&lt;group::serverName, Service&gt;&gt; ————&gt; 通过nameSpaceId + group::serviceName定位到具体的服务(Service) //其中的Service服务实例的结构： public class Service { private Map&lt;String, Cluster&gt; clusterMap = new HashMap&lt;&gt;(); //Map&lt;clusterName, Cluster&gt; ————&gt; 在具体的serviceInstance内通过clusterName定位到具体的Cluster集群 //而Cluster集群，又是这样的结构 public class Cluster { private Set&lt;Instance&gt; ephemeralInstances = new HashSet&lt;&gt;(); //这就是实际可以对外提供的单个服务（serviceInstanceItem） } } } 总结：Nacos底层数据结构，显示一个双层Map， —— 1、服务发现阶段，通过nameSpaceId, group::serviceName找到对应的服务 Service服务 —— 2、在服务Service内通过clusterName定位到具体的集群Cluster —— 3、在Cluster集群里面以HashSet的形式，存放着所有能够提供服务的每个实例Instance（这个Instance中有访问它的详细信息），最后把整个Set列表返回给客户端即可! 2 Nacos这么多层的配置，该如何使用？ 最佳实践一（中小型公司）： // namespace：用来区分不通的项目，如haier-iot / haier-code / cold-chain // group：用来区分不通项目的 prod / test / dev 等环境 // ————spring.application.name———— // cluster：可以以低于来划分集群：BJ / NJ / SH // 示例： spring: application: name: haier-iot-device-manager cloud: nacos: discovery: server-addr: 10.206.73.156:8848 namespace: haier-iot group: dev cluster-name: BJ //可以不区分 config: server-addr: 10.206.73.156:8848 file-extension: yaml namespace: haier-iot group: dev cluster-name: BJ //可以不区分 最佳实践二（大型公司）： //与最佳实践一的区别在于，直接使用nacos项目专用，直接使用 namespace 区分环境 // namespace：直接用来区分 prod / test / dev 等环境 // group：使用 DEFAULT_GROUP，因为微服务有可能太多，管理容易混乱；同时这一层可以做扩展，比如多个小服务可能属于另一个大服务下； // ————spring.application.name———— // cluster：可以以低于来划分集群：BJ / NJ / SH // 示例： spring: application: name: haier-iot-device-manager cloud: nacos: discovery: server-addr: 10.206.73.156:8848 namespace: dev group: DEFAULT_GROUP //默认GROUP可以不指定 cluster-name: BJ //可以不区分 config: server-addr: 10.206.73.156:8848 namespace: dev group: DEFAULT_GROUP //默认GROUP可以不指定 file-extension: yaml 3 为什么Nacos要设计这么复杂的数据结构 因为Nacos是一个开放的产品，为了适应绝大多数使用者的使用场景，所以扩展性一定要好，这么多层的设计，几乎可以满足任意复杂的业务场景！ 三 Nacos的注册表写入性能保证 1 Nacos怎么负责的注册表结构，如何支撑高并发场景？（阻塞队列、异步注册） // 使用内存阻塞队列实现异步注册 —— 当接收到provider的注册时，Nacos服务端会将任务封装成Task public class DistroConsistencyServiceImpl{ @PostConstruct public void init() { GlobalExecutor.submitDistroNotifyTask(notifier); } // 而notifier是一个线程，单线程处理服务注册任务，也避免了“并发覆盖”问题！ public class Notifier implements Runnable { private BlockingQueue&lt;Pair&lt;String, DataOperation&gt;&gt; tasks = new ArrayBlockingQueue&lt;&gt;(1024 * 1024); // run()方法就是在处理放入到tasks队列中的Task任务 @Override public void run() { for (; ; ) { // 死循环，即使出现异常也不会退出 try { Pair&lt;String, DataOperation&gt; pair = tasks.take(); //阻塞队列不消耗CPU handle(pair); } catch (Throwable e) { Loggers.DISTRO.error(&quot;[NACOS-DISTRO] Error while handling notifying task&quot;, e); } } } } // 新的Instance任务被封装成Task任务，放入到Notifier中 public void put(String key, Record value) throws NacosException { onPut(key, value); // 任务被封装成 distroProtocol.sync(new DistroKey(key, KeyBuilder.INSTANCE_LIST_KEY_PREFIX), DataOperation.CHANGE, globalConfig.getTaskDispatchPeriod() / 2); } public void onPut(String key, Record value) { ...边缘逻辑... notifier.addTask(key, DataOperation.CHANGE); } } 2 使用阻塞队列实现异步注册，会不会存在不一致问题，还没注册成功就给客户端返回结果？ 是的，肯定会存在这个问题，但是这是一个取舍，高性能的中间件内部都使用了大量的异步操作； 想一想，我们的应用程序可能依赖很多第三方服务，如果第三方中间件都用同步的方式去执行自己的内部逻辑，那么应用程序的启动将变得非常地缓慢，最后的效果肯定是难以接受的； —— 支持高并发！ —— 要说不及时，之前的Eureka更严重！ 其实正常情况下，几乎不会太过阻塞，因为几乎没有多少公司，是一次性增加n多台服务的，都是慢慢添加的，而且即使个别慢了，也是可以接受的，先用其它服务节点即可，站在服务消费者的角度，也就是provider服务起得有点慢而已。 3 为了解决高并发下的读写冲突问题，Nacos使用了CopyOnWrite方案 // 在Notifier.run()方法中： listener.onChange(datumKey, dataStore.get(datumKey).value); | com.alibaba.nacos.naming.core.Service#onChange(){ updateIPs(value.getInstanceList(), KeyBuilder.matchEphemeralInstanceListKey(key)); } | com.alibaba.nacos.naming.core.Service#updateIPs(){ clusterMap.get(entry.getKey()).updateIps(entryIPs, ephemeral); } | com.alibaba.nacos.naming.core.Cluster#updateIps{ Set&lt;Instance&gt; toUpdateInstances = ephemeral ? ephemeralInstances : persistentInstances; // 将旧的临时实例ephemeralInstances列表，复制转化为一个Map进行更新操作 HashMap&lt;String, Instance&gt; oldIpMap = new HashMap&lt;&gt;(toUpdateInstances.size()); for (Instance ip : toUpdateInstances) { oldIpMap.put(ip.getDatumKey(), ip); } //...对旧Set拷贝后转化为HashMap进行更新操作... toUpdateInstances = new HashSet&lt;&gt;(ips); if (ephemeral) { ephemeralInstances = toUpdateInstances; } else { persistentInstances = toUpdateInstances; } } 4 同时n多个实例注册或更新，都进行CopyOnWrite，岂不是会存在“更新覆盖”？ // 1、首先，根据上面的第1条，Notifier的执行是一个单线程执行任务： /// Notifier所在类DistroConsistencyServiceImpl是一个单例Service，@PostConstruct决定了init方法只会被调用一次： //// 而GlobalExecutor 是一个单线程的线程池，所以处理实例注册的最终线程只会有一个 @PostConstruct public void init() { GlobalExecutor.submitDistroNotifyTask(notifier); } // 2、CopyOnWrite后的集合中的元素不能直接修改，因为集合中的元素是引用！ // —— 当新增时，直接在新集合中新增Instance，然后替换原注册表中的集合即可！ // —— 当删除时，直接将新集合中的对应Instance删除，然后替换原注册表中的集合即可！ // —— 当更新时，新增一个Instance，然后删除原集合中的Instance元素，增加新的Instance元素即可！ // ————原则就是：永远是替换，不直接修改原Instance对象！ 5 随着注册表的不断增大，进行CopyOnWrite时候的成本是不是变得非常大？ // 当然不是每次直接Copy整张注册表，那样开销肯定很大 // 每次Copy的粒度是缩小到Service下对应的Cluster中的Set&lt;Instance&gt;集合，这个粒度是很小的！ public class Cluster extends com.alibaba.nacos.api.naming.pojo.Cluster implements Cloneable { private Set&lt;Instance&gt; persistentInstances = new HashSet&lt;&gt;(); // AP模式实例列表（服务发现得到的列表就是它） private Set&lt;Instance&gt; ephemeralInstances = new HashSet&lt;&gt;(); // CP模式实例列表（服务发现得到的列表就是它） // 更新节点的操作（Cluster级别） public void updateIps(List&lt;Instance&gt; ips, boolean ephemeral) { Set&lt;Instance&gt; toUpdateInstances = ephemeral ? ephemeralInstances : persistentInstances; HashMap&lt;String, Instance&gt; oldIpMap = new HashMap&lt;&gt;(toUpdateInstances.size()); //...对旧Set拷贝后转化为HashMap进行更新操作... toUpdateInstances = new HashSet&lt;&gt;(ips); if (ephemeral) { ephemeralInstances = toUpdateInstances; } else { persistentInstances = toUpdateInstances; } } } // 为了性能，CopyOnWrite的粒度一定要越小越好！ 四 Nacos的心跳机制（定时去调Nacos服务端Http接口） 核心类：NacosNamingService 1 Client在向服务端注册服务的同时，开启定时任务向服务端发送心跳请求 // com.alibaba.nacos.client.naming.NacosNamingService#registerInstance() // 既是注册服务的核心代码，也是发送心跳的核心代码 public void registerInstance(String serviceName, String groupName, Instance instance) throws NacosException { String groupedServiceName = NamingUtils.getGroupedName(serviceName, groupName); if (instance.isEphemeral()) { BeatInfo beatInfo = beatReactor.buildBeatInfo(groupedServiceName, instance); beatReactor.addBeatInfo(groupedServiceName, beatInfo); // 发送心跳 } serverProxy.registerService(groupedServiceName, groupName, instance); // 注册实例 } | public void addBeatInfo(String serviceName, BeatInfo beatInfo) { ... // 第一次调用 = 触发心跳任务 executorService.schedule(new BeatTask(beatInfo), beatInfo.getPeriod(), TimeUnit.MILLISECONDS); ... } | //BeatTask.run()任务核心代码： public void run() { if (!this.beatInfo.isStopped()) { // 计算下一次发送的时间 long nextTime = this.beatInfo.getPeriod(); try { //此处就是去调用“发送心跳API” JsonNode result = BeatReactor.this.serverProxy.sendBeat(this.beatInfo, BeatReactor.this.lightBeatEnabled); ...对心跳发送结果进行处理... } catch (NacosException var11) { ...log... } //第二次发送心跳，循环进行，就形成定时发送心跳的效果 BeatReactor.this.executorService.schedule(BeatReactor.this.new BeatTask(this.beatInfo), nextTime, TimeUnit.MILLISECONDS); } } 我们再看看执行“心跳任务”的线程长啥样： // 定时任务线程 this.executorService = new ScheduledThreadPoolExecutor(threadCount, new ThreadFactory() { @Override public Thread newThread(Runnable r) { Thread thread = new Thread(r); thread.setDaemon(true); // 守护线程（所有用户线程结束后，守护线程会自动结束） thread.setName(&quot;com.alibaba.nacos.naming.beat.sender&quot;); return thread; } }); 所以“心跳包”的核心就是：通过一个定时任务守护线程，定时去调用Nacos服务端的发送心跳包的API接口！ 2 “心跳包”的默认间隔时间是多少？（5-15-30） // 构建心跳包的信息 BeatInfo beatInfo = this.beatReactor.buildBeatInfo(groupedServiceName, instance); beatInfo.setPeriod(instance.getInstanceHeartBeatInterval()); public long getInstanceHeartBeatInterval() { return this.getMetaDataByKeyWithDefault(&quot;preserved.heart.beat.interval&quot;, Constants.DEFAULT_HEART_BEAT_INTERVAL); } //而Constants.DEFAULT_HEART_BEAT_INTERVAL是个常量 static { DEFAULT_HEART_BEAT_TIMEOUT = TimeUnit.SECONDS.toMillis(15L); //15秒 收不到心跳，则会被标记为“不健康” DEFAULT_IP_DELETE_TIMEOUT = TimeUnit.SECONDS.toMillis(30L); //30秒 收不到心跳，则“剔除”该实例IP DEFAULT_HEART_BEAT_INTERVAL = TimeUnit.SECONDS.toMillis(5L); //默认心跳时间 5秒 3 Nacos服务端对心跳包的处理逻辑(服务健康检查)？(定时任务，每5秒健康检查) // 以Service为单位，每个Service在被初始化时，都会创建一个健康检查器HealthCheckReactor public class Service { public void init() { // HealthCheckReactor 健康检查器，也是通过schduled线程池去做检查的 HealthCheckReactor.scheduleCheck(clientBeatCheckTask); for (Map.Entry&lt;String, Cluster&gt; entry : clusterMap.entrySet()) { entry.getValue().setService(this); entry.getValue().init(); } } } | // HealthCheckReactor.scheduleCheck()方法就是开启定时任务线程 |线程池大小为1~核数/2 public static void scheduleCheck(BeatCheckTask task) { futureMap.putIfAbsent(task.taskKey(), GlobalExecutor.scheduleNamingHealth(task, 5000, 5000, TimeUnit.MILLISECONDS)); //延迟5秒后，每5秒执行一次 } 健康检查任务的核心 run() 方法逻辑： public class ClientBeatCheckTask implements BeatCheckTask { @Override public void run() { //拿出Service中所有的实例（后面遍历检查） List&lt;Instance&gt; instances = service.allIPs(true); // 系统当前时间 - 最后一次心跳时间 &gt; 不健康阈值（15秒），标记为不健康 for (Instance instance : instances) { if (System.currentTimeMillis() - instance.getLastBeat() &gt; instance.getInstanceHeartBeatTimeOut()) { if (!instance.isMarked()) { if (instance.isHealthy()) { instance.setHealthy(false); Loggers.EVT_LOG .info(&quot;{POS} {IP-DISABLED} valid: {}:{}@{}@{}, region: {}, msg: client timeout after {}, last beat: {}&quot;, instance.getIp(), instance.getPort(), instance.getClusterName(), service.getName(), UtilsAndCommons.LOCALHOST_SITE, instance.getInstanceHeartBeatTimeOut(), instance.getLastBeat()); getPushService().serviceChanged(service); } } } } if (!getGlobalConfig().isExpireInstance()) { return; } // 系统当前时间 - 最后一次心跳时间 &gt; 可剔除阈值（30秒），直接剔除 for (Instance instance : instances) { if (System.currentTimeMillis() - instance.getLastBeat() &gt; instance.getIpDeleteTimeout()) { // delete instance Loggers.SRV_LOG.info(&quot;[AUTO-DELETE-IP] service: {}, ip: {}&quot;, service.getName(), JacksonUtils.toJson(instance)); deleteIp(instance); } } } } 所以“服务健康检查”的逻辑就是：服务端以Service为单位，使用定时任务线程池，每5秒检查一次Service中所有实例的状态： 最后心跳时间距当前超过15秒，标记为不健康； 最后心跳时间距当前超过30秒，将此实例踢除！ 五 服务发现 当服务消费者需要查询自己需要的服务列表时，会优先从本地缓存注册表获取数据，第一次获取为空时，才会从远程Server端获取； 从远程Server获取服务列表的粒度为Cluster粒度，同时还会将自己的udp端口告诉Server端，便于Server变化时的主动通知； 从远程Server获取列表的同时，还会启动定时任务，每隔10秒从Server端同步一次自己的注册表（只同步自己需要的）； udp通知的可靠性不能保证，但是影响不大，因为有定时任务同步托底！ 1 获取服务实例列表核心方法：NacosNamingService#getAllInstances() public List&lt;Instance&gt; getAllInstances(String serviceName, String groupName, List&lt;String&gt; clusters, boolean subscribe) { ServiceInfo serviceInfo; if (subscribe) { // 默认是开启订阅（udp通知），所以走这一分支 serviceInfo = hostReactor.getServiceInfo(NamingUtils.getGroupedName(serviceName, groupName), StringUtils.join(clusters, &quot;,&quot;)); } else { serviceInfo = hostReactor .getServiceInfoDirectlyFromServer(NamingUtils.getGroupedName(serviceName, groupName), StringUtils.join(clusters, &quot;,&quot;)); } List&lt;Instance&gt; list; if (serviceInfo == null || CollectionUtils.isEmpty(list = serviceInfo.getHosts())) { return new ArrayList&lt;Instance&gt;(); } return list; } | public ServiceInfo getServiceInfo(final String serviceName, final String clusters) { String key = ServiceInfo.getKey(serviceName, clusters); if (failoverReactor.isFailoverSwitch()) { return failoverReactor.getService(key); // 故障转移功能，从故障转移文件获取服务列表 } // 从本地缓存的注册表获取服务列表 ServiceInfo serviceObj = getServiceInfo0(serviceName, clusters); if (null == serviceObj) { // 第一次启动时候，缓存肯定为空，所以会走这一分支 serviceObj = new ServiceInfo(serviceName, clusters); serviceInfoMap.put(serviceObj.getKey(), serviceObj); updatingMap.put(serviceName, new Object()); updateServiceNow(serviceName, clusters); // 核心去远程获取服务列表的方法 updatingMap.remove(serviceName); } else if (updatingMap.containsKey(serviceName)) { if (UPDATE_HOLD_INTERVAL &gt; 0) { // hold a moment waiting for update finish synchronized (serviceObj) { try { serviceObj.wait(UPDATE_HOLD_INTERVAL); } catch (InterruptedException e) { NAMING_LOGGER .error(&quot;[getServiceInfo] serviceName:&quot; + serviceName + &quot;, clusters:&quot; + clusters, e); } } } } scheduleUpdateIfAbsent(serviceName, clusters); // 开启定时任务，定时更新本地注册表 return serviceInfoMap.get(serviceObj.getKey()); } 从远程获取服务列表没啥看的，我们重点看看定时任务更新本地缓存注册表的逻辑： public void scheduleUpdateIfAbsent(String serviceName, String clusters) { synchronized (futureMap) { if (futureMap.get(ServiceInfo.getKey(serviceName, clusters)) != null) { return; } // UpdateTask见名知意 ScheduledFuture&lt;?&gt; future = addTask(new UpdateTask(serviceName, clusters)); futureMap.put(ServiceInfo.getKey(serviceName, clusters), future); } } | // 看看UpdateTask.run()核心方法： public void run() { long delayTime = -1; try { ...一系列逻辑，但是最终都会走finally中的逻辑... delayTime = serviceObj.getCacheMillis(); } catch (Throwable e) { NAMING_LOGGER.warn(&quot;[NA] failed to update serviceName: &quot; + serviceName, e); } finally { if (delayTime &gt; 0) { // delayTime默认为10秒 executor.schedule(this, delayTime, TimeUnit.MILLISECONDS); } } } 所以“服务发现”的逻辑就是：在客户端启动时，会根据需要从Nacos服务端获取自己需要的服务列表（Cluster级别）， 并保存到本地缓存中的注册表中，并开启一个定时任务，每隔10秒去服务端同步一下对应的注册表； 之后每次需要时，都是从本地缓存中的注册表获取服务列表即可！ 2 如何尽可能地保证本地注册表的实时性？(开启订阅，开放udp端口) 从第一条中我们看到一个开启订阅的逻辑，在对应的分支中，从服务端获取服务列表时： updateServiceNow(serviceName, clusters); String result = serverProxy.queryList(serviceName, clusters, pushReceiver.getUdpPort(), false); // pushReceiver.getUdpPort() // 可以知道，从服务端获取服务列表时，顺便把自己的udp端口也传给了服务端 // 那么当服务端发现对应的服务列表有变动时，就可以通过此Udp端口通知到本Client 六 服务同步 ​ Nacos集群即使配置了外部mysql数据库，注册表信息也是存储在每个节点的内存中的，而不是存储在mysql中，而当Client向Nacos服务端注册时，只会选择一个Nacos Server节点注册，那么就必须有一套机制能够实现Nacos集群的各个节点都能同步到数据，Nacos自己实现了一套Distro协议，以实现分布式集群各节点之间的数据最终一致性！ 1 什么时Distro协议？ Distro协议时Nacos社区自研的一套AP分布式协议，为了集群的高可用，牺牲强一致性，只追求最终一致性！ Nacos集群的每个节点时平等的，都可以处理读写请求，同时把数据同步到其他节点； 每个节点只负责部分数据（服务健康检查等），定时发送自己负责的数据的校验值到其他节点，以保证数据的一致性； 每个节点独立处理请求，不需要经过其他节点同意，及时从本地发起对Client端的相应！ 2 Nacos集群中的节点，如何知道其它节点的存在？ 得熟悉Nacos AP集群得部署方式，Nacos集群在部署时，需要在配置 cluster.conf 文件中配置集群得各个节点，这样每台机器就都知道集群中得其它节点得ip:port了; @Component(&quot;serverListManager&quot;) public class ServerListManager extends MemberChangeListener { @PostConstruct public void init() { // 集群节点状态同步任务，它会每2秒调用集群其它节点的状态接口，以判断节点是否还在线！ GlobalExecutor.registerServerStatusReporter(new ServerStatusReporter(), 2000); GlobalExecutor.registerServerInfoUpdater(new ServerInfoUpdater()); } // 集群节点状态同步任务，每2秒执行一次， ServerStatusReporter.run(){ // 很简单，就是调用其它节点的状态接口，告诉其它机器自己还活着（集群中每两台机器直接都会互相调用）； // 如果某个节点在一定时间内，没有收到其它某个节点的状态报告，那就认为这个节点挂了，就会更新自己本地认为的集群存活节点数； // 集群存活节点数会直接影响到“服务健康检查”的目标机器核心变量，从而决定每个Service，将会在哪个Server节点被执行健康检查！ synchronizer.send(server.getAddress(), msg); } } 3 “服务注册”任务由哪个节点负责？如何同步数据到其他节点？ “服务注册”任务，有Client端发起，根据负载均衡算法挑选一台Server机器进行注册； 被挑选到的Server节点，处理自己的注册任务的同时，通过Distro协议，同步到集群中的其它节点； // com.alibaba.nacos.naming.consistency.ephemeral.distro.DistroConsistencyServiceImpl#put public void put(String key, Record value) throws NacosException { // 在本机处理服务注册请求 onPut(key, value); // 同步给其它机器进行注册 distroProtocol.sync(new DistroKey(key, KeyBuilder.INSTANCE_LIST_KEY_PREFIX), DataOperation.CHANGE, globalConfig.getTaskDispatchPeriod() / 2); } 4 如何判断各个Service的健康检查任务，由集群中的哪个节点负责检查？ // 我们找到心跳检查任务的run()方法： ClientBeatCheckTask.run(){ // 判断是否该由本节点负责该Service的心跳检查任务 if (!getDistroMapper().responsible(service.getName())) { return; } ...如果是自己负责该Service的心跳检查，才会继续执行心跳检查任务... } // 判断逻辑 public boolean responsible(String serviceName) { final List&lt;String&gt; servers = healthyList; if (!switchDomain.isDistroEnabled() || EnvUtil.getStandaloneMode()) { return true; } if (CollectionUtils.isEmpty(servers)) { // means distro config is not ready yet return false; } int index = servers.indexOf(EnvUtil.getLocalAddress()); int lastIndex = servers.lastIndexOf(EnvUtil.getLocalAddress()); if (lastIndex &lt; 0 || index &lt; 0) { return true; // 自己不在集群列表中，那可能当前就不是集群部署，所以自己得检查 } // 对serviceName进行hash后，对当前集群节点数量取余，看看是不是自己 // 如果不是自己，不用担心，其它机器在被注册时，也会走到这条逻辑，总有一台机器是负责该Service的“健康检查”的 int target = distroHash(serviceName) % servers.size(); return target &gt;= index &amp;&amp; target &lt;= lastIndex; } 5 集群间两个重要的同步任务 1. ServerListManager下的ServerStatusReporter任务： —— 上面已经讲过，在集群之间通过定时调用状态接口的方式，同步集群各节点的在线状态！ 2. ServiceManager下的ServiceReporter任务： —— 当某个节点执行完健康检查后，如果发现某个Service实例状态改变了，它必须要同步给集群中其它节点，修改各自注册表中的状态（通过调用InstanceController中的API接口实现） 6 如果有新节点加入集群，如果从其它节点同步数据？ // 每个节点启动时，会注入一个DistroProtocol的Bean @Component public class DistroProtocol { // 在DistroProtocol的构造函数中，会启动DistroTask数据同步任务 public DistroProtocol(ServerMemberManager memberManager, DistroComponentHolder distroComponentHolder, DistroTaskEngineHolder distroTaskEngineHolder, DistroConfig distroConfig) { this.memberManager = memberManager; this.distroComponentHolder = distroComponentHolder; this.distroTaskEngineHolder = distroTaskEngineHolder; this.distroConfig = distroConfig; startDistroTask(); } private void startDistroTask() { // 如果时单节点运行，就不用同步啦 if (EnvUtil.getStandaloneMode()) { isInitialized = true; return; } startVerifyTask(); startLoadTask(); // 开启数据加载任务 } private void startLoadTask() { DistroCallback loadCallback = new DistroCallback() { @Override public void onSuccess() { isInitialized = true; } @Override public void onFailed(Throwable throwable) { isInitialized = false; } }; GlobalExecutor.submitLoadDataTask( new DistroLoadDataTask(memberManager, distroComponentHolder, distroConfig, loadCallback)); } } //DistroLoadDataTask任务的核心run()方法： DistroLoadDataTask.run(){ try { load(); // 从其它节点加载数据 if (!checkCompleted()) { // 如果不成功，就开个延时任务，过会儿继续尝试去加载 GlobalExecutor.submitLoadDataTask(this, distroConfig.getLoadDataRetryDelayMillis()); } else { loadCallback.onSuccess(); Loggers.DISTRO.info(&quot;[DISTRO-INIT] load snapshot data success&quot;); } } catch (Exception e) { loadCallback.onFailed(e); Loggers.DISTRO.error(&quot;[DISTRO-INIT] load snapshot data failed. &quot;, e); } } 真正的load()从远程加载逻辑： private void load() throws Exception { while (memberManager.allMembersWithoutSelf().isEmpty()) { Loggers.DISTRO.info(&quot;[DISTRO-INIT] waiting server list init...&quot;); TimeUnit.SECONDS.sleep(1); } while (distroComponentHolder.getDataStorageTypes().isEmpty()) { Loggers.DISTRO.info(&quot;[DISTRO-INIT] waiting distro data storage register...&quot;); TimeUnit.SECONDS.sleep(1); } for (String each : distroComponentHolder.getDataStorageTypes()) { if (!loadCompletedMap.containsKey(each) || !loadCompletedMap.get(each)) { loadCompletedMap.put(each, loadAllDataSnapshotFromRemote(each)); } } } // for循环尝试从所有远程节点获取注册表全量文件，只要有一个成功，则跳出for循环 private boolean loadAllDataSnapshotFromRemote(String resourceType) { DistroTransportAgent transportAgent = distroComponentHolder.findTransportAgent(resourceType); DistroDataProcessor dataProcessor = distroComponentHolder.findDataProcessor(resourceType); if (null == transportAgent || null == dataProcessor) { Loggers.DISTRO.warn(&quot;[DISTRO-INIT] Can't find component for type {}, transportAgent: {}, dataProcessor: {}&quot;, resourceType, transportAgent, dataProcessor); return false; } for (Member each : memberManager.allMembersWithoutSelf()) { try { // 调取远程节点的获取DatumSnapshot快照数据接口 DistroData distroData = transportAgent.getDatumSnapshot(each.getAddress()); // 处理数据，加载到本节点内存的注册表中，完成新节点数据初始化 boolean result = dataProcessor.processSnapshot(distroData); if (result) { return true; // 有一个节点成功，则跳出全部for循环，直接返回成功结果 } } catch (Exception e) { ...... } } return false; } ","link":"https://tianxiawuhao.github.io/zMtbXOeeT/"},{"title":"上传文件到亚马逊云S3对象存储","content":"一 在亚马逊云S3创建一个存储桶，并设置权限 1 创建一个存储桶，并将权限设置为公开，因为我正常时候是用来存放网站图片 2 配置存储卷策略 { &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ { &quot;Sid&quot;: &quot;PublicReadGetObject&quot;, &quot;Effect&quot;: &quot;Allow&quot;, &quot;Principal&quot;: &quot;*&quot;, &quot;Action&quot;: &quot;s3:GetObject&quot;, &quot;Resource&quot;: &quot;arn:aws:s3:::test.jiguiquan.com/*&quot; } ] } 3 配置跨域策略 [ { &quot;AllowedHeaders&quot;: [ &quot;*&quot; ], &quot;AllowedMethods&quot;: [ &quot;PUT&quot;, &quot;POST&quot;, &quot;GET&quot; ], &quot;AllowedOrigins&quot;: [ &quot;*&quot; ], &quot;ExposeHeaders&quot;: [ &quot;x-amz-server-side-encryption&quot;, &quot;x-amz-request-id&quot;, &quot;x-amz-id-2&quot; ], &quot;MaxAgeSeconds&quot;: 3000 } ] 到这里，存储卷的配置就算是OK啦，满足正常网站图片的使用了！ 二 编写Java类，完成文件的上传 1 在项目中引入 aws-sdk 的依赖 &lt;dependency&gt; &lt;groupId&gt;com.amazonaws&lt;/groupId&gt; &lt;artifactId&gt;aws-java-sdk-s3&lt;/artifactId&gt; &lt;version&gt;1.11.347&lt;/version&gt; &lt;/dependency&gt; 2 编写上传文件的接口 @Api(tags = &quot;Auth——第三方服务模块&quot;) @RestController @RequiredArgsConstructor public class ThirdpartyController { @Value(&quot;${s3.accessKeyId}&quot;) private String s3AccessKeyId; @Value(&quot;${s3.accessKeySecret}&quot;) private String s3AccessKeySecret; @Value(&quot;${s3.bucketName}&quot;) private String s3BucketName; @Value(&quot;${s3.region}&quot;) private String s3Region; private static BasicAWSCredentials awsCreds; private static AmazonS3 s3; @PostConstruct private void init(){ awsCreds = new BasicAWSCredentials(s3AccessKeyId, s3AccessKeySecret); s3 = AmazonS3ClientBuilder.standard() .withCredentials(new AWSStaticCredentialsProvider(awsCreds)) //设置服务器所属地区 .withRegion(s3Region) .build(); } @ApiResponses({@ApiResponse(code = 200, message = &quot;文件url&quot;)}) @ApiOperation(&quot;后端直接上传文件到亚马逊云S3&quot;) @PostMapping(&quot;/auth/s3/upload&quot;) public BaseResponse&lt;String&gt; uploadFileToS3(@RequestParam(&quot;file&quot;) MultipartFile file) { if (file.getSize() == 0){ throw ZidanApiException.create(BmoonResponseCode.FILE_SIZE_ZERO); } if (StringUtils.isBlank(file.getOriginalFilename())){ throw ZidanApiException.create(BmoonResponseCode.FILE_NAME_EMPTY); } // String host = &quot;https://s3.&quot; + s3Region + &quot;.amazonaws.com/&quot; + s3BucketName; String host = &quot;https://s3.ap-northeast-2.amazonaws.com/test.jiguiquan.com&quot;; String format = new SimpleDateFormat(&quot;yyyyMMdd&quot;).format(new Date()); String uploadName = format + &quot;/&quot; + UUID.randomUUID().toString() + file.getOriginalFilename(); ObjectMetadata metadata = new ObjectMetadata(); metadata.setContentType(file.getContentType()); metadata.setContentLength(file.getSize()); InputStream inputStream = null; try { inputStream = file.getInputStream(); com.amazonaws.services.s3.model.PutObjectResult result = s3.putObject(new PutObjectRequest(s3BucketName, uploadName, inputStream, metadata)); System.out.println(uploadName + &quot;:文件的Md5为：&quot; + result.getContentMd5()); return BaseResponse.success(host + &quot;/&quot; + uploadName); }catch (Exception e) { e.printStackTrace(); throw ZidanApiException.create(BmoonResponseCode.FILE_UPLOAD_FAILED); } finally { if (inputStream != null){ try { inputStream.close(); } catch (IOException e) { e.printStackTrace(); } } } } } 三 测试文件的上传与访问 浏览器访问： ","link":"https://tianxiawuhao.github.io/PTmSAlH_U/"},{"title":"netty","content":"1. Netty简单介绍 1. 原生NIO存在的问题 为什么有了 NIO ，还会出现 Netty，因为 NIO 有如下问题： NIO 的类库和 API 繁杂，使用麻烦：需要熟练掌握 Selector、ServerSocketChannel、 SocketChannel、ByteBuffer 等。 需要具备其他的额外技能：要熟悉 Java 多线程编程，因为 NIO 编程涉及到 Reactor 模式，你必须对多线程和网络编程非常熟悉，才能编写出高质量的 NIO 程序。 开发工作量和难度都非常大：例如客户端面临断连重连、网络闪断、半包读写、失败缓存、网络拥塞和异常流的处理等等。 JDK NIO 的 Bug：例如臭名昭著的 Epoll Bug，它会导致 Selector 空轮询（死循环），最终导致 CPU 100%。直到 JDK 1.7 版本该问题仍旧存在，没有被根本解决 2. Netty官网说明 官网地址 ：https://netty.io/ Netty是一个异步事件驱动的网络应用程序框架，用于快速开发可维护的高性能的服务器和客户端。 Netty 是由 JBOSS 提供的一个 Java 开源框架。Netty 提供异步的、基于事件驱动的网络应用程序框架，用以快速开发高性能、高可靠性的网络 IO 程序。 Netty 可以帮助你快速、简单的开发出一个网络应用，相当于简化和流程化了 NIO 的 开发过程。 Netty 是目前最流行的 NIO 框架，Netty 在互联网领域、大数据分布式计算领域、游戏行业、通信行业等获得了广泛的应用，知名的 Elasticsearch 、Dubbo 框架内部都采 用了 Netty。 3. Netty的优点 Netty 对 JDK 自带的 NIO 的 API 进行了封装，解决了上述问题。 设计优雅：适用于各种传输类型的统一 API 阻塞和非阻塞 Socket；基于灵活且可扩展的事件模型，可以清晰地分离关注点；高度可定制的线程模型 —— 单线程，一个或多个 线程池. 使用方便：详细记录的 Javadoc，用户指南和示例；没有其他依赖项 (JDK 5 -&gt; Netty 3.x ; 6 -&gt; Netty 4.x 就可以支持了）。 高性能、吞吐量更高：延迟更低；减少资源消耗；最小化不必要的内存复制。 安全：完整的 SSL/TLS 和 StartTLS 支持。 社区活跃、不断更新：社区活跃，版本迭代周期短，发现的 Bug 可以被及时修复， 同时，更多的新功能会被加入 4. Netty版本说明 netty版本分为 netty3.x 和 netty4.x、netty5.x 因为Netty5出现重大bug，已经被官网废弃了，目前推荐使用的是Netty4.x的稳定版本 目前在官网可下载的版本 netty3.x netty4.0.x 和 netty4.1.x 这里使用的是 Netty4.1.x 版本 netty 下载地址： https://bintray.com/netty/downloads/netty/ 2. 各线程模式 1. 传统阻塞 I/O 服务模型 模型特点 采用阻塞IO模式获取输入的数据 每个连接都需要独立的线程完成数据的输入（read），业务处理， 数据返回（send） 问题分析 当并发数很大，就会创建大量的线程，占用很大系统资源 连接创建后，如果当前线程暂时没有数据可读，该线程会阻塞在read 操作，造成线程资源浪费 2. Reactor 模式 针对传统阻塞 I/O 服务模型的 2 个缺点，解决方案： 基于 I/O 复用模型：多个连接共用一个阻塞对象，应用程序只需要在一个阻塞对象等待，无需阻塞等待所有连接。当某个连接有新的数据可以处理时，操作系统通知应用程序，线程从阻塞状态返回，开始进行业务处理 基于线程池复用线程资源：不必再为每个连接创建线程，将连接完成后的业务处理任务分配给线程进行处理，一个线程可以处理多个连接的业务。 Reactor 对应的叫法: 反应器模式 分发者模式(Dispatcher) 通知者模式(notifier) 说明: Reactor 模式，通过一个或多个输入同时传递给服务处理器的模式 (基于事件驱动) 服务器端程序处理传入的多个请求, 并将它们同步分派到相应的处理线程， 因此Reactor（反应器）模式也叫 Dispatcher（分发者） 模式 Reactor 模式使用IO复用监听事件, 收到事件后，分发给某个线程(进程), 这点就是网络服务器高并发处理关键 核心组成： Reactor：Reactor 在一个单独的线程中运行，负责监听和分发事件，分发给适当的处 理程序来对 IO 事件做出反应。 它就像公司的电话接线员，它接听来自客户的电话并 将线路转移到适当的联系人； Handlers：处理程序执行 I/O 事件要完成的实际事件，类似于客户想要与之交谈的公司中的实际官员。Reactor 通过调度适当的处理程序来响应 I/O 事件，处理程序执行非阻塞操作 Reactor 模式分类： 单 Reactor 单线程 单 Reactor 多线程 主从 Reactor 多线程 3. 单Reactor-单线程 说明： Select 是前面 I/O 复用模型介绍的标准网络编程 API，可以实现应用程序通过一个阻塞对象监听多路连接请求 Reactor 对象通过 Select 监控客户端请求事件，收到事件后通过 Dispatch 进行分发 如果是建立连接请求事件，则由 Acceptor 通过 Accept 处理连接请求，然后创建一个 Handler 对象处理连接完成后的后续业务处理 如果不是建立连接事件，则 Reactor 会分发调用连接对应的 Handler 来响应 Handler 会完成 Read→业务处理→Send 的完整业务流程 结合实例：服务器端用一个线程通过多路复用搞定所有的 IO 操作（包括连接，读、写 等），编码简单，清晰明了，但是如果客户端连接数量较多，将无法支撑，前面的 NIO 案例就属于这种模型。 优点： 模型简单，没有多线程、进程通信、竞争的问题，全部都在一个线程中完成 缺点： 性能问题，只有一个线程，无法完全发挥多核 CPU 的性能。Handler 在处理某 个连接上的业务时，整个进程无法处理其他连接事件，很容易导致性能瓶颈 可靠性问题，线程意外终止，或者进入死循环，会导致整个系统通信模块不 可用，不能接收和处理外部消息，造成节点故障 使用场景：客户端的数量有限，业务处理非常快速，比如 Redis在业务处理的时间复 杂度 O(1) 的情况 4. 单 Reactor 多线程 说明： Reactor 对象通过 select 监控客户端请求事件, 收到事件后，通过 dispatch 进行分发 如果建立连接请求, 则由 Acceptor 通过 accept 处理连接请求, 然后创建一个Handler对象处理完成连接后的各种事件 如果不是连接请求，则由 Reactor 分发调用连接对 应的Handler 来处理 Handler 只负责响应事件，不做具体的业务处理, 通过 read 读取数据后，会分发给后面的 Worker 线程池的某个线程处理业务 Worker 线程池会分配独立线程完成真正的业务， 并将结果返回给 Handler Handler 收到响应后，通过 send 将结果返回给 Client 优点：可以充分的利用多核cpu 的处理能力 缺点：多线程数据共享和访问比较复杂,单 Reactor 处理所有的事件的监听和响应，在单线程运行， 在高并发场景容易出现性能瓶颈 5. 主从 Reactor 多线程 说明 1: Reactor 主线程 MainReactor 对象通过 select 监听连接事件, 收到事件后，通过 Acceptor 处理连接事件(主 Reactor 只处理连接事件) 2：当 Acceptor 处理连接事件后，MainReactor 将连接分配给 SubReactor 3：SubReactor 将连接加入到连接队列进行监听,并创建 Handler 进行各种事件处理 4：当有新事件发生时， SubReactor 就会调用对应的 Handler处理，Handler 通过 read 读取数据，分发给后面的 （Worker 线程池）处理 5：（Worker 线程池）分配独立的 （Worker 线程）进行业务处理，并返 回结果 6：Handler 收到响应的结果后，再通过 send 将结果返回给 Client ps：一个 MainReactor 可以关联多个 SubReactor 优点： 父线程与子线程的数据交互简单职责明确，父线程只需要接收新连接，子线程完成后续的业务处理。 父线程与子线程的数据交互简单，Reactor 主线程只需要把新连接传给子线程，子线程无需返回数据。 缺点：编程复杂度较高 结合实例：这种模型在许多项目中广泛使用，包括 Nginx 主从 Reactor 多进程模型， Memcached 主从多线程，Netty 主从多线程模型的支持 6. Reactor 模式小结 3 种模式用生活案例来理解 单 Reactor 单线程，前台接待员和服务员是同一个人，全程为顾客服 单 Reactor 多线程，1 个前台接待员，多个服务员，接待员只负责接待 主从 Reactor 多线程，多个前台接待员，多个服务生 Reactor 模式具有如下的优点： 响应快，不必为单个同步时间所阻塞，虽然 Reactor 本身依然是同步的 可以最大程度的避免复杂的多线程及同步问题，并且避免了多线程/进程 的切换开销 扩展性好，可以方便的通过增加 Reactor 实例个数来充分利用 CPU 资源 复用性好，Reactor 模型本身与具体事件处理逻辑无关，具有很高的复用性 3. Netty模型 1. 简单版 说明 ： BossGroup 线程维护Selector , 只关注Accecpt 当接收到Accept事件，获取到对应的 SocketChannel, 封装成 NIOScoketChannel并注册到 Worker 线程(事件循环), 并进行维护 当Worker线程监听到 Selector 中通道发生自己感 兴趣的事件后，就进行处理(就由 Handler 处理)， 注意 Handler 已经加入到通道 2. 进阶版 3. 完整版 非常重要 说明 ： 1：Netty 抽象出两组线程池 BossGroup 专门负责接收客户端的连接, WorkerGroup 专门负责网络的读写 2：BossGroup 和 WorkerGroup 类型都是 NioEventLoopGroup 3：NioEventLoopGroup 相当于一个事件循环组, 这个组中含有多个事件循环 ，每一个事件循环是 NioEventLoop 4：NioEventLoop 表示一个不断循环的执行处理任务的线程， 每个 NioEventLoop 都有一个 Selector , 用于监听绑定在其上的 Socket 的网络通讯 5:NioEventLoopGroup(BossGroup、WorkerGroup) 可以有多个线程, 即可以含有多个 NioEventLoop 6:每个Boss 的 NioEventLoop 循环执行的步骤有3步 (1):轮询accept 事件 (2):处理accept 事件 , 与client建立连接 , 生成NioScocketChannel , 并将其注册到 Worker 的 (3):NIOEventLoop 上的 Selector 处理任务队列的任务 ， 即 runAllTasks 7：每个 Worker 的 NIOEventLoop 循环执行的步骤 (1):轮询read, write 事件 (2):处理i/o事件， 即read , write 事件，在对应NioScocketChannel 处理 (3):处理任务队列的任务 ， 即 runAllTasks 每个Worker NIOEventLoop 处理业务时，会使用 Pipeline(管道), Pipeline 中包含了 Channel , 即通过 Pipeline 可以获取到对应通道, 管道中维护了很多的处理器。管道可以使用 Netty 提供的，也可以自定义 4. 理解重制版 服务器端首先创建一个ServerSocketChannel，bossGroup只处理客户端连接请求,workGroup处理读写事件，此二者为线程组，其中每一个NIOEventLoop都是线程组其中一个线程 客户端发送连接请求通过ServerSocketChannel被NIOLoopEventGroup线程组中的一个线程NIOLoopEvent的selector选择器监听，并将事件放入taskqueue队列进行轮询，一旦有accept事件就会封装NIOSocketChannel对象，并通过其去注册到workGroup的线程的selector中让其监听 一旦workGroup的线程的taskqueue轮询到读写事件，在对应的NIOSocketChannel进行处理 4. Netty实例分析 1. BossGroup 和 WorkGroup 怎么确定自己有多少个 NIOEventLoop BossGroup 和 WorkerGroup 含有的子线程数（NioEventLoop）默认为 CPU 核数*2 由源码中的构造方法可知 —— 想要设置线程数只要在参数中输入即可 2. WorkerGroup 是如何分配这些进程的 设置 BossGroup 进程数为 1 ； WorkerGroup 进程数为 4 ； Client 数位 8 在默认情况下，WorkerGroup 分配的逻辑就是按顺序循环分配的 3. BossGroup 和 WorkerGroup 中的 Selector 和 TaskQueue 打断点进行 Debug 每个子线程都具有自己的 Selector、TaskQueue…… 4. CTX 上下文、Channel、Pipeline 之间关系 修改 NettyServerHandler ，并添加端点 先看 CTX 上下文中的信息 Pipeline Channel CTX 上下文、Channel、Pipeline 三者关系示意图 5. 设置通道参数 childOption() 方法 给每条child channel 连接设置一些TCP底层相关的属性，比如上面，我们设置了两种TCP属性，其中 ChannelOption.SO_KEEPALIVE表示是否开启TCP底层心跳机制，true为开 option() 方法 对于server bootstrap而言，这个方法，是给parent channel 连接设置一些TCP底层相关的属性。 TCP连接的参数详细介绍如下。SO_RCVBUF ，SO_SNDBUF 这两个选项就是来设置TCP连接的两个buffer尺寸的。 每个TCP socket在内核中都有一个发送缓冲区和一个接收缓冲区，TCP的全双工的工作模式以及TCP的滑动窗口便是依赖于这两个独立的buffer以及此buffer的填充状态。 SO_SNDBUF Socket参数，TCP数据发送缓冲区大小。该缓冲区即TCP发送滑动窗口，linux操作系统可使用命令：cat /proc/sys/net/ipv4/tcp_smem 查询其大小。 TCP_NODELAY TCP参数，立即发送数据，默认值为Ture（Netty默认为True而操作系统默认为False）。该值设置Nagle算法的启用，改算法将小的碎片数据连接成更大的报文来最小化所发送的报文的数量，如果需要发送一些较小的报文，则需要禁用该算法。Netty默认禁用该算法，从而最小化报文传输延时。 这个参数，与是否开启Nagle算法是反着来的，true表示关闭，false表示开启。通俗地说，如果要求高实时性，有数据发送时就马上发送，就关闭，如果需要减少发送次数减少网络交互，就开启。 SO_KEEPALIVE 底层TCP协议的心跳机制。Socket参数，连接保活，默认值为False。启用该功能时，TCP会主动探测空闲连接的有效性。可以将此功能视为TCP的心跳机制，需要注意的是：默认的心跳间隔是7200s即2小时。Netty默认关闭该功能。 SO_REUSEADDR Socket参数，地址复用，默认值False。有四种情况可以使用： (1).当有一个有相同本地地址和端口的socket1处于TIME_WAIT状态时，而你希望启动的程序的socket2要占用该地址和端口，比如重启服务且保持先前端口。 (2).有多块网卡或用IP Alias技术的机器在同一端口启动多个进程，但每个进程绑定的本地IP地址不能相同。 (3).单个进程绑定相同的端口到多个socket上，但每个socket绑定的ip地址不同。(4).完全相同的地址和端口的重复绑定。但这只用于UDP的多播，不用于TCP。 SO_LINGER Socket参数，关闭Socket的延迟时间，默认值为-1，表示禁用该功能。-1表示socket.close()方法立即返回，但OS底层会将发送缓冲区全部发送到对端。0表示socket.close()方法立即返回，OS放弃发送缓冲区的数据直接向对端发送RST包，对端收到复位错误。非0整数值表示调用socket.close()方法的线程被阻塞直到延迟时间到或发送缓冲区中的数据发送完毕，若超时，则对端会收到复位错误。 SO_BACKLOG Socket参数，服务端接受连接的队列长度，如果队列已满，客户端连接将被拒绝。默认值，Windows为200，其他为128。 b.option(ChannelOption.SO_BACKLOG, 1024) 表示系统用于临时存放已完成三次握手的请求的队列的最大长度，如果连接建立频繁，服务器处理创建新连接较慢，可以适当调大这个参数. SO_BROADCAST Socket参数，设置广播模式。 5. TaskQueue 任务队列 任务队列中的 Task 有 3 种典型使用场景 用户程序自定义的普通任务 用户自定义定时任务 非当前 Reactor 线程调用 Channel 的各种方法 6. 异步模型 1. 工作示意图 说明 1:在使用 Netty 进行编程时，拦截操作和转换出入站数据只需要您提供 callback 或利用 future 即可。这使得链式操作简单、高效, 并有利于编写可重用的、通用的代码。 2:Netty 框架的目标就是让你的业务逻辑从网络基础应用编码中分离出来 2. Future-Listener 机制 当 Future 对象刚刚创建时，处于非完成状态，调用者可以通过返回的 ChannelFuture 来获取操作执行的状态，注册监听函数来执行完成后的操作。 常见有如下操作 • 通过 isDone 方法来判断当前操作是否完成； • 通过 isSuccess 方法来判断已完成的当前操作是否成功； • 通过 getCause 方法来获取已完成的当前操作失败的原因； • 通过 isCancelled 方法来判断已完成的当前操作是否被取消； • 通过 addListener 方法来注册监听器，当操作已完成(isDone 方法返回完成)，将会通知 指定的监听器；如果 Future 对象已完成，则通知指定的监听器 代码示例 给一个 ChannelFuture 注册监听器，来监控我们关系的事件 channelFuture.addListener(new ChannelFutureListener() { @Override public void operationComplete(ChannelFuture channelFuture) throws Exception { if (channelFuture.isSuccess()){ System.out.println(&quot;监听端口 6668 成功&quot;); }else { System.out.println(&quot;监听端口 6668 失败&quot;); } } }); 3. 快速入门实例-HTTP服务 Netty 可以做Http服务开发，并且理解Handler实例 和客户端及其请求的关系 编写代码 —— 服务端代码 # 编写服务端： HttpServer public static void main(String[] args) throws Exception { //创建BossGroup ,workGroup EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workGroup = new NioEventLoopGroup(); try { ServerBootstrap serverBootstrap = new ServerBootstrap(); serverBootstrap.group(bossGroup,workGroup) .channel(NioServerSocketChannel.class) .childHandler(new TestServerInitializer()); ChannelFuture channelFuture = serverBootstrap.bind(8080).sync(); channelFuture.channel().closeFuture().sync(); } finally { bossGroup.shutdownGracefully(); workGroup.shutdownGracefully(); } } 编写 服务初始化器 ：HttpServerInitialize public class TestServerInitializer extends ChannelInitializer&lt;SocketChannel&gt; { @Override protected void initChannel(SocketChannel socketChannel) throws Exception { //向管道加入处理器 ChannelPipeline pipeline = socketChannel.pipeline(); //加入netty提供的httpServerCodec,netty提供的处理Http的编-解码器 pipeline.addLast(&quot;MyHttpServerCodec&quot;,new HttpServerCodec()); pipeline.addLast(&quot;MyTestHttpServerHandler&quot;,new TestHttpServerHandler()); } } 编写 服务处理器 ：HttpServerHandler /** * * 1. SimpleChannelInboundHandler 是之前使用的 ChannelInboundHandlerAdapter 的子类 * 2. HttpObject 这个类型表示， 客户端、服务端 相互通信的数据需要被封装成什么类型 * */ public class TestHttpServerHandler extends SimpleChannelInboundHandler&lt;HttpObject&gt; { /** * 读取客户端数据 * @param ctx * @param msg * @throws Exception */ @Override protected void channelRead0(ChannelHandlerContext ctx, HttpObject msg) throws Exception { //判断msg 是不是HttpRequest 请求 if(msg instanceof HttpRequest){ System.out.println(&quot;msg 类型 = &quot;+msg.getClass()); System.out.println(&quot;客户端地址&quot;+ctx.channel().remoteAddress()); // 获取请求的 URI HttpRequest httpRequest = (HttpRequest) msg; URI uri = new URI(httpRequest.uri()); // 判断请求路径为 /favicon.ico，就不做处理 if (&quot;/favicon.ico&quot;.equals(uri.getPath())){ System.out.println(&quot;请求了 图标 资源，不做响应&quot;); return; } //回复信息给浏览器【HTTP协议】 ByteBuf content = Unpooled.copiedBuffer(&quot;Hello I am server&quot;, CharsetUtil.UTF_8); //构造一个http回应，即httpResponse,HttpResponseStatus:状态码 FullHttpResponse response = new DefaultFullHttpResponse(HttpVersion.HTTP_1_1, HttpResponseStatus.OK,content); response.headers().set(HttpHeaderNames.CONTENT_TYPE,&quot;text/plain&quot;); response.headers().set(HttpHeaderNames.CONTENT_LENGTH,content.readableBytes()); //将构建好的response返回 ctx.writeAndFlush(response); } } 编写代码 —— 对特定资源的过滤 上面的服务端启动后，在页面上不止接收到了文本，还接收到了一个网页的图标 现在把它过滤掉 // 修改 HttpServerHandler // 获取请求的 URI HttpRequest httpRequest = (HttpRequest) msg; URI uri = new URI(httpRequest.uri()); // 判断请求路径为 /favicon.ico，就不做处理 if (&quot;/favicon.ico&quot;.equals(uri.getPath())){ System.out.println(&quot;请求了 图标 资源，不做响应&quot;); return; } 7. netty核心组件 1. Bootstrap 和 ServerBootstrap Bootstrap 意思是引导，一个 Netty 应用通常由一个 Bootstrap 开始，主要作用是配置整个 Netty 程序，串联各个组件，Netty 中 Bootstrap类是客户端程序的启动引导类， ServerBootstrap是服务端启动引导类 常见的方法有 • public ServerBootstrap group(EventLoopGroup parentGroup, EventLoopGroup childGroup)，该方法用于服务器端，用来设置两个 EventLoop • public B channel(Class&lt;? extends C&gt; channelClass)，该方法用来设置一个服务器端的通道实现 • public ChannelFuture bind(int inetPort) ，该方法用于服务器端，用来设置占用的端口号 • public B option(ChannelOption option, T value)，用来给 ServerChannel 添加配置 • public B group(EventLoopGroup group) ，该方法用于客户端，用来设置一个 EventLoop • public ChannelFuture connect(String inetHost, int inetPort) ，该方法用于客户端，用来连接服务器 端 • public ServerBootstrap childOption(ChannelOption childOption, T value)，用来给接收到的通道添加配置 • public ServerBootstrap childHandler(ChannelHandler childHandler)，该方法用来设置业务处理类 （自定义的 handler） .childHandler(new TestServerInitializer())//对应workGroup .handler(null)//对应bossGroup 2. Future 和 ChannelFuture Netty 中所有的 IO 操作都是异步的，不能立刻得知消息是否被正确处理。但是可以过一会等它执行完成或者直接注册一个监听，具体的实现就是通过 Future 和 ChannelFutures，他们可以注册一个监听，当操作执行成功或失败时监听会自动触发注册的监听事件 常见的方法有 • Channel channel()，返回当前正在进行 IO 操作的通道 • ChannelFuture sync()，等待异步操作执行完毕 3. Channel Netty 网络通信的组件，能够用于执行网络 I/O 操作。 通过Channel 可获得当前网络连接的通道的状态 通过Channel 可获得网络连接的配置参数 （例如接收缓冲区大小） Channel 提供异步的网络 I/O 操作(如建立连接，读写，绑定端口)，异步调用意味着任何 I/O 调用都将立即返回，并且不保证在调用结束时所请求的 I/O 操作已完成 调用立即返回一个 ChannelFuture 实例，通过注册监听器到 ChannelFuture 上，可以 I/O 操作成功、失败或取消时回调通知调用方 支持关联 I/O 操作与对应的处理程序 不同协议、不同的阻塞类型的连接都有不同的 Channel 类型与之对应 常用的 Channel 类型 • NioSocketChannel，异步的客户端 TCP Socket 连接。 • NioServerSocketChannel，异步的服务器端 TCP Socket 连接。 • NioDatagramChannel，异步的 UDP 连接。 • NioSctpChannel，异步的客户端 Sctp 连接。 • NioSctpServerChannel，异步的 Sctp 服务器端连接，这些通道涵盖了 UDP 和 TCP 网络 IO 以及文件 IO。 4. Selector Netty 基于 Selector 对象实现 I/O 多路复用，通过 Selector 一个线程可以监听多个连接的 Channel 事件。 当向一个 Selector 中注册 Channel 后，Selector 内部的机制就可以自动不断地查询 (Select) 这些注册的 Channel 是否有已就绪的 I/O 事件（例如可读，可写，网络连接 完成等），这样程序就可以很简单地使用一个线程高效地管理多个 Channel 5. ChannelHandler 我们经常需要自定义一 个 Handler 类去继承 ChannelInboundHandlerA dapter，然后通过重写相应方法实现业务逻辑 常用的方法 public class ChannelInboundHandlerAdapter extends ChannelHandlerAdapter implements ChannelInboundHandler { // 通道注册事件 public void channelRegistered(ChannelHandlerContext ctx) throws Exception { ctx.fireChannelRegistered(); } // 通道注销事件 public void channelUnregistered(ChannelHandlerContext ctx) throws Exception { ctx.fireChannelUnregistered(); } // 通道就绪事件 public void channelActive(ChannelHandlerContext ctx) throws Exception { ctx.fireChannelActive(); } // 通道读取数据事件 public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception { ctx.fireChannelRead(msg); } // 通道读取数据完毕事件 public void channelReadComplete(ChannelHandlerContext ctx) throws Exception { ctx.fireChannelReadComplete(); } // 通道发生异常事件 public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception { ctx.fireExceptionCaught(cause); } } 6. Pipeline 和 ChannelPipeline ChannelPipeline 是一个 Handler 的集合，它负责处理和拦截 inbound（入栈） 或者 outbound（出栈） 的事件和操作，相当于一个贯穿 Netty 的链。(也可以这样理解： ChannelPipeline 是 保存 ChannelHandler 的 List，用于处理或拦截 Channel 的入站 和出站 事件 / 操作) ChannelPipeline 实现了一种高级形式的拦截过滤器模式，使用户可以完全控制事件的处理方式，以及 Channel 中各个的 ChannelHandler 如何相互交互 在 Netty 中每个 Channel 都有且仅有一个 ChannelPipeline 与之对应，它们的组成关系如下 说明 ： • 一个 Channel 包含了一个 ChannelPipeline，而 ChannelPipeline 中又维护了一个由 ChannelHandlerContext 组成的双向链表，并且每个 ChannelHandlerContext 中又关联着一个 ChannelHandler • 入站事件和出站事件在一个双向链表中，入站事件会从链表 head 往后传递到最后一个入站的 handler， 出站事件会从链表 tail 往前传递到最前一个出站的 handler，两种类型的 handler 互不干扰 常用方法 //把一个业务处理类（handler） 添加到链中的第一个位置 ChannelPipeline addFirst(ChannelHandler… handlers) //把一个业务处理类（handler） 添加到链中的最后一个位置 ChannelPipeline addLast(ChannelHandler… handlers) 8. Netty 群聊 要求： 编写一个 Netty 群聊系统，实现服务器端和客户端之间的数据简单通讯（非阻塞） 实现多人群聊 服务器端：可以监测用户上线，离线，并实现消息转发功能 客户端：通过channel 可以无阻塞发送消息给其它所有用户，同时可以接受其它用 户发送的消息(有服务器转发得到) 1. server端 public class ChatServer { private int port; public ChatServer(int port) { this.port = port; } public void run() throws Exception{ //创建bossGroup,workGroup EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workGroup = new NioEventLoopGroup(); try { //创建辅助工具 ServerBootstrap serverBootstrap = new ServerBootstrap(); //循环事件组 serverBootstrap.group(bossGroup,workGroup)//线程组 .channel(NioServerSocketChannel.class)//通道类型 .option(ChannelOption.SO_BACKLOG,128) .childOption(ChannelOption.SO_KEEPALIVE,true) .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() { @Override protected void initChannel(SocketChannel socketChannel) throws Exception { ChannelPipeline pipeline = socketChannel.pipeline(); //加入解码器 pipeline.addLast(&quot;decoder&quot;,new StringDecoder()); //加入编码器 pipeline.addLast(&quot;encoder&quot;,new StringEncoder()); pipeline.addLast(new ChatServerHandler()); } }); System.out.println(&quot;server is ok&quot;); ChannelFuture channelFuture = serverBootstrap.bind(port).sync(); channelFuture.channel().closeFuture().sync(); } finally { bossGroup.shutdownGracefully(); workGroup.shutdownGracefully(); } } public static void main(String[] args) throws Exception { new ChatServer(8080).run(); } } 2. serverHandler public class ChatServerHandler extends SimpleChannelInboundHandler&lt;String&gt; { /** * 定义一个 Channel 线程组，管理所有的 Channel, 参数 执行器 * GlobalEventExecutor =&gt; 全局事件执行器 * INSTANCE =&gt; 表示是单例的 */ private static ChannelGroup channelGroup = new DefaultChannelGroup(GlobalEventExecutor.INSTANCE); SimpleDateFormat simpleDateFormat = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); /** * 当连接建立之后，第一个被执行 * 一连接成功，就把当前的 Channel 加入到 ChannelGroup，并将上线消息推送给其他客户 */ @Override public void handlerAdded(ChannelHandlerContext ctx) throws Exception { Channel channel = ctx.channel(); // 将该客户上线的信息，推送给其他在线的 客户端 // 该方法，会将 ChannelGroup 中所有的 Channel 遍历，并发送消息 Date date = new Date(System.currentTimeMillis()); channelGroup.writeAndFlush(&quot;[client]&quot; +channel.remoteAddress()+&quot;[&quot;+simpleDateFormat.format(date)+&quot;]&quot;+&quot;加入聊天&quot;); channelGroup.add(channel); } /** * 当断开连接激活，将 XXX 退出群聊消息推送给当前在线的客户 * 当某个 Channel 执行到这个方法，会自动从 ChannelGroup 中移除 */ @Override public void handlerRemoved(ChannelHandlerContext ctx) throws Exception { Channel channel = ctx.channel(); Date date = new Date(System.currentTimeMillis()); channelGroup.writeAndFlush(&quot;[client]&quot;+channel.remoteAddress()+&quot;[&quot;+simpleDateFormat.format(date)+&quot;]&quot;+&quot;离开聊天&quot;); //channelGroup.remove(channel); 不需要，handlerRemoved（）直接删除了channel } /** * 提示客户端离线状态 * @param ctx * @throws Exception */ @Override public void channelInactive(ChannelHandlerContext ctx) throws Exception { Date date = new Date(System.currentTimeMillis()); System.out.println(ctx.channel().remoteAddress()+&quot;[&quot;+simpleDateFormat.format(date)+&quot;]&quot;+&quot;下线了&quot;); } /** * 提示客户端上线状态 * @param ctx * @throws Exception */ @Override public void channelActive(ChannelHandlerContext ctx) throws Exception { Date date = new Date(System.currentTimeMillis()); System.out.println(ctx.channel().remoteAddress()+&quot;[&quot;+simpleDateFormat.format(date)+&quot;]&quot;+&quot;上线了&quot;);; } /** * 读取消息，转发数据 * @param ctx * @param msg * @throws Exception */ @Override protected void channelRead0(ChannelHandlerContext ctx, String msg) throws Exception { Channel channel = ctx.channel(); Date date = new Date(System.currentTimeMillis()); //遍历，根据不同对象发送不同数据 channelGroup.forEach(ch -&gt;{ if(channel != ch){//不是自己的channel ch.writeAndFlush(&quot;[客户]&quot;+channel.remoteAddress()+&quot;[&quot;+simpleDateFormat.format(date)+&quot;]&quot;+&quot;发送&quot;+msg+&quot;/n&quot;); } else{ ch.writeAndFlush(&quot;[自己]发送了消息&quot;+&quot;[&quot;+simpleDateFormat.format(date)+&quot;]&quot;+msg+&quot;/n&quot;); } } ); } /** * 异常处理 * @param ctx * @param cause * @throws Exception */ @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception { ctx.close(); } } 3. client端 public class ChatClient { private final String HOST; private final int PORT; public ChatClient(String host, int port) { HOST = host; PORT = port; } public void run() throws InterruptedException { EventLoopGroup eventLoopGroup = new NioEventLoopGroup(); try { Bootstrap bootstrap = new Bootstrap(); bootstrap.group(eventLoopGroup) .channel(NioSocketChannel.class) .handler(new ChannelInitializer&lt;SocketChannel&gt;() { @Override protected void initChannel(SocketChannel socketChannel) throws Exception { ChannelPipeline pipeline = socketChannel.pipeline(); //加入解码器 pipeline.addLast(&quot;decoder&quot;,new StringDecoder()); //加入编码器 pipeline.addLast(&quot;encoder&quot;,new StringEncoder()); pipeline.addLast(new ChatClientHandler()); } }); ChannelFuture channelFuture = bootstrap.connect(HOST, PORT).sync(); System.out.println(&quot;client prepare is ok&quot;); Channel channel = channelFuture.channel(); //客户端需要输入信息，定义扫描器 Scanner scanner = new Scanner(System.in); while(scanner.hasNextLine()){ String s = scanner.nextLine(); channel.writeAndFlush(s); } channel.closeFuture().sync(); } finally { eventLoopGroup.shutdownGracefully(); } } public static void main(String[] args) throws InterruptedException { new ChatClient(&quot;localhost&quot;,8080).run(); } } 4. clientHandler public class ChatClientHandler extends SimpleChannelInboundHandler&lt;String&gt; { @Override protected void channelRead0(ChannelHandlerContext channelHandlerContext, String s) throws Exception { // 直接输出从服务端获得的信息 System.out.println(s.trim()); } } 9. 心跳机制 1. server端： public class MyServer { public static void main(String[] args) throws Exception{ //创建bossGroup,workGroup EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workGroup = new NioEventLoopGroup(); try { //创建辅助工具 ServerBootstrap serverBootstrap = new ServerBootstrap(); //循环事件组 serverBootstrap.group(bossGroup,workGroup)//线程组 .channel(NioServerSocketChannel.class)//通道类型 .handler(new LoggingHandler(LogLevel.INFO)) .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() { @Override protected void initChannel(SocketChannel socketChannel) throws Exception { ChannelPipeline pipeline = socketChannel.pipeline(); /* 说明： 1. IdleStateHandler 是 Netty 提供的 空闲状态处理器 2. 四个参数： readerIdleTime : 表示多久没有 读 事件后，就会发送一个心跳检测包，检测是否还是连接状态 writerIdleTime : 表示多久没有 写 事件后，就会发送一个心跳检测包，检测是否还是连接状态 allIdleTime : 表示多久时间既没读也没写 后，就会发送一个心跳检测包，检测是否还是连接状态 TimeUnit : 时间单位 1. 当 Channel 一段时间内没有执行 读 / 写 / 读写 事件后，就会触发一个 IdleStateEvent 空闲状态事件 2. 当 IdleStateEvent 触发后，就会传递给 Pipeline 中的下一个 Handler 去处理，通过回调下一个 Handler 的 userEventTriggered 方法，在该方法中处理 IdleStateEvent */ pipeline.addLast(new IdleStateHandler(3,5,7, TimeUnit.SECONDS)); pipeline.addLast(new MyServerHandler()); } }); System.out.println(&quot;server is ok&quot;); ChannelFuture channelFuture = serverBootstrap.bind(8080).sync(); channelFuture.channel().closeFuture().sync(); } finally { bossGroup.shutdownGracefully(); workGroup.shutdownGracefully(); } } } 2. handler public class MyServerHandler extends ChannelInboundHandlerAdapter { /** * @param ctx 上下文 @param evt 事件 * @throws Exception */ @Override public void userEventTriggered(ChannelHandlerContext ctx, Object evt) throws Exception { if(evt instanceof IdleStateEvent){ //将 evt 向下转型 IdleStateEvent IdleStateEvent event = (IdleStateEvent)evt; String eventType = null; //IdleStateEvent 枚举 switch (event.state()){ case READER_IDLE: eventType = &quot;读空闲&quot;; break; case WRITER_IDLE: eventType = &quot;写空闲&quot;; break; case ALL_IDLE: eventType = &quot;读写空闲&quot;; } System.out.println(ctx.channel().remoteAddress()+&quot;---超时事件---&quot;+eventType); } } } 说明： 1: IdleStateHandler 是 Netty 提供的 空闲状态处理器 2: 四个参数： readerIdleTime : 表示多久没有 读 事件后，就会发送一个心跳检测包，检测是否还是连接状态 writerIdleTime : 表示多久没有 写 事件后，就会发送一个心跳检测包，检测是否还是连接状态 allIdleTime : 表示多久时间既没读也没写 后，就会发送一个心跳检测包，检测是否还是连接状态 TimeUnit : 时间单位 3: 当 Channel 一段时间内没有执行 读 / 写 / 读写 事件后，就会触发一个 IdleStateEvent 空闲状态事件 4: 当 IdleStateEvent 触发后，就会传递给 Pipeline 中的下一个 Handler 去处理，通过回调下一个 Handler 的 userEventTriggered 方法，在该方法中处理 IdleStateEvent 10. 长连接 要求： 实现基于webSocket的长连接 的全双工的交互 改变Http协议多次请求的约束，实 现长连接了， 服务器可以发送消息 给浏览器 客户端浏览器和服务器端会相互感 知，比如服务器关闭了，浏览器会 感知，同样浏览器关闭了，服务器 会感知 代码实现 服务端 ：WebServer public class MyServer { public static void main(String[] args) throws InterruptedException { //创建bossGroup,workGroup EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workGroup = new NioEventLoopGroup(); try { //创建辅助工具 ServerBootstrap serverBootstrap = new ServerBootstrap(); //循环事件组 serverBootstrap.group(bossGroup,workGroup)//线程组 .channel(NioServerSocketChannel.class)//通道类型 .handler(new LoggingHandler(LogLevel.INFO)) .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() { @Override protected void initChannel(SocketChannel socketChannel) throws Exception { ChannelPipeline pipeline = socketChannel.pipeline(); //基于http协议使用http的编码和解码器 pipeline.addLast(new HttpServerCodec()); // 添加块处理器 pipeline.addLast(new ChunkedWriteHandler()); /* 说明： 1. 因为 HTTP 数据传输时是分段的，HttpObjectAggregator 可以将多个端聚合 2. 这就是为什么浏览器发送大量数据时，就会发出多次 HTTP 请求 */ pipeline.addLast(new HttpObjectAggregator(8192)); /* 说明： 1. 对于 WebSocket 是以 帧 的形式传递的 2. 后面的参数表示 ：请求的 URL 3. WebSocketServerProtocolHandler 将 HTTP 协议升级为 WebSocket 协议，即保持长连接 4. 切换协议通过一个状态码101 */ pipeline.addLast(new WebSocketServerProtocolHandler(&quot;/hello&quot;)); // 自定义的 Handler pipeline.addLast(new MyTextWebSocketFrameHandler()); } }); System.out.println(&quot;server is ok&quot;); ChannelFuture channelFuture = serverBootstrap.bind(8080).sync(); channelFuture.channel().closeFuture().sync(); } finally { bossGroup.shutdownGracefully(); workGroup.shutdownGracefully(); } } } 服务端的处理器 ：MyTextWebSocketFrameHandler /** * TextWebSocketFrame 类型，表示一个文本帧（flame） */ public class MyTextWebSocketFrameHandler extends SimpleChannelInboundHandler&lt;TextWebSocketFrame&gt; { @Override protected void channelRead0(ChannelHandlerContext ctx, TextWebSocketFrame msg) throws Exception { System.out.println(&quot;服务器收到消息&quot;+msg.text()); //回复消息 ctx.channel().writeAndFlush(new TextWebSocketFrame(&quot;服务器时间&quot;+ LocalDateTime.now()+msg.text())); } /** * 客户端连接后，触发方法 * @param ctx * @throws Exception */ @Override public void handlerAdded(ChannelHandlerContext ctx) throws Exception { //id表示唯一的值，Longtext是唯一的shortText 不是唯一 System.out.println(&quot;handlerAdded 被调用&quot;+ctx.channel().id().asLongText()); System.out.println(&quot;handlerAdded 被调用&quot;+ctx.channel().id().asShortText()); } @Override public void handlerRemoved(ChannelHandlerContext ctx) throws Exception { System.out.println(&quot;handlerRemove被调用&quot;+ctx.channel().id().asLongText()); } @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception { System.out.println(&quot;异常发生&quot;+cause.getMessage()); //关闭连接 ctx.close(); } } hello.html(浏览器) &lt;!DOCTYPE html&gt; &lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Title&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;form onsubmit=&quot;return false&quot;&gt; &lt;textarea id=&quot;message&quot; name=&quot;message&quot; style=&quot;height: 300px; width: 300px&quot;&gt;&lt;/textarea&gt; &lt;input type=&quot;button&quot; value=&quot;发送消息&quot; onclick=&quot;send(this.form.message.value)&quot;&gt; &lt;textarea id=&quot;responseText&quot; style=&quot;height: 300px; width: 300px&quot;&gt;&lt;/textarea&gt; &lt;input type=&quot;button&quot; value=&quot;清空内容&quot; onclick=&quot;document.getElementById('responseText').value=''&quot;&gt; &lt;/form&gt; &lt;script&gt; var socket; // 判断当前浏览器是否支持 WebSocket if (window.WebSocket){ socket = new WebSocket(&quot;ws://localhost:8080/hello&quot;); // 相当于 channelRead0 方法，ev 收到服务器端回送的消息 socket.onmessage = function (ev){ var rt = document.getElementById(&quot;responseText&quot;); rt.value = rt.value + &quot;\\n&quot; + ev.data; } // 相当于连接开启，感知到连接开启 socket.onopen = function (){ var rt = document.getElementById(&quot;responseText&quot;); rt.value = rt.value + &quot;\\n&quot; + &quot;连接开启……&quot;; } // 感知连接关闭 socket.onclose = function (){ var rt = document.getElementById(&quot;responseText&quot;); rt.value = rt.value + &quot;\\n&quot; + &quot;连接关闭……&quot;; } }else { alert(&quot;不支持 WebSocket&quot;); } // 发送消息到服务器 function send(message){ // 判断 WebSocket 是否创建好了 if (!window.socket){ return ; } // 判断 WebSocket 是否开启 if (socket.readyState == WebSocket.OPEN){ // 通过 Socket 发送消息 socket.send(message); }else { alert(&quot;连接未开启&quot;); } } &lt;/script&gt; &lt;/body&gt; &lt;/html&gt; ","link":"https://tianxiawuhao.github.io/Hbra5M-xo/"},{"title":"NIO","content":"1. 简介 Netty 是由 JBOSS 提供的一个 Java 开源框架，现为 Github上的独立项目。 Netty 是一个异步的、基于事件驱动（客户端的行为、读写事件）的网络应用框架，用以快速开发高性能、高可靠性的网络 IO 程序。 Netty主要针对在TCP协议下，面向Clients端的高并发应用，或者Peer-to-Peer场景下的大量数据持续传输的应用。 Netty本质是一个NIO框架，适用于服务器通讯相关的多种应用场景 要透彻理解Netty ， 需要先学习 NIO ， 这样我们才能阅读 Netty 的源码。 2. I/O 模型基本说明 I/O 模型简单的理解：就是用什么样的通道进行数据的发送和接收，很大程度上决定了程 序通信的性能 2.1. Java共支持3种网络编程模型/IO模式： BIO、NIO、AIO Java BIO ： 同步并阻塞(传统阻塞型)，服务器实现模式为一个连接一个线程，即客户端 有连接请求时服务器端就需要启动一个线程进行处理，如果这个连接不做任何事情会造成 不必要的线程开销 Java NIO ： 同步非阻塞，服务器实现模式为一个线程处理多个请求(连接)，即客户端发送的连接请求都会注册到多路复用器上，多路复用器轮询到连接有I/O请求就进行处理 Java AIO(NIO.2) ： 异步非阻塞，AIO 引入异步通道的概念，采用了 Proactor 模式，简化了程序编写，有效的请求才启动线程，它的特点是先由操作系统完成后才通知服务端程 2.2. BIO、NIO、AIO适用场景分析 BIO方式 适用于连接数目比较小且固定的架构，这种方式对服务器资源要求比较高， 并发局限于应用中，JDK1.4以前的唯一选择，但程序简单易理解。 NIO方式 适用于连接数目多且连接比较短（轻操作）的架构，比如聊天服务器，弹幕 系统，服务器间通讯等。编程比较复杂，JDK1.4开始支持。 AIO方式 使用于连接数目多且连接比较长（重操作）的架构，比如相册服务器，充分 调用OS参与并发操作，编程比较复杂，JDK7开始支持。 3. NIO具体介绍 Java NIO 全称 java non-blocking IO，是指 JDK 提供的新 API。从 JDK1.4 开始，Java 提供了一系列改进的输入/输出 的新特性，被统称为 NIO(即 New IO)，是同步非阻塞的 NIO 相关类都被放在 java.nio 包及子包下，并且对原 java.io 包中的很多类进行改写。 NIO 有三大核心部分：Channel(通道)，Buffer(缓冲区), Selector(选择器) NIO是 面向缓冲区 ，或者面向块编程的。数据读取到一个 它稍后处理的缓冲区，需要时可在缓冲区中前后移动，这就 增加了处理过程中的灵活性，使用它可以提供非阻塞式的高 伸缩性网络 Java NIO的非阻塞模式，使一个线程从某通道发送请求或者读取数据，但是它仅能得到目前可用的数据，如果目前没有数据可用时，就什么都不会获取，而不是保持线程阻塞，所以直至数据变的可以读取之前，该线程可以继续做其他的事情。 非阻塞写也是如此，一个线程请求写入一些数据到某通道，但不需要等待它完全写入，这 个线程同时可以去做别的事情。 通俗理解：NIO是可以做到用一个线程来处理多个操作的。假设有10000个请求过来, 根据实际情况，可以分配50或者100个线程来处理。不像之前的阻塞IO那样，非得分配10000个。 HTTP2.0使用了多路复用的技术，做到同一个连接并发处理多个请求，而且并发请求 的数量比HTTP1.1大了好几个数量级 3.1. NIO 三大核心 Selector 、 Channel 和 Buffer 的简单关系图 关系图的说明: 线程是非阻塞，buffer起很大的作用 每个 Channel 都会对应一个 Buffer Selector 对应一个线程， 一个 Selector 对应多个 Channel(连接) 该图反应了有三个 Channel 注册到该 selector 程序切换到哪个 Channel 是由事件决定的, Event 就是一个重要的概念 Selector 会根据不同的事件，在各个 Channel（通道）上切换 Buffer 就是一个内存块 ， 底层是有一个数组 数据的读取写入是通过 Buffer, 这个和BIO , BIO 中要么是输入流，或者是 输出流, 不能双向，但是NIO的 Buffer 是可以读也可以写, 需要 flip 方法切换 3.2. 常用Buffer子类一览 ByteBuffer，存储字节数据到缓冲区 =最常用= ShortBuffer，存储字符串数据到缓冲区 CharBuffer，存储字符数据到缓冲区 IntBuffer，存储整数数据到缓冲区 LongBuffer，存储长整型数据到缓冲区 DoubleBuffer，存储小数到缓冲区 FloatBuffer，存储小数到缓冲区 3.3. buffer常用方法 public abstract class Buffer { //JDK1.4时，引入的api public final int capacity( )// ★ 返回此缓冲区的容量 public final int position( )// ★ 返回此缓冲区的位置 public final Buffer position (int newPositio)// ★ 设置此缓冲区的位置 public final int limit( )// ★ 返回此缓冲区的限制 public final Buffer limit (int newLimit)// ★ 设置此缓冲区的限制 public final Buffer mark( )//在此缓冲区的位置设置标记 public final Buffer reset( )//将此缓冲区的位置重置为以前标记的位置 public final Buffer clear( )// ★ 清除此缓冲区, 即将各个标记恢复到初始状态，但是数据并没有真正擦除, 后面操作会覆盖 public final Buffer flip( )// ★ 反转此缓冲区 public final Buffer rewind( )//重绕此缓冲区 public final int remaining( )//返回当前位置与限制之间的元素数 public final boolean hasRemaining( )// ★ 告知在当前位置和限制之间是否有元素 public abstract boolean isReadOnly( );// ★ 告知此缓冲区是否为只读缓冲区 //JDK1.6时引入的api public abstract boolean hasArray();// ★ 告知此缓冲区是否具有可访问的底层实现数组 public abstract Object array();// ★ 返回此缓冲区的底层实现数组 public abstract int arrayOffset();//返回此缓冲区的底层实现数组中第一个缓冲区元素的偏移量 public abstract boolean isDirect();//告知此缓冲区是否为直接缓冲区 } 3.4. 通道(Channel) 1：基本介绍 NIO的通道类似于流，但有些区别如下： • 通道可以同时进行读写，而流只能读或者只能写 • 通道可以实现异步读写数据 • 通道可以从缓冲读数据，也可以写数据到缓冲: FileChannel主要用来对本地文件进行 IO 操作，常见的方法有 public int read(ByteBuffer dst) ，从通道读取数据并放到缓冲区中 public int write(ByteBuffer src) ，把缓冲区的数据写到通道中 public long transferFrom(ReadableByteChannel src, long position, long count)，从目标通道 中复制数据到当前通道 public long transferTo(long position, long count, WritableByteChannel target)，把数据从当 前通道复制给目标通道 首先channel与文件联立，判断是输入输出流看channel与buffer之间的关系 3.5. 关于Buffer 和 Channel的注意事项和细节 1：ByteBuffer 支持类型化的 put 和 get, put 放入的是什么数据类型，get 就应该使用相应的数据类型来取出（取出的顺序也要和存入的顺序一致），否则可能有 BufferUnderflowException 异常。 2： 可以将一个普通Buffer 转成只读Buffer，如果对一个只读类型的 Buffer 进行写操作会报错 ReadOnlyBufferException ByteBuffer buffer = ByteBuffer.allocate(3); ByteBuffer byteBuffer = buffer.asReadOnlyBuffer(); System.out.println(buffer); System.out.println(byteBuffer); 3：NIO 还提供了 MappedByteBuffer， 可以让文件直接在内存（堆外的内存）中进行修改， 而如何同步到文件由NIO 来完成. /* 说明 1. MappedByteBuffer 可以让文件直接在内存中修改，这样操作系统并不需要拷贝一次 2. MappedByteBuffer 实际类型是 DirectByteBuffer */ public static void main(String[] args) throws Exception { RandomAccessFile randomAccessFile = new RandomAccessFile(&quot;D:\\\\file01.txt&quot;, &quot;rw&quot;); // 获取对应的文件通道 FileChannel channel = randomAccessFile.getChannel(); // 参数 ：使用 只读/只写/读写 模式 ； 可以修改的起始位置 ； 映射到内存的大小，即可以将文件的多少个字节映射到内存 // 这里就表示，可以对 file01.txt 文件中 [0,5) 的字节进行 读写操作 MappedByteBuffer map = channel.map(FileChannel.MapMode.READ_WRITE, 0, 5); // 进行修改操作 map.put(0, (byte) 'A'); map.put(3, (byte) '3'); // 关闭通道 channel.close(); } 4：前面我们讲的读写操作，都是通过一个Buffer 完成的，NIO 还支持 通过多个 Buffer (即 Buffer 数组) 完成读写操作，即 Scattering 和 Gathering ，遵循 依次写入，依次读取。 public static void main(String[] args) throws Exception { // 使用 ServerSocketChannel 和 InetSocketAddress 网络 ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); InetSocketAddress inetSocketAddress = new InetSocketAddress(7000); //绑定端口到socket，启动 serverSocketChannel.socket().bind(inetSocketAddress); //创建buffer的2个数组 ByteBuffer[] byteBuffers = new ByteBuffer[2]; byteBuffers[0] = ByteBuffer.allocate(5); byteBuffers[1] = ByteBuffer.allocate(3); //等待客户端连接 SocketChannel socketChannel = serverSocketChannel.accept(); int messageLength = 8; //循环的读取 while(true){ // 表示累计读取的字节数 int byteRead = 0; // 假设从客户端最多接收 8 个字节 while (byteRead &lt; messageLength){ // 自动把数据分配到 byteBuffers-0、byteBuffers-1 long read = socketChannel.read(byteBuffers); byteRead += read; // 使用流打印，查看当前 Buffer 的 Position 和 Limit Arrays.asList(byteBuffers).stream(). map(byteBuffer -&gt; &quot;{position: &quot;+byteBuffer.position()+&quot;, limit: &quot;+byteBuffer.limit()+&quot;}&quot;) .forEach(System.out::println); } // 将所有的 Buffer 进行反转，为后面的其他操作做准备 Arrays.asList(byteBuffers).forEach(Buffer -&gt;Buffer.flip()); // 将数据读出，显示到客户端 int byteWrite = 0; while (byteWrite &lt; messageLength){ long write = socketChannel.write(byteBuffers); byteWrite += write; } // 将所有的 Buffer 进行清空，为后面的其他操作做准备 Arrays.asList(byteBuffers).forEach(Buffer-&gt;Buffer.clear()); // 打印处理的字节数 System.out.println(&quot;{byteRead: &quot;+byteRead+&quot;, byteWrite: &quot;+byteWrite+&quot;}&quot;); } } 3.6. Selector选择器 1. 基本介绍 1: Java 的 NIO，用非阻塞的 IO 方式。可以用一个线程，处理多个的客户端连接，就会使用到Selector(选择器) 2: Selector 能够检测多个注册的通道上是否有事件发生(注意:多个Channel以事件的方式可以注册到同一个Selector)，如果有事件发生，便获取事件然 后针对每个事件进行相应的处理。这样就可以只用一个单线程去管理多个通道，也就是管理多个连接和请求。 3: 只有在 连接/通道 真正有读写事件发生时，才会进行读写，就大大地减少 了系统开销，并且不必为每个连接都创建一个线程，不用去维护多个线程 4: 避免了多线程之间的上下文切换导致的开销 2. Selector(选择器) 示意图 说明: Netty 的 IO 线程 NioEventLoop 聚合了 Selector(选择器， 也叫多路复用器)，可以同时并发处理成百上千个客 户端连接。 当线程从某客户端 Socket 通道进行读写数据时，若没 有数据可用时，该线程可以进行其他任务。 线程通常将非阻塞 IO 的空闲时间用于在其他通道上 执行 IO 操作，所以单独的线程可以管理多个输入和 输出通道。 由于读写操作都是非阻塞的，这就可以充分提升 IO 线程的运行效率，避免由于频繁 I/O 阻塞导致的线程 挂起。 一个 I/O 线程可以并发处理 N 个客户端连接和读写操作，这从根本上解决了传统同步阻塞 I/O 一连接一线 程模型，架构的性能、弹性伸缩能力和可靠性都得到 了极大的提升。 3. Selector类相关方法 Selector 类是一个抽象类, 常用方法和说明如下 public abstract class Selector implements Closeable { public static Selector open();//得到一个选择器对象 public int select(long timeout);//监控所有注册的通道，当其 中有 IO 操作可以进行时，将 对应的 SelectionKey 加入到内部集合中并返回，参数用来 设置超时时间 public Set&lt;SelectionKey&gt; selectedKeys();//从内部集合中得 到所有的 SelectionKey } 4. 注意事项 NIO中的 ServerSocketChannel功能类似ServerSocket，SocketChannel功能类 似Socket selector 相关方法说明 selector.select()//阻塞 selector.select(1000);//阻塞1000毫秒，在1000毫秒后返回 selector.wakeup();//唤醒 selector selector.selectNow();//不阻塞，立马返还 3.7. NIO 非阻塞 网络编程原理分析 NIO 非阻塞 网络编程相关的(Selector、SelectionKey、 ServerScoketChannel和SocketChannel) 关系梳理图 说明： ServerSocketChannel 需要在selector注册，一旦有register事件（客户端一旦连接）就会建立SocketChannel 客户端连接时需要会通过ServerSocketChannel 在selector注册，一旦有读写事件，反向获取通道channel，把channel数据读出到buffer 事件发生通过SelectorKey来判断 代码演示：server端 public static void main(String[] args) throws Exception { //服务器端创建ServerSocketChannel -&gt;ServerSocketChannel ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); //得到一个Selector对象 Selector selector = Selector.open(); //绑定一个端口6666，在服务器端监听 serverSocketChannel.socket().bind(new InetSocketAddress(6666)); //设置为非阻塞 serverSocketChannel.configureBlocking(false); //把serverSocketChannel 注册到 selector 关心的事件：op_ACCEPT serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT); //循环等待客户端连接 while(true){ if(selector.select(1000) == 0){//没有事件发生 System.out.println(&quot;服务器等待1s,无连接&quot;); continue; } //返回大于0，获取相关的selectionKey集合(获取到关注的事件) //selector.selectedKeys() 返回关注的事件集合 Set&lt;SelectionKey&gt; selectionKeys = selector.selectedKeys(); //遍历,使用迭代器 Iterator&lt;SelectionKey&gt; keyIterator = selectionKeys.iterator(); while(keyIterator.hasNext()){ //获取到SelectionKey SelectionKey key = keyIterator.next(); //根据key 对应的通道发生的事件做相应的处理 if(key.isAcceptable()){//如果是OP_ACCEPT,有新的客户端连接 //该客户端生成一个SocketChannel SocketChannel socketChannel = serverSocketChannel.accept(); System.out.println(&quot;客户端连接成功，生成一个socketChannel&quot;); //将socketChannel设置为非阻塞，这时线程可以做其他事 socketChannel.configureBlocking(false); //将socketChannel 注册到Selector，关注OP_READ，同时给socketChannel关联一个buffer socketChannel.register(selector,SelectionKey.OP_READ, ByteBuffer.allocate(1024)); } if(key.isReadable()){//发生OP_READ //通过key 反向获取对应的channel SocketChannel channel = (SocketChannel) key.channel(); //获取改channel关联的buffer ByteBuffer buffer = (ByteBuffer) key.attachment(); channel.read(buffer); System.out.println(&quot;form client &quot;+ new String(buffer.array())); } //手动从集合中移动当前的selectionKey,防止重复操作 keyIterator.remove(); } } } client端： public static void main(String[] args) throws Exception { //得到一个网络通道 SocketChannel socketChannel = SocketChannel.open(); //设置非阻塞模式 socketChannel.configureBlocking(false); //提供服务器端的ip和端口 InetSocketAddress inetSocketAddress = new InetSocketAddress(&quot;127.0.0.1&quot;,6666); //连接服务器 if(!socketChannel.connect(inetSocketAddress)){ while(!socketChannel.finishConnect()){ System.out.println(&quot;连接需要时间，客户端不会阻塞，可做其他工作&quot;); } } //连接成功，发送数据 String str = &quot;hello,world&quot;; //获得字节数组到buffer中,且buffer大小与字节长度一致 ByteBuffer buffer = ByteBuffer.wrap(str.getBytes()); //发送数据，将buffer 数据写入channel socketChannel.write(buffer); System.in.read(); } 3.8. SelectionKey SelectionKey，表示 Selector 和网络通道的注册关系（OPS）, 共四种： int OP_ACCEPT：有新的网络连接可以 accept，值为 16 int OP_CONNECT：代表连接已经建立，值为 8 int OP_READ：代表读操作，值为 1 int OP_WRITE：代表写操作，值为 4 源码中： public static final int OP_READ = 1 &lt;&lt; 0; public static final int OP_WRITE = 1 &lt;&lt; 2; public static final int OP_CONNECT = 1 &lt;&lt; 3; public static final int OP_ACCEPT = 1 &lt;&lt; 4; SelectionKey相关方法 public abstract class SelectionKey { public abstract Selector selector();//得到与之关联的 Selector 对象 public abstract SelectableChannel channel();//得到与之关 联的通道 public final Object attachment();//得到与之关联的共享数 据 public abstract SelectionKey interestOps(int ops);//设置或改 变监听事件 public final boolean isAcceptable();//是否可以 accept public final boolean isReadable();//是否可以读 public final boolean isWritable();//是否可以写 } 3.9. ServerSocketChannel ServerSocketChannel 在服务器端监听新的客户端 Socket 连接 相关方法如下: public abstract class ServerSocketChannel extends AbstractSelectableChannel implements NetworkChannel{ public static ServerSocketChannel open()//得到一个 ServerSocketChannel 通道 public final ServerSocketChannel bind(SocketAddress local)//设置服务器端端口 号 public final SelectableChannel configureBlocking(boolean block)//设置阻塞或非 阻塞模式，取值 false 表示采用非阻塞模式 public SocketChannel accept()//接受一个连接，返回代表这个连接的通道对象 public final SelectionKey register(Selector sel, int ops)//注册一个选择器并设置 监听事件 } 3.10. SocketChannel SocketChannel，网络 IO 通道，具体负责进行读写操作。NIO 把缓冲区的数据写入通 道，或者把通道里的数据读到缓冲区。 相关方法如下 public abstract class SocketChannel extends AbstractSelectableChannel implements ByteChannel, ScatteringByteChannel, GatheringByteChannel, NetworkChannel{ public static SocketChannel open();//得到一个 SocketChannel 通道 public final SelectableChannel configureBlocking(boolean block);//设置阻塞或非阻塞 模式，取值 false 表示采用非阻塞模式 public boolean connect(SocketAddress remote);//连接服务器 public boolean finishConnect();//如果上面的方法连接失败，接下来就要通过该方法 完成连接操作 public int write(ByteBuffer src);//往通道里写数据 public int read(ByteBuffer dst);//从通道里读数据 public final SelectionKey register(Selector sel, int ops, Object att);//注册一个选择器并 设置监听事件，最后一个参数可以设置共享数据 public final void close();//关闭通道 } 3.11. NIO 网络编程应用实例-群聊系统 要求: 编写一个 NIO 群聊系统，实现服务器端和客户端之间的数据简单通讯（非阻塞） 实现多人群聊 服务器端：可以监测用户上线，离线， 并实现消息转发功能 客户端：通过channel 可以无阻塞发送 消息给其它所有用户，同时可以接受 其它用户发送的消息(有服务器转发得到) 目的：进一步理解NIO非阻塞网络编程 机制 代码实现：服务端 package com.atguigu.groupchat; import java.io.IOException; import java.net.InetSocketAddress; import java.nio.ByteBuffer; import java.nio.channels.*; import java.util.Iterator; public class GroupChatServer { //定义属性 private Selector selector; private ServerSocketChannel listenChannel; private static final int PORT = 6667; //构造器 //初始化工作 public GroupChatServer(){ try { //得到选择器 selector = Selector.open(); //得到ServerSocketChannel listenChannel = ServerSocketChannel.open(); //绑定端口 listenChannel.socket().bind(new InetSocketAddress(PORT)); //设置非阻塞 listenChannel.configureBlocking(false); //listenChannel注册到Selector listenChannel.register(selector, SelectionKey.OP_ACCEPT); } catch (IOException e) { e.printStackTrace(); } } //监听 public void listen(){ try { //循环处理监听 while(true){ int count = selector.select(); if(count&gt;0){//有事件 //遍历得到selectionKey 集合 Iterator&lt;SelectionKey&gt; iterator = selector.selectedKeys().iterator(); while(iterator.hasNext()){ //取得selectionKey SelectionKey key = iterator.next(); if(key.isAcceptable()){//连接事件 SocketChannel socketChannel = listenChannel.accept(); socketChannel.configureBlocking(false); socketChannel.register(selector,SelectionKey.OP_READ); //提示客户上线 System.out.println(socketChannel.getRemoteAddress()+&quot;上线了&quot;); } else if(key.isReadable()){//read事件 readDate(key); } //手动从集合中移动当前的selectionKey,防止重复操作 iterator.remove(); } } else{ System.out.println(&quot;waiting event&quot;); } } } catch (Exception e) { e.printStackTrace(); }finally { } } //读取client数据 public void readDate(SelectionKey key){ //定义一个SocketChannel SocketChannel socketChannel = null; try { //取到关联的channel socketChannel = (SocketChannel)key.channel(); //创建缓存 ByteBuffer byteBuffer = ByteBuffer.allocate(1024); int count = socketChannel.read(byteBuffer); if(count&gt;0){ //buffer的数据转成字符串 String msg = new String(byteBuffer.array()); //输出msg System.out.println(&quot;form client&quot;+msg); //向其他客户端转发消息 sendInfoToOtherClient(msg,socketChannel); } } catch (IOException e) { try { System.out.println(socketChannel.getRemoteAddress()+&quot;离线了&quot;); //取消注册 key.cancel(); //关闭通道 socketChannel.close(); } catch (IOException ioException) { ioException.printStackTrace(); } } } //转发消息给其他消息 private void sendInfoToOtherClient(String msg,SocketChannel self) throws IOException{ //遍历所有注册到selector的SocketChannel for(SelectionKey key: selector.keys()){ Channel targetChannel = key.channel(); //排除自己 if(targetChannel != self &amp;&amp; targetChannel instanceof SocketChannel){ //转发 SocketChannel dest = (SocketChannel)targetChannel; //写入buffer ByteBuffer buffer = ByteBuffer.wrap(msg.getBytes()); //写入通道 dest.write(buffer); } } } public static void main(String[] args) { // 创建一个服务器对象 GroupChatServer server = new GroupChatServer(); server.listen(); } } 代码实现：客户端 package com.atguigu.groupchat; import java.io.IOException; import java.net.InetSocketAddress; import java.nio.ByteBuffer; import java.nio.channels.SelectionKey; import java.nio.channels.Selector; import java.nio.channels.SocketChannel; import java.util.Iterator; import java.util.Scanner; public class GroupChatClient { // 定义相关属性 // 服务器的IP private final String HOST = &quot;127.0.0.1&quot;; // 服务器的端口 private final int PORT = 6667; private Selector selector; private SocketChannel socketChannel; private String username; // 构造器 public GroupChatClient() throws IOException { // 完成初始化 selector = Selector.open(); // 连接服务器 socketChannel = SocketChannel.open(new InetSocketAddress(HOST, PORT)); // 设置 非阻塞 socketChannel.configureBlocking(false); // 将 socketChannel 注册到 Selector socketChannel.register(selector, SelectionKey.OP_READ); // 得到 username username = socketChannel.getLocalAddress().toString().substring(1); System.out.println(username + &quot;is OK!&quot;); } // 向服务器发送消息 public void sendMessage(String message){ message = username + &quot;说：&quot;+ message; try { // 把 message 写入 buffer socketChannel.write(ByteBuffer.wrap(message.getBytes())); // 读取从服务器端回复的消息 }catch (Exception e){ e.printStackTrace(); }finally { } } public void readmessage(){ try { int select = selector.select(); if (select &gt; 0){ // 有事件发生的通道 Iterator&lt;SelectionKey&gt; iterator = selector.selectedKeys().iterator(); while (iterator.hasNext()){ SelectionKey key = iterator.next(); if (key.isReadable()){ // 得到相关的通道 SocketChannel channel = (SocketChannel) key.channel(); ByteBuffer buffer = ByteBuffer.allocate(1024); channel.read(buffer); String msg = new String(buffer.array()); System.out.println(msg.trim()); } } iterator.remove();//删除当前的selectionKey }else { // System.out.println(&quot;没有可用的通道&quot;); } }catch (Exception e){ e.printStackTrace(); }finally { } } public static void main(String[] args) throws IOException { // 启动客户端 GroupChatClient client = new GroupChatClient(); // 启动一个线程,每个三秒读取从服务器端读取数据 new Thread(){ public void run(){ while (true){ client.readmessage(); try { Thread.sleep(3000); }catch (Exception e){ e.printStackTrace(); } } } }.start(); // 发送数据给服务端 Scanner scanner = new Scanner(System.in); while (scanner.hasNextLine()){ String line = scanner.nextLine(); client.sendMessage(line); } } } ","link":"https://tianxiawuhao.github.io/X2beO2j1l/"},{"title":"前端简单部署","content":"部署前端项目，将打包后的文件dist放入nginx，基于nginx的基础镜像创建前端项目的docker镜像即可 一、前端项目的文件准备 1、文件准备： 1,dockerfile(docker部署镜像打包文件) 2,dist(待部署前端文件夹) 3,default.conf 2、Dockerfile文件： FROM nginx RUN rm /etc/nginx/conf.d/default.conf ADD default.conf /etc/nginx/conf.d/ COPY dist/ /usr/share/nginx/html/ EXPOSE 80 3、default.conf： server { listen 80; server_name localhost; # 修改为docker服务宿主机的ip location / { root /usr/share/nginx/html; index index.html index.htm; try_files $uri $uri/ /index.html =404; } error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } location /api/ { proxy_pass http://后端服务名:IP端口; } } 二、vue项目的自动化部署 1、安装Jenkins Jenkins的安装：https://tinaxiawuhao.github.io/post/MN_sOyfo3/ 2、安装node/npm/yarn 安装node与node/npm/yarn：https://tinaxiawuhao.github.io/post/XMNtsw8ig/ 3、Jenkins中配置harbor仓库的密钥 我们需要通过jenkinsfile文件推送docker镜像到指定的harbor仓库，需要在jenkins中配置仓库登录凭据 4、编写Jenkinsfile pipeline{ agent any environment { WS = &quot;${WORKSPACE}&quot; HARBOR_URL=&quot;harbor的url&quot; HARBOR_ID=&quot;harbor密钥&quot; } stages { stage(&quot;环境检查&quot;){ steps { sh 'docker version' sh 'npm -v' sh 'yarn -v' } } stage('yarn安装与编译') { steps { sh &quot;echo ${WS} &amp;&amp; ls -alh&quot; sh &quot;cd ${WS} &amp;&amp; yarn install&quot; sh &quot;cd ${WS} &amp;&amp; yarn build:hd&quot; } } stage('docker镜像与推送') { steps { sh &quot;cd ${WS} &amp;&amp; docker build -f Dockerfile -t ${HARBOR_URL}/test/web:latest .&quot; withCredentials([usernamePassword(credentialsId: &quot;${HARBOR_ID}&quot;, passwordVariable: 'password', usernameVariable: 'username')]) { sh &quot;docker login -u ${username} -p ${password} ${HARBOR_URL}&quot; sh &quot;docker push ${HARBOR_URL}/test/web:latest&quot; } } } } post { success { echo 'success!' } failure { echo 'failed...' } } } harbor密钥获取 点进入刚才添加的凭证获取凭证 最后创建一个pipeline项目（可选择Pipeline或者多分支流水线），pipeline脚本定义选择Pipeline script from SCM，选择git项目 修改关注分支 三、模仿后端实现环境变量注入 借助Nginx1.19以后docker镜像的template新功能，可以实现配置文件环境变量的动态注入 1、准备 default.conf.template： server { listen 80; listen [::]:80; server_name localhost; # 修改为docker服务宿主机的ip location / { root /usr/share/nginx/html; index index.html index.htm; ssi on; try_files $uri $uri/ /index.html; if ($request_filename ~* ^.*?\\.(eot)|(ttf)|(woff)|(svg)|(otf)$) { add_header Access-Control-Allow-Origin *; } } error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } location /api/ { ssi on; # 借助ssi proxy_pass ${BACKEND_SERVICE_URL}; } } 2、编写Dockerfile文件： FROM nginx:1.21 COPY dist /usr/share/nginx/html # 注意和上面文件对比位置的改变 COPY default.conf.template /etc/nginx/templates/ EXPOSE 80 3、设置环境变量 部署docker时候，设置环境变量-e BACKEND_SERVICE_URL= http://ip:port 后，实际的nginx配置就变为了： $BACKEND_SERVICE_URL = http://ip:port #后端服务的地址； server { listen 80; listen [::]:80; server_name localhost; # 修改为docker服务宿主机的ip location / { root /usr/share/nginx/html; index index.html index.htm; ssi on; try_files $uri $uri/ /index.html; if ($request_filename ~* ^.*?\\.(eot)|(ttf)|(woff)|(svg)|(otf)$) { add_header Access-Control-Allow-Origin *; } } error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } location /api/ { ssi on; # 借助ssi proxy_pass http://ip:port; } } 切换服务就可以修改环境变量，切换不同的服务后端 ","link":"https://tianxiawuhao.github.io/KEyilqyTn/"},{"title":"gitlab提交代码自动触发Jenkins构建操作","content":"Jenkins配置 插件安装 系统管理=====》插件管理=====》可选插件=====》搜索要按照的插件(gitlab hook-plugin和gitlab-plugin) 如果找不到上面两个插件，安装gitlab和gitlab hook即可 创建测试项目 gitlab配置 打开要关联Jenkins项目的设置选项找到webhooks选项，把Jenkins中的项目触发器url以及Secret token配置到gitlab的webhooks选项中 URL Secret token 验证效果 我们测试下点击刚刚关联的构建动作，Jenkins会不会自动构建 当出现请求状态码200的时候证明我们的关联动作已经执行 下面我们去到Jenkins中看下是否有构建历史 再试下手动push代码到gitlab会不会触发Jenkins的构建任务 手动去到服务器上向远程gitlab分支推送文件 ","link":"https://tianxiawuhao.github.io/ymh_BwIfn/"},{"title":"linux安装node","content":"以当前node版本16.13.0为例，新建演示目录/usr/local/node 1、下载 wget https://nodejs.org/dist/v16.13.0/node-v16.13.0-linux-x64.tar.xz 2、移动到指定目录 mv node-v16.13.0-linux-x64.tar.xz /usr/local/node 3、解压 xz -d node-v16.13.0-linux-x64.tar.xz tar -xvf node-v16.13.0-linux-x64.tar 测试下： /root/sdemo/node-v16.13.0-linux-x64/bin/node -v v16.13.0 /root/sdemo/node-v16.13.0-linux-x64/bin/npm -v 8.1.0 4、设置软连接，相当于windows设置环境变量 ln -s /usr/local/node/node-v16.13.0-linux-x64/bin/node /usr/bin/node ln -s /usr/local/node/node-v16.13.0-linux-x64/bin/npm /usr/bin/npm 5、安装yarn,并设置yarn的软连 npm install yarn -g ln -s /usr/local/node/node-v16.13.0-linux-x64/bin/yarn /usr/bin/yarn 测试下： yarn -v 1.22.17 6、安装cnpm npm install -g cnpm --registry=https://registry.npm.taobao.org ln -s /usr/local/node/lib/node-v16.13.0-linux-x64/bin/cnpm /usr/bin/cnpm ","link":"https://tianxiawuhao.github.io/XMNtsw8ig/"},{"title":"linux安装jenkins","content":"1、安装JDK yum install -y java 2、安装jenkins 添加Jenkins库到yum库，Jenkins将从这里下载安装。 wget -O /etc/yum.repos.d/jenkins.repo http://pkg.jenkins-ci.org/redhat/jenkins.repo rpm --import https://jenkins-ci.org/redhat/jenkins-ci.org.key yum install -y jenkins 或 yum install jenkins -y --nogpgcheck 3、修改jenkins端口 [root@tools ~]# rpm -qa | grep jenkins jenkins-2.340-1.1.noarch [root@tools ~]# rpm -ql jenkins-2.340-1.1.noarch /etc/init.d/jenkins /etc/logrotate.d/jenkins /etc/sysconfig/jenkins /usr/bin/jenkins /usr/lib/systemd/system/jenkins.service /usr/sbin/rcjenkins /usr/share/java/jenkins.war /usr/share/jenkins /usr/share/jenkins/migrate /var/cache/jenkins /var/lib/jenkins /var/log/jenkins [root@tools ~]# vim /usr/lib/systemd/system/jenkins.service vi /etc/sysconfig/jenkins 找到修改端口号： JENKINS_PORT=“9000” #此端口不冲突可以不修改 4、启动jenkins systemctl start jenkins.service [root@tools ~]# netstat -nultp 安装成功后Jenkins将作为一个守护进程随系统启动 系统会创建一个“jenkins”用户来允许这个服务，如果改变服务所有者，同时需要修改/var/log/jenkins, /var/lib/jenkins, 和/var/cache/jenkins的所有者 启动的时候将从/etc/sysconfig/jenkins获取配置参数 默认情况下，Jenkins运行在8080端口，在浏览器中直接访问该端进行服务配置 Jenkins的RPM仓库配置被加到/etc/yum.repos.d/jenkins.repo 5、打开jenkins 在浏览器中访问 http://192.168.3.200:9000/ 首次进入会要求输入初始密码 初始密码在：/var/lib/jenkins/secrets/initialAdminPassword 重置密码：admin/admin ","link":"https://tianxiawuhao.github.io/MN_sOyfo3/"},{"title":"k8s高可用集群搭建","content":"整体环境 3台master节点，3台node节点。采用了Centos 7，有网络，互相可以ping通。 准备工作查看单集群部署 1.安装keepalived 和 haproxy 1.1安装keepalived和 haproxy 1yum install keepalived haproxy -y 1.2 配置 keepalived cat &lt;&lt;EOF &gt; /etc/keepalived/keepalived.conf ! Configuration File for keepalived global_defs { router_id k8s } vrrp_script check_haproxy { script &quot;/bin/bash -c 'if [[ $(netstat -nlp | grep 16443) ]]; then exit 0; else exit 1; fi'&quot; interval 3 weight -2 fall 10 rise 2 } vrrp_instance VI_1 { state MASTER interface ens33 virtual_router_id 51 priority 250 advert_int 1 authentication { auth_type PASS auth_pass ceb1b3ec013d66163d6ab } virtual_ipaddress { 192.168.40.19 } track_script { check_haproxy } } EOF cat &lt;&lt;EOF &gt; /etc/keepalived/keepalived.conf ! Configuration File for keepalived global_defs { router_id k8s } vrrp_script check_haproxy { script &quot;/bin/bash -c 'if [[ $(netstat -nlp | grep 16443) ]]; then exit 0; else exit 1; fi'&quot; interval 3 weight -2 fall 10 rise 2 } vrrp_instance VI_1 { state BACKUP interface ens33 virtual_router_id 51 priority 150 advert_int 1 authentication { auth_type PASS auth_pass ceb1b3ec013d66163d6ab } virtual_ipaddress { 192.168.40.19 } track_script { check_haproxy } } EOF cat &lt;&lt;EOF &gt; /etc/keepalived/keepalived.conf ! Configuration File for keepalived global_defs { router_id k8s } vrrp_script check_haproxy { script &quot;/bin/bash -c 'if [[ $(netstat -nlp | grep 16443) ]]; then exit 0; else exit 1; fi'&quot; interval 3 weight -2 fall 10 rise 2 } vrrp_instance VI_1 { state BACKUP interface ens33 virtual_router_id 51 priority 200 advert_int 1 authentication { auth_type PASS auth_pass ceb1b3ec013d66163d6ab } virtual_ipaddress { 192.168.40.19 } track_script { check_haproxy } } EOF vrrp_script 用于检测 haproxy 是否正常。如果本机的 haproxy 挂掉，即使 keepalived 劫持vip，也无法将流量负载到 apiserver。 我所查阅的网络教程全部为检测进程, 类似 killall -0 haproxy。这种方式用在主机部署上可以，但容器部署时，在 keepalived 容器中无法知道另一个容器 haproxy 的活跃情况，因此我在此处通过检测端口号来判断 haproxy 的健康状况。 weight 可正可负。为正时检测成功 +weight，相当与节点检测失败时本身 priority 不变，但其他检测成功节点 priority 增加。为负时检测失败本身 priority 减少。 另外很多文章中没有强调 nopreempt 参数，意为不可抢占，此时 master 节点失败后，backup 节点也不能接管 vip，因此我将此配置删去 1.3 配置haproxy cat &lt;&lt;EOF &gt; /etc/haproxy/haproxy.cfg #--------------------------------------------------------------------- # Global settings #--------------------------------------------------------------------- global # to have these messages end up in /var/log/haproxy.log you will # need to: # 1) configure syslog to accept network log events. This is done # by adding the '-r' option to the SYSLOGD_OPTIONS in # /etc/sysconfig/syslog # 2) configure local2 events to go to the /var/log/haproxy.log # file. A line like the following can be added to # /etc/sysconfig/syslog # # local2.* /var/log/haproxy.log # log 127.0.0.1 local0 chroot /var/lib/haproxy pidfile /var/run/haproxy.pid maxconn 4000 user haproxy group haproxy daemon # turn on stats unix socket stats socket /var/lib/haproxy/stats #--------------------------------------------------------------------- # common defaults that all the 'listen' and 'backend' sections will # use if not designated in their block #--------------------------------------------------------------------- defaults mode http log global option httplog option dontlognull option http-server-close option forwardfor except 127.0.0.0/8 option redispatch retries 3 timeout http-request 10s timeout queue 1m timeout connect 10s timeout client 1m timeout server 1m timeout http-keep-alive 10s timeout check 10s maxconn 3000 #--------------------------------------------------------------------- # kubernetes apiserver frontend which proxys to the backends #--------------------------------------------------------------------- frontend kubernetes-apiserver mode tcp bind *:16443 option tcplog default_backend kubernetes-apiserver #--------------------------------------------------------------------- # round robin balancing between the various backends #--------------------------------------------------------------------- backend kubernetes-apiserver mode tcp balance roundrobin server gk8s-master 192.168.40.20:6443 check server gk8s-master2 192.168.40.21:6443 check server gk8s-master3 192.168.40.22:6443 check #--------------------------------------------------------------------- # collection haproxy statistics message #--------------------------------------------------------------------- listen stats bind *:1080 stats auth admin:awesomePassword stats refresh 5s stats realm HAProxy\\ Statistics stats uri /admin?stats EOF 设置开机启动 systemctl enable haproxy &amp;&amp; systemctl start haproxy systemctl enable keepalived &amp;&amp; systemctl start keepalived 2.安装kubeadm、kubelet、kubectl 1.配置文件修改 cat &lt;&lt;EOF | sudo tee /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF 2.安装启用 sudo yum install -y kubelet-1.24.1 kubeadm-1.24.1 kubectl-1.24.1 --disableexcludes=kubernetes sudo systemctl enable kubelet &amp;&amp; systemctl start kubelet 3.修改kubelet的配置文件 先查看配置文件位置 systemctl status kubelet vi /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf 并添加以下内容(使用和docker相同的cgroup-driver)。 Environment=&quot;KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --cgroup-driver=systemd&quot; 4.重启kubelet systemctl daemon-reload &amp;&amp; systemctl restart kubelet 3.获取K8S镜像（可忽略） 1.获取镜像列表 使用阿里云镜像仓库下载（国内环境该命令可不执行，下步骤kubeadm init --image-repository registry.cn-hangzhou.aliyuncs.com/google_containers已经默认为国内环境） 由于官方镜像地址被墙，所以我们需要首先获取所需镜像以及它们的版本。然后从国内镜像站获取。 kubeadm config images list 获取镜像列表后可以通过下面的脚本从阿里云获取： vi /usr/local/k8s/k8s-images.sh 下面的镜像应该去除&quot;k8s.gcr.io/&quot;的前缀，版本换成上面获取到的版本 images=( kube-apiserver:v1.24.1 kube-controller-manager:v1.24.1 kube-scheduler:v1.24.1 kube-proxy:v1.24.1 pause:3.7 etcd:3.5.3-0 coredns:v1.8.6 ) for imageName in ${images[@]} ; do docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName k8s.gcr.io/$imageName docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName done 2.赋权执行 chmod +x k8s-images.sh &amp;&amp; ./k8s-images.sh 以上操作在所有机器执行 4.初始化环境（master操作） 1.安装镜像 采用模板配置文件加载 kubeadm config print init-defaults &gt; kubeadm-config.yaml # 模板随版本更新 [root@master1 ~]# cat kubeadm-config.yaml apiVersion: kubeadm.k8s.io/v1beta2 bootstrapTokens: - groups: - system:bootstrappers:kubeadm:default-node-token token: abcdef.0123456789abcdef ttl: 24h0m0s usages: - signing - authentication kind: InitConfiguration localAPIEndpoint: advertiseAddress: 192.168.40.131 # 本机IP bindPort: 6443 nodeRegistration: criSocket: unix:///var/run/cri-docker.sock # 此处千万不要忘记修改，如果不修改等于没有替换。(此处已经更改完了) #criSocket: unix:///run/containerd/containerd.sock # 此处千万不要忘记修改，如果不修改等于没有替换。(此处已经更改完了) name: master1 # 本主机名 taints: - effect: NoSchedule key: node-role.kubernetes.io/master --- apiServer: certSANs: - k8s-master-01 - k8s-master-02 - k8s-master-03 - master.k8s.io - 192.168.40.19 - 192.168.40.20 - 192.168.40.21 - 192.168.40.22 - 127.0.0.1 extraArgs: authorization-mode: Node,RBAC timeoutForControlPlane: 4m0s apiVersion: kubeadm.k8s.io/v1beta2 certificatesDir: /etc/kubernetes/pki clusterName: kubernetes controlPlaneEndpoint: &quot;192.168.40.19:16443&quot; # 虚拟IP和haproxy端口 controllerManager: {} dns: type: CoreDNS etcd: local: dataDir: /var/lib/etcd imageRepository: registry.aliyuncs.com/google_containers # 镜像仓库源要根据自己实际情况修改 kind: ClusterConfiguration kubernetesVersion: v1.24.1 # k8s版本 networking: dnsDomain: cluster.local podSubnet: &quot;10.244.0.0/16&quot; #设置网段，和下面网络插件对应 serviceSubnet: 10.96.0.0/12 scheduler: {} --- apiVersion: kubeproxy.config.k8s.io/v1alpha1 kind: KubeProxyConfiguration featureGates: SupportIPVSProxyMode: true mode: ipvs 2.查看kubeadm版本，修改命令参数 kubeadm version 这个就很简单了，只需要简单的一个命令： #直接使用已经下载好的镜像 kubeadm init --kubernetes-version=v1.24.1 --control-plane-endpoint &quot;192.168.40.19:16443&quot; --apiserver-advertise-address=192.168.40.20 --pod-network-cidr=10.244.0.0/16 --service-cidr=10.96.0.0/12 --ignore-preflight-errors=Swap --cri-socket unix:///var/run/cri-docker.sock | tee kubeadm-init.log #或者采用aliyuncs镜像下载 kubeadm init --kubernetes-version=v1.24.1 --control-plane-endpoint &quot;192.168.40.19:16443&quot; --apiserver-advertise-address=192.168.40.20 --image-repository registry.aliyuncs.com/google_containers --service-cidr=10.1.0.0/16 --pod-network-cidr=10.244.0.0/16 --cri-socket unix:///var/run/cri-docker.sock| tee kubeadm-init.log #使用上面系统生成配置文件加载 kubeadm init --config kubeadm-config.yaml 3.初始化命令说明： 虚拟ip节点端口号 --control-plane-endpoint 指明用 Master 的哪个 interface 与 Cluster 的其他节点通信。如果 Master 有多个 interface，建议明确指定，如果不指定，kubeadm 会自动选择有默认网关的 interface。 --apiserver-advertise-address 指定 Pod 网络的范围。Kubernetes 支持多种网络方案，而且不同网络方案对 --pod-network-cidr 有自己的要求，这里设置为 10.244.0.0/16 是因为我们将使用 flannel 网络方案，必须设置成这个 CIDR。 --pod-network-cidr Kubenetes默认Registries地址是 k8s.gcr.io，在国内并不能访问 gcr.io，在1.19.3版本中我们可以增加–image-repository参数，默认值是 k8s.gcr.io，将其指定为阿里云镜像地址：registry.aliyuncs.com/google_containers。 --image-repository 关闭版本探测，因为它的默认值是stable-1，会导致从https://dl.k8s.io/release/stable-1.txt下载最新的版本号，我们可以将其指定为固定版本（最新版：v1.24.1）来跳过网络请求。 --kubernetes-version=v1.24.1 指定启动时使用cri-docker调用docker --cri-socket unix:///var/run/cri-docker.sock 4.错误启动重置 # 重置 如果有需要 kubeadm reset --cri-socket unix:///var/run/cri-docker.sock 5.初始化成功后，为顺利使用kubectl，执行以下命令： mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config 6.添加其他主节点 1.复制密钥及相关文件 ssh root@192.168.40.21 mkdir -p /etc/kubernetes/pki/etcd scp /etc/kubernetes/admin.conf root@192.168.40.21:/etc/kubernetes scp /etc/kubernetes/pki/{ca.*,sa.*,front-proxy-ca.*}root@192.168.40.21:/etc/kubernetes/pki scp /etc/kubernetes/pki/etcd/ca.* root@192.168.40.21:/etc/kubernetes/pki/etcd ssh root@192.168.40.22 mkdir -p /etc/kubernetes/pki/etcd scp /etc/kubernetes/admin.conf root@192.168.40.22:/etc/kubernetes scp /etc/kubernetes/pki/{ca.*,sa.*,front-proxy-ca.*} root@192.168.40.22:/etc/kubernetes/pki scp /etc/kubernetes/pki/etcd/ca.* root@192.168.40.22:/etc/kubernetes/pki/etcd 2.添加节点 kubeadm join 192.168.40.19:16443 --token pqir66.66fy6pexw3kprt2b --discovery-token-ca-cert-hash sha256:cd4c42e956fdc7e0ad48c990484c22cfd43da63cb3f3887bedc481e7f33a0be1 --control-plane --cri-socket unix:///var/run/cri-docker.sock 7.执行kubectl get nodes，查看master节点状态： kubectl get node 8.通过如下命令查看kubelet状态： journalctl -xef -u kubelet -n 20 提示未安装cni 网络插件。 5.1安装flannel网络插件(CNI) master执行以下命令安装flannel即可： kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml kube-flannel.yaml文件中的net-conf.json-&gt;Network地址默认为命令中–pod-network-cidr=值相同 输入命令kubectl get pods -n kube-system,等待所有插件为running状态。 待所有pod status为Running的时候，再次执行kubectl get nodes： [root@k8s-master ~]# kubectl get node NAME STATUS ROLES AGE VERSION k8s-master Ready master 16m v1.24.1 如上所示，master状态变为，表明Master节点部署成功！ 5.2安装calico网络(功能更完善) 1.在master上下载配置calico网络的yaml。 kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml 2.提前下载所需要的镜像。 # 查看此文件用哪些镜像： [root@k8s-master ~]# grep image calico.yaml image: docker.io/calico/cni:v3.23.1 image: docker.io/calico/node:v3.23.1 image: docker.io/calico/kube-controllers:v3.23.1 3.安装calico网络。 在master上执行如下命令： kubectl apply -f calico.yaml 5.验证结果。 再次在master上运行命令 kubectl get nodes查看运行结果： [root@k8s-master ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION master01 Ready control-plane,master 21h v1.23.4 worker01 Ready control-plane,master 16h v1.23.4 worker02 Ready control-plane,master 16h v1.23.4 6.部署k8s-node1、k8s-node2、k8s-node3集群 1、在k8s-node1、k8s-node2、k8s-node3等三台虚拟机中重复执行上面的步骤，安装好docker、kubelet、kubectl、kubeadm。 1.node节点加入集群 在上面第初始化master节点成功后，输出了下面的kubeadm join命令： kubeadm join 192.168.40.131:6443 --token zj0u08.ge77y7uv76flqgdk --discovery-token-ca-cert-hash sha256:7cd23cec6afb192b2d34c5c719b378082a6315a9d91a22d91b83066c870d4db5 --cri-socket unix:///var/run/cri-docker.sock 该命令就是node加入集群的命令，分别在k8s-node1、k8s-node2上执行该命令加入集群。 如果忘记该命令，可以通过以下命令重新生成： kubeadm token create --print-join-command 2.在master节点执行下面命令查看集群状态： kubectl get nodes [root@k8s-master ~]# kubectl get node NAME STATUS ROLES AGE VERSION k8s-master Ready master 24m v1.24.1 k8s-master2 Ready master 24m v1.24.1 k8s-master3 Ready master 24m v1.24.1 k8s-node1 Ready &lt;none&gt; 5m50s v1.24.1 k8s-node2 Ready &lt;none&gt; 5m21s v1.24.1 k8s-node3 Ready &lt;none&gt; 5m21s v1.24.1 如上所示，所有节点都为ready，集群搭建成功。 卸载集群命令 #建议所有服务器都执行 #!/bin/bash kubeadm reset -f modprobe -r ipip lsmod rm -rf ~/.kube/ rm -rf /etc/kubernetes/ rm -rf /etc/systemd/system/kubelet.service.d rm -rf /etc/systemd/system/kubelet.service rm -rf /usr/bin/kube* rm -rf /etc/cni rm -rf /opt/cni rm -rf /var/lib/etcd rm -rf /var/etcd yum -y remove kubeadm* kubectl* kubelet* docker* reboot ","link":"https://tianxiawuhao.github.io/McgbXRFnx/"},{"title":"Helm简介","content":"一、简介 Helm是Kubernetes的包管理器。包管理器类似于Ubuntu中使用的apt、Python中的pip一样，能快速查找、下载和安装软件包。 Helm解决的痛点 在Kubernetes中部署一个可以使用的应用，需要涉及到很多的Kubernetes资源的共同协作。比如安装一个WordPress，用到了一些Kubernetes的一些资源对象，包括Deployment用于部署应用、Service提供服务发现、Secret配置 WordPress的用户名和密码，可能还需要pv和pvc来提供持久化服务。并且WordPress数据是存储在mariadb里面的，所以需要mariadb启动就绪后才能启动 WordPress。这些k8s资源过于分散，不方便进行管理。 Helm把Kubernetes资源(比如deployments、services或ingress等) 打包到一个chart中，而chart被保存到chart仓库。通过chart仓库可用来存储和分享chart。Helm使发布可配置，支持发布应用配置的版本管理，简化了Kubernetes部署应用的版本控制、打包、发布、删除、更新等操作。 Helm相关组件及概念 helm是一个命令行工具，主要用于Kubernetes应用程序Chart的创建、打包、发布以及创建和管理本地和远程的Chart仓库。chart Helm的软件包，采用TAR格式。类似于APT的DEB包或者YUM的RPM包，其包含了一组定义Kubernetes资源相关的 YAML文件。 Repoistory Helm的软件仓库，Repository本质上是一个Web服务器，该服务器保存了一系列的Chart软件包以供用户下载，并且提供了一个该Repository的Chart包的清单文件以供查询。Helm可以同时管理多个不同的Repository。 release使用helm install命令在Kubernetes集群中部署的Chart称为Release。可以理解为Helm使用Chart包部署的一个应用实例。 创建release helm 客户端从指定的目录或本地tar文件或远程repo仓库解析出chart的结构信息 helm 客户端根据 chart 和 values 生成一个 release helm 将install release请求直接传递给 kube-apiserver 删除release helm 客户端从指定的目录或本地tar文件或远程repo仓库解析出chart的结构信息 helm 客户端根据 chart 和 values 生成一个 release helm 将delete release请求直接传递给 kube-apiserver 更新release helm 客户端从指定的目录或本地tar文件或远程repo仓库解析出chart的结构信息 helm 将收到的信息生成新的 release，并同时更新这个 release 的 history helm 将新的 release 传递给 kube-apiserver 进行更新 chart的基本结构 ## Helm的打包格式叫做chart，所谓chart就是一系列文件, 它描述了一组相关的 k8s 集群资源。 ## Chart中的文件安装特定的目录结构组织, 最简单的chart 目录如下所示： ./ ├── charts # 目录存放依赖的chart ├── Chart.yaml # 包含Chart的基本信息，包括chart版本，名称等 ├── templates # 目录下存放应用一系列k8s资源的yaml模板 │ ├── deployment.yaml │ ├── _helpers.tpl # 此文件中定义一些可重用的模板片断，此文件中的定义在任何资源定义模板中可用 │ ├── ingress.yaml │ ├── NOTES.txt # 介绍chart部署后的帮助信息，如何使用chart等 │ ├── serviceaccount.yaml │ ├── service.yaml │ └── tests │ └── test-connection.yaml └── values.yaml # 包含了必要的值定义（默认值）, 用于存储templates目录中模板文件中用到变量的值 二、安装Helm 1 安装Helm [root@Ansible01 ~]# wget https://get.helm.sh/helm-v3.8.2-linux-amd64.tar.gz [root@Ansible01 ~]# tar -zxvf helm-v3.0.0-linux-amd64.tar.gz [root@Ansible01 ~]# mv linux-amd64/helm /usr/local/bin/helm [root@Ansible01 ~]# helm version version.BuildInfo{Version:&quot;v3.8.2&quot;, GitCommit:&quot;6e3701edea09e5d55a8ca2aae03a68917630e91b&quot;, GitTreeState:&quot;clean&quot;, GoVersion:&quot;go1.17.5&quot;} 2 添加常用repo 添加存储库 [root@Ansible01 ~]# helm repo add aliyun https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts &quot;aliyun&quot; has been added to your repositories 更新存储库 [root@Ansible01 ~]# helm repo update Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the &quot;aliyun&quot; chart repository ...Successfully got an update from the &quot;stable&quot; chart repository ...Successfully got an update from the &quot;prometheus-community&quot; chart repository ...Successfully got an update from the &quot;bitnami&quot; chart repository Update Complete. ⎈Happy Helming!⎈ 查看存储库 [root@Ansible01 ~]# helm repo list NAME URL bitnami https://charts.bitnami.com/bitnami prometheus-community https://prometheus-community.github.io/helm-charts stable http://mirror.azure.cn/kubernetes/charts aliyun https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts 删除存储库 [root@Ansible01 ~]# helm repo remove aliyun &quot;aliyun&quot; has been removed from your repositories 三、使用Helm 1 使用chart部署一个mysql # 查找所有repo下的所有chart helm search repo # 查找stable这个repo下的chart mysql helm search repo stable/mysql # 查看chart信息： helm show chart stable/mysql # 安装包： helm install db stable/mysql** # 查看发布状态： helm status db [root@Ansible01 ~]# kubectl get pod -n default NAME READY STATUS RESTARTS AGE db-mysql-599d764c8c-knfqc 0/1 Pending 0 2m10s [root@Ansible01 ~]# kubectl describe pod db-mysql-599d764c8c-knfqc -n default ...... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 17s (x3 over 2m33s) default-scheduler 0/4 nodes are available: 4 pod has unbound immediate PersistentVolumeClaims. 2 安装前自定义chart配置选项 上面部署的mysql并没有成功，这是因为并不是所有的chart都能按照默认配置运行成功，可能会需要一些环境依赖，例如PV。 所以需要自定义chart配置选项，安装过程中有两种方法可以传递配置数据：–values（或-f）：指定带有覆盖的YAML文件。这可以多次指定，最右边的文件优先。 –set：在命令行上指定替代。如果两者都用，–set优先级高 创建满足的PV [root@Ansible01 mysql]# cat pv.yaml apiVersion: v1 kind: PersistentVolume metadata: name: mysql-pv-volume labels: type: local spec: storageClassName: &quot;managed-nfs-storage&quot; capacity: storage: 10Gi accessModes: - ReadWriteOnce hostPath: path: &quot;/mnt/data&quot; [root@Ansible01 hello-world]# kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE mysql-pv-volume 10Gi RWO Retain Available managed-nfs-storage 4s [root@Ansible01 hello-world]# helm uninstall db release &quot;db&quot; uninstalled [root@Ansible01 hello-world]# cat config.yaml persistence: enabled: true storageClass: &quot;managed-nfs-storage&quot; accessMode: ReadWriteOnce size: 8Gi mysqlUser: &quot;k8s&quot; mysqlPassword: &quot;123456&quot; mysqlDatabase: &quot;k8s&quot; [root@Ansible01 hello-world]# helm install db -f config.yaml stable/mysql [root@Ansible01 hello-world]# kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE mysql-pv-volume 10Gi RWO Retain Bound default/db-mysql managed-nfs-storage 17s [root@Ansible01 hello-world]# kubectl get pod NAME READY STATUS RESTARTS AGE db-mysql-f7fbfdd68-tc8cm 1/1 Running 0 11s 3 构建一个Helm Chart 命令来创建一个名为mychart 的helm chart [root@Ansible01 2022-05-21]# helm create mychart Creating mychart [root@Ansible01 2022-05-21]# ls mychart 创建后会在目录创建一个mychart目录 [root@Ansible01 2022-05-21]# tree mychart/ mychart/ ├── charts ├── Chart.yaml ├── templates │ ├── deployment.yaml │ ├── _helpers.tpl │ ├── hpa.yaml │ ├── ingress.yaml │ ├── NOTES.txt │ ├── serviceaccount.yaml │ ├── service.yaml │ └── tests │ └── test-connection.yaml └── values.yaml 3 directories, 10 files 其中mychart目录下的templates目录中保存有部署的模板文件，values.yaml中定义了部署的变量，Chart.yaml文件包含有version（chart版本）和appVersion（包含应用的版本）。 [root@Ansible01 2022-05-21]# cat mychart/Chart.yaml |grep -v &quot;^#&quot; |grep -v ^$ apiVersion: v2 name: mychart description: A Helm chart for Kubernetes type: application version: 0.1.0 appVersion: &quot;1.16.0&quot; 选择镜像及标签和副本数（这里设置1个） [root@Ansible01 2022-05-21]# vi mychart/values.yaml replicaCount: 1 image: repository: myapp pullPolicy: IfNotPresent # Overrides the image tag whose default is the chart appVersion. tag: &quot;v1&quot; 编辑完成后检查依赖及模板配置是否正确 [root@Ansible01 2022-05-21]# cd mychart/ [root@Ansible01 mychart]# helm lint . ==&gt; Linting . [INFO] Chart.yaml: icon is recommended 1 chart(s) linted, 0 chart(s) failed 打包应用，其中0.1.0为在Chart.yaml文件中定义的version（chart版本）信息 [root@Ansible01 2022-05-21]# helm package mychart/ Successfully packaged chart and saved it to: /root/2022-05-21/mychart-0.1.0.tgz [root@Ansible01 2022-05-21]# ls mychart mychart-0.1.0.tgz 升级、回滚和删除 # 发布新版本的chart时，或者当您要更改发布的配置时，可以使用该helm upgrade 命令。 helm upgrade --set imageTag=1.17 web mychart helm upgrade -f values.yaml web mychart # 如果在发布后没有达到预期的效果，则可以使用helm rollback回滚到之前的版本。 例如将应用回滚到第一个版本： helm rollback web 2 # 卸载发行版，请使用以下helm uninstall命令： helm uninstall web # 查看历史版本配置信息 helm get --revision 1 web 下载、上传tar包 [root@Ansible01 2022-05-21]# helm pull stable/traefik [root@Ansible01 2022-05-21]# ls traefik-1.87.7.tgz # 一种方式是直接上传charts文件夹，seldon-mab是charts目录，harbor-test-helm是harbor charts repo名称。 helm push seldon-mab harbor-test-helm # 另一种是将charts package文件包push helm push seldon-core-operator-1.5.1.tgz harbor-test-helm 安装到k8s的其它空间：--namespace=monitoring helm install --name prometheus-operator --set rbacEnable=true --namespace=monitoring stable/prometheus-operator 四、Helm使用minio搭建私有仓库 minio介绍 我们一般是从本地的目录结构中的chart去进行部署，如果要集中管理chart,就需要涉及到repository的问题，因为helmrepository都是指到外面的地址，接下来我们可以通过minio建立一个企业私有的存放仓库。 Minio提供对象存储服务。它的应用场景被设定在了非结构化的数据的存储之上了。众所周知，非结构化对象诸如图像/音频/视频/log文件/系统备份/镜像文件…等等保存起来管理总是不那么方便，size变化很大，类型很多，再有云端的结合会使得情况更加复杂，minio就是解决此种场景的一个解决方案。Minio号称其能很好的适应非结构化的数据，支持AWS的S3，非结构化的文件从数KB到5TB都能很好的支持。 Minio的使用比较简单，只有两个文件，服务端minio,客户访问端mc,比较简单。 在项目中，我们可以直接找一台虚拟机作为Minio Server,提供服务，当然minio也支持作为Pod部署。 1 helm3存储库更改 在Helm 2中，默认情况下包括稳定的图表存储库。在Helm 3中，默认情况下不包含任何存储库。因此需要做的第一件事就是添加一个存储库。官方图表存储库将在有限的时间内继续接收补丁，但是将不再作为默认存储库包含在Helm客户端中。 2 minio介绍 MinIO 是一个基于Apache License v2.0开源协议的对象存储服务。它兼容亚马逊S3云存储服务接口，非常适合于存储大容量非结构化的数据，例如图片、视频、日志文件、备份数据和容器/虚拟机镜像等，而一个对象文件可以是任意大小，从几kb到最大5T不等。 MinIO是一个非常轻量的服务,可以很简单的和其他应用的结合，类似 NodeJS, Redis 或者 MySQL。 3 安装minio服务端 1使用容器安装服务端 docker pull minio/minio docker run -p 9000:9000 minio/minio server /data 2使用二进制安装服务端 wget https://dl.min.io/server/minio/release/linux-amd64/minio chmod +x minio mkdir -p /chart ./minio server /chart 访问Browser Access地址： 在启动日志中获取access key和secret key 看到这个页面则表示登陆成功 至此服务端部署完成。 4 安装minio客户端 1.使用容器安装客户端 docker pull minio/mc docker run minio/mc ls play 2.使用二进制安装客户端 wget https://dl.min.io/client/mc/release/linux-amd64/mc chmod +x mc ./mc 5 连接至服务端 ./mc config host add myminio http://172.17.0.1:9000 XH2LCA4AJIP52RDB4P5M CDDCuoS2FNsdW8S0bodkcs2729N+TH5lFov+rrT3 服务端启动时候的access key和secret key 6 mc的shell使用别名 ls=mc ls cp=mc cp cat=mc cat mkdir=mc mb pipe=mc pipe find=mc find 7 创建bucket ./mc mb myminio/minio-helm-repo 8 设置bucket和objects匿名访问 ./mc policy set download myminio/minio-helm-repo 9 helm创建与仓库连接的index.yaml文件 mkdir /root/helm/repo helm repo index helm/repo/ 10 helm与minio仓库进行连接 1.将index.yaml文件推送到backet中去 ./mc cp helm/repo/index.yaml myminio/minio-helm-repo 2.helm连接私仓 helm repo add fengnan http://192.168.0.119:9000/minio-helm-repo 3.更新repo仓库 helm repo update 4.查看repo helm repo list 5.查看repo中的文件 ./mc ls myminio/minio-helm-repo 6.登录服务端web界面查看 ","link":"https://tianxiawuhao.github.io/zIWVYloAQ/"},{"title":"k8s资源操作","content":"1.资源类型 资源分类 类型 具体资源 名称空间级别 工作负载型资源 Pod(pod:k8s 系统中可以创建和管理的最小单元) 名称空间级别 工作负载型资源 ReplicaSet(rs:用来确保容器应用的副本数始终保持在用户定义的副本数) 名称空间级别 工作负载型资源 Deployment(deployment:为 Pod 和 ReplicaSet 提供了一个 声明式定义 (declarative) 方法) 名称空间级别 工作负载型资源 StatefulSet(sts:为了解决有状态服务的问题) 名称空间级别 工作负载型资源 DaemonSet(ds:) 名称空间级别 工作负载型资源 Job(jobs:负责批处理任务，即仅执行一次的任务，它保证批处理任务的一个或多个 Pod) 名称空间级别 工作负载型资源 CronJob(jobs:管理基于时间的 Job) 名称空间级别 服务发现及负载均衡型资源 Service(svc:为一组功能相同的Pod提供一个供外界访问的地址) 名称空间级别 服务发现及负载均衡型资源 Ingress(ing:官方只能实现四层代理，INGRESS 可以实现七层代理) 名称空间级别 配置与存储型资源 Volume(存储卷) 名称空间级别 配置与存储型资源 CSI(容器存储接口-- 第三方存储卷) 名称空间级别 特殊类型的存储卷 ConfigMap(当配置中心来使用的资源类型) 名称空间级别 特殊类型的存储卷 Secret(保存敏感数据) 名称空间级别 特殊类型的存储卷 DownwardApi(把外部环境中的信息输出给容器) 集群级资源 集群级资源 Namespace(命名空间) 集群级资源 集群级资源 Node(集群节点) 集群级资源 集群级资源 Role(角色) 集群级资源 集群级资源 ClusterRole(集群角色) 集群级资源 集群级资源 RoleBinding(角色绑定) 集群级资源 集群级资源 ClusterRoleBinging(集群角色绑定) 元数据型资源 元数据型资源 HPA(根据 Pod的 CPU 利用率扩所容) 元数据型资源 元数据型资源 PodTemplate(pod资源模板) 元数据型资源 元数据型资源 LimitRange(资源限制) 2.操作指令 命令名 类型 作用 get 查 列出某个类型的下属资源 describe 查 查看某个资源的详细信息 logs 查 查看某个 pod 的日志 create 增 新建资源 explain 查 查看某个资源的配置项 delete 删 删除某个资源 edit 改 修改某个资源的配置项 apply 改 应用某个资源的配置项 3.查看和进入空间 命令名 作用 kubectl get pod -n名称空间 查看对应名称空间内的pod kubectl exec -it pod名字 -n名称空间 bash 进入对应名称空间的pod内 kubectl get nodes -o wide 获取节点和服务版本信息，并查看附加信息 kubectl get pod 获取pod信息，默认是default名称空间 kubectl get pod -o wide 获取pod信息，默认是default名称空间，并查看附加信息 kubectl get pod -n kube-system 获取指定名称空间的pod kubectl get pod -n kube-system podName 获取指定名称空间中的指定pod kubectl get pod -A 获取所有名称空间的pod kubectl get pods -o yamlkubectl get pods -o json 查看pod的详细信息，以yaml格式或json格式显示 kubectl get pod -A --show-labels 查看pod的标签信息 kubectl get pod -A --selector=“k8s-app=kube-dns” 根据Selector（label query）来查询pod kubectl exec podName env 查看运行pod的环境变量 kubectl logs -f --tail 500 -n kube-system kube-apiserver-k8s-master 查看指定pod的日志 kubectl get svc -A 查看所有名称空间的service信息 kubectl get svc -n kube-system 查看指定名称空间的service信息 kubectl get cs 查看componentstatuses信息 kubectl get cm -A 查看所有configmaps信息 kubectl get sa -A 查看所有serviceaccounts信息 kubectl get ds -A 查看所有daemonsets信息 kubectl get deploy -A 查看所有deployments信息 kubectl get rs -A 查看所有replicasets信息 kubectl get sts -A 查看所有statefulsets信息 kubectl get jobs -A 查看所有jobs信息 kubectl get ing -A 查看所有ingresses信息 kubectl get ns 查看有哪些名称空间 kubectl describe pod podNamekubectl describe pod -n kube-system kube-apiserver-k8s-master 查看pod的描述信息 kubectl describe deploy -n kube-system coredns 查看指定名称空间中指定deploy的描述信息 kubectl top nodekubectl top pod 查看node或pod的资源使用情况（需要heapster 或metrics-server支持） kubectl cluster-info kubectl cluster-info dump 查看集群信息 kubectl -s https://172.16.1.110:6443 get componentstatuses 查看各组件信息【172.16.1.110为master机器】 4.进入pod启动的容器 命令名 作用 kubectl exec -it podName -n nsName /bin/sh 进入容器 kubectl exec -it podName -n nsName /bin/bash 进入容器 5.添加label值 命令名 作用 kubectl label nodes k8s-node01 zone=north 为指定节点添加标签 kubectl label nodes k8s-node01 zone 为指定节点删除标签 kubectl label pod podName -n nsName role-name=test 为指定pod添加标签 kubectl label pod podName -n nsName role-name=dev --overwrite 修改lable标签值 kubectl label pod podName -n nsName role-name 删除lable标签 6.滚动升级 命令名 作用 kubectl apply -f myapp-deployment-v2.yaml 通过配置文件滚动升级 kubectl set image deploy/myapp-deployment myapp=“registry.cn-beijing.aliyuncs.com/google_registry/myapp:v3” 通过命令滚动升级 kubectl rollout undo deploy/myapp-deployment kubectl rollout undo deploy myapp-deployment pod回滚到前一个版本 kubectl rollout undo deploy/myapp-deployment --to-revision=2 回滚到指定历史版本 7.动态伸缩 命令名 作用 kubectl scale deploy myapp-deployment --replicas=5 动态伸缩 kubectl scale --replicas=8 -f myapp-deployment-v2.yaml 动态伸缩（根据yaml文件） 8.操作类命令 命令名 作用 kubectl create -f xxx.yaml 创建资源 kubectl apply -f xxx.yaml 应用资源 kubectl apply -f 应用资源，该目录下的所有 .yaml, .yml, 或 .json 文件都会被使用 kubectl create namespace test 创建test名称空间 kubectl delete -f xxx.yamlkubectl delete -f 删除资源 kubectl delete pod podName 删除指定的pod kubectl delete pod -n test podName 删除指定名称空间的指定pod kubectl delete svc svcNamekubectl delete deploy deployNamekubectl delete ns nsName 删除其他资源 kubectl delete pod podName -n nsName --grace-period=0 --forcekubectl delete pod podName -n nsName --grace-period=1kubectl delete pod podName -n nsName --now 强制删除 kubectl edit pod podName 编辑资源 9.状态 状态名 含义 Running 运行中 Error 异常，无法提供服务 Pending 准备中，暂时无法提供服务 Terminaling 结束中，即将被移除 Unknown 未知状态，多发生于节点宕机 PullImageBackOff 镜像拉取失败 必须存在的属性 参数名 字段类型 说明 version String K8S API版本, 可以使用kubectl api-version命令查询 kind String 指的是yaml文件定义的资源类型和角色, 比如Pod metadata Object 元数据对象, 固定值就写metadata metadata.name String 元数据对象的名字, 比如命名Pod的名字 metadata.namespace String 元数据对象的命名空间 Spec Object 详细定义对象, 固定值就写Spec spec.containers[] list Spec对象的容器列表定义, 是个列表 spec.containers[].name String 容器的名字 spec.containers[].image String 使用到的镜像名称 主要对象 apiVersion: v1 #必选，版本号，例如v1，可以用 kubectl api-versions 查询到 kind: Pod #必选，指yaml文件定义的k8s 资源类型或角色，比如：Pod metadata: #必选，元数据对象 name: string #必选，元数据对象的名字，自己定义，比如命名Pod的名字 namespace: string #必选，元数据对象的名称空间，默认为&quot;default&quot; labels: #自定义标签 key1: value1 #自定义标签键值对1 key2: value2 #自定义标签键值对2 annotations: #自定义注解 key1: value1 #自定义注解键值对1 key2: value2 #自定义注解键值对2 spec: #必选，对象【如pod】的详细定义 containers: #必选，spec对象的容器信息 - name: string #必选，容器名称 image: string #必选，要用到的镜像名称 imagePullPolicy: [Always|Never|IfNotPresent] #获取镜像的策略；(1)Always：意思是每次都尝试重新拉取镜像；(2)Never：表示仅使用本地镜像，即使本地没有镜像也不拉取；(3) IfNotPresent：如果本地有镜像就使用本地镜像，没有就拉取远程镜像。默认：Always command: [string] #指定容器启动命令，由于是数组因此可以指定多个。不指定则使用镜像打包时指定的启动命令。 args: [string] #指定容器启动命令参数，由于是数组因此可以指定多个 workingDir: string #指定容器的工作目录 volumeMounts: #指定容器内部的存储卷配置 - name: string #指定可以被容器挂载的存储卷的名称。跟下面volume字段的name值相同表示使用这个存储卷 mountPath: string #指定可以被容器挂载的存储卷的路径，应少于512字符 readOnly: boolean #设置存储卷路径的读写模式，true或者false，默认为读写模式false ports: #需要暴露的端口号列表 - name: string #端口的名称 containerPort: int #容器监听的端口号 #除非绝对必要，否则不要为 Pod 指定 hostPort。将 Pod 绑定到hostPort时，它会限制 Pod 可以调度的位置数 #DaemonSet 中的 Pod 可以使用 hostPort，从而可以通过节点 IP 访问到 Pod；因为DaemonSet模式下Pod不会被调度到其他节点。 #一般情况下 containerPort与hostPort值相同 hostPort: int #可以通过宿主机+hostPort的方式访问该Pod。例如：pod在/调度到了k8s-node02【172.16.1.112】，hostPort为8090，那么该Pod可以通过172.16.1.112:8090方式进行访问。 protocol: string #端口协议，支持TCP和UDP，默认TCP env: #容器运行前需设置的环境变量列表 - name: string #环境变量名称 value: string #环境变量的值 resources: #资源限制和资源请求的设置（设置容器的资源上线） limits: #容器运行时资源使用的上线 cpu: string #CPU限制，单位为core数，允许浮点数，如0.1等价于100m，0.5等价于500m；因此如果小于1那么优先选择如100m的形式，精度为1m。这个数字用作 docker run 命令中的 --cpu-quota 参数。 memory: string #内存限制，单位：E,P,T,G,M,K；或者Ei,Pi,Ti,Gi,Mi,Ki；或者字节数。将用于docker run --memory参数 requests: #容器启动和调度时的限制设定 cpu: string #CPU请求，容器启动时初始化可用数量，单位为core数，允许浮点数，如0.1等价于100m，0.5等价于500m；因此如果小于1那么优先选择如100m的形式，精度为1m。这个数字用作 docker run 命令中的 --cpu-shares 参数。 memory: string #内存请求,容器启动的初始化可用数量。单位：E,P,T,G,M,K；或者Ei,Pi,Ti,Gi,Mi,Ki；或者字节数 # 参见官网地址：https://kubernetes.io/zh/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/ livenessProbe: #对Pod内各容器健康检查的设置，当探测无响应几次后将自动重启该容器，检查方法有exec、httpGet和tcpSocket，对一个容器【只需设置其中一种方法即可】 exec: #对Pod内容器健康检查方式设置为exec方式 command: [string] #exec方式需要制定的命令或脚本 httpGet: #对Pod内容器健康检查方法设置为HttpGet，需要制定Path、port path: string #访问 HTTP 服务的路径 port: number #访问容器的端口号或者端口名。如果数字必须在 1 ~ 65535 之间。 host: string #当没有定义 &quot;host&quot; 时，使用 &quot;PodIP&quot; scheme: string #当没有定义 &quot;scheme&quot; 时，使用 &quot;HTTP&quot;，scheme 只允许 &quot;HTTP&quot; 和 &quot;HTTPS&quot; HttpHeaders: #请求中自定义的 HTTP 头。HTTP 头字段允许重复。 - name: string value: string tcpSocket: #对Pod内容器健康检查方式设置为tcpSocket方式 port: number initialDelaySeconds: 5 #容器启动完成后，kubelet在执行第一次探测前应该等待 5 秒。默认是 0 秒，最小值是 0。 periodSeconds: 60 #指定 kubelet 每隔 60 秒执行一次存活探测。默认是 10 秒。最小值是 1 timeoutSeconds: 3 #对容器健康检查探测等待响应的超时时间为 3 秒，默认1秒 successThreshold: 1 #检测到有1次成功则认为服务是`就绪` failureThreshold: 5 #检测到有5次失败则认为服务是`未就绪`。默认值是 3，最小值是 1。 restartPolicy: [Always|Never|OnFailure] #Pod的重启策略，默认Always。Always表示一旦不管以何种方式终止运行，kubelet都将重启；OnFailure表示只有Pod以非0退出码退出才重启；Nerver表示不再重启该Pod nodeSelector: #定义Node的label过滤标签，以key：value的格式指定。节点选择，先给主机打标签kubectl label nodes kube-node01 key1=value1 key1: value1 imagePullSecrets: #Pull镜像时使用的secret名称，以name：secretKeyName格式指定 - name: string hostNetwork: false #是否使用主机网络模式，默认为false。如果设置为true，表示使用宿主机网络，不使用docker网桥 # volumes 和 containers 是同层级 ****************************** # 参见官网地址：https://kubernetes.io/zh/docs/concepts/storage/volumes/ volumes: #定义了paues容器关联的宿主机或分布式文件系统存储卷列表 （volumes类型有很多种，选其中一种即可） - name: string #共享存储卷名称。 emptyDir: {} #类型为emtyDir的存储卷，与Pod同生命周期的一个临时目录。当Pod因为某些原因被从节点上删除时，emptyDir卷中的数据也会永久删除。 hostPath: string #类型为hostPath的存储卷，表示挂载Pod所在宿主机的文件或目录 path: string #在宿主机上文件或目录的路径 type: [|DirectoryOrCreate|Directory|FileOrCreate|File] #空字符串（默认）用于向后兼容，这意味着在安装 hostPath 卷之前不会执行任何检查。DirectoryOrCreate：如果给定目录不存在则创建，权限设置为 0755，具有与 Kubelet 相同的组和所有权。Directory：给定目录必须存在。FileOrCreate：如果给定文件不存在，则创建空文件，权限设置为 0644，具有与 Kubelet 相同的组和所有权。File：给定文件必须存在。 secret: #类型为secret的存储卷，挂载集群预定义的secre对象到容器内部。Secret 是一种包含少量敏感信息例如密码、token 或 key 的对象。放在一个 secret 对象中可以更好地控制它的用途，并降低意外暴露的风险。 secretName: string #secret 对象的名字 items: #可选，修改key 的目标路径 - key: username #username secret存储在/etc/foo/my-group/my-username 文件中而不是 /etc/foo/username 中。【此时存在spec.containers[].volumeMounts[].mountPath为/etc/foo】 path: my-group/my-username configMap: #类型为configMap的存储卷，挂载预定义的configMap对象到容器内部。ConfigMap 允许您将配置文件与镜像文件分离，以使容器化的应用程序具有可移植性。 name: string #提供你想要挂载的 ConfigMap 的名字 kubectl explain pod命令可以查看资源的模板 kubectl explain pod.apiVersion 查看详细字段 ","link":"https://tianxiawuhao.github.io/HhauOMn26/"},{"title":"k8s-ingress安装使用","content":"1.Nginx ingress安装 首先需要安装Nginx Ingress Controller控制器，控制器安装方式包含两种：DaemonSets和Deployments。 DaemonSets通过hostPort的方式暴露80和443端口，可通过Node的调度由专门的节点实现部署 Deployments则通过NodePort的方式实现控制器端口的暴露，借助外部负载均衡实现高可用负载均衡 除此之外，还需要部署Namespace，ServiceAccount，RBAC，Secrets，Custom Resource Definitions等资源，如下开始部署。 1.1 基础依赖环境准备 1、github中下载源码包,安装部署文件在kubernetes-ingress/deployments/目录下 git clone https://github.com/nginxinc/kubernetes-ingress.git kubernetes-ingress/deployments/ ├── common │ ├── custom-resource-definitions.yaml 自定义资源 │ ├── default-server-secret.yaml Secrets │ ├── nginx-config.yaml │ └── ns-and-sa.yaml Namspace+ServiceAccount ├── daemon-set │ ├── nginx-ingress.yaml DaemonSets控制器 │ └── nginx-plus-ingress.yaml ├── deployment │ ├── nginx-ingress.yaml Deployments控制器 │ └── nginx-plus-ingress.yaml ├── helm-chart Helm安装包 │ ├── chart-icon.png │ ├── Chart.yaml │ ├── README.md │ ├── templates │ │ ├── controller-configmap.yaml │ │ ├── controller-custom-resources.yaml │ │ ├── controller-daemonset.yaml │ │ ├── controller-deployment.yaml │ │ ├── controller-leader-election-configmap.yaml │ │ ├── controller-secret.yaml │ │ ├── controller-serviceaccount.yaml │ │ ├── controller-service.yaml │ │ ├── controller-wildcard-secret.yaml │ │ ├── _helpers.tpl │ │ ├── NOTES.txt │ │ └── rbac.yaml │ ├── values-icp.yaml │ ├── values-plus.yaml │ └── values.yaml ├── rbac RBAC认证授权 │ └── rbac.yaml ├── README.md └── service Service定义 ├── loadbalancer-aws-elb.yaml ├── loadbalancer.yaml DaemonSets暴露服务方式 └── nodeport.yaml Deployments暴露服务方式 **2、创建Namespace和ServiceAccount ** cat common/ns-and-sa.yaml apiVersion: v1 kind: Namespace metadata: name: nginx-ingress --- apiVersion: v1 kind: ServiceAccount metadata: name: nginx-ingress namespace: nginx-ingress kubectl apply -f common/ns-and-sa.yaml 3、创建Secrets自签名证书 kubectl apply -f common/default-server-secret.yaml apiVersion: v1 kind: Secret metadata: name: default-server-secret namespace: nginx-ingress type: Opaque data: tls.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN2akNDQWFZQ0NRREFPRjl0THNhWFhEQU5CZ2txaGtpRzl3MEJBUXNGQURBaE1SOHdIUVlEVlFRRERCWk8KUjBsT1dFbHVaM0psYzNORGIyNTBjbTlzYkdWeU1CNFhEVEU0TURreE1qRTRNRE16TlZvWERUSXpNRGt4TVRFNApNRE16TlZvd0lURWZNQjBHQTFVRUF3d1dUa2RKVGxoSmJtZHlaWE56UTI5dWRISnZiR3hsY2pDQ0FTSXdEUVlKCktvWklodmNOQVFFQkJRQURnZ0VQQURDQ0FRb0NnZ0VCQUwvN2hIUEtFWGRMdjNyaUM3QlBrMTNpWkt5eTlyQ08KR2xZUXYyK2EzUDF0azIrS3YwVGF5aGRCbDRrcnNUcTZzZm8vWUk1Y2Vhbkw4WGM3U1pyQkVRYm9EN2REbWs1Qgo4eDZLS2xHWU5IWlg0Rm5UZ0VPaStlM2ptTFFxRlBSY1kzVnNPazFFeUZBL0JnWlJVbkNHZUtGeERSN0tQdGhyCmtqSXVuektURXUyaDU4Tlp0S21ScUJHdDEwcTNRYzhZT3ExM2FnbmovUWRjc0ZYYTJnMjB1K1lYZDdoZ3krZksKWk4vVUkxQUQ0YzZyM1lma1ZWUmVHd1lxQVp1WXN2V0RKbW1GNWRwdEMzN011cDBPRUxVTExSakZJOTZXNXIwSAo1TmdPc25NWFJNV1hYVlpiNWRxT3R0SmRtS3FhZ25TZ1JQQVpQN2MwQjFQU2FqYzZjNGZRVXpNQ0F3RUFBVEFOCkJna3Foa2lHOXcwQkFRc0ZBQU9DQVFFQWpLb2tRdGRPcEsrTzhibWVPc3lySmdJSXJycVFVY2ZOUitjb0hZVUoKdGhrYnhITFMzR3VBTWI5dm15VExPY2xxeC9aYzJPblEwMEJCLzlTb0swcitFZ1U2UlVrRWtWcitTTFA3NTdUWgozZWI4dmdPdEduMS9ienM3bzNBaS9kclkrcUI5Q2k1S3lPc3FHTG1US2xFaUtOYkcyR1ZyTWxjS0ZYQU80YTY3Cklnc1hzYktNbTQwV1U3cG9mcGltU1ZmaXFSdkV5YmN3N0NYODF6cFErUyt1eHRYK2VBZ3V0NHh3VlI5d2IyVXYKelhuZk9HbWhWNThDd1dIQnNKa0kxNXhaa2VUWXdSN0diaEFMSkZUUkk3dkhvQXprTWIzbjAxQjQyWjNrN3RXNQpJUDFmTlpIOFUvOWxiUHNoT21FRFZkdjF5ZytVRVJxbStGSis2R0oxeFJGcGZnPT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo= tls.key: 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 4、创建ConfigMap自定义配置文 kubectl apply -f common/nginx-config.yaml kind: ConfigMap apiVersion: v1 metadata: name: nginx-config namespace: nginx-ingress data: 5、为主机和主机路由定义自定义资源，支持自定义主机和路由 kubectl apply -f common/custom-resource-definitions.yaml apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: virtualservers.k8s.nginx.org spec: group: k8s.nginx.org versions: - name: v1 served: true storage: true scope: Namespaced names: plural: virtualservers singular: virtualserver kind: VirtualServer shortNames: - vs --- apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: virtualserverroutes.k8s.nginx.org spec: group: k8s.nginx.org versions: - name: v1 served: true storage: true scope: Namespaced names: plural: virtualserverroutes singular: virtualserverroute kind: VirtualServerRoute shortNames: - vsr 6、配置RBAC认证授权，实现ingress控制器访问集群中的其他资源 kubectl apply -f rbac/rbac.yaml kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: nginx-ingress rules: - apiGroups: - &quot;&quot; resources: - services - endpoints verbs: - get - list - watch - apiGroups: - &quot;&quot; resources: - secrets verbs: - get - list - watch - apiGroups: - &quot;&quot; resources: - configmaps verbs: - get - list - watch - update - create - apiGroups: - &quot;&quot; resources: - pods verbs: - list - watch - apiGroups: - &quot;&quot; resources: - events verbs: - create - patch - apiGroups: - extensions resources: - ingresses verbs: - list - watch - get - apiGroups: - &quot;extensions&quot; resources: - ingresses/status verbs: - update - apiGroups: - k8s.nginx.org resources: - virtualservers - virtualserverroutes verbs: - list - watch - get --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: nginx-ingress subjects: - kind: ServiceAccount name: nginx-ingress namespace: nginx-ingress roleRef: kind: ClusterRole name: nginx-ingress apiGroup: rbac.authorization.k8s.io 1.2 部署Ingress控制器 1、 部署控制器，控制器可以DaemonSets和Deployment的形式部署，如下是DaemonSets的配置文件 apiVersion: apps/v1 kind: DaemonSet metadata: name: nginx-ingress namespace: nginx-ingress spec: selector: matchLabels: app: nginx-ingress template: metadata: labels: app: nginx-ingress #annotations: #prometheus.io/scrape: &quot;true&quot; #prometheus.io/port: &quot;9113&quot; spec: serviceAccountName: nginx-ingress containers: - image: nginx/nginx-ingress:edge imagePullPolicy: Always name: nginx-ingress ports: - name: http containerPort: 80 hostPort: 80 #通过hostPort的方式暴露端口 - name: https containerPort: 443 hostPort: 443 #- name: prometheus #containerPort: 9113 securityContext: allowPrivilegeEscalation: true runAsUser: 101 #nginx capabilities: drop: - ALL add: - NET_BIND_SERVICE env: - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name args: - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret #- -v=3 # Enables extensive logging. Useful for troubleshooting. #- -report-ingress-status #- -external-service=nginx-ingress #- -enable-leader-election #- -enable-prometheus-metrics Deployments的配置文件 apiVersion: apps/v1 kind: Deployment metadata: name: nginx-ingress namespace: nginx-ingress spec: replicas: 1 #副本的个数 selector: matchLabels: app: nginx-ingress template: metadata: labels: app: nginx-ingress #annotations: #prometheus.io/scrape: &quot;true&quot; #prometheus.io/port: &quot;9113&quot; spec: serviceAccountName: nginx-ingress containers: - image: nginx/nginx-ingress:edge imagePullPolicy: Always name: nginx-ingress ports: #内部暴露的服务端口，需要通过NodePort的方式暴露给外部 - name: http containerPort: 80 - name: https containerPort: 443 #- name: prometheus #containerPort: 9113 securityContext: allowPrivilegeEscalation: true runAsUser: 101 #nginx capabilities: drop: - ALL add: - NET_BIND_SERVICE env: - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name args: - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret #- -v=3 # Enables extensive logging. Useful for troubleshooting. #- -report-ingress-status #- -external-service=nginx-ingress #- -enable-leader-election #- -enable-prometheus-metrics 2、我们以DaemonSets的方式部署，DaemonSet部署集群中各个节点都是对等，如果有外部LoadBalancer则通过外部负载均衡路由至Ingress中 [root@node-1 deployments]# kubectl apply -f daemon-set/nginx-ingress.yaml daemonset.apps/nginx-ingress created [root@node-1 deployments]# kubectl get daemonsets -n nginx-ingress NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE nginx-ingress 3 3 3 3 3 &lt;none&gt; 15s [root@node-1 ~]# kubectl get pods -n nginx-ingress -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-ingress-7mpfc 1/1 Running 0 2m44s 10.244.0.50 node-1 &lt;none&gt; &lt;none&gt; nginx-ingress-l2rtj 1/1 Running 0 2m44s 10.244.1.144 node-2 &lt;none&gt; &lt;none&gt; nginx-ingress-tgf6r 1/1 Running 0 2m44s 10.244.2.160 node-3 &lt;none&gt; &lt;none&gt; 3、校验Nginx Ingress安装情况 此时三个节点均是对等，即访问任意一个节点均能实现相同的效果，统一入口则通过外部负载均衡，如果在云环境下执行kubectl apply -f service/loadbalancer.yaml创建外部负载均衡实现入口调度，自建的可以通过lvs或nginx等负载均衡实现接入。 备注说明：如果以Deployments的方式部署，则需要执行service/nodeport.yaml创建NodePort类型的Service，实现的效果和DaemonSets类似。 2. Ingress资源定义 上面的已安装了一个Nginx Ingress Controller控制器，有了Ingress控制器后，我们就可以定义Ingress资源来实现七层负载转发了，大体上Ingress支持三种使用方式：1. 基于虚拟主机转发，2. 基于虚拟机主机URI转发，3. 支持TLS加密转发。 2.1 Ingress定义 1、环境准备，先创建一个nginx的Deployment应用，包含2个副本 [root@node-1 ~]# kubectl run ingress-demo --image=nginx:1.7.9 --port=80 --replicas=2 [root@node-1 ~]# kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE ingress-demo 2/2 2 2 116s 2、以service方式暴露服务端口 [root@node-1 ~]# kubectl expose deployment ingress-demo --port=80 --protocol=TCP --target-port=80 service/ingress-demo exposed [root@node-1 ~]# kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-demo ClusterIP 10.109.33.91 &lt;none&gt; 80/TCP 2m15s 3、上述两个步骤已创建了一个service，如下我们定义一个ingress对象将流量转发至ingress-demo这个service，通过ingress.class指定控制器的类型为nginx [root@node-1 nginx-ingress]# cat nginx-ingress-demo.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: nginx-ingress-demo labels: ingres-controller: nginx annotations: kubernets.io/ingress.class: nginx spec: rules: - host: www.test.cn http: paths: - path: / backend: serviceName: ingress-demo servicePort: 80 4、创建ingress对象 [root@node-1 nginx-ingress]# kubectl apply -f nginx-ingress-demo.yaml ingress.extensions/nginx-ingress-demo created 查看ingress资源列表 [root@node-1 nginx-ingress]# kubectl get ingresses NAME HOSTS ADDRESS PORTS AGE nginx-ingress-demo www.test.cn 80 4m4s 5、查看ingress详情，可以在Rules规则中看到后端Pod的列表，自动发现和关联相关Pod [root@node-1 ~]# kubectl describe ingresses nginx-ingress-demo Name: nginx-ingress-demo Namespace: default Address: Default backend: default-http-backend:80 (&lt;none&gt;) Rules: Host Path Backends ---- ---- -------- www.test.cn / ingress-demo:80 (10.244.1.146:80,10.244.2.162:80) Annotations: kubectl.kubernetes.io/last-applied-configuration: {&quot;apiVersion&quot;:&quot;extensions/v1beta1&quot;,&quot;kind&quot;:&quot;Ingress&quot;,&quot;metadata&quot;:{&quot;annotations&quot;:{&quot;kubernets.io/ingress.class&quot;:&quot;nginx&quot;},&quot;labels&quot;:{&quot;ingres-controller&quot;:&quot;nginx&quot;},&quot;name&quot;:&quot;nginx-ingress-demo&quot;,&quot;namespace&quot;:&quot;default&quot;},&quot;spec&quot;:{&quot;rules&quot;:[{&quot;host&quot;:&quot;www.happylaulab.cn&quot;,&quot;http&quot;:{&quot;paths&quot;:[{&quot;backend&quot;:{&quot;serviceName&quot;:&quot;ingress-demo&quot;,&quot;servicePort&quot;:80},&quot;path&quot;:&quot;/&quot;}]}}]}} kubernets.io/ingress.class: nginx Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal AddedOrUpdated 9m7s nginx-ingress-controller Configuration for default/nginx-ingress-demo was added or updated Normal AddedOrUpdated 9m7s nginx-ingress-controller Configuration for default/nginx-ingress-demo was added or updated Normal AddedOrUpdated 9m7s nginx-ingress-controller Configuration for default/nginx-ingress-demo was added or updated 6、测试验证 ingress规则的配置信息已注入到Ingress Controller中，环境中Ingress Controller是以DaemonSets的方式部署在集群中，如果有外部的负载均衡，则将www.test.cn域名的地址解析为负载均衡VIP。由于测试环境没有搭建负载均衡，将hosts解析执行node-1，node-2或者node-3任意一个IP都能实现相同的功能。 上述测试解析正常，当然也可以解析为node-1和node-2的IP，如下： [root@node-1 ~]# curl -I http://www.happylau.cn --resolve www.happylau.cn:80:10.254.100.101 HTTP/1.1 200 OK Server: nginx/1.17.6 Date: Tue, 24 Dec 2019 10:32:22 GMT Content-Type: text/html Content-Length: 612 Connection: keep-alive Last-Modified: Tue, 23 Dec 2014 16:25:09 GMT ETag: &quot;54999765-264&quot; Accept-Ranges: bytes [root@node-1 ~]# curl -I http://www.happylau.cn --resolve www.happylau.cn:80:10.254.100.102 HTTP/1.1 200 OK Server: nginx/1.17.6 Date: Tue, 24 Dec 2019 10:32:24 GMT Content-Type: text/html Content-Length: 612 Connection: keep-alive Last-Modified: Tue, 23 Dec 2014 16:25:09 GMT ETag: &quot;54999765-264&quot; Accept-Ranges: bytes 3 Ingress动态配置 上面介绍了ingress资源对象的申明配置，这里我们探究一下Nginx Ingress Controller的实现机制和动态配置更新机制，以方便了解Ingress控制器的工作机制。 1、 查看Nginx Controller控制器的配置文件，在nginx-ingress pod中存储着ingress的配置文件 [root@node-1 ~]# kubectl get pods -n nginx-ingress NAME READY STATUS RESTARTS AGE nginx-ingress-7mpfc 1/1 Running 0 6h15m nginx-ingress-l2rtj 1/1 Running 0 6h15m nginx-ingress-tgf6r 1/1 Running 0 6h15m #查看配置文件，每个ingress生成一个配置文件，文件名为：命名空间-ingres名称.conf [root@node-1 ~]# kubectl exec -it nginx-ingress-7mpfc -n nginx-ingress -- ls -l /etc/nginx/conf.d total 4 -rw-r--r-- 1 nginx nginx 1005 Dec 24 10:06 default-nginx-ingress-demo.conf #查看配置文件 [root@node-1 ~]# kubectl exec -it nginx-ingress-7mpfc -n nginx-ingress -- cat /etc/nginx/conf.d/default-nginx-ingress-demo.conf # configuration for default/nginx-ingress-demo #upstream的配置，会用least_conn算法，通过service服务发现机制动态识别到后端的Pod upstream default-nginx-ingress-demo-www.test.cn-ingress-demo-80 { zone default-nginx-ingress-demo-www.test.cn-ingress-demo-80 256k; random two least_conn; server 10.244.1.146:80 max_fails=1 fail_timeout=10s max_conns=0; server 10.244.2.162:80 max_fails=1 fail_timeout=10s max_conns=0; } server { listen 80; server_tokens on; server_name www.test.cn; location / { proxy_http_version 1.1; proxy_connect_timeout 60s; proxy_read_timeout 60s; proxy_send_timeout 60s; client_max_body_size 1m; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Host $host; proxy_set_header X-Forwarded-Port $server_port; proxy_set_header X-Forwarded-Proto $scheme; proxy_buffering on; proxy_pass http://default-nginx-ingress-demo-www.test.cn-ingress-demo-80; #调用upstream实现代理 } } 通过上述查看配置文件可得知，Nginx Ingress Controller实际是根据ingress规则生成对应的nginx配置文件，以实现代理转发的功能，加入Deployments的副本数变更后nginx的配置文件会发生什么改变呢？ 2、更新控制器的副本数，由2个Pod副本扩容至3个 [root@node-1 ~]# kubectl scale --replicas=3 deployment ingress-demo deployment.extensions/ingress-demo scaled [root@node-1 ~]# kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE ingress-demo 3/3 3 3 123m 3、再次查看nginx的配置文件，ingress借助于service的服务发现机制，将加入的Pod自动加入到nginx upstream中 4、查看nginx pod的日志（kubectl logs nginx-ingress-7mpfc -n nginx-ingress），有reload优雅重启的记录，即通过更新配置文件+reload实现配置动态更新。 通过上述的配置可知，ingress调用kubernetes api去感知kubernetes集群中的变化情况，Pod的增加或减少这些变化，然后动态更新nginx ingress controller的配置文件，并重新载入配置。当集群规模越大时，会频繁涉及到配置文件的变动和重载，因此nginx这方面会存在先天的劣势，专门为微服务负载均衡应运而生，如Traefik，Envoy，Istio，这些负载均衡工具能够提供大规模，频繁动态更新的场景，但性能相比Nginx，HAproxy还存在一定的劣势。 4 Ingress路径转发 Ingress支持URI格式的转发方式，同时支持URL重写，如下以两个service为例演示，service-1安装nginx，service-2安装httpd，分别用http://demo.happylau.cn/news和http://demo.happylau.cn/sports转发到两个不同的service 1、环境准备，创建两个应用并实现service暴露，创建deployments时指定--explose创建service [root@node-1 ~]# kubectl run service-1 --image=nginx:1.7.9 --port=80 --replicas=1 --expose=true service/service-1 created deployment.apps/service-1 created [root@node-1 ~]# kubectl run service-2 --image=httpd --port=80 --replicas=1 --expose=true service/service-2 created deployment.apps/service-2 created 查看deployment状态 [root@node-1 ~]# kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE ingress-demo 4/4 4 4 4h36m service-1 1/1 1 1 65s service-2 1/1 1 1 52s 查看service状态，服务已经正常 [root@node-1 ~]# kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-demo ClusterIP 10.109.33.91 &lt;none&gt; 80/TCP 4h36m kubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 101d service-1 ClusterIP 10.106.245.71 &lt;none&gt; 80/TCP 68s service-2 ClusterIP 10.104.204.158 &lt;none&gt; 80/TCP 55s 2、创建ingress对象，通过一个域名将请求转发至后端两个service [root@node-1 nginx-ingress]# cat nginx-ingress-uri-demo.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: nginx-ingress-uri-demo labels: ingres-controller: nginx annotations: kubernets.io/ingress.class: nginx nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - host: http: paths: - path: /news backend: serviceName: service-1 servicePort: 80 - path: /sports backend: serviceName: service-2 servicePort: 80 3、创建ingress规则，查看详情 [root@node-1 nginx-ingress]# kubectl apply -f nginx-ingress-uri-demo.yaml ingress.extensions/nginx-ingress-uri-demo created #查看详情 [root@node-1 nginx-ingress]# kubectl get ingresses. NAME HOSTS ADDRESS PORTS AGE nginx-ingress-demo www.happylau.cn 80 4h35m nginx-ingress-uri-demo demo.happylau.cn 80 4s [root@node-1 nginx-ingress]# kubectl describe ingresses nginx-ingress-uri-demo Name: nginx-ingress-uri-demo Namespace: default Address: Default backend: default-http-backend:80 (&lt;none&gt;) Rules: #对应的转发url规则 Host Path Backends ---- ---- -------- demo.happylau.cn /news service-1:80 (10.244.2.163:80) /sports service-2:80 (10.244.1.148:80) Annotations: kubectl.kubernetes.io/last-applied-configuration: {&quot;apiVersion&quot;:&quot;extensions/v1beta1&quot;,&quot;kind&quot;:&quot;Ingress&quot;,&quot;metadata&quot;:{&quot;annotations&quot;:{&quot;kubernets.io/ingress.class&quot;:&quot;nginx&quot;,&quot;nginx.ingress.kubernetes.io/rewrite-target&quot;:&quot;/&quot;},&quot;labels&quot;:{&quot;ingres-controller&quot;:&quot;nginx&quot;},&quot;name&quot;:&quot;nginx-ingress-uri-demo&quot;,&quot;namespace&quot;:&quot;default&quot;},&quot;spec&quot;:{&quot;rules&quot;:[{&quot;host&quot;:&quot;demo.happylau.cn&quot;,&quot;http&quot;:{&quot;paths&quot;:[{&quot;backend&quot;:{&quot;serviceName&quot;:&quot;service-1&quot;,&quot;servicePort&quot;:80},&quot;path&quot;:&quot;/news&quot;},{&quot;backend&quot;:{&quot;serviceName&quot;:&quot;service-2&quot;,&quot;servicePort&quot;:80},&quot;path&quot;:&quot;/sports&quot;}]}}]}} kubernets.io/ingress.class: nginx nginx.ingress.kubernetes.io/rewrite-target: / Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal AddedOrUpdated 11s nginx-ingress-controller Configuration for default/nginx-ingress-uri-demo was added or updated Normal AddedOrUpdated 11s nginx-ingress-controller Configuration for default/nginx-ingress-uri-demo was added or updated Normal AddedOrUpdated 11s nginx-ingress-controller Configuration for default/nginx-ingress-uri-demo was added or updated 4、准备测试，站点中创建对应的路径 [root@node-1 ~]# kubectl exec -it service-1-7b66bf758f-xj9jh /bin/bash root@service-1-7b66bf758f-xj9jh:/# echo &quot;service-1 website page&quot; &gt;/usr/share/nginx/html/news [root@node-1 ~]# kubectl exec -it service-2-7c7444684d-w9cv9 /bin/bash root@service-2-7c7444684d-w9cv9:/usr/local/apache2# echo &quot;service-2 website page&quot; &gt;/usr/local/apache2/htdocs/sports 5、测试验证 [root@node-1 ~]# curl http://demo.happylau.cn/news --resolve demo.happylau.cn:80:10.254.100.101 service-1 website page [root@node-1 ~]# curl http://demo.happylau.cn/sports --resolve demo.happylau.cn:80:10.254.100.101 service-2 website page 6、总结 通过上述的验证测试可以得知，ingress支持URI的路由方式转发，其对应在ingress中的配置文件内容是怎样的呢，我们看下ingress controller生成对应的nginx配置文件内容，实际是通过ingress的location来实现，将不同的localtion转发至不同的upstream以实现service的关联，配置文件如下： [root@node-1 ~]# kubectl exec -it nginx-ingress-7mpfc -n nginx-ingress /bin/bash nginx@nginx-ingress-7mpfc:/$ cat /etc/nginx/conf.d/default-nginx-ingress-uri-demo.conf |grep -v &quot;^$&quot; # configuration for default/nginx-ingress-uri-demo #定义两个upstream和后端的service关联 upstream default-nginx-ingress-uri-demo-demo.happylau.cn-service-1-80 { zone default-nginx-ingress-uri-demo-demo.happylau.cn-service-1-80 256k; random two least_conn; server 10.244.2.163:80 max_fails=1 fail_timeout=10s max_conns=0; } upstream default-nginx-ingress-uri-demo-demo.happylau.cn-service-2-80 { zone default-nginx-ingress-uri-demo-demo.happylau.cn-service-2-80 256k; random two least_conn; server 10.244.1.148:80 max_fails=1 fail_timeout=10s max_conns=0; } server { listen 80; server_tokens on; server_name demo.happylau.cn; #定义location实现代理，通过proxy_pass和后端的service关联 location /news { proxy_http_version 1.1; proxy_connect_timeout 60s; proxy_read_timeout 60s; proxy_send_timeout 60s; client_max_body_size 1m; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Host $host; proxy_set_header X-Forwarded-Port $server_port; proxy_set_header X-Forwarded-Proto $scheme; proxy_buffering on; proxy_pass http://default-nginx-ingress-uri-demo-demo.happylau.cn-service-1-80; } location /sports { proxy_http_version 1.1; proxy_connect_timeout 60s; proxy_read_timeout 60s; proxy_send_timeout 60s; client_max_body_size 1m; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Host $host; proxy_set_header X-Forwarded-Port $server_port; proxy_set_header X-Forwarded-Proto $scheme; proxy_buffering on; proxy_pass http://default-nginx-ingress-uri-demo-demo.happylau.cn-service-2-80; } } 5 Ingress虚拟主机 ingress支持基于名称的虚拟主机，实现单个IP多个域名转发的需求，通过请求头部携带主机名方式区分开，将上个章节的ingress删除，使用service-1和service-2两个service来做演示。 1、创建ingress规则，通过主机名实现转发规则 apiVersion: extensions/v1beta1 kind: Ingress metadata: name: nginx-ingress-virtualname-demo labels: ingres-controller: nginx annotations: kubernets.io/ingress.class: nginx spec: rules: - host: news.happylau.cn http: paths: - path: / backend: serviceName: service-1 servicePort: 80 - host: sports.happylau.cn http: paths: - path: / backend: serviceName: service-2 servicePort: 80 2、生成ingress规则并查看详情，一个ingress对应两个HOSTS [root@node-1 nginx-ingress]# kubectl apply -f nginx-ingress-virtualname.yaml ingress.extensions/nginx-ingress-virtualname-demo created #查看列表 [root@node-1 nginx-ingress]# kubectl get ingresses nginx-ingress-virtualname-demo NAME HOSTS ADDRESS PORTS AGE nginx-ingress-virtualname-demo news.happylau.cn,sports.happylau.cn 80 12s #查看详情 [root@node-1 nginx-ingress]# kubectl describe ingresses nginx-ingress-virtualname-demo Name: nginx-ingress-virtualname-demo Namespace: default Address: Default backend: default-http-backend:80 (&lt;none&gt;) Rules: Host Path Backends ---- ---- -------- news.happylau.cn / service-1:80 (10.244.2.163:80) sports.happylau.cn / service-2:80 (10.244.1.148:80) Annotations: kubectl.kubernetes.io/last-applied-configuration: {&quot;apiVersion&quot;:&quot;extensions/v1beta1&quot;,&quot;kind&quot;:&quot;Ingress&quot;,&quot;metadata&quot;:{&quot;annotations&quot;:{&quot;kubernets.io/ingress.class&quot;:&quot;nginx&quot;},&quot;labels&quot;:{&quot;ingres-controller&quot;:&quot;nginx&quot;},&quot;name&quot;:&quot;nginx-ingress-virtualname-demo&quot;,&quot;namespace&quot;:&quot;default&quot;},&quot;spec&quot;:{&quot;rules&quot;:[{&quot;host&quot;:&quot;news.happylau.cn&quot;,&quot;http&quot;:{&quot;paths&quot;:[{&quot;backend&quot;:{&quot;serviceName&quot;:&quot;service-1&quot;,&quot;servicePort&quot;:80},&quot;path&quot;:&quot;/&quot;}]}},{&quot;host&quot;:&quot;sports.happylau.cn&quot;,&quot;http&quot;:{&quot;paths&quot;:[{&quot;backend&quot;:{&quot;serviceName&quot;:&quot;service-2&quot;,&quot;servicePort&quot;:80},&quot;path&quot;:&quot;/&quot;}]}}]}} kubernets.io/ingress.class: nginx Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal AddedOrUpdated 28s nginx-ingress-controller Configuration for default/nginx-ingress-virtualname-demo was added or updated Normal AddedOrUpdated 28s nginx-ingress-controller Configuration for default/nginx-ingress-virtualname-demo was added or updated Normal AddedOrUpdated 28s nginx-ingress-controller Configuration for default/nginx-ingress-virtualname-demo was added or updated 3、准备测试数据并测试 [root@node-1 ~]# kubectl exec -it service-1-7b66bf758f-xj9jh /bin/bash root@service-1-7b66bf758f-xj9jh:/# echo &quot;news demo&quot; &gt;/usr/share/nginx/html/index.html [root@node-1 ~]# kubectl exec -it service-2-7c7444684d-w9cv9 /bin/bash root@service-2-7c7444684d-w9cv9:/usr/local/apache2# echo &quot;sports demo&quot; &gt;/usr/local/apache2/htdocs/index.html 测试： [root@node-1 ~]# curl http://news.happylau.cn --resolve news.happylau.cn:80:10.254.100.102 news demo [root@node-1 ~]# curl http://sports.happylau.cn --resolve sports.happylau.cn:80:10.254.100.102 sports demo 4、查看nginx的配置文件内容，通过在server中定义不同的server_name以区分，代理到不同的upstream以实现service的代理。 # configuration for default/nginx-ingress-virtualname-demo upstream default-nginx-ingress-virtualname-demo-news.happylau.cn-service-1-80 { zone default-nginx-ingress-virtualname-demo-news.happylau.cn-service-1-80 256k; random two least_conn; server 10.244.2.163:80 max_fails=1 fail_timeout=10s max_conns=0; } upstream default-nginx-ingress-virtualname-demo-sports.happylau.cn-service-2-80 { zone default-nginx-ingress-virtualname-demo-sports.happylau.cn-service-2-80 256k; random two least_conn; server 10.244.1.148:80 max_fails=1 fail_timeout=10s max_conns=0; } server { listen 80; server_tokens on; server_name news.happylau.cn; location / { proxy_http_version 1.1; proxy_connect_timeout 60s; proxy_read_timeout 60s; proxy_send_timeout 60s; client_max_body_size 1m; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Host $host; proxy_set_header X-Forwarded-Port $server_port; proxy_set_header X-Forwarded-Proto $scheme; proxy_buffering on; proxy_pass http://default-nginx-ingress-virtualname-demo-news.happylau.cn-service-1-80; } } server { listen 80; server_tokens on; server_name sports.happylau.cn; location / { proxy_http_version 1.1; proxy_connect_timeout 60s; proxy_read_timeout 60s; proxy_send_timeout 60s; client_max_body_size 1m; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Host $host; proxy_set_header X-Forwarded-Port $server_port; proxy_set_header X-Forwarded-Proto $scheme; proxy_buffering on; proxy_pass http://default-nginx-ingress-virtualname-demo-sports.happylau.cn-service-2-80; } } 6 Ingress TLS加密 四层的负载均衡无法支持https请求，当前大部分业务都要求以https方式接入，Ingress能支持https的方式接入，通过Secrets存储证书+私钥，实现https接入，同时还能支持http跳转功能。对于用户的请求流量来说，客户端到ingress controller是https流量，ingress controller到后端service则是http，提高用户访问性能，如下介绍ingress TLS功能实现步骤。 1、生成自签名证书和私钥 [root@node-1 ~]# openssl req -x509 -newkey rsa:2048 -nodes -days 365 -keyout tls.key -out tls.crt Generating a 2048 bit RSA private key ....................................................+++ ........................................+++ writing new private key to 'tls.key' ----- You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank. ----- Country Name (2 letter code) [XX]:CN #国家 State or Province Name (full name) []:GD #省份 Locality Name (eg, city) [Default City]:ShenZhen #城市 Organization Name (eg, company) [Default Company Ltd]:Tencent #公司 Organizational Unit Name (eg, section) []:HappyLau #组织 Common Name (eg, your name or your server's hostname) []:www.happylau.cn #域名 Email Address []:573302346@qq.com #邮箱地址 #tls.crt为证书，tls.key为私钥 [root@node-1 ~]# ls tls.* -l -rw-r--r-- 1 root root 1428 12月 26 13:21 tls.crt -rw-r--r-- 1 root root 1708 12月 26 13:21 tls.key 2、配置Secrets，将证书和私钥配置到Secrets中 [root@node-1 ~]# kubectl create secret tls happylau-sslkey --cert=tls.crt --key=tls.key secret/happylau-sslkey created 查看Secrets详情,证书和私要包含在data中，文件名为两个不同的key：tls.crt和tls.key [root@node-1 ~]# kubectl describe secrets happylau-sslkey Name: happylau-sslkey Namespace: default Labels: &lt;none&gt; Annotations: &lt;none&gt; Type: kubernetes.io/tls Data ==== tls.crt: 1428 bytes tls.key: 1708 bytes 3、配置ingress调用Secrets实现SSL证书加密 apiVersion: extensions/v1beta1 kind: Ingress metadata: name: nginx-ingress-tls-demo labels: ingres-controller: nginx annotations: kubernets.io/ingress.class: nginx spec: tls: - hosts: - news.happylau.cn - sports.happylau.cn secretName: happylau-sslkey rules: - host: news.happylau.cn http: paths: - path: / backend: serviceName: service-1 servicePort: 80 - host: sports.happylau.cn http: paths: - path: / backend: serviceName: service-2 servicePort: 80 4、创建ingress并查看ingress详情 [root@node-1 nginx-ingress]# kubectl describe ingresses nginx-ingress-tls-demo Name: nginx-ingress-tls-demo Namespace: default Address: Default backend: default-http-backend:80 (&lt;none&gt;) TLS: happylau-sslkey terminates news.happylau.cn,sports.happylau.cn Rules: Host Path Backends ---- ---- -------- news.happylau.cn / service-1:80 (10.244.2.163:80) sports.happylau.cn / service-2:80 (10.244.1.148:80) Annotations: kubectl.kubernetes.io/last-applied-configuration: {&quot;apiVersion&quot;:&quot;extensions/v1beta1&quot;,&quot;kind&quot;:&quot;Ingress&quot;,&quot;metadata&quot;:{&quot;annotations&quot;:{&quot;kubernets.io/ingress.class&quot;:&quot;nginx&quot;},&quot;labels&quot;:{&quot;ingres-controller&quot;:&quot;nginx&quot;},&quot;name&quot;:&quot;nginx-ingress-tls-demo&quot;,&quot;namespace&quot;:&quot;default&quot;},&quot;spec&quot;:{&quot;rules&quot;:[{&quot;host&quot;:&quot;news.happylau.cn&quot;,&quot;http&quot;:{&quot;paths&quot;:[{&quot;backend&quot;:{&quot;serviceName&quot;:&quot;service-1&quot;,&quot;servicePort&quot;:80},&quot;path&quot;:&quot;/&quot;}]}},{&quot;host&quot;:&quot;sports.happylau.cn&quot;,&quot;http&quot;:{&quot;paths&quot;:[{&quot;backend&quot;:{&quot;serviceName&quot;:&quot;service-2&quot;,&quot;servicePort&quot;:80},&quot;path&quot;:&quot;/&quot;}]}}],&quot;tls&quot;:[{&quot;hosts&quot;:[&quot;news.happylau.cn&quot;,&quot;sports.happylau.cn&quot;],&quot;secretName&quot;:&quot;happylau-sslkey&quot;}]}} kubernets.io/ingress.class: nginx Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal AddedOrUpdated 22s nginx-ingress-controller Configuration for default/nginx-ingress-tls-demo was added or updated Normal AddedOrUpdated 22s nginx-ingress-controller Configuration for default/nginx-ingress-tls-demo was added or updated Normal AddedOrUpdated 22s nginx-ingress-controller Configuration for default/nginx-ingress-tls-demo was added or updated 5、访问 将news.happylau.cn和sports.happylau.cn写入到hosts文件中，并通过https://news.happylau.cn 的方式访问，浏览器访问内容提示证书如下，信任证书即可访问到站点内容。 查看证书详情，正是我们制作的自签名证书，生产实际使用时，推荐使用CA机构颁发签名证书。 6、接下来查看一下tls配置https的nginx配置文件内容，可以看到在server块启用了https并配置证书，同时配置了http跳转，因此直接访问http也能够实现自动跳转到https功能。 # configuration for default/nginx-ingress-tls-demo upstream default-nginx-ingress-tls-demo-news.happylau.cn-service-1-80 { zone default-nginx-ingress-tls-demo-news.happylau.cn-service-1-80 256k; random two least_conn; server 10.244.2.163:80 max_fails=1 fail_timeout=10s max_conns=0; } upstream default-nginx-ingress-tls-demo-sports.happylau.cn-service-2-80 { zone default-nginx-ingress-tls-demo-sports.happylau.cn-service-2-80 256k; random two least_conn; server 10.244.1.148:80 max_fails=1 fail_timeout=10s max_conns=0; } server { listen 80; listen 443 ssl; #https监听端口，证书和key，实现和Secrets关联 ssl_certificate /etc/nginx/secrets/default-happylau-sslkey; ssl_certificate_key /etc/nginx/secrets/default-happylau-sslkey; server_tokens on; server_name news.happylau.cn; #http跳转功能，即访问http会自动跳转至https if ($scheme = http) { return 301 https://$host:443$request_uri; } location / { proxy_http_version 1.1; proxy_connect_timeout 60s; proxy_read_timeout 60s; proxy_send_timeout 60s; client_max_body_size 1m; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Host $host; proxy_set_header X-Forwarded-Port $server_port; proxy_set_header X-Forwarded-Proto $scheme; proxy_buffering on; proxy_pass http://default-nginx-ingress-tls-demo-news.happylau.cn-service-1-80; } } server { listen 80; listen 443 ssl; ssl_certificate /etc/nginx/secrets/default-happylau-sslkey; ssl_certificate_key /etc/nginx/secrets/default-happylau-sslkey; server_tokens on; server_name sports.happylau.cn; if ($scheme = http) { return 301 https://$host:443$request_uri; } location / { proxy_http_version 1.1; proxy_connect_timeout 60s; proxy_read_timeout 60s; proxy_send_timeout 60s; client_max_body_size 1m; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Host $host; proxy_set_header X-Forwarded-Port $server_port; proxy_set_header X-Forwarded-Proto $scheme; proxy_buffering on; proxy_pass http://default-nginx-ingress-tls-demo-sports.happylau.cn-service-2-80; } } 7. Nginx Ingress高级功能 7.1 定制化参数 ingress controller提供了基础反向代理的功能，如果需要定制化nginx的特性或参数，需要通过ConfigMap和Annotations来实现，两者实现的方式有所不同，ConfigMap用于指定整个ingress集群资源的基本参数，修改后会被所有的ingress对象所继承；Annotations则被某个具体的ingress对象所使用，修改只会影响某个具体的ingress资源，冲突时其优先级高于ConfigMap。 7.1.1 ConfigMap自定义参数 安装nginx ingress controller时默认会包含一个空的ConfigMap，可以通过ConfigMap来自定义nginx controller的默认参数，如下以修改一些参数为例： 1、 定义ConfigMap参数 kind: ConfigMap apiVersion: v1 metadata: name: nginx-config namespace: nginx-ingress data: proxy-connect-timeout: &quot;10s&quot; proxy-read-timeout: &quot;10s&quot; proxy-send-timeout: &quot;10&quot; client-max-body-size: &quot;3m&quot; 2、 应用配置并查看ConfigMap配置 [root@node-1 ~]# kubectl get configmaps -n nginx-ingress nginx-config -o yaml apiVersion: v1 data: client-max-body-size: 3m proxy-connect-timeout: 10s proxy-read-timeout: 10s proxy-send-timeout: 10s kind: ConfigMap metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {&quot;apiVersion&quot;:&quot;v1&quot;,&quot;data&quot;:{&quot;client-max-body-size&quot;:&quot;3m&quot;,&quot;proxy-connect-timeout&quot;:&quot;10s&quot;,&quot;proxy-read-timeout&quot;:&quot;10s&quot;,&quot;proxy-send-timeout&quot;:&quot;10&quot;},&quot;kind&quot;:&quot;ConfigMap&quot;,&quot;metadata&quot;:{&quot;annotations&quot;:{},&quot;name&quot;:&quot;nginx-config&quot;,&quot;namespace&quot;:&quot;nginx-ingress&quot;}} creationTimestamp: &quot;2019-12-24T04:39:23Z&quot; name: nginx-config namespace: nginx-ingress resourceVersion: &quot;13845543&quot; selfLink: /api/v1/namespaces/nginx-ingress/configmaps/nginx-config uid: 9313ae47-a0f0-463e-a25a-1658f1ca0d57 3 、此时，ConfigMap定义的配置参数会被集群中所有的Ingress资源继承（除了annotations定义之外） 有很多参数可以定义，详情配置可参考方文档说明：https://github.com/nginxinc/kubernetes-ingress/blob/master/docs/configmap-and-annotations.md#Summary-of-ConfigMap-and-Annotations 7.1.2 Annotations自定义参数 ConfigMap定义的是全局的配置参数，修改后所有的配置都会受影响，如果想针对某个具体的ingress资源自定义参数，则可以通过Annotations来实现，下面开始以实际的例子演示Annotations的使用。 1、修改ingress资源，添加annotations的定义,通过nginx.org组修改了一些参数，如proxy-connect-timeout，调度算法为round_robin（默认为least _conn） apiVersion: extensions/v1beta1 kind: Ingress metadata: name: nginx-ingress-demo labels: ingres-controller: nginx annotations: kubernets.io/ingress.class: nginx nginx.org/proxy-connect-timeout: &quot;30s&quot; nginx.org/proxy-send-timeout: &quot;20s&quot; nginx.org/proxy-read-timeout: &quot;20s&quot; nginx.org/client-max-body-size: &quot;2m&quot; nginx.org/fail-timeout: &quot;5s&quot; nginx.org/lb-method: &quot;round_robin&quot; spec: rules: - host: www.happylau.cn http: paths: - path: / backend: serviceName: ingress-demo servicePort: 80 2、 重新应用ingress对象并查看参数配置情况 由上面的演示可得知，Annotations的优先级高于ConfigMapMap，Annotations修改参数只会影响到某一个具体的ingress资源，其定义的方法和ConfigMap相相近似，但又有差别，部分ConfigMap的参数Annotations无法支持，反过来Annotations定义的参数ConfigMap也不一定支持，下图列举一下常规支持参数情况： 通用参数 日志支持 请求头部 认证和安全 upstream支持 7.2 虚拟主机和路由 安装nginx ingress时我们安装了一个customresourcedefinitions自定义资源，其能够提供除了默认ingress功能之外的一些高级特性如 虚拟主机VirtualServer 虚拟路由VirtualServerRoute 健康检查Healthcheck 流量切割Split 会话保持SessionCookie 重定向Redirect 这些功能大部分依赖于Nginx Plus高级版本的支持，社区版本仅支持部分，对于企业级开发而言，丰富更多的功能可以购买企业级Nginx Plus版本。如下以通过VirtualServer和VirtualServerRoute定义upstream配置为例演示功能使用。 1、定义VirtualServer资源,其配置和ingress资源对象类似，能支持的功能会更丰富一点 apiVersion: k8s.nginx.org/v1 kind: VirtualServer metadata: name: cafe spec: host: cafe.example.com tls: secret: cafe-secret upstreams: - name: tea service: tea-svc port: 80 name: tea service: ingress-demo subselector: version: canary lb-method: round_robin fail-timeout: 10s max-fails: 1 max-conns: 32 keepalive: 32 connect-timeout: 30s read-timeout: 30s send-timeout: 30s next-upstream: &quot;error timeout non_idempotent&quot; next-upstream-timeout: 5s next-upstream-tries: 10 client-max-body-size: 2m tls: enable: true routes: - path: /tea action: pass: tea 2、 应用资源并查看VirtualServer资源列表 [root@node-1 ~]# kubectl apply -f vs.yaml virtualserver.k8s.nginx.org/cafe unchanged [root@node-1 ~]# kubectl get virtualserver NAME AGE cafe 2m52s 3、检查ingress控制器的配置文件情况,生成的配置和upstream定义一致 nginx@nginx-ingress-7mpfc:/etc/nginx/conf.d$ cat vs_default_cafe.conf upstream vs_default_cafe_tea { zone vs_default_cafe_tea 256k; server 10.244.0.51:80 max_fails=1 fail_timeout=10s max_conns=32; server 10.244.1.146:80 max_fails=1 fail_timeout=10s max_conns=32; server 10.244.1.147:80 max_fails=1 fail_timeout=10s max_conns=32; server 10.244.2.162:80 max_fails=1 fail_timeout=10s max_conns=32; keepalive 32; } server { listen 80; server_name cafe.example.com; listen 443 ssl; ssl_certificate /etc/nginx/secrets/default; ssl_certificate_key /etc/nginx/secrets/default; ssl_ciphers NULL; server_tokens &quot;on&quot;; location /tea { proxy_connect_timeout 30s; proxy_read_timeout 30s; proxy_send_timeout 30s; client_max_body_size 2m; proxy_max_temp_file_size 1024m; proxy_buffering on; proxy_http_version 1.1; set $default_connection_header &quot;&quot;; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection $vs_connection_header; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Host $host; proxy_set_header X-Forwarded-Port $server_port; proxy_set_header X-Forwarded-Proto $scheme; proxy_pass https://vs_default_cafe_tea; proxy_next_upstream error timeout non_idempotent; proxy_next_upstream_timeout 5s; proxy_next_upstream_tries 10; } } ","link":"https://tianxiawuhao.github.io/TX9ZIPxbq/"},{"title":"k8s界面kubesphere安装","content":"在 Kubernetes 上安装 KubeSphere 3.2.1，您的 Kubernetes 版本必须为：1.19.x、1.20.x、1.21.x 或 1.22.x（实验性支持） 1 搭建NFS作为默认sc（（主节点作为服务器，主节点操作） 1.1 配置NFS服务器 yum install -y nfs-utils rpcbind &amp;&amp; echo &quot;/nfs *(insecure,rw,sync,no_root_squash)&quot; &gt; /etc/exports 1.2 创建nfs服务器目录 mkdir -p /nfs 1.3 启动rpcbind,nfs服务命令 systemctl restart rpcbind &amp;&amp; systemctl enable rpcbind systemctl restart nfs-server &amp;&amp; systemctl enable nfs-server 1.4 检查配置是否生效 exportfs -r exportfs 1.5 测试Pod直接挂载NFS了（主节点操作） 1.5.1 在opt目录下创建一个nginx.yaml的文件 vi nginx.yaml 1.5.2 写入以下的命令 apiVersion: v1 kind: Pod metadata: name: vol-nfs namespace: default spec: volumes: - name: html nfs: path: /nfs server: 192.168.40.131 #自己的nfs服务器地址 containers: - name: myapp image: nginx volumeMounts: - name: html mountPath: /usr/share/nginx/html/ 1.5.3 应用该yaml的pod服务 kubectl apply -f nginx.yaml 1.5.4 检查该pod是否允许状态 kubectl get pod kubectl get pods -A 这里需要注意的是，必须等所有的状态为Runing才能进行下一步操作。 2 搭建NFS-Client（node节点操作） 服务器端防火墙开放111、662、875、892、2049的 tcp / udp 允许，否则远端客户无法连接。 2.1 安装客户端工具 yum install -y nfs-utils rpcbind showmount -e 192.168.40.131 该IP地址是master的IP地址 2.2 创建同步文件夹 mkdir /nfs/data/ 2.3 将客户端的/nfs/data和/nfs/做同步（node节点操作） mount -t nfs 192.168.40.131:/nfs/ /nfs/data/ 192.168.40.131：是nfs的服务器的地址，这里是master的IP地址。 3 设置动态供应 3.1 创建provisioner（NFS环境前面已经搭好） 字段名称 填入内容 备注 名称 nfs-storage 自定义存储类名称 NFS Server 192.168.40.131 NFS服务的IP地址 NFS Path /nfs NFS服务所共享的路径 3.1.1 先创建授权（master节点操作） vim nfs-rbac.yaml #在opt目录下 新建内容如下： --- apiVersion: v1 kind: ServiceAccount metadata: name: nfs-provisioner --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: nfs-provisioner-runner rules: - apiGroups: [&quot;&quot;] resources: [&quot;persistentvolumes&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;delete&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;persistentvolumeclaims&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;update&quot;] - apiGroups: [&quot;storage.k8s.io&quot;] resources: [&quot;storageclasses&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;events&quot;] verbs: [&quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;services&quot;, &quot;endpoints&quot;] verbs: [&quot;get&quot;,&quot;create&quot;,&quot;list&quot;, &quot;watch&quot;,&quot;update&quot;] - apiGroups: [&quot;extensions&quot;] resources: [&quot;podsecuritypolicies&quot;] resourceNames: [&quot;nfs-provisioner&quot;] verbs: [&quot;use&quot;] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: run-nfs-provisioner subjects: - kind: ServiceAccount name: nfs-provisioner namespace: default roleRef: kind: ClusterRole name: nfs-provisioner-runner apiGroup: rbac.authorization.k8s.io --- kind: Deployment apiVersion: apps/v1 metadata: name: nfs-client-provisioner spec: replicas: 1 strategy: type: Recreate selector: matchLabels: app: nfs-client-provisioner template: metadata: labels: app: nfs-client-provisioner spec: serviceAccount: nfs-provisioner containers: - name: nfs-client-provisioner image: lizhenliang/nfs-client-provisioner volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: storage.pri/nfs - name: NFS_SERVER value: 192.168.40.131 - name: NFS_PATH value: /nfs volumes: - name: nfs-client-root nfs: server: 192.168.40.131 path: /nfs 这个镜像中volume的mountPath默认为/persistentvolumes，不能修改，否则运行时会报错。ip的必须是自己的master的IP地址。 3.1.2 执行创建nfs的yaml文件信息 kubectl apply -f nfs-rbac.yaml 3.1.3 如果发现pod有问题，想删除pod进行重新kubectl apply-f nfs-rbac.yaml的话，可以参照这个博客文档： https://blog.csdn.net/qq_43542988/article/details/101277263?utm_medium=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param 3.1.4 查看pod的状态信息 kubectl get pods -A # 如果报错：查看报错信息，这个命令： kubectl describe pod xxx -n kube-system 3.1.5 创建storageclass（master节点操作） vim storageclass-nfs.yaml apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: storage-nfs provisioner: storage.pri/nfs reclaimPolicy: Delete 3.1.6 应用storageclass-nfs.yaml文件 kubectl apply -f storageclass-nfs.yaml 3.1.7 修改默认的驱动 kubectl patch storageclass storage-nfs -p '{&quot;metadata&quot;: {&quot;annotations&quot;:{&quot;storageclass.kubernetes.io/is-default-class&quot;:&quot;true&quot;}}}' kubectl get sc 4 安装metrics-server 4.1 准备metrics-server.yaml文件（主节点操作） vim metrics-server.yaml 4.2 编写以下的内容 --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: system:aggregated-metrics-reader labels: rbac.authorization.k8s.io/aggregate-to-view: &quot;true&quot; rbac.authorization.k8s.io/aggregate-to-edit: &quot;true&quot; rbac.authorization.k8s.io/aggregate-to-admin: &quot;true&quot; rules: - apiGroups: [&quot;metrics.k8s.io&quot;] resources: [&quot;pods&quot;, &quot;nodes&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;] --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: metrics-server:system:auth-delegator roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:auth-delegator subjects: - kind: ServiceAccount name: metrics-server namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: metrics-server-auth-reader namespace: kube-system roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: extension-apiserver-authentication-reader subjects: - kind: ServiceAccount name: metrics-server namespace: kube-system --- apiVersion: apiregistration.k8s.io/v1 kind: APIService metadata: name: v1beta1.metrics.k8s.io spec: service: name: metrics-server namespace: kube-system group: metrics.k8s.io version: v1beta1 insecureSkipTLSVerify: true groupPriorityMinimum: 100 versionPriority: 100 --- apiVersion: v1 kind: ServiceAccount metadata: name: metrics-server namespace: kube-system --- apiVersion: apps/v1 kind: Deployment metadata: name: metrics-server namespace: kube-system labels: k8s-app: metrics-server spec: selector: matchLabels: k8s-app: metrics-server template: metadata: name: metrics-server labels: k8s-app: metrics-server spec: serviceAccountName: metrics-server volumes: # mount in tmp so we can safely use from-scratch images and/or read-only containers - name: tmp-dir emptyDir: {} containers: - name: metrics-server image: mirrorgooglecontainers/metrics-server-amd64:v0.3.6 imagePullPolicy: IfNotPresent args: - --cert-dir=/tmp - --secure-port=4443 - --kubelet-insecure-tls - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname ports: - name: main-port containerPort: 4443 protocol: TCP securityContext: readOnlyRootFilesystem: true runAsNonRoot: true runAsUser: 1000 volumeMounts: - name: tmp-dir mountPath: /tmp nodeSelector: kubernetes.io/os: linux kubernetes.io/arch: &quot;amd64&quot; --- apiVersion: v1 kind: Service metadata: name: metrics-server namespace: kube-system labels: kubernetes.io/name: &quot;Metrics-server&quot; kubernetes.io/cluster-service: &quot;true&quot; spec: selector: k8s-app: metrics-server ports: - port: 443 protocol: TCP targetPort: main-port --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: system:metrics-server rules: - apiGroups: - &quot;&quot; resources: - pods - nodes - nodes/stats - namespaces - configmaps verbs: - get - list - watch --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: system:metrics-server roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:metrics-server subjects: - kind: ServiceAccount name: metrics-server namespace: kube-system 4.3 应用该文件pod kubectl apply -f metrics-server.yaml 4.4 查看部署的应用信息状态 kubectl get pod -A 4.5 查看系统的监控状态 kubectl top nodes 如果运行kubectl top nodes这个命令，爆metrics not available yet 这个命令还没有用，那就稍等一会，就能用了 这里，kubesphere3.0的前置环境全部结束。 5 安装kubesphere v3.0.0 5.1 文档地址 https://kubesphere.com.cn/ 1.5.2 部署文档地址 https://kubesphere.com.cn/docs/quick-start/minimal-kubesphere-on-k8s/ 1.5.3 安装步骤说明（master节点） 1.5.3.1 安装集群配置文件 1.5.3.1.1 准备配置文件cluster-configuration.yaml vim cluster-configuration.yaml 1.5.3.1.2 编写以下的内容配置 --- apiVersion: installer.kubesphere.io/v1alpha1 kind: ClusterConfiguration metadata: name: ks-installer namespace: kubesphere-system labels: version: v3.2.1 spec: persistence: storageClass: &quot;&quot; # If there is no default StorageClass in your cluster, you need to specify an existing StorageClass here. authentication: jwtSecret: &quot;&quot; # Keep the jwtSecret consistent with the Host Cluster. Retrieve the jwtSecret by executing &quot;kubectl -n kubesphere-system get cm kubesphere-config -o yaml | grep -v &quot;apiVersion&quot; | grep jwtSecret&quot; on the Host Cluster. local_registry: &quot;&quot; # Add your private registry address if it is needed. # dev_tag: &quot;&quot; # Add your kubesphere image tag you want to install, by default it's same as ks-install release version. etcd: monitoring: false # Enable or disable etcd monitoring dashboard installation. You have to create a Secret for etcd before you enable it. endpointIps: localhost # etcd cluster EndpointIps. It can be a bunch of IPs here. port: 2379 # etcd port. tlsEnable: true common: core: console: enableMultiLogin: true # Enable or disable simultaneous logins. It allows different users to log in with the same account at the same time. port: 30880 type: NodePort # apiserver: # Enlarge the apiserver and controller manager's resource requests and limits for the large cluster # resources: {} # controllerManager: # resources: {} redis: enabled: false volumeSize: 2Gi # Redis PVC size. openldap: enabled: false volumeSize: 2Gi # openldap PVC size. minio: volumeSize: 20Gi # Minio PVC size. monitoring: # type: external # Whether to specify the external prometheus stack, and need to modify the endpoint at the next line. endpoint: http://prometheus-operated.kubesphere-monitoring-system.svc:9090 # Prometheus endpoint to get metrics data. GPUMonitoring: # Enable or disable the GPU-related metrics. If you enable this switch but have no GPU resources, Kubesphere will set it to zero. enabled: false gpu: # Install GPUKinds. The default GPU kind is nvidia.com/gpu. Other GPU kinds can be added here according to your needs. kinds: - resourceName: &quot;nvidia.com/gpu&quot; resourceType: &quot;GPU&quot; default: true es: # Storage backend for logging, events and auditing. # master: # volumeSize: 4Gi # The volume size of Elasticsearch master nodes. # replicas: 1 # The total number of master nodes. Even numbers are not allowed. # resources: {} # data: # volumeSize: 20Gi # The volume size of Elasticsearch data nodes. # replicas: 1 # The total number of data nodes. # resources: {} logMaxAge: 7 # Log retention time in built-in Elasticsearch. It is 7 days by default. elkPrefix: logstash # The string making up index names. The index name will be formatted as ks-&lt;elk_prefix&gt;-log. basicAuth: enabled: false username: &quot;&quot; password: &quot;&quot; externalElasticsearchUrl: &quot;&quot; externalElasticsearchPort: &quot;&quot; alerting: # (CPU: 0.1 Core, Memory: 100 MiB) It enables users to customize alerting policies to send messages to receivers in time with different time intervals and alerting levels to choose from. enabled: false # Enable or disable the KubeSphere Alerting System. # thanosruler: # replicas: 1 # resources: {} auditing: # Provide a security-relevant chronological set of records，recording the sequence of activities happening on the platform, initiated by different tenants. enabled: false # Enable or disable the KubeSphere Auditing Log System. # operator: # resources: {} # webhook: # resources: {} devops: # (CPU: 0.47 Core, Memory: 8.6 G) Provide an out-of-the-box CI/CD system based on Jenkins, and automated workflow tools including Source-to-Image &amp; Binary-to-Image. enabled: false # Enable or disable the KubeSphere DevOps System. # resources: {} jenkinsMemoryLim: 2Gi # Jenkins memory limit. jenkinsMemoryReq: 1500Mi # Jenkins memory request. jenkinsVolumeSize: 8Gi # Jenkins volume size. jenkinsJavaOpts_Xms: 512m # The following three fields are JVM parameters. jenkinsJavaOpts_Xmx: 512m jenkinsJavaOpts_MaxRAM: 2g events: # Provide a graphical web console for Kubernetes Events exporting, filtering and alerting in multi-tenant Kubernetes clusters. enabled: false # Enable or disable the KubeSphere Events System. # operator: # resources: {} # exporter: # resources: {} # ruler: # enabled: true # replicas: 2 # resources: {} logging: # (CPU: 57 m, Memory: 2.76 G) Flexible logging functions are provided for log query, collection and management in a unified console. Additional log collectors can be added, such as Elasticsearch, Kafka and Fluentd. enabled: false # Enable or disable the KubeSphere Logging System. containerruntime: docker logsidecar: enabled: true replicas: 2 # resources: {} metrics_server: # (CPU: 56 m, Memory: 44.35 MiB) It enables HPA (Horizontal Pod Autoscaler). enabled: false # Enable or disable metrics-server. monitoring: storageClass: &quot;&quot; # If there is an independent StorageClass you need for Prometheus, you can specify it here. The default StorageClass is used by default. # kube_rbac_proxy: # resources: {} # kube_state_metrics: # resources: {} # prometheus: # replicas: 1 # Prometheus replicas are responsible for monitoring different segments of data source and providing high availability. # volumeSize: 20Gi # Prometheus PVC size. # resources: {} # operator: # resources: {} # adapter: # resources: {} # node_exporter: # resources: {} # alertmanager: # replicas: 1 # AlertManager Replicas. # resources: {} # notification_manager: # resources: {} # operator: # resources: {} # proxy: # resources: {} gpu: # GPU monitoring-related plug-in installation. nvidia_dcgm_exporter: # Ensure that gpu resources on your hosts can be used normally, otherwise this plug-in will not work properly. enabled: false # Check whether the labels on the GPU hosts contain &quot;nvidia.com/gpu.present=true&quot; to ensure that the DCGM pod is scheduled to these nodes. # resources: {} multicluster: clusterRole: none # host | member | none # You can install a solo cluster, or specify it as the Host or Member Cluster. network: networkpolicy: # Network policies allow network isolation within the same cluster, which means firewalls can be set up between certain instances (Pods). # Make sure that the CNI network plugin used by the cluster supports NetworkPolicy. There are a number of CNI network plugins that support NetworkPolicy, including Calico, Cilium, Kube-router, Romana and Weave Net. enabled: false # Enable or disable network policies. ippool: # Use Pod IP Pools to manage the Pod network address space. Pods to be created can be assigned IP addresses from a Pod IP Pool. type: none # Specify &quot;calico&quot; for this field if Calico is used as your CNI plugin. &quot;none&quot; means that Pod IP Pools are disabled. topology: # Use Service Topology to view Service-to-Service communication based on Weave Scope. type: none # Specify &quot;weave-scope&quot; for this field to enable Service Topology. &quot;none&quot; means that Service Topology is disabled. openpitrix: # An App Store that is accessible to all platform tenants. You can use it to manage apps across their entire lifecycle. store: enabled: false # Enable or disable the KubeSphere App Store. servicemesh: # (0.3 Core, 300 MiB) Provide fine-grained traffic management, observability and tracing, and visualized traffic topology. enabled: false # Base component (pilot). Enable or disable KubeSphere Service Mesh (Istio-based). kubeedge: # Add edge nodes to your cluster and deploy workloads on edge nodes. enabled: false # Enable or disable KubeEdge. cloudCore: nodeSelector: {&quot;node-role.kubernetes.io/worker&quot;: &quot;&quot;} tolerations: [] cloudhubPort: &quot;10000&quot; cloudhubQuicPort: &quot;10001&quot; cloudhubHttpsPort: &quot;10002&quot; cloudstreamPort: &quot;10003&quot; tunnelPort: &quot;10004&quot; cloudHub: advertiseAddress: # At least a public IP address or an IP address which can be accessed by edge nodes must be provided. - &quot;&quot; # Note that once KubeEdge is enabled, CloudCore will malfunction if the address is not provided. nodeLimit: &quot;100&quot; service: cloudhubNodePort: &quot;30000&quot; cloudhubQuicNodePort: &quot;30001&quot; cloudhubHttpsNodePort: &quot;30002&quot; cloudstreamNodePort: &quot;30003&quot; tunnelNodePort: &quot;30004&quot; edgeWatcher: nodeSelector: {&quot;node-role.kubernetes.io/worker&quot;: &quot;&quot;} tolerations: [] edgeWatcherAgent: nodeSelector: {&quot;node-role.kubernetes.io/worker&quot;: &quot;&quot;} tolerations: [] endpointIps: 192.168.40.131：master节点的地址。 1.5.3.1.3 准备配置文件kubesphere-installer.yaml vim kubesphere-installer.yaml 1.5.3.1.4 编写以下的内容配置 --- apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition metadata: name: clusterconfigurations.installer.kubesphere.io spec: group: installer.kubesphere.io versions: - name: v1alpha1 served: true storage: true schema: openAPIV3Schema: type: object properties: spec: type: object x-kubernetes-preserve-unknown-fields: true status: type: object x-kubernetes-preserve-unknown-fields: true scope: Namespaced names: plural: clusterconfigurations singular: clusterconfiguration kind: ClusterConfiguration shortNames: - cc --- apiVersion: v1 kind: Namespace metadata: name: kubesphere-system --- apiVersion: v1 kind: ServiceAccount metadata: name: ks-installer namespace: kubesphere-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: ks-installer rules: - apiGroups: - &quot;&quot; resources: - '*' verbs: - '*' - apiGroups: - apps resources: - '*' verbs: - '*' - apiGroups: - extensions resources: - '*' verbs: - '*' - apiGroups: - batch resources: - '*' verbs: - '*' - apiGroups: - rbac.authorization.k8s.io resources: - '*' verbs: - '*' - apiGroups: - apiregistration.k8s.io resources: - '*' verbs: - '*' - apiGroups: - apiextensions.k8s.io resources: - '*' verbs: - '*' - apiGroups: - tenant.kubesphere.io resources: - '*' verbs: - '*' - apiGroups: - certificates.k8s.io resources: - '*' verbs: - '*' - apiGroups: - devops.kubesphere.io resources: - '*' verbs: - '*' - apiGroups: - monitoring.coreos.com resources: - '*' verbs: - '*' - apiGroups: - logging.kubesphere.io resources: - '*' verbs: - '*' - apiGroups: - jaegertracing.io resources: - '*' verbs: - '*' - apiGroups: - storage.k8s.io resources: - '*' verbs: - '*' - apiGroups: - admissionregistration.k8s.io resources: - '*' verbs: - '*' - apiGroups: - policy resources: - '*' verbs: - '*' - apiGroups: - autoscaling resources: - '*' verbs: - '*' - apiGroups: - networking.istio.io resources: - '*' verbs: - '*' - apiGroups: - config.istio.io resources: - '*' verbs: - '*' - apiGroups: - iam.kubesphere.io resources: - '*' verbs: - '*' - apiGroups: - notification.kubesphere.io resources: - '*' verbs: - '*' - apiGroups: - auditing.kubesphere.io resources: - '*' verbs: - '*' - apiGroups: - events.kubesphere.io resources: - '*' verbs: - '*' - apiGroups: - core.kubefed.io resources: - '*' verbs: - '*' - apiGroups: - installer.kubesphere.io resources: - '*' verbs: - '*' - apiGroups: - storage.kubesphere.io resources: - '*' verbs: - '*' - apiGroups: - security.istio.io resources: - '*' verbs: - '*' - apiGroups: - monitoring.kiali.io resources: - '*' verbs: - '*' - apiGroups: - kiali.io resources: - '*' verbs: - '*' - apiGroups: - networking.k8s.io resources: - '*' verbs: - '*' - apiGroups: - kubeedge.kubesphere.io resources: - '*' verbs: - '*' - apiGroups: - types.kubefed.io resources: - '*' verbs: - '*' - apiGroups: - monitoring.kubesphere.io resources: - '*' verbs: - '*' - apiGroups: - application.kubesphere.io resources: - '*' verbs: - '*' --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: ks-installer subjects: - kind: ServiceAccount name: ks-installer namespace: kubesphere-system roleRef: kind: ClusterRole name: ks-installer apiGroup: rbac.authorization.k8s.io --- apiVersion: apps/v1 kind: Deployment metadata: name: ks-installer namespace: kubesphere-system labels: app: ks-install spec: replicas: 1 selector: matchLabels: app: ks-install template: metadata: labels: app: ks-install spec: serviceAccountName: ks-installer containers: - name: installer image: kubesphere/ks-installer:v3.2.1 imagePullPolicy: &quot;Always&quot; resources: limits: cpu: &quot;1&quot; memory: 1Gi requests: cpu: 20m memory: 100Mi volumeMounts: - mountPath: /etc/localtime name: host-time readOnly: true volumes: - hostPath: path: /etc/localtime type: &quot;&quot; name: host-time 1.5.3.1.5 分别执行两个文件 kubectl apply -f kubesphere-installer.yaml kubectl apply -f cluster-configuration.yaml #或者直接用网络文件安装 kubectl apply -f https://github.com/kubesphere/ks-installer/releases/download/v3.2.1/kubesphere-installer.yaml kubectl apply -f https://github.com/kubesphere/ks-installer/releases/download/v3.2.1/cluster-configuration.yaml 1.5.3.1.6 监控安装的日志信息 kubectl logs -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-install -o jsonpath='{.items[0].metadata.name}') -f 1.5.3.1.7 查看pod启动状态信息 kubectl get pods -A 需要等待漫长的时间。 1.5.4 访问验证是否安装成功 访问地址： http://192.168.40.131:30880/login 帐号：admin 密码：P@88w0rd 1.5.5 harbor目录下重新启动harbor docker-compose stop docker-compose up -d ","link":"https://tianxiawuhao.github.io/ULtZ5rQXA/"},{"title":"harbor自建镜像仓库","content":"centos网络配置 1.设置主机名 [root@localhost ~]# hostnamectl set-hostname harbor 2.添加 Host 解析 [root@harbor ~]# cat /etc/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 192.168.40.131 k8s-master 192.168.40.132 k8s-node1 192.168.40.133 k8s-node2 192.168.40.150 hub.test.com k8s 集群每个节点添加解析（注意：K8s 每个节点，不是 Harbor） [root@k8s-master ~]# echo &quot;192.168.40.150 hub.test.com&quot; &gt;&gt; /etc/hosts [root@k8s-node1 ~]# echo &quot;192.168.40.150 hub.test.com&quot; &gt;&gt; /etc/hosts [root@k8s-node2 ~]# echo &quot;192.168.40.150 hub.test.com&quot; &gt;&gt; /etc/hosts 3.网络环境设置 vi /etc/sysconfig/network-scripts/ifcfg-ens33 内容替换如下： BOOTPROTO=static #静态连接 ONBOOT=yes #网络设备开机启动 IPADDR=192.168.40.150 NETMASK=255.255.255.0 #子网掩码 GATEWAY=192.168.40.2 #网关 DNS1=114.114.114.114 #DNS解析 网络服务重启 service network restart 安装环境 1.安装Docker-CE # 卸载旧版本 yum remove docker docker-client docker-client-latest docker-common docker-latest docker-latest-logrotate docker-logrotate docker-engine # 安装所需的软件包 yum install -y yum-utils device-mapper-persistent-data lvm2 # 添加docker存储库 yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo #安装最新版的docker-ce yum install -y docker-ce docker-ce-cli containerd.io #启动docker并设置为开机自启动 systemctl enable --now docker 2.配置Docker镜像加速器 tee /etc/docker/daemon.json &lt;&lt;-'EOF' { &quot;registry-mirrors&quot;: [&quot;https://hub-mirror.c.163.com/&quot;], &quot;insecure-registries&quot;: [&quot;https://hub.test.com&quot;] } EOF # 重启 systemctl daemon-reload &amp;&amp; systemctl restart docker 3.安装docker-compose #1、安装pip yum -y install epel-release yum install python3-pip pip3 install --upgrade pip #2、安装docker-compose pip3 install docker-compose #3、查看版本 docker-compose version 4.创建 https 证书 安装 openssl [root@harbor]# yum install openssl -y 创建证书目录，并赋予权限 [root@harbor ~]# mkdir -p /cert/harbor [root@harbor ~]# chmod -R 777 /cert/harbor [root@harbor ~]# cd /cert/harbor 创建服务器证书密钥文件 harbor.key [root@harbor harbor]# openssl genrsa -des3 -out harbor.key 2048 输入密码，确认密码，自己随便定义，但是要记住，后面会用到。 创建服务器证书的申请文件 harbor.csr [root@harbor harbor]# openssl req -new -key harbor.key -out harbor.csr 输入密钥文件的密码, 然后一路回车。 备份一份服务器密钥文件 [root@harbor harbor]# cp harbor.key harbor.key.org 去除文件口令 [root@harbor harbor]# openssl rsa -in harbor.key.org -out harbor.key 输入密钥文件的密码 创建一个自当前日期起为期十年的证书 harbor.crt [root@harbor harbor]# openssl x509 -req -days 3650 -in harbor.csr -signkey harbor.key -out harbor.crt 5.下载Harbor安装包并解压 链接：https://pan.baidu.com/s/1AUEw0Qw3w9lZP-rnAYaWjA 提取码：qutg wget https://github.com/goharbor/harbor/releases/download/v2.5.0/harbor-offline-installer-v2.5.0.tgz tar zxvf harbor-offline-installer-v2.5.0.tgz 6.配置harbor.cfg和安装Harbor vi harbor.yml 将http端口改成10086，因为默认用的80端口已经被占用，http可以指定任意端口； 接下来运行install.sh安装和启动harbor ./prepare ./install.sh 7.测试 Harbor [root@k8s-master01 ~]# docker login https://hub.test.com Username: admin Password: Harbor12345 # 默认密码，可通过 harbor.yml 配置文件修改 登录时报：Error response from daemon: Get https://hub.test.com/v2/: x509: certificate is not valid for any names, but wanted to match hub.test.com 解决：修改客户端（即需要登陆harbor的机器）的docker.service 文件 vi /lib/systemd/system/docker.service 添加 --insecure-registry hub.test.com 重新加载服务配置文件，并且重启docker服务 systemctl daemon-reload &amp;&amp; systemctl restart docker 下载镜像推送到 Harbor [root@k8s-node01 ~]# docker pull nginx [root@k8s-node01 ~]# docker tag nginx:latest hub.test.com/library/test:v1 [root@k8s-node01 ~]# docker push hub.test.com/library/test:v1 8.Windows 访问 Harbor Web界面 Windows 添加 hosts 解析路径 C:\\Windows\\System32\\drivers\\etc\\hosts 添加信息 192.168.40.150 hub.test.com 浏览器访问测试 https://hub.test.com 用户密码：admin / Harbor12345 ","link":"https://tianxiawuhao.github.io/WOO39eqh2/"},{"title":"k8s1.24.1安装（基于Centos 7）","content":"注意：除了Master节点初始化及Node节点添加分别在Master节点和Node节点执行外，其余所有命令均在所有节点执行 整体环境 一台master节点，2台node节点。采用了Centos 7，有网络，互相可以ping通。 1.内核升级（可忽略） 为避免出现不可预知的问题，提升centos 7内核到最新版本 联网升级内核 1. 查看内核版本 uname -r 2. 导入ELRepo软件仓库的公共秘钥 rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org 3. 安装ELRepo软件仓库的yum源 rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm 4. 启用 elrepo 软件源并下载安装最新稳定版内核 yum --enablerepo=elrepo-kernel install kernel-ml -y 5. 查看系统可用内核，并设置内核启动顺序 sudo awk -F\\' '$1==&quot;menuentry &quot; {print i++ &quot; : &quot; $2}' /etc/grub2.cfg 6. 生成 grub 配置文件 机器上存在多个内核，我们要使用最新版本，可以通过 grub2-set-default 0 命令生成 grub 配置文件 grub2-set-default 0 #初始化页面的第一个内核将作为默认内核 grub2-mkconfig -o /boot/grub2/grub.cfg #重新创建内核配置 7. 重启系统并验证 yum update reboot uname -r 8. 删除旧内核 yum -y remove kernel kernel-tools 2.centos网络配置文件 网络配置文件名可能会有不同，在输入到ifcfg时，可以连续按两下tab键，获取提示，比如我的机器 为 ifcfg-ens33 vi /etc/sysconfig/network-scripts/ifcfg-ens33 1.内容替换如下： BOOTPROTO=static #静态连接 ONBOOT=yes #网络设备开机启动 IPADDR=192.168.40.131 #192.168.40.132,192.168.40.133. NETMASK=255.255.255.0 #子网掩码 GATEWAY=192.168.40.2 #网关 DNS1=114.114.114.114 #DNS解析 2.网络服务重启 service network restart 3.查看IP地址 3.安装依赖包 yum install -y wget 4.修改yum源（视网络情况操作） 1.备份 cp /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup 2.下载 wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo 或者使用清华大学站 sudo sed -e 's|^mirrorlist=|#mirrorlist=|g' -e 's|^#baseurl=http://mirror.centos.org/$contentdir|baseurl=https://mirrors.tuna.tsinghua.edu.cn/centos|g' -i.bak /etc/yum.repos.d/CentOS-Base.repo 3.清除生成缓存 yum clean all # 清除系统所有的yum缓存 yum makecache # 生成yum缓存 5.修改主机名 hostnamectl set-hostname k8s-master hostnamectl set-hostname k8s-node1 hostnamectl set-hostname k8s-node2 查看主机名称 6.添加hosts解析 echo -e &quot;192.168.40.131 k8s-master\\n192.168.40.132 k8s-node1\\n192.168.40.133 k8s-node2&quot; &gt;&gt; /etc/hosts 7.关闭防火墙firewalld systemctl stop firewalld &amp;&amp; systemctl disable firewalld 8.关闭selinux setenforce 0 &amp;&amp; sed -i 's/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/config 9.关闭swap分区交换 swapoff -a &amp;&amp; sed -i '/ swap / s/^\\(.*\\)$/#\\1/g' /etc/fstab 10.配置内核参数 将桥接的IPv4流量传递倒iptables的链 1.设置内核参数 cat &gt; /etc/sysctl.d/k8s.conf &lt;&lt;EOF net.bridge.bridge-nf-call-iptables=1 net.bridge.bridge-nf-call-ip6tables=1 net.ipv4.ip_forward=1 net.ipv4.tcp_tw_recycle=0 vm.swappiness=0 # 禁止使用swap空间，只有当系统00M时才允许使用它 vm.overcommit_memory=1 # 不检查物理内存是否够用 vm.panic_on_oom=0 # 开启oom fs.inotify.max_user_instances=8192 fs.inotify.max_user_watches=1048576 fs.file-max=52786963 fs.nr_open=52706963 net.ipv6.conf.all.disable_ipv6=1 net.netfilter.nf_conntrack_max=2310720 EOF 2.加载内核模块 modprobe br_netfilter &amp;&amp; echo &quot;modprobe br_netfilter&quot; &gt;&gt; /etc/rc.local 3.使内核参数生效 sysctl -p /etc/sysctl.d/k8s.conf 11.时间同步 timedatectl set-timezone Asia/Shanghai &amp;&amp; timedatectl set-local-rtc 0 #重启依赖于系统时间的服务 systemctl restart rsyslog &amp;&amp; systemctl restart crond 12.安装iptables，设置空表 yum -y install iptables-services &amp;&amp; systemctl start iptables &amp;&amp; systemctl enable iptables &amp;&amp; iptables -F &amp;&amp; service iptables save 检查服务的的规则：iptables -L -n 13.开启IPVS 由于ipvs已经加入到了内核的主干，所以为kube-proxy开启ipvs的前提需要加载以下的内核模块 cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOF #！/bin/bash modprobe -- ip_vs modprobe -- ip_vs_rr modprobe -- ip_vs_wrr modprobe -- ip_vs_sh modprobe -- nf_conntrack_ipv4 EOF 授权启动 chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod 接下来还需要确保各个节点上已经安装了ipset软件包。 为了便于查看ipvs的代理规则，最好安装一下管理工具ipvsadm。 yum install ipset ipvsadm -y service底层实现主要由两个网络模式组成：iptables与IPVS。他们都是有kube-proxy维护 Iptables VS IPVS Iptables： • 灵活，功能强大 • 规则遍历匹配和更新，呈线性时延 IPVS： • 工作在内核态，有更好的性能 • 调度算法丰富：rr，wrr，lc，wlc，ip hash 等集群部署成功，mode由空值修改成ipvs模式 kubectl edit configmap kube-proxy -n kube-system configmap/kube-proxy edited 删除所有kube-proxy的pod,等待重启 kubectl delete pod/kube-proxy-84p9n -n kube-system # 查看ipvs相关规则 ipvsadm 14.安装docker软件 自1.20版本被弃用之后，dockershim组件终于在1.24的kubelet中被删除。从1.24开始，大家需要使用其他受到支持的运行时选项（例如containerd或CRI-O）；如果您选择Docker Engine作为运行时，则需要使用cri-dockerd。 1.删除自带的docker yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-selinux \\ docker-engine-selinux \\ docker-engine 2.安装依赖包 yum install -y yum-utils device-mapper-persistent-data lvm2 3.安装yum源 yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo 4.安装docker-ce yum -y install docker-ce 5.设置docker cat &gt; /etc/docker/daemon.conf &lt;&lt;EOF { &quot;registry-mirrors&quot;: [&quot;https://registry.docker-cn.com&quot;,&quot;https://heusyzko.mirror.aliyuncs.com&quot;], &quot;insecure-registries&quot;: [&quot;https://hub.test.com&quot;], &quot;exec-opts&quot;:[&quot;native.cgroupdriver=systemd&quot;], &quot;log-driver&quot;:&quot;json-file&quot;, &quot;log-opts&quot;:{ &quot;max-size&quot;: &quot;100m&quot; }, &quot;storage-driver&quot;: &quot;overlay2&quot; } EOF mkdir -p /etc/systemd/system/docker.service.d 6.重启docker服务 systemctl daemon-reload &amp;&amp; systemctl start docker &amp;&amp; systemctl enable docker systemctl daemon-reload &amp;&amp; systemctl restart docker 以下的NO_PROXY表示对这些网段的服务器不使用代理，如果不需要用到代理服务器，以下的配置可以不写，注意，以下的代理是不通的。不建议使用代理，因为国内有资源可以访问到gcr.io需要的镜像，下文会介绍 #放在Type=notify下面 vi /usr/lib/systemd/system/docker.service Environment=&quot;HTTPS_PROXY=http://www.ik8s.io:10080&quot; Environment=&quot;HTTP_PROXY=http://www.ik8s.io:10080&quot; Environment=&quot;NO_PROXY=127.0.0.0/8,172.20.0.0/16&quot; #保存退出后，执行 systemctl daemon-reload #确保如下两个参数值为1，默认为1。 cat /proc/sys/net/bridge/bridge-nf-call-ip6tables cat /proc/sys/net/bridge/bridge-nf-call-iptables #启动docker-ce systemctl restart docker #设置开机启动 systemctl enable docker.service #想要删除容器，则要先停止所有容器（当然，也可以加-f强制删除，但是不推荐）： docker stop $(docker ps -a -q) #删除所有容器 docker rm $(docker ps -a -q) #.删除所有镜像（慎重） docker rmi $(docker images -q) 14.2.cri-dockerd安装 CRI-Dockerd 其实就是从被移除的 Docker Shim 中，独立出来的一个项目，用于解决历史遗留的节点升级 Kubernetes 的问题。 kubelet并没有直接和dockerd交互，而是通过了一个dockershim的组件间接操作dockerd。dockershim提供了一个标准的接口，让kubelet能够专注于容器调度逻辑本身，而不用去适配dockerd的接口变动。而其他实现了相同标准接口的容器技术也可以被kubelet集成使用，这个接口称作CRI。dockershim和CRI的出现也是容器生态系统演化的历史产物。在k8s最早期的版本中是不存在dockershim的，kubelet直接和dockerd交互。但为了支持更多不同的容器技术（避免完全被docker控制容器技术市场），kubelet在之后的版本开始支持另一种容器技术rkt。这给kubelet的维护工作造成了巨大的挑战，因为两种容器技术没有统一的接口和使用逻辑，kubelet同时支持两种技术的使用还要保证一致的容器功能表现，对代码逻辑和功能可靠性都有很大的影响。为了解决这个问题，k8s提出了一个统一接口CRI，kubelet统一通过这个接口来调用容器功能。但是dockerd并不支持CRI，k8s就自己实现了配套的dockershim将CRI接口调用转换成dockerd接口调用来支持CRI。因此，dockershim并不是docker技术的一部分，而是k8s系统的一部分 使用 CRI-Dockerd 项目 项目地址：https://github.com/Mirantis/cri-dockerd 1.下载cri-dockerd二进制包或者源码自己编译 # 下载文件 wget https://github.com/Mirantis/cri-dockerd/releases/download/v0.2.0/cri-dockerd-v0.2.0-linux-amd64.tar.gz # 解压文件 tar -xvf cri-dockerd-v0.2.0-linux-amd64.tar.gz # 复制二进制文件到指定目录 cp cri-dockerd /usr/bin/ 2.配置启动文件 # 配置启动文件 cat &lt;&lt;&quot;EOF&quot; &gt; /usr/lib/systemd/system/cri-docker.service [Unit] Description=CRI Interface for Docker Application Container Engine Documentation=https://docs.mirantis.com After=network-online.target firewalld.service docker.service Wants=network-online.target Requires=cri-docker.socket [Service] Type=notify ExecStart=/usr/bin/cri-dockerd --container-runtime-endpoint=unix:///var/run/cri-docker.sock --network-plugin=cni --cni-bin-dir=/opt/cni/bin \\ --cni-conf-dir=/etc/cni/net.d --image-pull-progress-deadline=30s --pod-infra-container-image=registry.aliyuncs.com/google_containers/pause:3.7 \\ --docker-endpoint=unix:///var/run/docker.sock --cri-dockerd-root-directory=/var/lib/docker ExecReload=/bin/kill -s HUP $MAINPID TimeoutSec=0 RestartSec=2 Restart=always # Note that StartLimit* options were moved from &quot;Service&quot; to &quot;Unit&quot; in systemd 229. # Both the old, and new location are accepted by systemd 229 and up, so using the old location # to make them work for either version of systemd. StartLimitBurst=3 # Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230. # Both the old, and new name are accepted by systemd 230 and up, so using the old name to make # this option work for either version of systemd. StartLimitInterval=60s # Having non-zero Limit*s causes performance problems due to accounting overhead # in the kernel. We recommend using cgroups to do container-local accounting. LimitNOFILE=infinity LimitNPROC=infinity LimitCORE=infinity # Comment TasksMax if your systemd version does not support it. # Only systemd 226 and above support this option. TasksMax=infinity Delegate=yes KillMode=process [Install] WantedBy=multi-user.target EOF # 生成socket 文件 cat &lt;&lt;&quot;EOF&quot; &gt; /usr/lib/systemd/system/cri-docker.socket [Unit] Description=CRI Docker Socket for the API PartOf=cri-docker.service [Socket] ListenStream=/var/run/cri-dockerd.sock SocketMode=0660 SocketUser=root SocketGroup=docker [Install] WantedBy=sockets.target EOF # 启动 cri-dockerd systemctl daemon-reload systemctl start cri-docker #设置开机启动 systemctl enable cri-docker # 查看启动状态 systemctl status cri-docker 3.下载cri-tools验证cri-docker 是否正常 # 下载二进制文件 wget https://github.com/kubernetes-sigs/cri-tools/releases/download/v1.24.0/crictl-v1.24.0-linux-amd64.tar.gz # 解压 tar -xvf crictl-v1.24.0-linux-amd64.tar.gz # 复制二进制文件到指定目录 cp crictl /usr/bin/ # 创建配置文件 vim /etc/crictl.yaml runtime-endpoint: &quot;unix:///var/run/cri-docker.sock&quot; image-endpoint: &quot;unix:///var/run/cri-docker.sock&quot; timeout: 10 debug: false pull-image-on-create: true disable-pull-on-run: false # 测试能否访问docker # 查看运行的容器 crictl ps # 查看拉取的镜像 crictl images # 拉取镜像 crictl pull busybox [root@k8s-node-4 ~]# crictl pull busybox Image is up to date for busybox@sha256:5ecba83a746c7608ed511dc1533b87c737a0b0fb730301639a0179f9344b13448 返回一切正常cri-dockerd接入docker完整 15.部署 containerd(k8s-1.24版本以上) 服务版本 服务名称 版本号 内核 5.14.3-1.el7.elrepo.x86_64 containerd v1.6.4（加入） ctr v1.6.4 k8s 1.24 1.安装containerd 创建配置文件 mkdir /etc/modules-load.d/containerd.conf 创建完配置文件执行以下命令 modprobe overlay &amp;&amp; modprobe br_netfilter 立即生效 sysctl --system 下载 docker-ce 源 wget http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 或者 yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 安装 containerd 服务并加入开机启动 yum install -y containerd.io systemctl enable containerd &amp;&amp; systemctl start containerd 2.配置 containerd 创建路径 mkdir -p /etc/containerd 获取默认配置文件 containerd config default | sudo tee /etc/containerd/config.toml 修改配置文件，新增 &quot;SystemdCgroup = true&quot;，使用 systemd 作为 cgroup 驱动程序 [root@master1 ~]# vi /etc/containerd/config.toml #修改以下内容 [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.runc.options] SystemdCgroup = true ## 修改为true 替换默认pause镜像地址 默认情况下k8s.gcr.io无法访问，所以使用阿里云镜像仓库地址即可 # 所有节点更换默认镜像地址 sed -i 's/k8s.gcr.io/registry.cn-beijing.aliyuncs.com\\/abcdocker/' /etc/containerd/config.toml 重启 containerd systemctl restart containerd 查看 containerd 运行状态(以下状态视为正常) [root@master1 ~]# systemctl status containerd ● containerd.service - containerd container runtime Loaded: loaded (/usr/lib/systemd/system/containerd.service; enabled; vendor preset: disabled) Active: active (running) since Sun 2022-03-06 08:09:00 CST; 1h 43min ago Docs: https://containerd.io Process: 931 ExecStartPre=/sbin/modprobe overlay (code=exited, status=0/SUCCESS) Main PID: 941 (containerd) Tasks: 11 Memory: 61.4M CGroup: /system.slice/containerd.service └─941 /usr/bin/containerd 16.安装kubeadm、kubelet、kubectl 1.配置文件修改 cat &lt;&lt;EOF | sudo tee /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF 2.安装启用 sudo yum install -y kubelet-1.24.1 kubeadm-1.24.1 kubectl-1.24.1 --disableexcludes=kubernetes sudo systemctl enable kubelet &amp;&amp; systemctl start kubelet 3.修改kubelet的配置文件 先查看配置文件位置 systemctl status kubelet vi /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf 并添加以下内容(使用和docker相同的cgroup-driver)。 Environment=&quot;KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --cgroup-driver=systemd&quot; 4.重启kubelet systemctl daemon-reload &amp;&amp; systemctl restart kubelet 17.获取K8S镜像（可忽略） 1.获取镜像列表 使用阿里云镜像仓库下载（国内环境该命令可不执行，下步骤kubeadm init --image-repository registry.cn-hangzhou.aliyuncs.com/google_containers已经默认为国内环境） 由于官方镜像地址被墙，所以我们需要首先获取所需镜像以及它们的版本。然后从国内镜像站获取。 kubeadm config images list 获取镜像列表后可以通过下面的脚本从阿里云获取： vi /usr/local/k8s/k8s-images.sh 下面的镜像应该去除&quot;k8s.gcr.io/&quot;的前缀，版本换成上面获取到的版本 images=( kube-apiserver:v1.24.1 kube-controller-manager:v1.24.1 kube-scheduler:v1.24.1 kube-proxy:v1.24.1 pause:3.7 etcd:3.5.3-0 coredns:v1.8.6 ) for imageName in ${images[@]} ; do docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName k8s.gcr.io/$imageName docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName done 2.赋权执行 chmod +x k8s-images.sh &amp;&amp; ./k8s-images.sh 以上操作在所有机器执行 18.初始化环境（master操作） 1.安装镜像 采用模板配置文件加载 kubeadm config print init-defaults &gt; kubeadm-config.yaml [root@master1 ~]# cat kubeadm-config.yaml apiVersion: kubeadm.k8s.io/v1beta2 bootstrapTokens: - groups: - system:bootstrappers:kubeadm:default-node-token token: abcdef.0123456789abcdef ttl: 24h0m0s usages: - signing - authentication kind: InitConfiguration localAPIEndpoint: advertiseAddress: 192.168.40.131 # 本机IP bindPort: 6443 nodeRegistration: criSocket: unix:///var/run/cri-docker.sock # 此处千万不要忘记修改，如果不修改等于没有替换。(此处已经更改完了) #criSocket: unix:///run/containerd/containerd.sock # 此处千万不要忘记修改，如果不修改等于没有替换。(此处已经更改完了) name: master1 # 本主机名 taints: - effect: NoSchedule key: node-role.kubernetes.io/master --- apiServer: timeoutForControlPlane: 4m0s apiVersion: kubeadm.k8s.io/v1beta2 certificatesDir: /etc/kubernetes/pki clusterName: kubernetes controlPlaneEndpoint: &quot;192.168.40.151:16443&quot; # 虚拟IP和haproxy端口 controllerManager: {} dns: type: CoreDNS etcd: local: dataDir: /var/lib/etcd imageRepository: registry.aliyuncs.com/google_containers # 镜像仓库源要根据自己实际情况修改 kind: ClusterConfiguration kubernetesVersion: v1.24.1 # k8s版本 networking: dnsDomain: cluster.local podSubnet: &quot;10.244.0.0/16&quot; #设置网段，和下面网络插件对应 serviceSubnet: 10.96.0.0/12 scheduler: {} --- apiVersion: kubeproxy.config.k8s.io/v1alpha1 kind: KubeProxyConfiguration featureGates: SupportIPVSProxyMode: true mode: ipvs 2.查看kubeadm版本，修改命令参数 kubeadm version 这个就很简单了，只需要简单的一个命令： #直接使用已经下载好的镜像 kubeadm init --kubernetes-version=v1.24.1 --apiserver-advertise-address=192.168.40.131 --pod-network-cidr=10.244.0.0/16 --service-cidr=10.96.0.0/12 --ignore-preflight-errors=Swap --cri-socket unix:///var/run/cri-docker.sock | tee kubeadm-init.log #或者采用aliyuncs镜像下载 kubeadm init --kubernetes-version=v1.24.1 --apiserver-advertise-address=192.168.40.131 --image-repository registry.aliyuncs.com/google_containers --service-cidr=10.1.0.0/16 --pod-network-cidr=10.244.0.0/16 --cri-socket unix:///var/run/cri-docker.sock| tee kubeadm-init.log #使用上面系统生成配置文件加载 kubeadm init --config kubeadm-config.yaml 3.初始化命令说明： 指明用 Master 的哪个 interface 与 Cluster 的其他节点通信。如果 Master 有多个 interface，建议明确指定，如果不指定，kubeadm 会自动选择有默认网关的 interface。 --apiserver-advertise-address 指定 Pod 网络的范围。Kubernetes 支持多种网络方案，而且不同网络方案对 --pod-network-cidr 有自己的要求，这里设置为 10.244.0.0/16 是因为我们将使用 flannel 网络方案，必须设置成这个 CIDR。 --pod-network-cidr Kubenetes默认Registries地址是 k8s.gcr.io，在国内并不能访问 gcr.io，在1.19.3版本中我们可以增加–image-repository参数，默认值是 k8s.gcr.io，将其指定为阿里云镜像地址：registry.aliyuncs.com/google_containers。 --image-repository 关闭版本探测，因为它的默认值是stable-1，会导致从https://dl.k8s.io/release/stable-1.txt下载最新的版本号，我们可以将其指定为固定版本（最新版：v1.24.1）来跳过网络请求。 --kubernetes-version=v1.24.1 指定启动时使用cri-docker调用docker --cri-socket unix:///var/run/cri-docker.sock 4.错误启动重置 # 重置 如果有需要 kubeadm reset --cri-socket unix:///var/run/cri-docker.sock 5.初始化成功后，为顺利使用kubectl，执行以下命令： mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config 6.添加节点 kubeadm join 192.168.40.131:6443 --token eyr8v6.j84sxak8aptse8j9 --discovery-token-ca-cert-hash sha256:c082f3c546bdbac02d0d0a3b696de4004b0d449e37838fa38d4752b39682676b --cri-socket unix:///var/run/cri-docker.sock 7.执行kubectl get nodes，查看master节点状态： kubectl get node 8.通过如下命令查看kubelet状态： journalctl -xef -u kubelet -n 20 提示未安装cni 网络插件。 19.1安装flannel网络插件(CNI) master执行以下命令安装flannel即可： kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml kube-flannel.yaml文件中的net-conf.json-&gt;Network地址默认为命令中–pod-network-cidr=值相同 输入命令kubectl get pods -n kube-system,等待所有插件为running状态。 待所有pod status为Running的时候，再次执行kubectl get nodes： [root@k8s-master ~]# kubectl get node NAME STATUS ROLES AGE VERSION k8s-master Ready master 16m v1.19.3 如上所示，master状态变为，表明Master节点部署成功！ 19.2安装calico网络(功能更完善) 1.在master上下载配置calico网络的yaml。 kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml 2.提前下载所需要的镜像。 # 查看此文件用哪些镜像： [root@k8s-master ~]# grep image calico.yaml image: docker.io/calico/cni:v3.23.1 image: docker.io/calico/node:v3.23.1 image: docker.io/calico/kube-controllers:v3.23.1 3.安装calico网络。 在master上执行如下命令： kubectl apply -f calico.yaml 5.验证结果。 再次在master上运行命令 kubectl get nodes查看运行结果： [root@k8s-master ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION master01 Ready control-plane,master 21h v1.23.4 worker01 Ready &lt;none&gt; 16h v1.23.4 worker02 Ready &lt;none&gt; 16h v1.23.4 20.部署k8s-node1、k8s-node2集群 1、在k8s-node1、k8s-node2等两台虚拟机中重复执行上面的步骤，安装好docker、kubelet、kubectl、kubeadm。 1.node节点加入集群 在上面第初始化master节点成功后，输出了下面的kubeadm join命令： kubeadm join 192.168.40.131:6443 --token zj0u08.ge77y7uv76flqgdk --discovery-token-ca-cert-hash sha256:7cd23cec6afb192b2d34c5c719b378082a6315a9d91a22d91b83066c870d4db5 --cri-socket unix:///var/run/cri-docker.sock 该命令就是node加入集群的命令，分别在k8s-node1、k8s-node2上执行该命令加入集群。 如果忘记该命令，可以通过以下命令重新生成： kubeadm token create --print-join-command 2.在master节点执行下面命令查看集群状态： kubectl get nodes [root@k8s-master ~]# kubectl get node NAME STATUS ROLES AGE VERSION k8s-master Ready master 24m v1.19.3 k8s-node1 Ready &lt;none&gt; 5m50s v1.19.3 k8s-node2 Ready &lt;none&gt; 5m21s v1.19.3 如上所示，所有节点都为ready，集群搭建成功。 21.安装ingress-nginx vi ingress-nginx-deploy.yaml apiVersion: v1 kind: Namespace metadata: labels: app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx name: ingress-nginx --- apiVersion: v1 automountServiceAccountToken: true kind: ServiceAccount metadata: labels: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.2.0 name: ingress-nginx namespace: ingress-nginx --- apiVersion: v1 kind: ServiceAccount metadata: labels: app.kubernetes.io/component: admission-webhook app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.2.0 name: ingress-nginx-admission namespace: ingress-nginx --- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: labels: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.2.0 name: ingress-nginx namespace: ingress-nginx rules: - apiGroups: - &quot;&quot; resources: - namespaces verbs: - get - apiGroups: - &quot;&quot; resources: - configmaps - pods - secrets - endpoints verbs: - get - list - watch - apiGroups: - &quot;&quot; resources: - services verbs: - get - list - watch - apiGroups: - networking.k8s.io resources: - ingresses verbs: - get - list - watch - apiGroups: - networking.k8s.io resources: - ingresses/status verbs: - update - apiGroups: - networking.k8s.io resources: - ingressclasses verbs: - get - list - watch - apiGroups: - &quot;&quot; resourceNames: - ingress-controller-leader resources: - configmaps verbs: - get - update - apiGroups: - &quot;&quot; resources: - configmaps verbs: - create - apiGroups: - &quot;&quot; resources: - events verbs: - create - patch --- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: labels: app.kubernetes.io/component: admission-webhook app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.2.0 name: ingress-nginx-admission namespace: ingress-nginx rules: - apiGroups: - &quot;&quot; resources: - secrets verbs: - get - create --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: labels: app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.2.0 name: ingress-nginx rules: - apiGroups: - &quot;&quot; resources: - configmaps - endpoints - nodes - pods - secrets - namespaces verbs: - list - watch - apiGroups: - &quot;&quot; resources: - nodes verbs: - get - apiGroups: - &quot;&quot; resources: - services verbs: - get - list - watch - apiGroups: - networking.k8s.io resources: - ingresses verbs: - get - list - watch - apiGroups: - &quot;&quot; resources: - events verbs: - create - patch - apiGroups: - networking.k8s.io resources: - ingresses/status verbs: - update - apiGroups: - networking.k8s.io resources: - ingressclasses verbs: - get - list - watch --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: labels: app.kubernetes.io/component: admission-webhook app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.2.0 name: ingress-nginx-admission rules: - apiGroups: - admissionregistration.k8s.io resources: - validatingwebhookconfigurations verbs: - get - update --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: labels: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.2.0 name: ingress-nginx namespace: ingress-nginx roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: ingress-nginx subjects: - kind: ServiceAccount name: ingress-nginx namespace: ingress-nginx --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: labels: app.kubernetes.io/component: admission-webhook app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.2.0 name: ingress-nginx-admission namespace: ingress-nginx roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: ingress-nginx-admission subjects: - kind: ServiceAccount name: ingress-nginx-admission namespace: ingress-nginx --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: labels: app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.2.0 name: ingress-nginx roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: ingress-nginx subjects: - kind: ServiceAccount name: ingress-nginx namespace: ingress-nginx --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: labels: app.kubernetes.io/component: admission-webhook app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.2.0 name: ingress-nginx-admission roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: ingress-nginx-admission subjects: - kind: ServiceAccount name: ingress-nginx-admission namespace: ingress-nginx --- apiVersion: v1 data: allow-snippet-annotations: &quot;true&quot; kind: ConfigMap metadata: labels: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.2.0 name: ingress-nginx-controller namespace: ingress-nginx --- apiVersion: v1 kind: Service metadata: labels: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.2.0 name: ingress-nginx-controller namespace: ingress-nginx spec: ports: - appProtocol: http name: http port: 80 protocol: TCP targetPort: http - appProtocol: https name: https port: 443 protocol: TCP targetPort: https selector: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx type: NodePort --- apiVersion: v1 kind: Service metadata: labels: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.2.0 name: ingress-nginx-controller-admission namespace: ingress-nginx spec: ports: - appProtocol: https name: https-webhook port: 443 targetPort: webhook selector: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx type: ClusterIP --- apiVersion: apps/v1 kind: Deployment metadata: labels: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.2.0 name: ingress-nginx-controller namespace: ingress-nginx spec: minReadySeconds: 0 revisionHistoryLimit: 10 selector: matchLabels: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx template: metadata: labels: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx spec: containers: - args: - /nginx-ingress-controller - --election-id=ingress-controller-leader - --controller-class=k8s.io/ingress-nginx - --ingress-class=nginx - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller - --validating-webhook=:8443 - --validating-webhook-certificate=/usr/local/certificates/cert - --validating-webhook-key=/usr/local/certificates/key env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: LD_PRELOAD value: /usr/local/lib/libmimalloc.so image: registry.aliyuncs.com/google_containers/nginx-ingress-controller:v1.2.0 imagePullPolicy: IfNotPresent lifecycle: preStop: exec: command: - /wait-shutdown livenessProbe: failureThreshold: 5 httpGet: path: /healthz port: 10254 scheme: HTTP initialDelaySeconds: 10 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 name: controller ports: - containerPort: 80 name: http protocol: TCP - containerPort: 443 name: https protocol: TCP - containerPort: 8443 name: webhook protocol: TCP readinessProbe: failureThreshold: 3 httpGet: path: /healthz port: 10254 scheme: HTTP initialDelaySeconds: 10 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 resources: requests: cpu: 100m memory: 90Mi securityContext: allowPrivilegeEscalation: true capabilities: add: - NET_BIND_SERVICE drop: - ALL runAsUser: 101 volumeMounts: - mountPath: /usr/local/certificates/ name: webhook-cert readOnly: true dnsPolicy: ClusterFirst nodeSelector: kubernetes.io/os: linux serviceAccountName: ingress-nginx terminationGracePeriodSeconds: 300 volumes: - name: webhook-cert secret: secretName: ingress-nginx-admission --- apiVersion: batch/v1 kind: Job metadata: labels: app.kubernetes.io/component: admission-webhook app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.2.0 name: ingress-nginx-admission-create namespace: ingress-nginx spec: template: metadata: labels: app.kubernetes.io/component: admission-webhook app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.2.0 name: ingress-nginx-admission-create spec: containers: - args: - create - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc - --namespace=$(POD_NAMESPACE) - --secret-name=ingress-nginx-admission env: - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace image: registry.aliyuncs.com/google_containers/kube-webhook-certgen:v1.1.1 imagePullPolicy: IfNotPresent name: create securityContext: allowPrivilegeEscalation: false nodeSelector: kubernetes.io/os: linux restartPolicy: OnFailure securityContext: fsGroup: 2000 runAsNonRoot: true runAsUser: 2000 serviceAccountName: ingress-nginx-admission --- apiVersion: batch/v1 kind: Job metadata: labels: app.kubernetes.io/component: admission-webhook app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.2.0 name: ingress-nginx-admission-patch namespace: ingress-nginx spec: template: metadata: labels: app.kubernetes.io/component: admission-webhook app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.2.0 name: ingress-nginx-admission-patch spec: containers: - args: - patch - --webhook-name=ingress-nginx-admission - --namespace=$(POD_NAMESPACE) - --patch-mutating=false - --secret-name=ingress-nginx-admission - --patch-failure-policy=Fail env: - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace image: registry.aliyuncs.com/google_containers/kube-webhook-certgen:v1.1.1 imagePullPolicy: IfNotPresent name: patch securityContext: allowPrivilegeEscalation: false nodeSelector: kubernetes.io/os: linux restartPolicy: OnFailure securityContext: fsGroup: 2000 runAsNonRoot: true runAsUser: 2000 serviceAccountName: ingress-nginx-admission --- apiVersion: networking.k8s.io/v1 kind: IngressClass metadata: labels: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.2.0 name: nginx spec: controller: k8s.io/ingress-nginx --- apiVersion: admissionregistration.k8s.io/v1 kind: ValidatingWebhookConfiguration metadata: labels: app.kubernetes.io/component: admission-webhook app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.2.0 name: ingress-nginx-admission webhooks: - admissionReviewVersions: - v1 clientConfig: service: name: ingress-nginx-controller-admission namespace: ingress-nginx path: /networking/v1/ingresses failurePolicy: Fail matchPolicy: Equivalent name: validate.nginx.ingress.kubernetes.io rules: - apiGroups: - networking.k8s.io apiVersions: - v1 operations: - CREATE - UPDATE resources: - ingresses sideEffects: None kubectl create -f ingress-nginx-deploy.yaml 卸载集群命令 #建议所有服务器都执行 #!/bin/bash kubeadm reset -f modprobe -r ipip lsmod rm -rf ~/.kube/ rm -rf /etc/kubernetes/ rm -rf /etc/systemd/system/kubelet.service.d rm -rf /etc/systemd/system/kubelet.service rm -rf /usr/bin/kube* rm -rf /etc/cni rm -rf /opt/cni rm -rf /var/lib/etcd rm -rf /var/etcd yum -y remove kubeadm* kubectl* kubelet* docker* reboot ","link":"https://tianxiawuhao.github.io/oqAZb-4wU/"},{"title":"centos7安装Docker详细步骤","content":"一、安装前必读 在安装 Docker 之前，先说一下配置，我这里是Centos7 Linux 内核：官方建议 3.10 以上，3.8以上貌似也可。 注意：本文的命令使用的是 root 用户登录执行，不是 root 的话所有命令前面要加 sudo 1.查看当前的内核版本 uname -r 2.使用 root 权限更新 yum 包（生产环境中此步操作需慎重，看自己情况，学习的话随便搞） yum -y update 这个命令不是必须执行的，看个人情况，后面出现不兼容的情况的话就必须update了 注意 yum -y update：升级所有包同时也升级软件和系统内核； yum -y upgrade：只升级所有包，不升级软件和系统内核 3.卸载旧版本（如果之前安装过的话） yum remove docker docker-common docker-selinux docker-engine 二、安装Docker的详细步骤 1.安装需要的软件包， yum-util 提供yum-config-manager功能，另两个是devicemapper驱动依赖 yum install -y yum-utils device-mapper-persistent-data lvm2 2.设置 yum 源 设置一个yum源，下面两个都可用 yum-config-manager --add-repo http://download.docker.com/linux/centos/docker-ce.repo（中央仓库） yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo（阿里仓库） 3.选择docker版本并安装 （1）查看可用版本有哪些 yum list docker-ce --showduplicates | sort -r （2）选择一个版本并安装：yum install docker-ce-版本号 yum -y install docker-ce-18.03.1.ce 4.启动 Docker 并设置开机自启 systemctl start docker systemctl enable docker ","link":"https://tianxiawuhao.github.io/4kG3rvwit/"},{"title":"HTTP3","content":"HTTP3协议发布 自2017年起HTTP3协议已发布了29个Draft，推出在即，Chrome、Nginx等软件都在跟进实现最新的草案。本文将介绍HTTP3协议规范、应用场景及实现原理。 2015年HTTP2协议正式推出后，已经有接近一半的互联网站点在使用它： HTTP2协议虽然大幅提升了HTTP/1.1的性能，然而，基于TCP实现的HTTP2遗留下3个问题： 有序字节流引出的队头阻塞（Head-of-line blocking），使得HTTP2的多路复用能力大打折扣； TCP与TLS叠加了握手时延，建链时长还有1倍的下降空间； 基于TCP四元组确定一个连接，这种诞生于有线网络的设计，并不适合移动状态下的无线网络，这意味着IP地址的频繁变动会导致TCP连接、TLS会话反复握手，成本高昂。 HTTP3协议解决了这些问题： HTTP3基于UDP协议重新定义了连接，在QUIC层实现了无序、并发字节流的传输，解决了队头阻塞问题（包括基于QPACK解决了动态表的队头阻塞）； HTTP3重新定义了TLS协议加密QUIC头部的方式，既提高了网络攻击成本，又降低了建立连接的速度（仅需1个RTT就可以同时完成建链与密钥协商）； HTTP3 将Packet、QUIC Frame、HTTP3 Frame分离，实现了连接迁移功能，降低了5G环境下高速移动设备的连接维护成本。 本文将会从HTTP3协议的概念讲起，从连接迁移的实现上学习HTTP3的报文格式，再围绕着队头阻塞问题来分析多路复用与QPACK动态表的实现。虽然正式的RFC规范还未推出，但最近的草案Change只有微小的变化，所以现在学习HTTP3正当其时，这将是下一代互联网最重要的基础设施。 HTTP3协议到底是什么？ 就像HTTP2协议一样，HTTP3并没有改变HTTP1的语义。那什么是HTTP语义呢？在我看来，它包括以下3个点： 请求只能由客户端发起，而服务器针对每个请求返回一个响应； 请求与响应都由Header、Body（可选）组成，其中请求必须含有URL和方法，而响应必须含有响应码； Header中各Name对应的含义保持不变。 HTTP3在保持HTTP1语义不变的情况下，更改了编码格式，这由2个原因所致：首先，是为了减少编码长度。下图中HTTP1协议的编码使用了ASCII码，用空格、冒号以及\\r\\n作为分隔符，编码效率很低： HTTP2与HTTP3采用二进制、静态表、动态表与Huffman算法对HTTP Header编码，不只提供了高压缩率，还加快了发送端编码、接收端解码的速度。 其次，由于HTTP1协议不支持多路复用，这样高并发只能通过多开一些TCP连接实现。然而，通过TCP实现高并发有3个弊端： 实现成本高。TCP是由操作系统内核实现的，如果通过多线程实现并发，并发线程数不能太多，否则线程间切换成本会以指数级上升；如果通过异步、非阻塞socket实现并发，开发效率又太低； 每个TCP连接与TLS会话都叠加了2-3个RTT的建链成本； TCP连接有一个防止出现拥塞的慢启动流程，它会对每个TCP连接都产生减速效果。 因此，HTTP2与HTTP3都在应用层实现了多路复用功能： HTTP2协议基于TCP有序字节流实现，因此应用层的多路复用并不能做到无序地并发，在丢包场景下会出现队头阻塞问题。如下面的动态图片所示，服务器返回的绿色响应由5个TCP报文组成，而黄色响应由4个TCP报文组成，当第2个黄色报文丢失后，即使客户端接收到完整的5个绿色报文，但TCP层不会允许应用进程的read函数读取到最后5个报文，并发成了一纸空谈： 当网络繁忙时，丢包概率会很高，多路复用受到了很大限制。因此，HTTP3采用UDP作为传输层协议，重新实现了无序连接，并在此基础上通过有序的QUIC Stream提供了多路复用，如下图所示： 最早这一实验性协议由Google推出，并命名为gQUIC，因此，IETF草案中仍然保留了QUIC概念，用来描述HTTP3协议的传输层和表示层。HTTP3协议规范由以下5个部分组成： QUIC层由https://tools.ietf.org/html/draft-ietf-quic-transport-29描述，它定义了连接、报文的可靠传输、有序字节流的实现； TLS协议会将QUIC层的部分报文头部暴露在明文中，方便代理服务器进行路由。https://tools.ietf.org/html/draft-ietf-quic-tls-29规范定义了QUIC与TLS的结合方式； 丢包检测、RTO重传定时器预估等功能由https://tools.ietf.org/html/draft-ietf-quic-recovery-29定义，目前拥塞控制使用了类似TCP New RENO的算法，未来有可能更换为基于带宽检测的算法（例如BBR）； 基于以上3个规范，https://tools.ietf.org/html/draft-ietf-quic-http-29定义了HTTP语义的实现，包括服务器推送、请求响应的传输等； 在HTTP2中，由HPACK规范定义HTTP头部的压缩算法。由于HPACK动态表的更新具有时序性，无法满足HTTP3的要求。在HTTP3中，QPACK定义HTTP头部的编码：https://tools.ietf.org/html/draft-ietf-quic-qpack-16。注意，以上规范的最新草案都到了29，而QPACK相对简单，它目前更新到16。 自1991年诞生的HTTP/0.9协议已不再使用，但1996推出的HTTP/1.0、1999年推出的HTTP/1.1、2015年推出的HTTP2协议仍然共存于互联网中（HTTP/1.0在企业内网中还在广为使用，例如Nginx与上游的默认协议还是1.0版本），即将面世的HTTP3协议的加入，将会进一步增加协议适配的复杂度。接下来，我们将深入HTTP3协议的细节。 连接迁移功能是怎样实现的？ 对于当下的HTTP1和HTTP2协议，传输请求前需要先完成耗时1个RTT的TCP三次握手、耗时1个RTT的TLS握手（TLS1.3），由于它们分属内核实现的传输层、openssl库实现的表示层，所以难以合并在一起，如下图所示： 在IoT时代，移动设备接入的网络会频繁变动，从而导致设备IP地址改变。对于通过四元组（源IP、源端口、目的IP、目的端口）定位连接的TCP协议来说，这意味着连接需要断开重连，所以上述2个RTT的建链时延、TCP慢启动都需要重新来过。而HTTP3的QUIC层实现了连接迁移功能，允许移动设备更换IP地址后，只要仍保有上下文信息（比如连接ID、TLS密钥等），就可以复用原连接。 在UDP报文头部与HTTP消息之间，共有3层头部，定义连接且实现了Connection Migration主要是在Packet Header中完成的，如下图所示： 这3层Header实现的功能各不相同： Packet Header实现了可靠的连接。当UDP报文丢失后，通过Packet Header中的Packet Number实现报文重传。连接也是通过其中的Connection ID字段定义的； QUIC Frame Header在无序的Packet报文中，基于QUIC Stream概念实现了有序的字节流，这允许HTTP消息可以像在TCP连接上一样传输； HTTP3 Frame Header定义了HTTP Header、Body的格式，以及服务器推送、QPACK编解码流等功能。 为了进一步提升网络传输效率，Packet Header又可以细分为两种： Long Packet Header用于首次建立连接； Short Packet Header用于日常传输数据。 其中，Long Packet Header的格式如下图所示： 建立连接时，连接是由服务器通过Source Connection ID字段分配的，这样，后续传输时，双方只需要固定住Destination Connection ID，就可以在客户端IP地址、端口变化后，绕过UDP四元组（与TCP四元组相同），实现连接迁移功能。下图是Short Packet Header头部的格式，这里就不再需要传输Source Connection ID字段了： 上图中的Packet Number是每个报文独一无二的序号，基于它可以实现丢失报文的精准重发。如果你通过抓包观察Packet Header，会发现Packet Number被TLS层加密保护了，这是为了防范各类网络攻击的一种设计。下图给出了Packet Header中被加密保护的字段： 其中，显示为E（Encrypt）的字段表示被TLS加密过。当然，Packet Header只是描述了最基本的连接信息，其上的Stream层、HTTP消息也是被加密保护的： 现在我们已经对HTTP3协议的格式有了基本的了解，接下来我们通过队头阻塞问题，看看Packet之上的QUIC Frame、HTTP3 Frame帧格式。 Stream多路复用时的队头阻塞是怎样解决的？ 其实，解决队头阻塞的方案，就是允许微观上有序发出的Packet报文，在接收端无序到达后也可以应用于并发请求中。比如上文的动态图中，如果丢失的黄色报文对其后发出的绿色报文不造成影响，队头阻塞问题自然就得到了解决： 在Packet Header之上的QUIC Frame Header，定义了有序字节流Stream，而且Stream之间可以实现真正的并发。HTTP3的Stream，借鉴了HTTP2中的部分概念，所以在讨论QUIC Frame Header格式之前，我们先来看看HTTP2中的Stream长成什么样子： 每个Stream就像HTTP1中的TCP连接，它保证了承载的HEADERS frame（存放HTTP Header）、DATA frame（存放HTTP Body）是有序到达的，多个Stream之间可以并行传输。在HTTP3中，上图中的HTTP2 frame会被拆解为两层，我们先来看底层的QUIC Frame。 一个Packet报文中可以存放多个QUIC Frame，当然所有Frame的长度之和不能大于PMTUD（Path Maximum Transmission Unit Discovery，这是大于1200字节的值），你可以把它与IP路由中的MTU概念对照理解： 每一个Frame都有明确的类型： 前4个字节的Frame Type字段描述的类型不同，接下来的编码也不相同，下表是各类Frame的16进制Type值： 在上表中，我们只要分析0x08-0x0f这8种STREAM类型的Frame，就能弄明白Stream流的实现原理，自然也就清楚队头阻塞是怎样解决的了。Stream Frame用于传递HTTP消息，它的格式如下所示： 可见，Stream Frame头部的3个字段，完成了多路复用、有序字节流以及报文段层面的二进制分隔功能，包括： Stream ID标识了一个有序字节流。当HTTP Body非常大，需要跨越多个Packet时，只要在每个Stream Frame中含有同样的Stream ID，就可以传输任意长度的消息。多个并发传输的HTTP消息，通过不同的Stream ID加以区别； 消息序列化后的“有序”特性，是通过Offset字段完成的，它类似于TCP协议中的Sequence序号，用于实现Stream内多个Frame间的累计确认功能； Length指明了Frame数据的长度。 你可能会奇怪，为什么会有8种Stream Frame呢？这是因为0x08-0x0f 这8种类型其实是由3个二进制位组成，它们实现了以下3 标志位的组合： 第1位表示是否含有Offset，当它为0时，表示这是Stream中的起始Frame，这也是上图中Offset是可选字段的原因； 第2位表示是否含有Length字段； 第3位Fin，表示这是Stream中最后1个Frame，与HTTP2协议Frame帧中的FIN标志位相同。 Stream数据中并不会直接存放HTTP消息，因为HTTP3还需要实现服务器推送、权重优先级设定、流量控制等功能，所以Stream Data中首先存放了HTTP3 Frame： 其中，Length指明了HTTP消息的长度，而Type字段（请注意，低2位有特殊用途，在QPACK章节中会详细介绍）包含了以下类型： 0x00：DATA帧，用于传输HTTP Body包体； 0x01：HEADERS帧，通过QPACK 编码，传输HTTP Header头部； 0x03：CANCEL_PUSH控制帧，用于取消1次服务器推送消息，通常客户端在收到PUSH_PROMISE帧后，通过它告知服务器不需要这次推送； 0x04：SETTINGS控制帧，设置各类通讯参数； 0x05：PUSH_PROMISE帧，用于服务器推送HTTP Body前，先将HTTP Header头部发给客户端，流程与HTTP2相似； 0x07：GOAWAY控制帧，用于关闭连接（注意，不是关闭Stream）； 0x0d：MAX_PUSH_ID，客户端用来限制服务器推送消息数量的控制帧。 总结一下，QUIC Stream Frame定义了有序字节流，且多个Stream间的传输没有时序性要求，这样，HTTP消息基于QUIC Stream就实现了真正的多路复用，队头阻塞问题自然就被解决掉了。 QPACK编码是如何解决队头阻塞问题的？ 最后，我们再看下HTTP Header头部的编码方式，它需要面对另一种队头阻塞问题。 与HTTP2中的HPACK编码方式相似，HTTP3中的QPACK也采用了静态表、动态表及Huffman编码： 先来看静态表的变化。在上图中，GET方法映射为数字2，这是通过客户端、服务器协议实现层的硬编码完成的。在HTTP2中，共有61个静态表项： 而在QPACK中，则上升为98个静态表项，比如Nginx上的ngx_htt_v3_static_table数组所示： 你也可以从这里找到完整的HTTP3静态表。对于Huffman以及整数的编码，QPACK与HPACK并无多大不同，但动态表编解码方式差距很大。 所谓动态表，就是将未包含在静态表中的Header项，在其首次出现时加入动态表，这样后续传输时仅用1个数字表示，大大提升了编码效率。因此，动态表是天然具备时序性的，如果首次出现的请求出现了丢包，后续请求解码HPACK头部时，一定会被阻塞！ QPACK是如何解决队头阻塞问题的呢？事实上，QPACK将动态表的编码、解码独立在单向Stream中传输，仅当单向Stream中的动态表编码成功后，接收端才能解码双向Stream上HTTP消息里的动态表索引。 我们又引入了单向Stream和双向Stream概念，不要头疼，它其实很简单。单向指只有一端可以发送消息，双向则指两端都可以发送消息。还记得上一小节的QUIC Stream Frame头部吗？其中的Stream ID别有玄机，除了标识Stream外，它的低2位还可以表达以下组合： 因此，当Stream ID是0、4、8、12时，这就是客户端发起的双向Stream（HTTP3不支持服务器发起双向Stream），它用于传输HTTP请求与响应。单向Stream有很多用途，所以它在数据前又多出一个Stream Type字段： Stream Type有以下取值： 0x00：控制Stream，传递各类Stream控制消息； 0x01：服务器推送消息； 0x02：用于编码QPACK动态表，比如面对不属于静态表的HTTP请求头部，客户端可以通过这个Stream发送动态表编码； 0x03：用于通知编码端QPACK动态表的更新结果。 由于HTTP3的STREAM之间是乱序传输的，因此，若先发送的编码Stream后到达，双向Stream中的QPACK头部就无法解码，此时传输HTTP消息的双向Stream就会进入Block阻塞状态（两端可以通过控制帧定义阻塞Stream的处理方式）。 小结 最后对本文内容做个小结。 基于四元组定义连接并不适用于下一代IoT网络，HTTP3创造出Connection ID概念实现了连接迁移，通过融合传输层、表示层，既缩短了握手时长，也加密了传输层中的绝大部分字段，提升了网络安全性。 HTTP3在Packet层保障了连接的可靠性，在QUIC Frame层实现了有序字节流，在HTTP3 Frame层实现了HTTP语义，这彻底解开了队头阻塞问题，真正实现了应用层的多路复用。 QPACK使用独立的单向Stream分别传输动态表编码、解码信息，这样乱序、并发传输HTTP消息的Stream既不会出现队头阻塞，也能基于时序性大幅压缩HTTP Header的体积。 ","link":"https://tianxiawuhao.github.io/Um_5N4eGG/"},{"title":"binlog、redolog、undolog","content":"日志是 mysql 数据库的重要组成部分，记录着数据库运行期间各种状态信息。mysql日志主要包括错误日志、查询日志、慢查询日志、事务日志、二进制日志几大类。 作为开发，我们重点需要关注的是二进制日志( binlog )和事务日志(包括redo log 和 undo log )，本文接下来会详细介绍这三种日志。 binlog binlog 用于记录数据库执行的写入性操作(不包括查询)信息，以二进制的形式保存在磁盘中。binlog 是 mysql的逻辑日志，并且由 Server 层进行记录，使用任何存储引擎的 mysql 数据库都会记录 binlog 日志。 逻辑日志：可以简单理解为记录的就是sql语句 。 物理日志：mysql 数据最终是保存在数据页中的，物理日志记录的就是数据页变更 。 binlog 是通过追加的方式进行写入的，可以通过max_binlog_size 参数设置每个 binlog文件的大小，当文件大小达到给定值之后，会生成新的文件来保存日志。 binlog使用场景 在实际应用中， binlog 的主要使用场景有两个，分别是 主从复制 和 数据恢复 。 主从复制 ：在 Master 端开启 binlog ，然后将 binlog发送到各个 Slave 端， Slave 端重放 binlog 从而达到主从数据一致。 数据恢复 ：通过使用 mysqlbinlog 工具来恢复数据。 binlog刷盘时机 对于 InnoDB 存储引擎而言，只有在事务提交时才会记录binlog ，此时记录还在内存中，那么 binlog是什么时候刷到磁盘中的呢？ mysql 通过 sync_binlog 参数控制 binlog 的刷盘时机，取值范围是 0-N： 0：不去强制要求，由系统自行判断何时写入磁盘； 1：每次 commit 的时候都要将 binlog 写入磁盘； N：每N个事务，才会将 binlog 写入磁盘。 从上面可以看出， sync_binlog 最安全的是设置是 1 ，这也是MySQL 5.7.7之后版本的默认值。但是设置一个大一些的值可以提升数据库性能，因此实际情况下也可以将值适当调大，牺牲一定的一致性来获取更好的性能。 binlog日志格式 binlog 日志有三种格式，分别为 STATMENT 、 ROW 和 MIXED。 在 MySQL 5.7.7 之前，默认的格式是 STATEMENT ， MySQL 5.7.7 之后，默认值是 ROW。日志格式通过 binlog-format 指定。 STATMENT：基于SQL 语句的复制( statement-based replication, SBR )，每一条会修改数据的sql语句会记录到binlog 中 。 优点：不需要记录每一行的变化，减少了 binlog 日志量，节约了 IO , 从而提高了性能； 缺点：在某些情况下会导致主从数据不一致，比如执行sysdate() 、 slepp() 等 。 ROW：基于行的复制(row-based replication, RBR )，不记录每条sql语句的上下文信息，仅需记录哪条数据被修改了 。 优点：不会出现某些特定情况下的存储过程、或function、或trigger的调用和触发无法被正确复制的问题 ； 缺点：会产生大量的日志，尤其是alter table 的时候会让日志暴涨 MIXED：基于STATMENT 和 ROW 两种模式的混合复制(mixed-based replication, MBR )，一般的复制使用STATEMENT 模式保存 binlog ，对于 STATEMENT 模式无法复制的操作使用 ROW 模式保存 binlog redo log 我们都知道，事务的四大特性里面有一个是 持久性 ，具体来说就是只要事务提交成功，那么对数据库做的修改就被永久保存下来了，不可能因为任何原因再回到原来的状态 。 那么 mysql是如何保证一致性的呢？ 最简单的做法是在每次事务提交的时候，将该事务涉及修改的数据页全部刷新到磁盘中。但是这么做会有严重的性能问题，主要体现在两个方面： 因为 Innodb 是以 页 为单位进行磁盘交互的，而一个事务很可能只修改一个数据页里面的几个字节，这个时候将完整的数据页刷到磁盘的话，太浪费资源了！ 一个事务可能涉及修改多个数据页，并且这些数据页在物理上并不连续，使用随机IO写入性能太差！ 因此 mysql 设计了 redo log ， 具体来说就是只记录事务对数据页做了哪些修改，这样就能完美地解决性能问题了(相对而言文件更小并且是顺序IO)。 redo log基本概念 redo log 包括两部分：一个是内存中的日志缓冲( redo log buffer )，另一个是磁盘上的日志文件( redo logfile)。 mysql 每执行一条 DML 语句，先将记录写入 redo log buffer，后续某个时间点再一次性将多个操作记录写到 redo log file。这种 先写日志，再写磁盘 的技术就是 MySQL里经常说到的 WAL(Write-Ahead Logging) 技术。 在计算机操作系统中，用户空间( user space )下的缓冲区数据一般情况下是无法直接写入磁盘的，中间必须经过操作系统内核空间( kernel space )缓冲区( OS Buffer )。 因此， redo log buffer 写入 redo logfile 实际上是先写入 OS Buffer ，然后再通过系统调用 fsync() 将其刷到 redo log file 中，过程如下： mysql 支持三种将 redo log buffer 写入 redo log file 的时机，可以通过 innodb_flush_log_at_trx_commit 参数配置，各参数值含义如下： redo log记录形式 前面说过， redo log 实际上记录数据页的变更，而这种变更记录是没必要全部保存，因此 redo log实现上采用了大小固定，循环写入的方式，当写到结尾时，会回到开头循环写日志。如下图： 同时我们很容易得知， 在innodb中，既有redo log 需要刷盘，还有 数据页 也需要刷盘， redo log存在的意义主要就是降低对 数据页 刷盘的要求 。 在上图中， write pos 表示 redo log 当前记录的 LSN (逻辑序列号)位置， check point 表示 数据页更改记录 刷盘后对应 redo log 所处的 LSN(逻辑序列号)位置。 write pos 到 check point 之间的部分是 redo log 空着的部分，用于记录新的记录；check point 到 write pos 之间是 redo log 待落盘的数据页更改记录。当 write pos追上check point 时，会先推动 check point 向前移动，空出位置再记录新的日志。 启动 innodb 的时候，不管上次是正常关闭还是异常关闭，总是会进行恢复操作。因为 redo log记录的是数据页的物理变化，因此恢复的时候速度比逻辑日志(如 binlog )要快很多。 重启innodb 时，首先会检查磁盘中数据页的 LSN ，如果数据页的LSN 小于日志中的 LSN ，则会从 checkpoint 开始恢复。 还有一种情况，在宕机前正处于checkpoint 的刷盘过程，且数据页的刷盘进度超过了日志页的刷盘进度，此时会出现数据页中记录的 LSN 大于日志中的 LSN，这时超出日志进度的部分将不会重做，因为这本身就表示已经做过的事情，无需再重做。 redo log与binlog区别 由 binlog 和 redo log 的区别可知：binlog 日志只用于归档，只依靠 binlog 是没有 crash-safe 能力的。 但只有 redo log 也不行，因为 redo log 是 InnoDB特有的，且日志上的记录落盘后会被覆盖掉。因此需要 binlog和 redo log二者同时记录，才能保证当数据库发生宕机重启时，数据不会丢失。 undo log 数据库事务四大特性中有一个是 原子性 ，具体来说就是 原子性是指对数据库的一系列操作，要么全部成功，要么全部失败，不可能出现部分成功的情况。 实际上， 原子性 底层就是通过 undo log 实现的。undo log主要记录了数据的逻辑变化，比如一条 INSERT 语句，对应一条DELETE 的 undo log ，对于每个 UPDATE 语句，对应一条相反的 UPDATE 的 undo log ，这样在发生错误时，就能回滚到事务之前的数据状态。 同时， undo log 也是 MVCC(多版本并发控制)实现的关键。 ","link":"https://tianxiawuhao.github.io/HHdQUUQvV/"},{"title":"RocketMQ","content":"可用性评估 系统可用性(Availability)是信息工业界用来衡量一个信息系统提供持续服务的能力，它表示的是在给定时间区间内系统或者系统某一能力在特定环境中能够正常工作的概率。 简单地说， 可用性是平均故障间隔时间(MTBF)除以平均故障间隔时间(MTBF)和平均故障修复时间(MTTR)之和所得的结果， 即： 通常业界习惯用N个9来表征系统可用性，表示系统可以正常使用时间与总时间(1年)之比，比如： 99.9%代表3个9的可用性，意味着全年不可用时间在8.76小时以内，表示该系统在连续运行1年时间里最多可能的业务中断时间是8.76小时； 99.99%代表4个9的可用性，意味着全年不可用时间在52.6分钟以内,表示该系统在连续运行1年时间里最多可能的业务中断时间是52.6分钟； 99.999%代表5个9的可用性，意味着全年不可用时间必须保证在5.26分钟以内，缺少故障自动恢复机制的系统将很难达到5个9的高可用性。 那么X个9里的X只代表数字35，为什么没有12，也没有大于6的呢？ 我们接着往下计算： 1个9：(1-90%)*365=36.5天 *2个9：(1-99%)*365=3.65天 6个9：(1-99.9999%)*365*24*60*60=31秒 可以看到1个9和、2个9分别表示一年时间内业务可能中断的时间是36.5天、3.65天，这种级别的可靠性或许还不配使用“可靠性”这个词； 而6个9则表示一年内业务中断时间最多是31秒，那么这个级别的可靠性并非实现不了，而是要做到从“5个9” 到“6个9”的可靠性提升的话，后者需要付出比前者几倍的成本。 RocketMQ架构设计 在介绍RocketMQ高可用之前，首先了解一下RocketMQ架构设计 技术架构 部署架构 技术架构 RocketMQ架构上主要分为四部分，如图所示: Producer：消息发布的角色，支持分布式集群方式部署。Producer通过MQ的负载均衡模块选择相应的Broker集群队列进行消息投递，投递的过程支持快速失败并且低延迟。 Consumer：消息消费的角色，支持分布式集群方式部署。支持以push推，pull拉两种模式对消息进行消费。同时也支持集群方式和广播方式的消费，它提供实时消息订阅机制，可以满足大多数用户的需求。 NameServer：NameServer是一个非常简单的Topic路由注册中心，其角色类似Dubbo中的zookeeper，支持Broker的动态注册与发现。主要包括两个功能：Broker管理，NameServer接受Broker集群的注册信息并且保存下来作为路由信息的基本数据。然后提供心跳检测机制，检查Broker是否还存活；路由信息管理，每个NameServer将保存关于Broker集群的整个路由信息和用于客户端查询的队列信息。然后Producer和Conumser通过NameServer就可以知道整个Broker集群的路由信息，从而进行消息的投递和消费。NameServer通常也是集群的方式部署，各实例间相互不进行信息通讯。Broker是向每一台NameServer注册自己的路由信息，所以每一个NameServer实例上面都保存一份完整的路由信息。当某个NameServer因某种原因下线了，Broker仍然可以向其它NameServer同步其路由信息，Producer,Consumer仍然可以动态感知Broker的路由的信息。 BrokerServer：Broker主要负责消息的存储、投递和查询以及服务高可用保证，为了实现这些功能，Broker包含了以下几个重要子模块。 Remoting Module：整个Broker的实体，负责处理来自clients端的请求。 Client Manager：负责管理客户端(Producer/Consumer)和维护Consumer的Topic订阅信息 Store Service：提供方便简单的API接口处理消息存储到物理硬盘和查询功能。 HA Service：高可用服务，提供Master Broker 和 Slave Broker之间的数据同步功能。 Index Service：根据特定的Message key对投递到Broker的消息进行索引服务，以提供消息的快速查询。 部署架构 RocketMQ的Broker有三种集群部署方式： 1.单台Master部署； 2.多台Master部署； 3.多Master多Slave部署； 基础的rocket高可用，主要采用第3种部署方式 下图是第3种部署方式的简单图： 第3种部署方式网络部署特点 NameServer是一个几乎无状态节点，可集群部署，节点之间无任何信息同步。 Broker部署相对复杂，Broker分为Master与Slave，一个Master可以对应多个Slave，但是一个Slave只能对应一个Master，Master与Slave 的对应关系通过指定相同的BrokerName，不同的BrokerId 来定义，BrokerId为0表示Master，非0表示Slave。Master也可以部署多个。每个Broker与NameServer集群中的所有节点建立长连接，定时注册Topic信息到所有NameServer。 注意：当前RocketMQ版本在部署架构上支持一Master多Slave，但只有BrokerId=1的从服务器才会参与消息的读负载。 Producer与NameServer集群中的其中一个节点（随机选择）建立长连接，定期从NameServer获取Topic路由信息，并向提供Topic 服务的Master建立长连接，且定时向Master发送心跳。Producer完全无状态，可集群部署。 Consumer与NameServer集群中的其中一个节点（随机选择）建立长连接，定期从NameServer获取Topic路由信息，并向提供Topic服务的Master、Slave建立长连接，且定时向Master、Slave发送心跳。Consumer既可以从Master订阅消息，也可以从Slave订阅消息，消费者在向Master拉取消息时，Master服务器会根据拉取偏移量与最大偏移量的距离（判断是否读老消息，产生读I/O），以及从服务器是否可读等因素建议下一次是从Master还是Slave拉取。 结合部署架构图，描述集群工作流程： 启动NameServer，NameServer起来后监听端口，等待Broker、Producer、Consumer连上来，相当于一个路由控制中心。 Broker启动，跟所有的NameServer保持长连接，定时发送心跳包。心跳包中包含当前Broker信息(IP+端口等)以及存储所有Topic信息。注册成功后，NameServer集群中就有Topic跟Broker的映射关系。 收发消息前，先创建Topic，创建Topic时需要指定该Topic要存储在哪些Broker上，也可以在发送消息时自动创建Topic。 Producer发送消息，启动时先跟NameServer集群中的其中一台建立长连接，并从NameServer中获取当前发送的Topic存在哪些Broker上，轮询从队列列表中选择一个队列，然后与队列所在的Broker建立长连接从而向Broker发消息。 Consumer跟Producer类似，跟其中一台NameServer建立长连接，获取当前订阅Topic存在哪些Broker上，然后直接跟Broker建立连接通道，开始消费消息。 汇总：RocketMQ 集群部署模式 前面介绍到，RocketMQ的Broker有三种集群部署方式： 1.单台Master部署； 2.多台Master部署； 3.多Master多Slave部署； 第三种模式，根据Master和Slave之节的数据同步方式可以分为： 多 master 多 slave 异步复制模式 多 master 多 slave 同步复制模式 同步方式：同步复制和异步复制（指的一组 master 和 slave 之间数据的同步） 所以，总体来说，RocketMQ 集群部署模式为四种： 1.单 master 模式 也就是只有一个 master 节点，如果master节点挂掉了，会导致整个服务不可用，线上不宜使用，适合个人学习使用。 2.多 master 模式 多个 master 节点组成集群，单个 master 节点宕机或者重启对应用没有影响。 优点：所有模式中性能最高 缺点：单个 master 节点宕机期间，未被消费的消息在节点恢复之前不可用，消息的实时性就受到影响。 注意：使用同步刷盘可以保证消息不丢失，同时 Topic 相对应的 queue 应该分布在集群中各个 master 节点，而不是只在某各 master 节点上，否则，该节点宕机会对订阅该 topic 的应用造成影响。 3.多 master 多 slave 异步复制模式 在多 master 模式的基础上，每个 master 节点都有至少一个对应的 slave。 master 节点可读可写，但是 slave 只能读不能写，类似于 mysql 的主备模式。 优点： 在 master 宕机时，消费者可以从 slave 读取消息，消息的实时性不会受影响，性能几乎和多 master 一样。 缺点：使用异步复制的同步方式有可能会有消息丢失的问题。 4.多 master 多 slave 同步双写模式 同多 master 多 slave 异步复制模式类似，区别在于 master 和 slave 之间的数据同步方式。 优点：同步双写的同步模式能保证数据不丢失。 缺点：发送单个消息 RT 会略长，性能相比异步复制低10%左右。 刷盘策略：同步刷盘和异步刷盘（指的是节点自身数据是同步还是异步存储） 注意：要保证数据可靠，需采用同步刷盘和同步双写的方式，但性能会较其他方式低。 RocketMQ与ZooKeeper的爱恨纠葛 说到高性能消息中间件，第一个想到的肯定是LinkedIn开源的Kafka，虽然最初Kafka是为日志传输而生，但也非常适合互联网公司消息服务的应用场景，他们不要求数据实时的强一致性（事务），更多是希望达到数据的最终一致性。 RocketMQ是MetaQ的3.0版本，而MetaQ最初的设计又参考了Kafka。最初的MetaQ 1.x版本由阿里的原作者庄晓丹开发，后面的MetaQ 2.x版本才进行了开源。 MetaQ 1.x和MetaQ 2.x是依赖ZooKeeper的，但RocketMQ（即MetaQ 3.x）却去掉了ZooKeeper依赖，转而采用自己的NameServer。 ZooKeeper是著名的分布式协作框架，提供了Master选举、分布式锁、数据的发布和订阅等诸多功能。为什么RocketMQ没有选择ZooKeeper，而是自己开发了NameServer，我们来具体看看NameServer在RocketMQ集群中的作用就明了了。 RocketMQ的Broker有三种集群部署方式 RocketMQ的Broker有三种集群部署方式： 1.单台Master部署； 2.多台Master部署； 3.多Master多Slave部署； 采用第3种部署方式时，Master和Slave可以采用同步复制和异步复制两种方式。 下图是第3种部署方式的简单图： 当采用多Master方式时，Master与Master之间是不需要知道彼此的，这样的设计直接降低了Broker实现的复杂性。 你可以试想，如果Master与Master之间需要知道彼此的存在，这会需要在Master之中维护一个网络的Master列表，而且必然设计到Master发现和活跃Master数量变更等诸多状态更新问题，所以最简单也最可靠的做法就是Master只做好自己的事情（比如和Slave进行数据同步）即可。 这样，在分布式环境中，某台Master宕机或上线，不会对其他Master造成任何影响。 那么怎么才能知道网络中有多少台Master和Slave呢？ 你会很自然想到用ZooKeeper，每个活跃的Master或Slave都去约定的ZooKeeper节点下注册一个状态节点，但RocketMQ没有使用ZooKeeper，所以这件事就交给了NameServer来做了（看上图）。 NameServer的功能 功能一：NameServer用来保存活跃的broker列表，包括Master和Slave。 功能二：NameServer用来保存所有topic和该topic所有队列的列表。 功能三：NameServer用来保存所有broker的Filter列表。 功能四：NameServer可以理解承担了注册中心的职能 NameServer注册中心职能 NameServer是一个非常简单的路由注册中心，其角色类似Dubbo中的zookeeper，支持Broker的动态注册与发现。 Broker管理，NameServer接受Broker集群的注册信息并且保存下来作为路由信息的基本数据。然后提供心跳检测机制，检查Broker是否还存活； 路由信息管理，每个NameServer将保存关于Broker集群的整个路由信息和用于客户端查询的队列信息。然后Producer和Conumser通过NameServer就可以知道整个Broker集群的路由信息，从而进行消息的投递和消费 整个Rocketmq集群的工作原理如下图所示： 可以看到，Broker集群、Producer集群、Consumer集群都需要与NameServer集群进行通信： Broker集群: Broker用于接收生产者发送消息，或者消费者消费消息的请求。一个Broker集群由多组Master/Slave组成，Master可写可读，Slave只可以读，Master将写入的数据同步给Slave。 每个Broker节点，在启动时，都会遍历NameServer列表，与每个NameServer建立长连接，注册自己的信息，之后定时上报。 Producer集群: 消息的生产者，通过NameServer集群获得Topic的路由信息，包括Topic下面有哪些Queue，这些Queue分布在哪些Broker上等。Producer只会将消息发送到Master节点上，因此只需要与Master节点建立连接。 Consumer集群: 消息的消费者，通过NameServer集群获得Topic的路由信息，连接到对应的Broker上消费消息。注意，由于Master和Slave都可以读取消息，因此Consumer会与Master和Slave都建立连接。 总之： Name Server 是专为 RocketMQ 设计的轻量级注册中心，具有简单、可集群横吐扩展、无状态，节点之间互不通信等特点。 RocketMQ为什么不使用ZooKeeper 来看看RocketMQ为什么不使用ZooKeeper？ ZooKeeper可以提供Master选举功能。比如Kafka用来给每个分区选一个broker作为leader。 但对于RocketMQ来说，topic的数据在每个Master上是对等的，没有哪个Master上有topic上的全部数据，所以这里选举leader没有意义； RockeqMQ集群中，需要有构件来处理一些通用数据，比如broker列表，broker刷新时间。 虽然ZooKeeper也能存放数据，并有一致性保证。但处理数据之间的一些逻辑关系却比较麻烦，而且数据的逻辑解析操作得交给ZooKeeper客户端来做，如果有多种角色的客户端存在，自己解析多级数据确实是个麻烦事情； 既然RocketMQ集群中没有用到ZooKeeper的一些重量级的功能，只是使用ZooKeeper的数据一致性和发布订阅的话，与其依赖重量级的ZooKeeper，还不如写个轻量级的NameServer，NameServer也可以集群部署，NameServer与NameServer之间无任何信息同步，不需要保障数据一致性， 比zk简单太多。 NameServer特性 NameServer通常也是集群的方式部署，各实例间相互不进行信息通讯。Broker是向每一台NameServer注册自己的路由信息，所以每一个NameServer实例上面都保存一份完整的路由信息。当某个NameServer因某种原因下线了，Broker仍然可以向其它NameServer同步其路由信息，Producer，Consumer仍然可以动态感知Broker的路由的信息。 NameServer实例时间互不通信，这本身也是其设计亮点之一，即允许不同NameServer之间数据不同步(像Zookeeper那样保证各节点数据强一致性会带来额外的性能消耗) RocketMQ 的消息类型 RocketMQ 支持普通消息，顺序消息、事务消息，等等多种消息类型： 普通消息：没有特殊功能的消息。 分区顺序消息：以分区纬度保持顺序进行消费的消息。 全局顺序消息：全局顺序消息可以看作是只分一个区，始终在同一个分区上进行消费。 定时/延时消息：消息可以延迟一段特定时间进行消费。 事务消息：二阶段事务消息，先进行prepare投递消息，此时不能进行消息消费，当二阶段发出commit或者rollback的时候才会进行消息的消费或者回滚。 虽然配置种类比较繁多，但是使用的还是普通消息和分区顺序消息。 本文的主要介绍高可用，主要介绍普通消息，其他消息的高可用策略，也是类似侧。 RocketMQ高可用 NameServer 高可用 由于 NameServer 节点是无状态的，且各个节点直接的数据是一致的，故存在多个 NameServer 节点的情况下，部分 NameServer 不可用也可以保证 MQ 服务正常运行 BrokerServer 高可用 RocketMQ是通过 Master 和 Slave 的配合达到 BrokerServer 模块的高可用性的 一个 Master 可以配置多个 Slave，同时也支持配置多个 Master-Slave 组。 当其中一个 Master 出现问题时： 由于Slave只负责读，当 Master 不可用，它对应的 Slave 仍能保证消息被正常消费 由于配置多组 Master-Slave 组，其他的 Master-Slave 组也会保证消息的正常发送和消费 老版本的RocketMQ不支持把Slave自动转成Master，如果机器资源不足， 需要把Slave转成Master，则要手动停止Slave角色的Broker，更改配置文 件，用新的配置文件启动Broker。 新版本的RocketMQ，支持Slave自动转成Master。 consumer高可用 Consumer 的高可用是依赖于 Master-Slave 配置的，由于 Master 能够支持读写消息，Slave 支持读消息，当 Master 不可用或繁忙时， Consumer 会被自动切换到从 Slave 读取(自动切换，无需配置)。 故当 Master 的机器故障后，消息仍可从 Slave 中被消费 producer高可用 在创建Topic的时候，把Topic的多个Message Queue创建在多个Broker组上（相同Broker名称，不同 brokerId的机器组成一个Broker组）. 这样当一个Broker组的Master不可用后，其他组的Master仍然可用，Producer仍然可以发送消息。 实现分布式集群多副本的三种方式 M/S模式 即Master/Slaver模式。 该模式在过去使用的最多，RocketMq之前也是使用这样的主从模式来实现的。 主从模式分为同步模式和异步模式，区别是在同步模式下只有主从复制完毕才会返回给客户端；而在异步模式中，主从的复制是异步的，不用等待即可返回。 同步模式 同步模式特点 异步模式 异步模式特点 基于zookeeper服务 和M/S模式相比zookeeper模式是自动选举的主节点，新版本rocketMq暂时不支持zookeeper。 基于raft 相比zookeeper，raft自身就可以实现选举，raft通过投票的方式实现自身选举leader。去除额外依赖。目前RocketMq 4.5.0已经支持 可用性与可靠性 可用性 由于消息分布在各个broker上，一旦某个broker宕机，则该broker上的消息读写都会受到影响。所以rocketmq提供了master/slave的结构，salve定时从master同步数据，如果master宕机，则slave提供消费服务，但是不能写入消息，此过程对应用透明，由rocketmq内部解决。 这里有两个关键点： 一旦某个broker master宕机，生产者和消费者多久才能发现？受限于rocketmq的网络连接机制，默认情况下，最多需要30秒，但这个时间可由应用设定参数来缩短时间。这个时间段内，发往该broker的消息都是失败的，而且该broker的消息无法消费，因为此时消费者不知道该broker已经挂掉。 消费者得到master宕机通知后，转向slave消费，但是slave不能保证master的消息100%都同步过来了，因此会有少量的消息丢失。但是消息最终不会丢的，一旦master恢复，未同步过去的消息会被消费掉。 可靠性 所有发往broker的消息，有同步刷盘和异步刷盘机制，总的来说，可靠性非常高 同步刷盘时，消息写入物理文件才会返回成功，因此非常可靠 异步刷盘时，只有机器宕机，才会产生消息丢失，broker挂掉可能会发生，但是机器宕机崩溃是很少发生的，除非突然断电 Broker消息的零丢失方案 同步刷盘、异步刷盘 RocketMQ的消息是存储到磁盘上的，这样既能保证断电后恢复，又可以让存储的消息量超出内存的限制。RocketMQ为了提高性能，会尽可能地保证磁盘的顺序写。 消息在通过Producer写入RocketMQ的时候，有两种写磁盘方式： 异步刷盘方式： 在返回写成功状态时，消息可能只是被写入了内存的PAGECACHE，写操作的返回快，吞吐量大；当内存里的消息量积累到一定程度时，统一触发写磁盘操作，快速写入 优点：性能高 缺点：Master宕机，磁盘损坏的情况下，会丢失少量的消息, 导致MQ的消息状态和生产者/消费者的消息状态不一致 同步刷盘方式： 在返回应用写成功状态前，消息已经被写入磁盘。 具体流程是，消息写入内存的PAGECACHE后，立刻通知刷盘线程刷盘，然后等待刷盘完成，刷盘线程执行完成后唤醒等待的线程，给应用返回消息写成功的状态。 优点：可以保持MQ的消息状态和生产者/消费者的消息状态一致 缺点：性能比异步的低 同步刷盘还是异步刷盘，是通过Broker配置文件里的flushDiskType参数设置的，这个参数被设置成SYNC_FLUSH, ASYNC_FLUSH中的一个。 同步复制、异步复制 如果一个broker组有Master和Slave，消息需要从Master复制到Slave上，有同步和异步两种复制方式。 同步复制方式： 等Master和Slave均写成功后才反馈给客户端写成功状态 优点：如果Master出故障，Slave上有全部的备份数据，容易恢复，消费者仍可以从Slave消费, 消息不丢失 缺点：增大数据写入延迟，降低系统吞吐量，性能比异步复制模式略低，大约低10%左右，发送单个Master的响应时间会略高 异步复制方式： 只要Master写成功即可反馈给客户端写成功状态 优点：系统拥有较低的延迟和较高的吞吐量. Master宕机之后，消费者仍可以从Slave消费，此过程对应用透明，不需要人工干预，性能同多个Master模式几乎一样 缺点：如果Master出了故障，有些数据因为没有被写入Slave，而丢失少量消息。 若一个 Broker 组有一个 Master 和 Slave，消息需要从 Master 复制到 Slave 上，有同步复制和异步复制两种方式 同步复制 异步复制 概念 即等 Master 和 Slave 均写成功后才反馈给客户端写成功状态 只要 Master 写成功，就反馈客户端写成功状态 可靠性 可靠性高，若 Master 出现故障，Slave 上有全部的备份数据，容易恢复 若 Master 出现故障，可能存在一些数据还没来得及写入 Slave，可能会丢失 效率 由于是同步复制，会增加数据写入延迟，降低系统吞吐量 由于只要写入 Master 即可，故数据写入延迟较低，吞吐量较高 同步复制和异步复制是通过Broker配置文件里的brokerRole参数进行设置的，这个参数可以被设置成ASYNC_MASTER、SYNC_MASTER、SLAVE三个值中的一个。 三个值的说明： sync_master是同步方式，Master角色Broker中的消息要立刻同步过去。 async_master是异步方式，Master角色Broker中的消息通过异步处理的方式同步到Slave角色的机器上。 SLAVE 表明当前是从节点，无需配置 brokerRole 消息零丢失方案 消息零丢失是一把双刃剑，要想用好，还是要视具体的业务场景，在性能和消息零丢失上做平衡。 实际应用中的推荐把Master和Slave设置成ASYNC_FLUSH的异步刷盘方式，主从之间配置成SYNC_MASTER的同步复制方式，这样即使有一台机器出故障，仍然可以保证数据不丢。 刷盘方式 Master和Slave都设置成ASYNC_FLUSH的异步刷盘 复制方式 Master配置成SYNC_MASTER 同步复制 异步刷盘能够避免频繁触发磁盘写操作，除非服务器宕机，否则不会造成消息丢失。 主从同步复制能够保证消息不丢失，即使 Master 节点异常，也能保证 Slave 节点存储所有消息并被正常消费掉。 producer高可用 producer具备发送到全部master的能力，如果有多个master，消息会发送到所有的master 另外，在topic的不同的queue之间，producer还具备负载均衡能力。 在实例发送消息时，默认会轮询所有订阅了改 Topic 的 broker 节点上的 message queue，让消息平均落在不同的 queue 上，而由于这些 queue 散落在不同的 broker 节点中，即使某个 broker 节点异常，其他存在订阅了这个 Topic 的 message queue 的 broker 依然能消费消息 消息者业务代码出现异常怎么办？ 再来看一下消费者的代码中监听器的部分，它说如果消息处理成功，那么就返回消息状态为 CONSUME_SUCCESS，也有可能发放优惠券、积分等操作出现了异常，比如说数据库挂掉了。这个时候应该怎么处理呢？ consumer.registerMessageListener(new MessageListenerConcurrently() { @Override public ConsumeConcurrentlyStatus consumeMessage(List &lt;MessageExt&gt; list, ConsumeConcurrentlyContext consumeConcurrentlyContext) { // 对消息的处理，比如发放优惠券、积分等 return ConsumeConcurrentlyStatus.CONSUME_SUCCESS; } }); 我们可以把代码改一改，捕获异常之后返回消息的状态为 RECONSUME_LATER 表示稍后重试。 // 这次回调接口，接收消息 consumer.registerMessageListener(new MessageListenerConcurrently() { @Override public ConsumeConcurrentlyStatus consumeMessage(List &lt;MessageExt&gt; list, ConsumeConcurrentlyContext consumeConcurrentlyContext) { try { // 对消息的处理，比如发放优惠券、积分等 return ConsumeConcurrentlyStatus.CONSUME_SUCCESS; } catch (Exception e) { // 万一发生数据库宕机等异常，返回稍后重试消息的状态 return ConsumeConcurrentlyStatus.RECONSUME_LATER; } } }); 这个时候，消息会进入到 RocketMQ 的重试队列中。 重试队列 比如说消费者所属的消息组名称为AAAConsumerGroup 其重试队列名称就叫做**%RETRY%AAAConsumerGroup** 重试队列中的消息过一段时间会再次发送给消费者，如果还是无法正常执行会再次进入重试队列 默认重试16次，还是无法执行，消息就会从重试队列进入到死信队列 死信队列 重试队列中的消息重试16次任然无法执行，将会进入到死信队列 死信队列的名字是 %DLQ%AAAConsumerGroup 死信队列中的消息可以后台开一个线程，订阅**%DLQ%AAAConsumerGroup**，并不停重试 Customer 负载均衡 集群模式 在集群消费模式下，存在多个消费者同时消费消息，同一条消息只会被某一个消费者获取。即消息只需要被投递到订阅了这个 Topic 的消费者Group下的一个实例中即可。 消费者采用主动拉去的方式拉去并消费，在拉取的时候需要明确指定拉取那一条消息队列中的消息。 每当有实例变更，都会触发一次所有消费者实例的负载均衡，这是会按照queue的数量和实例的数量平均分配 queue 给每个消费者实例。 注意： 1）在集群模式下，一个 queue 只允许分配给一个消费者实例，这是由于若多个实例同时消费一个 queue 的小，由于拉取操作是由 consumer 主动发生的，可能导致同一个消息在不同的 consumer 实例中被消费。故算法保证了一个 queue 只会被一个 consumer 实例消费，但一个 consumer 实例能够消费多个 queue 2）控制 consumer 数量，应小于 queue 数量。这是由于一个 queue 只允许分配给一个 consumer 实例，若 consumer 实例数量多于 queue，则多出的 consumer 实例无法分配到 queue消费，会浪费系统资源 广播模式 广播模式其实不是负载均衡，由于每个消费者都能够拿到所有消息，故不能达到负载均衡的要求 消费者的消息重试 顺序消息重试 对于顺序消息，为了保证消息消费的顺序性，当consumer消费失败后，消息队列会自动不断进行消息重试(每次间隔时间为1s)， 这时会导致consumer消费被阻塞的情况，故必须保证应用能够及时监控并处理消费失败的情况，避免阻塞现象的发生 无序消息重试 概述 无序消息即普通、定时、延时、事务消息，当consumer消费消息失败时，可以通过设置返回状态实现消息重试 注意：无序消息的重试只针对集群消费方式（非广播方式）生效 广播方式不提供失败重试特性，即消费失败后，失败的消息不再重试，而是继续消费新消息 重试次数 消息队列 RocketMQ 默认允许每条消息最多重试 16 次，每次重试的间隔时间如下： 第几次重试 与上次重试的间隔时间 第几次重试 与上次重试的间隔时间 1 10 秒 9 7 分钟 2 30 秒 10 8 分钟 3 1 分钟 11 9 分钟 4 2 分钟 12 10 分钟 5 3 分钟 13 20 分钟 6 4 分钟 14 30 分钟 7 5 分钟 15 1 小时 8 6 分钟 16 2 小时 如果消息重试 16 次后仍然失败，消息将不再投递。 如果严格按照上述重试时间间隔计算，某条消息在一直消费失败的前提下，将会在接下来的 4 小时 46 分钟之内进行 16 次重试，超过这个时间范围消息将不再重试投递。 注意： 一条消息无论重试多少次，这些重试消息的 Message ID 不会改变。 消息重试相关的处理方式 消费失败后，需要重试的处理方式 集群消费方式（非广播方式）下，消息消费失败后期望消息重试，需要在消息监听器接口的实现中明确进行配置（三种方式任选一种）： 方式 1：返回 Action.ReconsumeLater（推荐） 方式 2：返回 Null 方式 3：抛出异常 示例代码 public class MessageListenerImpl implements MessageListener { @Override public Action consume(Message message, ConsumeContext context) { //消息处理逻辑抛出异常，消息将重试 doConsumeMessage(message); //方式 1：返回 Action.ReconsumeLater，消息将重试 return Action.ReconsumeLater; //方式 2：返回 null，消息将重试 return null; //方式 3：直接抛出异常，消息将重试 throw new RuntimeException(&quot;Consumer Message exception&quot;); } } 集群消费方式下，消息消费失败后期望消息重试，需要在消息监听器接口的实现中明确进行配置 消费失败后，无需重试的处理方式 集群消费方式下，消息失败后期望消息不重试，需要捕获消费逻辑中可能抛出的异常，最终返回 Action.CommitMessage，此后这条消息将不会再重试。 public class MessageListenerImpl implements MessageListener { @Override public Action consume(Message message, ConsumeContext context) { try { doConsumeMessage(message); } catch (Throwable e) { //捕获消费逻辑中的所有异常，并返回 Action.CommitMessage; return Action.CommitMessage; } //消息处理正常，直接返回 Action.CommitMessage; return Action.CommitMessage; } } 3）自定义消息最大重试次数 消息队列 RocketMQ 允许 Consumer 启动的时候设置最大重试次数，重试时间间隔将按照如下策略： 最大重试次数小于等于 16 次，则重试时间间隔同上表描述。 最大重试次数大于 16 次，超过 16 次的重试时间间隔均为每次 2 小时。 设置方式： consumer.setMaxReconsumeTimes(20); 或者： Properties properties = new Properties(); //配置对应 Group ID 的最大消息重试次数为 20 次，最大重试次数为字符串类型 properties.put(PropertyKeyConst.MaxReconsumeTimes,&quot;20&quot;); Consumer consumer =ONSFactory.createConsumer(properties); 注意： 消息最大重试次数设置，对相同 Group ID 下的所有 Consumer 实例有效。 如果只对相同 Group ID 下两个 Consumer 实例中的其中一个设置了 MaxReconsumeTimes，那么该配置对两个 Consumer 实例均生效。 配置采用覆盖的方式生效，即最后启动的 Consumer 实例会覆盖之前的启动实例的配置 获取消息重试次数 消费者收到消息后，可以获取到消息的重试次数 设置方式： public class MessageListenerImpl implements MessageListener { @Override public Action consume(Message message, ConsumeContext context) { //获取消息的重试次数 System.out.println(message.getReconsumeTimes()); return Action.CommitMessage; } } 死信队列 死信队列概念 在正常情况下无法被消费(超过最大重试次数)的消息称为死信消息(Dead-Letter Message)，存储死信消息的特殊队列就称为死信队列(Dead-Letter Queue) 当一条消息初次消费失败，消息队列 RocketMQ 会自动进行消息重试； 达到最大重试次数后，若消费依然失败，则表明消费者在正常情况下无法正确地消费该消息，此时，消息队列 RocketMQ 不会立刻将消息丢弃，而是将其发送到该消费者对应的死信队列中。 代码正常执行返回消息状态为CONSUME_SUCCESS，执行异常返回RECONSUME_LATER 状态为RECONSUME_LATER的消息会进入到重试队列，重试队列的名称为 %RETRY% + ConsumerGroupName； 重试16次消息任然没有处理成功，消息就会进入到死信队列%DLQ% + ConsumerGroupName; 死信特性 死信消息有以下特点： 不会再被消费者正常消费 有效期与正常消息相同，均为 3 天，3 天后会被自动删除。故死信消息应在产生的 3 天内及时处理 死信队列有以下特点： 一个死信队列对应一个消费者组，而不是对应单个消费者实例 一个死信队列包含了对应的 Group ID 所产生的所有死信消息，不论该消息属于哪个 Topic 若一个 Group ID 没有产生过死信消息，则 RocketMQ 不会为其创建相应的死信队列 查看死信信息和重发 在控制台查看死信队列的主题信息 重发消息 消息幂等性 消费幂等 消费幂等即无论消费者消费多少次，其结果都是一样的。 RocketMQ 是通过业务上的唯一 Key 来对消息做幂等处理 消费幂等的必要性 在网络环境中，由于网络不稳定等因素，消息队列的消息有可能出现重复，大概有以下几种： 发送时消息重复 当一条消息已被成功发送到服务端并完成持久化，此时出现了网络闪断或者客户端宕机，导致服务端对客户端应答失败。 如果此时生产者意识到消息发送失败并尝试再次发送消息，消费者后续会收到两条内容相同并且 Message ID 也相同的消息。 投递时消息重复 消息消费的场景下，消息已投递到消费者并完成业务处理，当客户端给服务端反馈应答的时候网络闪断。 为了保证消息至少被消费一次，消息队列 RocketMQ 的服务端将在网络恢复后再次尝试投递之前已被处理过的消息，消费者后续会收到两条内容相同并且 Message ID 也相同的消息。 负载均衡时消息重复（包括但不限于网络抖动、Broker 重启以及订阅方应用重启） 当消息队列 RocketMQ 的 Broker 或客户端重启、扩容或缩容时，会触发 Rebalance，此时消费者可能会收到重复消息。 结合三种情况，可以发现消息重发的最后结果都是，消费者接收到了重复消息，那么，我们只需要在消费者端统一进行幂等处理就能够实现消息幂等。 处理方式 消费端实现消息幂等性 RocketMQ 只能够保证消息丢失，但不能保证消息不重复投递，且由于高可用和高性能的考虑，应该在消费端实现消息幂等性。 那么 RocketMQ 是怎样解决消息重复的问题呢？还是“恰好”不解决。 造成消息重复的根本原因是：网络不可达。只要通过网络交换数据，就无法避免这个问题。所以解决这个问题的办法就是绕过这个问题。那么问题就变成了：如果消费端收到两条一样的消息，应该怎样处理？ 消费端处理消息的业务逻辑保持幂等性 保证每条消息都有唯一编号且保证消息处理成功与去重表的日志同时出现 第1条很好理解，只要保持幂等性，不管来多少条重复消息，最后处理的结果都一样。 第2条原理就是利用一张日志表来记录已经处理成功的消息的ID，如果新到的消息ID已经在日志表中，那么就不再处理这条消息。 第1条解决方案，很明显应该在消费端实现，不属于消息系统要实现的功能。第2条可以消息系统实现，也可以业务端实现。正常情况下出现重复消息的概率其实很小，如果由消息系统来实现的话，肯定会对消息系统的吞吐量和高可用有影响，所以最好还是由业务端自己处理消息重复的问题，这也是 RocketMQ 不解决消息重复的问题的原因。 RocketMQ 不保证消息不重复，如果你的业务需要保证严格的不重复消息，需要你自己在业务端去重。 在消费端通过业务逻辑实现幂等性操作，最常用的方式就是唯一ID的形式，若已经消费过的消息就不进行处理。例如在秒杀系统中使用订单ID作为关键ID，分布式系统中常用雪花算法生成ID。 注：如果需要彻底了解雪花算法，以及里边的位运算逻辑，请参见尼恩的秒杀视频。 在发送消息时，可以对 Message 设置标识唯一标识： Message message = new Message(); # 设置唯一标识，标识由雪花算法生成message.setKey(idWorker.nextId()); 订阅方收到消息时，可以获取到这个 Key consumer.registerMessageListener(new MessageListenerConcurrently() { @Override public ConsumeConcurrentlyStatus consumeMessage(List&lt;MessageExt&gt; msgs, ConsumeConcurrentlyContext context) { System.out.printf(&quot;%s Receive New Messages: %s %n&quot;, Thread.currentThread().getName(), msgs); for (MessageExt ext : msgs) { System.out.println(ext.getKeys()); } return ConsumeConcurrentlyStatus.CONSUME_SUCCESS; } }); 一、顺序消息 顺序消息（FIFO 消息）是消息队列 RocketMQ 提供的一种严格按照顺序来发布和消费的消息。顺序发布和顺序消费是指对于指定的一个 Topic，生 产者按照一定的先后顺序发布消息；消费者按照既定的先后顺序订阅消息，即先发布的消息一定会先被客户端接收到。 顺序消息分为全局顺序消息和分区顺序消息。 1.1、全局顺序消息 RocketMQ 在默认情况下不保证顺序，要保证全局顺序，需要把 Topic 的读写队列数设置为 1，然后生产者和消费者的并发设置也是 1。所以这样的话 高并发，高吞吐量的功能完全用不上。 1.1.1、适用场景 适用于性能要求不高，所有的消息严格按照 FIFO 原则来发布和消费的场景。 1.1.2、示例 要确保全局顺序消息，需要先把 Topic 的读写队列数设置为 1，然后生产者和消费者的并发设置也是 1。 mqadmin update Topic -t AllOrder -c DefaultCluster -r 1 -w 1 -n 127.0.0.1:9876 在证券处理中，以人民币兑换美元为 Topic，在价格相同的情况下，先出价者优先处理，则可以按照 FIFO 的方式发布和消费全局顺序消息。 1.2、部分顺序消息 对于指定的一个 Topic，所有消息根据 Sharding Key 进行区块分区。同一个分区内的消息按照严格的 FIFO 顺序进行发布和消费。Sharding Key 是顺 序消息中用来区分不同分区的关键字段，和普通消息的 Key 是完全不同的概念。 二、延时消息 2.1、概念介绍 延时消息：Producer 将消息发送到消息队列 RocketMQ 服务端，但并不期望这条消息立马投递，而是延迟一定时间后才投递到 Consumer 进行消费， 该消息即延时消息。 2.2、适用场景 消息生产和消费有时间窗口要求：比如在电商交易中超时未支付关闭订单的场景，在订单创建时会发送一条延时消息。这条消息将会在 30 分钟以 后投递给消费者，消费者收到此消息后需要判断对应的订单是否已完成支付。 如支付未完成，则关闭订单。如已完成支付则忽略。 2.3、使用方式 Apache RocketMQ 目前只支持固定精度的定时消息，因为如果要支持任意的时间精度，在 Broker 层面，必须要做消息排序，如果再涉及到持久化， 那么消息排序要不可避免的产生巨大性能开销。（阿里云 RocketMQ 提供了任意时刻的定时消息功能，Apache 的 RocketMQ 并没有,阿里并没有开源） 发送延时消息时需要设定一个延时时间长度，消息将从当前发送时间点开始延迟固定时间之后才开始投递。 延迟消息是根据延迟队列的 level 来的，延迟队列默认是 **msg.setDelayTimeLevel(5)**代表延迟一分钟 &quot;1s 5s 10s 30s 1m 2m 3m 4m 5m 6m 7m 8m 9m 10m 20m 30m 1h 2h&quot; 是这 18 个等级（秒（s）、分（m）、小时（h）），level 为 1，表示延迟 1 秒后消费，level 为 5 表示延迟 1 分钟后消费，level 为 18 表示延迟 2 个 小时消费。生产消息跟普通的生产消息类似，只需要在消息上设置延迟队列的 level 即可。消费消息跟普通的消费消息一致。 三、死信队列 3.1、概念介绍 死信队列用于处理无法被正常消费的消息。当一条消息初次消费失败，消息队列 MQ 会自动进行消息重试；达到最大重试次数后，若消费依然失败， 则表明Consumer 在正常情况下无法正确地消费该消息。此时，消息队列MQ不会立刻将消息丢弃，而是将这条消息发送到该 Consumer 对应的特殊队列中。 消息队列 MQ 将这种正常情况下无法被消费的消息称为死信消息（Dead-Letter Message），将存储死信消息的特殊队列称为死信队列 （Dead-Letter Queue)。 3.2适用场景 3.2.1、死信消息的特性 不会再被消费者正常消费。 有效期与正常消息相同，均为 3 天，3 天后会被自动删除。因此，请在死信消息产生后的 3 天内及时处理。 3.2.2、死信队列的特性 一个死信队列对应一个 Group ID， 而不是对应单个消费者实例。 如果一个 Group ID 未产生死信消息，消息队列 MQ 不会为其创建相应的死信队列。 一个死信队列包含了对应 Group ID 产生的所有死信消息，不论该消息属于哪个 Topic。 消息队列 MQ 控制台提供对死信消息的查询的功能。 一般控制台直接查看死信消息会报错。 进入RocketMQ中服务器对应的 RocketMQ 中的/bin 目录，执行以下脚本 sh mqadmin updateTopic -b 192.168.0.128:10911 -n 192.168.0.128:9876 -t %DLQ%group1 -p 6 四、消费幂等 为了防止消息重复消费导致业务处理异常，消息队列 MQ 的消费者在接收到消息后，有必要根据业务上的唯一 Key 对消息做幂等处理。本文介绍消息幂 等的概念、适用场景以及处理方法。 4.1、什么是消息幂等 当出现消费者对某条消息重复消费的情况时，重复消费的结果与消费一次的结果是相同的，并且多次消费并未对业务系统产生任何负面影响，那么 这整个过程就实现可消息幂等。 例如，在支付场景下，消费者消费扣款消息，对一笔订单执行扣款操作，扣款金额为 100 元。如果因网络不稳定等原因导致扣款消息重复投递，消 费者重复消费了该扣款消息，但最终的业务结果是只扣款一次，扣费 100 元，且用户的扣款记录中对应的订单只有一条扣款流水，不会多次扣除费用。 那么这次扣款操作是符合要求的，整个消费过程实现了消费幂等。 4.2、需要处理的场景 在互联网应用中，尤其在网络不稳定的情况下，消息队列 MQ 的消息有可能会出现重复。如果消息重复会影响您的业务处理，请对消息做幂等处理。 消息重复的场景如下： **1. 发送时消息重复 ** 当一条消息已被成功发送到服务端并完成持久化，此时出现了网络闪断或者客户端宕机，导致服务端对客户端应答失败。 如果此时生产者意识到消 息发送失败并尝试再次发送消息，消费者后续会收到两条内容相同并且 Message ID 也相同的消息。 **2. 投递时消息重复 ** 消息消费的场景下，消息已投递到消费者并完成业务处理，当客户端给服务端反馈应答的时候网络闪断。为了保证消息至少被消费一次，消息队列 MQ 的服务端将在网络恢复后再次尝试投递之前已被处理过的消息，消费者后续会收到两条内容相同并且 Message ID 也相同的消息。 3. 负载均衡时消息重复（包括但不限于网络抖动、Broker 重启以及消费者应用重启） 当消息队列 MQ 的 Broker 或客户端重启、扩容或缩容时，会触发 Rebalance，此时消费者可能会收到重复消息。 4.3、处理方法 因为 Message ID 有可能出现冲突（重复）的情况，所以真正安全的幂等处理，不建议以 Message ID 作为处理依据。最好的方式是以业务唯一标识 作为幂等处理的关键依据，而业务的唯一标识可以通过消息 Key 设置。 以支付场景为例，可以将消息的 Key 设置为订单号，作为幂等处理的依据。具体代码示例如下： Message message = new Message(); message.setKey(&quot;ORDERID_100&quot;); SendResult sendResult = producer.send(message); 消费者收到消息时可以根据消息的 Key，即订单号来实现消息幂等： consumer.subscribe(&quot;ons_test&quot;, &quot;*&quot;, new MessageListener() { public Action consume(Message message, ConsumeContext context) { String key = message.getKey() // 根据业务唯一标识的 Key 做幂等处理 } }); ","link":"https://tianxiawuhao.github.io/BvbuvYGnx/"},{"title":"CountDownLatch,CyclicBarrier,Semaphore的用法和区别","content":"CountDownLatch CountDownLatch（也叫闭锁）是一个同步协助类，允许一个或多个线程等待，直到其他线程完成操作集。 CountDownLatch 使用给定的计数值（count）初始化。await 方法会阻塞直到当前的计数值（count）由于 countDown 方法的调用达到 0，count 为 0 之后所有等待的线程都会被释放，并且随后对await方法的调用都会立即返回。 构造方法 //参数count为计数值 public CountDownLatch(int count) {}; 常用方法 // 调用 await() 方法的线程会被挂起，它会等待直到 count 值为 0 才继续执行 public void await() throws InterruptedException {}; // 和 await() 类似，若等待 timeout 时长后，count 值还是没有变为 0，不再等待，继续执行 public boolean await(long timeout, TimeUnit unit) throws InterruptedException {}; // 会将 count 减 1，直至为 0 public void countDown() {}; 使用案例 首先是创建实例 CountDownLatch countDown = new CountDownLatch(2)； 需要同步的线程执行完之后，计数 -1， countDown.countDown()； 需要等待其他线程执行完毕之后，再运行的线程，调用 countDown.await()实现阻塞同步。 应用场景 CountDownLatch 一般用作多线程倒计时计数器，强制它们等待其他一组（CountDownLatch的初始化决定）任务执行完成。 CountDownLatch的两种使用场景： 让多个线程等待，模拟并发。 让单个线程等待，多个线程（任务）完成后，进行汇总合并。 场景 1：模拟并发 import java.util.concurrent.CountDownLatch; /** * 让多个线程等待：模拟并发，让并发线程一起执行 */ public class CountDownLatchTest { public static void main(String[] args) throws InterruptedException { CountDownLatch countDownLatch = new CountDownLatch(1); for (int i = 0; i &lt; 5; i++) { new Thread(() -&gt; { try { // 等待 countDownLatch.await(); String parter = &quot;【&quot; + Thread.currentThread().getName() + &quot;】&quot;; System.out.println(parter + &quot;开始执行……&quot;); } catch (InterruptedException e) { e.printStackTrace(); } }).start(); } Thread.sleep(2000); countDownLatch.countDown(); } } 场景 2：多个线程完成后，进行汇总合并 很多时候，我们的并发任务，存在前后依赖关系；比如数据详情页需要同时调用多个接口获取数据，并发请求获取到数据后、需要进行结果合并；或者多个数据操作完成后，需要数据 check；这其实都是：在多个线程(任务)完成后，进行汇总合并的场景。 import java.util.Map; import java.util.concurrent.ConcurrentHashMap; import java.util.concurrent.CountDownLatch; /** * 让单个线程等待：多个线程(任务)完成后，进行汇总合并 */ public class CountDownLatchTest3 { //用于聚合所有的统计指标 private static Map map = new ConcurrentHashMap(); //创建计数器，这里需要统计4个指标 private static CountDownLatch countDownLatch = new CountDownLatch(4); public static void main(String[] args) throws Exception { //记录开始时间 long startTime = System.currentTimeMillis(); Thread countUserThread = new Thread(() -&gt; { try { System.out.println(&quot;正在统计新增用户数量&quot;); Thread.sleep(3000);//任务执行需要3秒 map.put(&quot;userNumber&quot;, 100);//保存结果值 System.out.println(&quot;统计新增用户数量完毕&quot;); countDownLatch.countDown();//标记已经完成一个任务 } catch (InterruptedException e) { e.printStackTrace(); } }); Thread countOrderThread = new Thread(() -&gt; { try { System.out.println(&quot;正在统计订单数量&quot;); Thread.sleep(3000);//任务执行需要3秒 map.put(&quot;countOrder&quot;, 20);//保存结果值 System.out.println(&quot;统计订单数量完毕&quot;); countDownLatch.countDown();//标记已经完成一个任务 } catch (InterruptedException e) { e.printStackTrace(); } }); Thread countGoodsThread = new Thread(() -&gt; { try { System.out.println(&quot;正在商品销量&quot;); Thread.sleep(3000);//任务执行需要3秒 map.put(&quot;countGoods&quot;, 300);//保存结果值 System.out.println(&quot;统计商品销量完毕&quot;); countDownLatch.countDown();//标记已经完成一个任务 } catch (InterruptedException e) { e.printStackTrace(); } }); Thread countmoneyThread = new Thread(() -&gt; { try { System.out.println(&quot;正在总销售额&quot;); Thread.sleep(3000);//任务执行需要3秒 map.put(&quot;countMoney&quot;, 40000);//保存结果值 System.out.println(&quot;统计销售额完毕&quot;); countDownLatch.countDown();//标记已经完成一个任务 } catch (InterruptedException e) { e.printStackTrace(); } }); //启动子线程执行任务 countUserThread.start(); countGoodsThread.start(); countOrderThread.start(); countmoneyThread.start(); try { //主线程等待所有统计指标执行完毕 countDownLatch.await(); long endTime = System.currentTimeMillis();//记录结束时间 System.out.println(&quot;------统计指标全部完成--------&quot;); System.out.println(&quot;统计结果为：&quot; + map); System.out.println(&quot;任务总执行时间为&quot; + (endTime - startTime) + &quot;ms&quot;); } catch (InterruptedException e) { e.printStackTrace(); } } } CylicBarrier 从字面上的意思可以知道，这个类的中文意思是“循环栅栏”。大概的意思就是一个可循环利用的屏障。 它的作用就是会让所有线程都等待完成后才会继续下一步行动。 现实生活中我们经常会遇到这样的情景，在进行某个活动前需要等待人全部都齐了才开始。例如吃饭时要等全家人都上座了才动筷子，旅游时要等全部人都到齐了才出发，比赛时要等运动员都上场后才开始。 在JUC包中为我们提供了一个同步工具类能够很好的模拟这类场景，它就是CyclicBarrier类。利用CyclicBarrier类可以实现一组线程相互等待，当所有线程都到达某个屏障点后再进行后续的操作。 CyclicBarrier字面意思是“可重复使用的栅栏”，CyclicBarrier相比 CountDownLatch 来说，要简单很多，其源码没有什么高深的地方，它是 ReentrantLock 和 Condition 的组合使用。 看如下示意图，CyclicBarrier 和 CountDownLatch 是不是很像，只是 CyclicBarrier 可以有不止一个栅栏，因为它的栅栏（Barrier）可以重复使用（Cyclic）。 就好比以前的那种客车一样，当第一轮车坐满之后发车，然后接着等第二辆车坐满之后在发车。 构造方法 // parties表示屏障拦截的线程数量，每个线程调用 await 方法告诉 CyclicBarrier 我已经到达了屏障，然后当前线程被阻塞。 public CyclicBarrier(int parties) // 用于在线程到达屏障时，优先执行 barrierAction，方便处理更复杂的业务场景(该线程的执行时机是在到达屏障之后再执行) public CyclicBarrier(int parties, Runnable barrierAction) 常用方法 //屏障 指定数量的线程全部调用await()方法时，这些线程不再阻塞 // BrokenBarrierException 表示栅栏已经被破坏，破坏的原因可能是其中一个线程 await() 时被中断或者超时 public int await() throws InterruptedException, BrokenBarrierException public int await(long timeout, TimeUnit unit) throws InterruptedException, B rokenBarrierException, TimeoutException //循环 通过reset()方法可以进行重置 public void reset() 使用案例 import java.util.concurrent.CyclicBarrier; /** * CyclicBarrier(回环栅栏)允许一组线程互相等待，直到到达某个公共屏障点 (Common Barrier Point) * CountDownLatch 用于等待countDown事件，而栅栏用于等待其他线程。 */ public class CyclicBarrierTest { public static void main(String[] args) { CyclicBarrier cyclicBarrier = new CyclicBarrier(3); for (int i = 0; i &lt; 5; i++) { new Thread(new Runnable() { @Override public void run() { try { System.out.println(Thread.currentThread().getName() + &quot;开始等待其他线程&quot;); cyclicBarrier.await(); System.out.println(Thread.currentThread().getName() + &quot;开始执行&quot;); //TODO 模拟业务处理 Thread.sleep(5000); System.out.println(Thread.currentThread().getName() + &quot;执行完毕&quot;); } catch (Exception e) { e.printStackTrace(); } } }).start(); } } } 应用场景 可以用于多线程计算数据，最后合并计算结果的场景。 import java.util.Set; import java.util.concurrent.*; public class CyclicBarrierTest2 { //保存每个学生的平均成绩 private ConcurrentHashMap&lt;String, Integer&gt; map = new ConcurrentHashMap&lt;String, Integer&gt;(); private ExecutorService threadPool = Executors.newFixedThreadPool(3); private CyclicBarrier cb = new CyclicBarrier(3, () -&gt; { int result = 0; Set&lt;String&gt; set = map.keySet(); for (String s : set) { result += map.get(s); } System.out.println(&quot;三人平均成绩为:&quot; + (result / 3) + &quot;分&quot;); }); public void count() { for (int i = 0; i &lt; 3; i++) { threadPool.execute(new Runnable() { @Override public void run() { //获取学生平均成绩 int score = (int) (Math.random() * 40 + 60); map.put(Thread.currentThread().getName(), score); System.out.println(Thread.currentThread().getName() + &quot;同学的平均成绩为：&quot; + score); try { //执行完运行await(),等待所有学生平均成绩都计算完毕 cb.await(); } catch (InterruptedException | BrokenBarrierException e) { e.printStackTrace(); } } }); } } public static void main(String[] args) { CyclicBarrierTest2 cb = new CyclicBarrierTest2(); cb.count(); } } 测试结果： Semaphore Semaphore，俗称信号量，基于 AbstractQueuedSynchronizer 实现。使用 Semaphore 可以控制同时访问资源的线程个数。 比如：停车场入口立着的那个显示屏，每有一辆车进入停车场显示屏就会显示剩余车位减 1，每有一辆车从停车场出去，显示屏上显示的剩余车辆就会加 1，当显示屏上的剩余车位为 0 时，停车场入口的栏杆就不会再打开，车辆就无法进入停车场了，直到有一辆车从停车场出去为止。 比如：在学生时代都去餐厅打过饭，假如有 3 个窗口可以打饭，同一时刻也只能有 3 名同学打饭。第 4 个人来了之后就必须在外面等着，只要有打饭的同学好了，就可以去相应的窗口了 。 构造方法 //创建具有给定的许可数和非公平的公平设置的 Semaphore。 Semaphore(int permits) //创建具有给定的许可数和给定的公平设置的 Semaphore。 Semaphore(int permits, boolean fair) permits 表示许可证的数量（资源数），就好比一个学生可以占用 3 个打饭窗口。 fair 表示公平性，如果这个设为 true 的话，下次执行的线程会是等待最久的线程。 常用方法 public void acquire() throws InterruptedException public boolean tryAcquire() public void release() public int availablePermits() public final int getQueueLength() public final boolean hasQueuedThreads() protected void reducePermits(int reduction) protected Collection&lt;Thread&gt; getQueuedThreads() acquire()：表示阻塞并获取许可。 tryAcquire()：方法在没有许可的情况下会立即返回 false，要获取许可的线程不会阻塞。 release()：表示释放许可。 int availablePermits()：返回此信号量中当前可用的许可证数。 int getQueueLength()：返回正在等待获取许可证的线程数。 boolean hasQueuedThreads()：是否有线程正在等待获取许可证。 void reducePermit(int reduction)：减少 reduction 个许可证。 Collection getQueuedThreads()：返回所有等待获取许可证的线程集合。 使用案例 我们可以模拟车站买票，假如车站有 3 个窗口售票，那么同一时刻每个窗口只能存在一个人买票，其他人则等待前面的人完成后才可以去买票。 import java.util.concurrent.Semaphore; public class SemaphoreTest { public static void main(String[] args) { // 3 个窗口 Semaphore windows = new Semaphore(3); // 模拟 5 个人购票 for (int i = 0; i &lt; 5; i++) { new Thread(new Runnable() { @Override public void run() { // 占用窗口，加锁 try { windows.acquire(); System.out.println(Thread.currentThread().getName() + &quot;：开始购票&quot;); // 买票 Thread.sleep(5000); System.out.println(Thread.currentThread().getName() + &quot;：购票成功&quot;); } catch (InterruptedException e) { e.printStackTrace(); } finally { // 释放许可，释放窗口 windows.release(); } } }, &quot;Thread&quot; + i).start(); } } } 测试结果如下： 很明显可以看到当前面 3 个线程购票成功之后，剩余的线程再开始购票。 应用场景 可以用于做流量控制，特别是公用资源有限的应用场景。 如我们实现一个同时只能处理 5 个请求的限流器。 import java.util.concurrent.LinkedBlockingDeque; import java.util.concurrent.Semaphore; import java.util.concurrent.ThreadPoolExecutor; import java.util.concurrent.TimeUnit; public class SemaphoneTest2 { /** * 实现一个同时只能处理5个请求的限流器 */ private static Semaphore semaphore = new Semaphore(5); /** * 定义一个线程池 * 0 */ private static ThreadPoolExecutor executor = new ThreadPoolExecutor(10, 50, 1 , TimeUnit.SECONDS, new LinkedBlockingDeque&lt;&gt;(200)); /** * 模拟执行方法 */ public static void exec() { try { semaphore.acquire(1); // 模拟真实方法执行 System.out.println(&quot;执行exec方法&quot;); Thread.sleep(2000); } catch (Exception e) { e.printStackTrace(); } finally { semaphore.release(1); } } public static void main(String[] args) throws InterruptedException { { for (;;) { Thread.sleep(100); // 模拟请求以10个/s的速度 executor.execute(() -&gt; exec()); } } } } 总结 1、CountDownLatch、CyclicBarrier、Semaphore的区别 CountDownLatch 和 CyclicBarrier 都能够实现线程之间的等待，只不过它们侧重点不同： CountDownLatch 一般用于某个线程 A 等待若干个其他线程执行完任务之后，它才执行； 而CyclicBarrier一般用于一组线程互相等待至某个状态，然后这一组线程再同时执行； 另外，CountDownLatch是不能够重用的，而 CyclicBarrier 是可以重用的（reset）。 Semaphore和锁有点类似，它一般用于控制对某组资源的访问权限。 2、CountDownLatch 与 Thread.join 的区别 CountDownLatch 的作用就是允许一个或多个线程等待其他线程完成操作，看起来有点类似 join() 方法，但其提供了比 join() 更加灵活的API。 CountDownLatch 可以手动控制在n个线程里调用 n 次 countDown() 方法使计数器进行减一操作，也可以在一个线程里调用 n 次执行减一操作。 而 join() 的实现原理是不停检查 join 线程是否存活，如果 join 线程存活则让当前线程永远等待。所以两者之间相对来说还是 CountDownLatch 使用起来较为灵活。 3、CyclicBarrier 与 CountDownLatch 区别 CountDownLatch的计数器只能使用一次，而CyclicBarrier的计数器可以使用reset()方法重置。所以CyclicBarrier能处理更为复杂的业务场景，比如如果计算发生错误，可以重置计数器，并让线程们重新执行一次。 CyclicBarrier还提供getNumberWaiting(可以获得CyclicBarrier阻塞的线程数量)、isBroken(用来知道阻塞的线程是否被中断)等方法。 CountDownLatch会阻塞主线程，CyclicBarrier不会阻塞主线程，只会阻塞子线程。 CountDownLatch和CyclicBarrier都能够实现线程之间的等待，只不过它们侧重点不同。CountDownLatch一般用于一个或多个线程，等待其他线程执行完任务后，再执行。CyclicBarrier一般用于一组线程互相等待至某个状态，然后这一组线程再同时执行。 CyclicBarrier 还可以提供一个 barrierAction，合并多线程计算结果。 CyclicBarrier是通过ReentrantLock的&quot;独占锁&quot;和Conditon来实现一组线程的阻塞唤醒的，而CountDownLatch则是通过AQS的“共享锁”实现。 ","link":"https://tianxiawuhao.github.io/bPYM5y-vS/"},{"title":"异步编程的 7 种实现方式","content":"早期的系统是同步的，容易理解，我们来看个例子 同步编程 当用户创建一笔电商交易订单时，要经历的业务逻辑流程还是很长的，每一步都要耗费一定的时间，那么整体的RT就会比较长。 于是，聪明的人们开始思考能不能将一些非核心业务从主流程中剥离出来，于是有了异步编程雏形。 异步编程是让程序并发运行的一种手段。它允许多个事件同时发生，当程序调用需要长时间运行的方法时，它不会阻塞当前的执行流程，程序可以继续运行。 核心思路：采用多线程优化性能，将串行操作变成并行操作。异步模式设计的程序可以显著减少线程等待，从而在高吞吐量场景中，极大提升系统的整体性能，显著降低时延。 接下来，我们来讲下异步有哪些编程实现方式 一、线程 Thread 直接继承 Thread类 是创建异步线程最简单的方式。 首先，创建Thread子类，普通类或匿名内部类方式；然后创建子类实例；最后通过start()方法启动线程。 public class AsyncThread extends Thread{ @Override public void run() { System.out.println(&quot;当前线程名称:&quot; + this.getName() + &quot;, 执行线程名称:&quot; + Thread.currentThread().getName() + &quot;-hello&quot;); } } public static void main(String[] args) { // 模拟业务流程 // ....... // 创建异步线程 AsyncThread asyncThread = new AsyncThread(); // 启动异步线程 asyncThread.start(); } 当然如果每次都创建一个 Thread线程，频繁的创建、销毁，浪费系统资源。我们可以采用线程池 @Bean(name = &quot;executorService&quot;) public ExecutorService downloadExecutorService() { return new ThreadPoolExecutor(20, 40, 60, TimeUnit.SECONDS, new ArrayBlockingQueue&lt;&gt;(2000), new ThreadFactoryBuilder().setNameFormat(&quot;defaultExecutorService-%d&quot;).build(), (r, executor) -&gt; log.error(&quot;defaultExecutor pool is full! &quot;)); } 将业务逻辑封装到 Runnable 或 Callable 中，交由 线程池 来执行 二、Future 上述方式虽然达到了多线程并行处理，但有些业务不仅仅要执行过程，还要获取执行结果。 Java 从1.5版本开始，提供了 Callable 和 Future，可以在任务执行完毕之后得到任务执行结果。 当然也提供了其他功能，如：取消任务、查询任务是否完成等 Future类位于java.util.concurrent包下，接口定义： public interface Future&lt;V&gt; { boolean cancel(boolean mayInterruptIfRunning); boolean isCancelled(); boolean isDone(); V get() throws InterruptedException, ExecutionException; V get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException; } 方法描述： cancel()：取消任务，如果取消任务成功返回true，如果取消任务失败则返回false isCancelled()：表示任务是否被取消成功，如果在任务正常完成前被取消成功，则返回 true isDone()：表示任务是否已经完成，如果完成，返回true get()：获取执行结果，这个方法会产生阻塞，会一直等到任务执行完毕才返回 get(long timeout, TimeUnit unit)：用来获取执行结果，如果在指定时间内，还没获取到结果，就直接返回null 代码示例： public class CallableAndFuture { public static ExecutorService executorService = new ThreadPoolExecutor(4, 40, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;(1024), new ThreadFactoryBuilder() .setNameFormat(&quot;demo-pool-%d&quot;).build(), new ThreadPoolExecutor.AbortPolicy()); static class MyCallable implements Callable&lt;String&gt; { @Override public String call() throws Exception { return &quot;异步处理，Callable 返回结果&quot;; } } public static void main(String[] args) { Future&lt;String&gt; future = executorService.submit(new MyCallable()); try { System.out.println(future.get()); } catch (Exception e) { // nodo } finally { executorService.shutdown(); } } } Future 表示一个可能还没有完成的异步任务的结果，通过 get 方法获取执行结果，该方法会阻塞直到任务返回结果。 三、FutureTask FutureTask 实现了 RunnableFuture 接口，则 RunnableFuture 接口继承了 Runnable 接口和 Future 接口，所以可以将 FutureTask 对象作为任务提交给 ThreadPoolExecutor 去执行，也可以直接被 Thread 执行；又因为实现了 Future 接口，所以也能用来获得任务的执行结果。 FutureTask 构造函数： public FutureTask(Callable&lt;V&gt; callable) public FutureTask(Runnable runnable, V result) FutureTask 常用来封装 Callable 和 Runnable，可以作为一个任务提交到线程池中执行。除了作为一个独立的类之外，也提供了一些功能性函数供我们创建自定义 task 类使用。 FutureTask 线程安全由CAS来保证。 ExecutorService executor = Executors.newCachedThreadPool(); // FutureTask包装callbale任务，再交给线程池执行 FutureTask&lt;Integer&gt; futureTask = new FutureTask&lt;&gt;(() -&gt; { System.out.println(&quot;子线程开始计算：&quot;); Integer sum = 0; for (int i = 1; i &lt;= 100; i++) sum += i; return sum; }); // 线程池执行任务， 运行结果在 futureTask 对象里面 executor.submit(futureTask); try { System.out.println(&quot;task运行结果计算的总和为：&quot; + futureTask.get()); } catch (Exception e) { e.printStackTrace(); } executor.shutdown(); Callable 和 Future 的区别：Callable 用于产生结果，Future 用于获取结果 如果是对多个任务多次自由串行、或并行组合，涉及多个线程之间同步阻塞获取结果，Future 代码实现会比较繁琐，需要我们手动处理各个交叉点，很容易出错。 四、异步框架 CompletableFuture Future 类通过 get() 方法阻塞等待获取异步执行的运行结果，性能比较差。 JDK1.8 中，Java 提供了 CompletableFuture 类，它是基于异步函数式编程。相对阻塞式等待返回结果，CompletableFuture 可以通过回调的方式来处理计算结果，实现了异步非阻塞，性能更优。 优点： 异步任务结束时，会自动回调某个对象的方法 异步任务出错时，会自动回调某个对象的方法 主线程设置好回调后，不再关心异步任务的执行 泡茶示例： (内容摘自：极客时间的《Java 并发编程实战》) //任务1：洗水壶-&gt;烧开水 CompletableFuture&lt;Void&gt; f1 = CompletableFuture.runAsync(() -&gt; { System.out.println(&quot;T1:洗水壶...&quot;); sleep(1, TimeUnit.SECONDS); System.out.println(&quot;T1:烧开水...&quot;); sleep(15, TimeUnit.SECONDS); }); //任务2：洗茶壶-&gt;洗茶杯-&gt;拿茶叶 CompletableFuture&lt;String&gt; f2 = CompletableFuture.supplyAsync(() -&gt; { System.out.println(&quot;T2:洗茶壶...&quot;); sleep(1, TimeUnit.SECONDS); System.out.println(&quot;T2:洗茶杯...&quot;); sleep(2, TimeUnit.SECONDS); System.out.println(&quot;T2:拿茶叶...&quot;); sleep(1, TimeUnit.SECONDS); return &quot;龙井&quot;; }); //任务3：任务1和任务2完成后执行：泡茶 CompletableFuture&lt;String&gt; f3 = f1.thenCombine(f2, (__, tf) -&gt; { System.out.println(&quot;T1:拿到茶叶:&quot; + tf); System.out.println(&quot;T1:泡茶...&quot;); return &quot;上茶:&quot; + tf; }); //等待任务3执行结果 System.out.println(f3.join()); } CompletableFuture 提供了非常丰富的API，大约有50种处理串行，并行，组合以及处理错误的方法。 五、 SpringBoot 注解 @Async 除了硬编码的异步编程处理方式，SpringBoot 框架还提供了 注解式 解决方案，以 方法体 为边界，方法体内部的代码逻辑全部按异步方式执行。 首先，使用 @EnableAsync 启用异步注解 @SpringBootApplication @EnableAsync public class StartApplication { public static void main(String[] args) { SpringApplication.run(StartApplication.class, args); } } 自定义线程池： @Configuration @Slf4j public class ThreadPoolConfiguration { @Bean(name = &quot;defaultThreadPoolExecutor&quot;, destroyMethod = &quot;shutdown&quot;) public ThreadPoolExecutor systemCheckPoolExecutorService() { return new ThreadPoolExecutor(3, 10, 60, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;Runnable&gt;(10000), new ThreadFactoryBuilder().setNameFormat(&quot;default-executor-%d&quot;).build(), (r, executor) -&gt; log.error(&quot;system pool is full! &quot;)); } } 在异步处理的方法上添加注解 @Async ，当对 execute 方法 调用时，通过自定义的线程池 defaultThreadPoolExecutor 异步化执行 execute 方法 @Service public class AsyncServiceImpl implements AsyncService { @Async(&quot;defaultThreadPoolExecutor&quot;) public Boolean execute(Integer num) { System.out.println(&quot;线程：&quot; + Thread.currentThread().getName() + &quot; , 任务：&quot; + num); return true; } } 用 @Async 注解标记的方法，称为异步方法。在spring boot应用中使用 @Async 很简单： 调用异步方法类上或者启动类加上注解 @EnableAsync 在需要被异步调用的方法外加上 @Async 所使用的 @Async 注解方法的类对象应该是Spring容器管理的bean对象； 六、Spring ApplicationEvent 事件 事件机制在一些大型项目中被经常使用，Spring 专门提供了一套事件机制的接口，满足了架构原则上的解耦。 ApplicationContext 通过 ApplicationEvent 类和 ApplicationListener 接口进行事件处理。如果将实现 ApplicationListener 接口的 bean 注入到上下文中，则每次使用 ApplicationContext 发布 ApplicationEvent 时，都会通知该 bean。本质上，这是标准的观察者设计模式。 ApplicationEvent 是由 Spring 提供的所有 Event 类的基类 首先，自定义业务事件子类，继承自 ApplicationEvent，通过泛型注入业务模型参数类。相当于 MQ 的消息体。 public class OrderEvent extends AbstractGenericEvent&lt;OrderModel&gt; { public OrderEvent(OrderModel source) { super(source); } } 然后，编写事件监听器。ApplicationListener 接口是由 Spring 提供的事件订阅者必须实现的接口，我们需要定义一个子类，继承 ApplicationListener。相当于 MQ 的消费端 @Component public class OrderEventListener implements ApplicationListener&lt;OrderEvent&gt; { @Override public void onApplicationEvent(OrderEvent event) { System.out.println(&quot;【OrderEventListener】监听器处理！&quot; + JSON.toJSONString(event.getSource())); } } 最后，发布事件，把某个事件告诉所有与这个事件相关的监听器。相当于 MQ 的生产端。 OrderModel orderModel = new OrderModel(); orderModel.setOrderId((long) i); orderModel.setBuyerName(&quot;Tom-&quot; + i); orderModel.setSellerName(&quot;judy-&quot; + i); orderModel.setAmount(100L); // 发布Spring事件通知 SpringUtils.getApplicationContext().publishEvent(new OrderEvent(orderModel)); 加个餐： [消费端]线程：http-nio-8090-exec-1，消费事件 {&quot;amount&quot;:100.0,&quot;buyerName&quot;:&quot;Tom-1&quot;,&quot;orderId&quot;:1,&quot;sellerName&quot;:&quot;judy-1&quot;} [生产端]线程：http-nio-8090-exec-1，发布事件 1 [消费端]线程：http-nio-8090-exec-1，消费事件 {&quot;amount&quot;:100.0,&quot;buyerName&quot;:&quot;Tom-2&quot;,&quot;orderId&quot;:2,&quot;sellerName&quot;:&quot;judy-2&quot;} [生产端]线程：http-nio-8090-exec-1，发布事件 2 [消费端]线程：http-nio-8090-exec-1，消费事件 {&quot;amount&quot;:100.0,&quot;buyerName&quot;:&quot;Tom-3&quot;,&quot;orderId&quot;:3,&quot;sellerName&quot;:&quot;judy-3&quot;} [生产端]线程：http-nio-8090-exec-1，发布事件 3 上面是跑了个demo的运行结果，我们发现无论生产端还是消费端，使用了同一个线程 http-nio-8090-exec-1，Spring 框架的事件机制默认是同步阻塞的。只是在代码规范方面做了解耦，有较好的扩展性，但底层还是采用同步调用方式。 那么问题来了，如果想实现异步调用，如何处理？ 我们需要手动创建一个 SimpleApplicationEventMulticaster，并设置 TaskExecutor，此时所有的消费事件采用异步线程执行。 @Component public class SpringConfiguration { @Bean public SimpleApplicationEventMulticaster applicationEventMulticaster(@Qualifier(&quot;defaultThreadPoolExecutor&quot;) ThreadPoolExecutor defaultThreadPoolExecutor) { SimpleApplicationEventMulticaster simpleApplicationEventMulticaster = new SimpleApplicationEventMulticaster(); simpleApplicationEventMulticaster.setTaskExecutor(defaultThreadPoolExecutor); return simpleApplicationEventMulticaster; } } 我们看下改造后的运行结果： [生产端]线程：http-nio-8090-exec-1，发布事件 1 [生产端]线程：http-nio-8090-exec-1，发布事件 2 [生产端]线程：http-nio-8090-exec-1，发布事件 3 [消费端]线程：default-executor-1，消费事件 {&quot;amount&quot;:100.0,&quot;buyerName&quot;:&quot;Tom-2&quot;,&quot;orderId&quot;:2,&quot;sellerName&quot;:&quot;judy-2&quot;} [消费端]线程：default-executor-2，消费事件 {&quot;amount&quot;:100.0,&quot;buyerName&quot;:&quot;Tom-1&quot;,&quot;orderId&quot;:1,&quot;sellerName&quot;:&quot;judy-1&quot;} [消费端]线程：default-executor-0，消费事件 {&quot;amount&quot;:100.0,&quot;buyerName&quot;:&quot;Tom-3&quot;,&quot;orderId&quot;:3,&quot;sellerName&quot;:&quot;judy-3&quot;} SimpleApplicationEventMulticaster 这个我们自己实例化的 Bean 与系统默认的加载顺序如何？会不会有冲突？ 查了下 Spring 源码，处理逻辑在 AbstractApplicationContext#initApplicationEventMulticaster 方法中，通过 beanFactory 查找是否有自定义的 Bean，如果没有，容器会自己 new 一个 SimpleApplicationEventMulticaster 对象注入到容器中。 代码地址：https://github.com/aalansehaiyang/wx-project 七、消息队列 异步架构是互联网系统中一种典型架构模式，与同步架构相对应。而消息队列天生就是这种异步架构，具有超高吞吐量和超低时延。 消息队列异步架构的主要角色包括消息生产者、消息队列和消息消费者。 消息生产者就是主应用程序，生产者将调用请求封装成消息发送给消息队列。 消息队列的职责就是缓冲消息，等待消费者消费。根据消费方式又分为点对点模式和发布订阅模式两种。 消息消费者，用来从消息队列中拉取、消费消息，完成业务逻辑处理。 当然市面上消息队列框架非常多，常见的有RabbitMQ、Kafka、RocketMQ、ActiveMQ 和 Pulsar 等 不同的消息队列的功能特性会略有不同，但整体架构类似，这里就不展开了。 我们只需要记住一个关键点，借助消息队列这个中间件可以高效的实现异步编程。 ","link":"https://tianxiawuhao.github.io/9hE3Lg3SU/"},{"title":"RabbitMQ","content":"思维导图： 1. 消息队列 1.1 消息队列模式 消息队列目前主要 2 种模式，分别为“点对点模式”和“发布/订阅模式”。 1.1.1 点对点模式 一个具体的消息只能由一个消费者消费，多个生产者可以向同一个消息队列发送消息，但是一个消息在被一个消息者处理的时候，这个消息在队列上会被锁住或者被移除并且其他消费者无法处理该消息。 需要额外注意的是，如果消费者处理一个消息失败了，消息系统一般会把这个消息放回队列，这样其他消费者可以继续处理。 1.1.2 发布/订阅模式 单个消息可以被多个订阅者并发的获取和处理。一般来说，订阅有两种类型： 临时（ephemeral）订阅：这种订阅只有在消费者启动并且运行的时候才存在。一旦消费者退出，相应的订阅以及尚未处理的消息就会丢失。 持久（durable）订阅：这种订阅会一直存在，除非主动去删除。消费者退出后，消息系统会继续维护该订阅，并且后续消息可以被继续处理。 1.2 衡量标准 对消息队列进行技术选型时，需要通过以下指标衡量你所选择的消息队列，是否可以满足你的需求： 消息顺序：发送到队列的消息，消费时是否可以保证消费的顺序，比如A先下单，B后下单，应该是A先去扣库存，B再去扣，顺序不能反。 消息路由：根据路由规则，只订阅匹配路由规则的消息，比如有A/B两者规则的消息，消费者可以只订阅A消息，B消息不会消费。 消息可靠性：是否会存在丢消息的情况，比如有A/B两个消息，最后只有B消息能消费，A消息丢失。 消息时序：主要包括“消息存活时间”和“延迟/预定的消息”，“消息存活时间”表示生产者可以对消息设置TTL，如果超过该TTL，消息会自动消失；“延迟/预定的消息”指的是可以延迟或者预订消费消息，比如延时5分钟，那么消息会5分钟后才能让消费者消费，时间未到的话，是不能消费的。 消息留存：消息消费成功后，是否还会继续保留在消息队列。 容错性：当一条消息消费失败后，是否有一些机制，保证这条消息是一种能成功，比如异步第三方退款消息，需要保证这条消息消费掉，才能确定给用户退款成功，所以必须保证这条消息消费成功的准确性。 伸缩：当消息队列性能有问题，比如消费太慢，是否可以快速支持库容；当消费队列过多，浪费系统资源，是否可以支持缩容。 吞吐量：支持的最高并发数。 2. RabbitMQ 原理初探 RabbitMQ 2007 年发布，是使用 Erlang 语言开发的开源消息队列系统，基于 AMQP 协议来实现。 2.1 基本概念 提到RabbitMQ，就不得不提AMQP协议。AMQP协议是具有现代特征的二进制协议。是一个提供统一消息服务的应用层标准高级消息队列协议，是应用层协议的一个开放标准，为面向消息的中间件设计。 先了解一下AMQP协议中间的几个重要概念： Server：接收客户端的连接，实现AMQP实体服务。 Connection：连接，应用程序与Server的网络连接，TCP连接。 Channel：信道，消息读写等操作在信道中进行。客户端可以建立多个信道，每个信道代表一个会话任务。 Message：消息，应用程序和服务器之间传送的数据，消息可以非常简单，也可以很复杂。由Properties和Body组成。Properties为外包装，可以对消息进行修饰，比如消息的优先级、延迟等高级特性；Body就是消息体内容。 Virtual Host：虚拟主机，用于逻辑隔离。一个虚拟主机里面可以有若干个Exchange和Queue，同一个虚拟主机里面不能有相同名称的Exchange或Queue。 Exchange：交换器，接收消息，按照路由规则将消息路由到一个或者多个队列。如果路由不到，或者返回给生产者，或者直接丢弃。RabbitMQ常用的交换器常用类型有direct、topic、fanout、headers四种，后面详细介绍。 Binding：绑定，交换器和消息队列之间的虚拟连接，绑定中可以包含一个或者多个RoutingKey。 RoutingKey：路由键，生产者将消息发送给交换器的时候，会发送一个RoutingKey，用来指定路由规则，这样交换器就知道把消息发送到哪个队列。路由键通常为一个“.”分割的字符串，例如“com.rabbitmq”。 Queue：消息队列，用来保存消息，供消费者消费。 2.2 工作原理 AMQP 协议模型由三部分组成：生产者、消费者和服务端，执行流程如下： 生产者是连接到 Server，建立一个连接，开启一个信道。 生产者声明交换器和队列，设置相关属性，并通过路由键将交换器和队列进行绑定。 消费者也需要进行建立连接，开启信道等操作，便于接收消息。 生产者发送消息，发送到服务端中的虚拟主机。 虚拟主机中的交换器根据路由键选择路由规则，发送到不同的消息队列中。 订阅了消息队列的消费者就可以获取到消息，进行消费。 2.3 常用交换器 RabbitMQ常用的交换器类型有direct、topic、fanout、headers四种： Direct Exchange：见文知意，直连交换机意思是此交换机需要绑定一个队列，要求该消息与一个特定的路由键完全匹配。简单点说就是一对一的，点对点的发送。 Fanout Exchange：这种类型的交换机需要将队列绑定到交换机上。一个发送到交换机的消息都会被转发到与该交换机绑定的所有队列上。很像子网广播，每台子网内的主机都获得了一份复制的消息。简单点说就是发布订阅。 Topic Exchange：直接翻译的话叫做主题交换机，如果从用法上面翻译可能叫通配符交换机会更加贴切。这种交换机是使用通配符去匹配，路由到对应的队列。通配符有两种：&quot;*&quot; 、 &quot;#&quot;。需要注意的是通配符前面必须要加上&quot;.&quot;符号。 *符号：有且只匹配一个词。比如 a.*可以匹配到&quot;a.b&quot;、&quot;a.c&quot;，但是匹配不了&quot;a.b.c&quot;。 #符号：匹配一个或多个词。比如&quot;rabbit.#&quot;既可以匹配到&quot;rabbit.a.b&quot;、&quot;rabbit.a&quot;，也可以匹配到&quot;rabbit.a.b.c&quot;。 Headers Exchange：这种交换机用的相对没这么多。它跟上面三种有点区别，它的路由不是用routingKey进行路由匹配，而是在匹配请求头中所带的键值进行路由。创建队列需要设置绑定的头部信息，有两种模式：全部匹配和部分匹配。如上图所示，交换机会根据生产者发送过来的头部信息携带的键值去匹配队列绑定的键值，路由到对应的队列。 2.4 消费原理 我们先看几个基本概念： broker：每个节点运行的服务程序，功能为维护该节点的队列的增删以及转发队列操作请求。 master queue：每个队列都分为一个主队列和若干个镜像队列。 mirror queue：镜像队列，作为master queue的备份。在master queue所在节点挂掉之后，系统把mirror queue提升为master queue，负责处理客户端队列操作请求。注意，mirror queue只做镜像，设计目的不是为了承担客户端读写压力。 集群中有两个节点，每个节点上有一个broker，每个broker负责本机上队列的维护，并且borker之间可以互相通信。集群中有两个队列A和B，每个队列都分为master queue和mirror queue（备份）。那么队列上的生产消费怎么实现的呢？ 对于消费队列，如下图有两个consumer消费队列A，这两个consumer连在了集群的不同机器上。RabbitMQ集群中的任何一个节点都拥有集群上所有队列的元信息，所以连接到集群中的任何一个节点都可以，主要区别在于有的consumer连在master queue所在节点，有的连在非master queue节点上。 因为mirror queue要和master queue保持一致，故需要同步机制，正因为一致性的限制，导致所有的读写操作都必须都操作在master queue上（想想，为啥读也要从master queue中读？和数据库读写分离是不一样的），然后由master节点同步操作到mirror queue所在的节点。即使consumer连接到了非master queue节点，该consumer的操作也会被路由到master queue所在的节点上，这样才能进行消费。 对于生成队列，原理和消费一样，如果连接到非 master queue 节点，则路由过去。 所以，到这里小伙伴们就可以看到 RabbitMQ的不足：由于master queue单节点，导致性能瓶颈，吞吐量受限。虽然为了提高性能，内部使用了Erlang这个语言实现，但是终究摆脱不了架构设计上的致命缺陷。 2.5 高级特性 2.5.1 过期时间 Time To Live，也就是生存时间，是一条消息在队列中的最大存活时间，单位是毫秒，下面看看RabbitMQ过期时间特性： RabbitMQ可以对消息和队列设置TTL。 RabbitMQ支持设置消息的过期时间，在消息发送的时候可以进行指定，每条消息的过期时间可以不同。 RabbitMQ支持设置队列的过期时间，从消息入队列开始计算，直到超过了队列的超时时间配置，那么消息会变成死信，自动清除。 如果两种方式一起使用，则过期时间以两者中较小的那个数值为准。 当然也可以不设置TTL，不设置表示消息不会过期；如果设置为0，则表示除非此时可以直接将消息投递到消费者，否则该消息将被立即丢弃。 2.5.2 消息确认 为了保证消息从队列可靠地到达消费者，RabbitMQ提供了消息确认机制。 消费者订阅队列的时候，可以指定autoAck参数，当autoAck为true的时候，RabbitMQ采用自动确认模式，RabbitMQ自动把发送出去的消息设置为确认，然后从内存或者硬盘中删除，而不管消费者是否真正消费到了这些消息。 当autoAck为false的时候，RabbitMQ会等待消费者回复的确认信号，收到确认信号之后才从内存或者磁盘中删除消息。 消息确认机制是RabbitMQ消息可靠性投递的基础，只要设置autoAck参数为false，消费者就有足够的时间处理消息，不用担心处理消息的过程中消费者进程挂掉后消息丢失的问题。 2.5.3 持久化 消息的可靠性是RabbitMQ的一大特色，那么RabbitMQ是如何保证消息可靠性的呢？答案就是消息持久化。持久化可以防止在异常情况下丢失数据。RabbitMQ的持久化分为三个部分：交换器持久化、队列持久化和消息的持久化。 交换器持久化可以通过在声明队列时将durable参数设置为true。如果交换器不设置持久化，那么在RabbitMQ服务重启之后，相关的交换器元数据会丢失，不过消息不会丢失，只是不能将消息发送到这个交换器了。 队列的持久化能保证其本身的元数据不会因异常情况而丢失，但是不能保证内部所存储的消息不会丢失。要确保消息不会丢失，需要将其设置为持久化。队列的持久化可以通过在声明队列时将durable参数设置为true。 设置了队列和消息的持久化，当RabbitMQ服务重启之后，消息依然存在。如果只设置队列持久化或者消息持久化，重启之后消息都会消失。 当然，也可以将所有的消息都设置为持久化，但是这样做会影响RabbitMQ的性能，因为磁盘的写入速度比内存的写入要慢得多。 对于可靠性不是那么高的消息可以不采用持久化处理以提高整体的吞吐量。鱼和熊掌不可兼得，关键在于选择和取舍。在实际中，需要根据实际情况在可靠性和吞吐量之间做一个权衡。 2.5.4 死信队列 当消息在一个队列中变成死信之后，他能被重新发送到另一个交换器中，这个交换器成为死信交换器，与该交换器绑定的队列称为死信队列。 消息变成死信有下面几种情况： 消息被拒绝。 消息过期 队列达到最大长度 DLX也是一个正常的交换器，和一般的交换器没有区别，他能在任何的队列上面被指定，实际上就是设置某个队列的属性。当这个队列中有死信的时候，RabbitMQ会自动将这个消息重新发送到设置的交换器上，进而被路由到另一个队列，我们可以监听这个队列中消息做相应的处理。 死信队列有什么用？当发生异常的时候，消息不能够被消费者正常消费，被加入到了死信队列中。后续的程序可以根据死信队列中的内容分析当时发生的异常，进而改善和优化系统。 2.5.5 延迟队列 一般的队列，消息一旦进入队列就会被消费者立即消费。延迟队列就是进入该队列的消息会被消费者延迟消费，延迟队列中存储的对象是的延迟消息，“延迟消息”是指当消息被发送以后，等待特定的时间后，消费者才能拿到这个消息进行消费。 延迟队列用于需要延迟工作的场景。最常见的使用场景：淘宝或者天猫我们都使用过，用户在下单之后通常有30分钟的时间进行支付，如果这30分钟之内没有支付成功，那么订单就会自动取消。 除了延迟消费，延迟队列的典型应用场景还有延迟重试。比如消费者从队列里面消费消息失败了，可以延迟一段时间以后进行重试。 2.6 特性分析 这里才是内容的重点，不仅需要知道Rabbit的特性，还需要知道支持这些特性的原因： 消息路由（支持）：RabbitMQ可以通过不同的交换器支持不同种类的消息路由； 消息有序（不支持）：当消费消息时，如果消费失败，消息会被放回队列，然后重新消费，这样会导致消息无序； 消息时序（非常好）：通过延时队列，可以指定消息的延时时间，过期时间TTL等； 容错处理（非常好）：通过交付重试和死信交换器（DLX）来处理消息处理故障； 伸缩（一般）：伸缩其实没有非常智能，因为即使伸缩了，master queue还是只有一个，负载还是只有这一个master queue去抗，所以我理解RabbitMQ的伸缩很弱（个人理解）。 持久化（不太好）：没有消费的消息，可以支持持久化，这个是为了保证机器宕机时消息可以恢复，但是消费过的消息，就会被马上删除，因为RabbitMQ设计时，就不是为了去存储历史数据的。 消息回溯（不支持）：因为消息不支持永久保存，所以自然就不支持回溯。 高吞吐（中等）：因为所有的请求的执行，最后都是在master queue，它的这个设计，导致单机性能达不到十万级的标准。 3. RabbitMQ环境搭建 因为我用的是Mac，所以直接可以参考官网： https://www.rabbitmq.com/install-homebrew.html 需要注意的是，一定需要先执行： brew update 然后再执行： brew install rabbitmq 之前没有执行brew update，直接执行brew install rabbitmq时，会报各种各样奇怪的错误，其中“403 Forbidde”居多。 但是在执行“brew install rabbitmq”，会自动安装其它的程序，如果你使用源码安装Rabbitmq，因为启动该服务依赖erlang环境，所以你还需手动安装erlang，但是目前官方已经一键给你搞定，会自动安装Rabbitmq依赖的所有程序，是不是很棒！ 最后执行成功的输出如下： 启动服务： # 启动方式1：后台启动 brew services start rabbitmq # 启动方式2：当前窗口启动 cd /usr/local/Cellar/rabbitmq/3.8.19 rabbitmq-server 在浏览器输入： http://localhost:15672/ 会出现RabbitMQ后台管理界面（用户名和密码都为guest）： 通过brew安装，一行命令搞定，真香！ 4. RabbitMQ测试 4.1 添加账号 首先得启动mq ## 添加账号 ./rabbitmqctl add_user admin admin ## 添加访问权限 ./rabbitmqctl set_permissions -p &quot;/&quot; admin &quot;.*&quot; &quot;.*&quot; &quot;.*&quot; ## 设置超级权限 ./rabbitmqctl set_user_tags admin administrator 4.2 编码实测 因为代码中引入了java 8的特性，pom引入依赖： &lt;dependency&gt; &lt;groupId&gt;com.rabbitmq&lt;/groupId&gt; &lt;artifactId&gt;amqp-client&lt;/artifactId&gt; &lt;version&gt;5.5.1&lt;/version&gt; &lt;/dependency&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;source&gt;8&lt;/source&gt; &lt;target&gt;8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; 开始写代码： public class RabbitMqTest { //消息队列名称 private final static String QUEUE_NAME = &quot;hello&quot;; @Test public void send() throws java.io.IOException, TimeoutException { //创建连接工程 ConnectionFactory factory = new ConnectionFactory(); factory.setHost(&quot;127.0.0.1&quot;); factory.setPort(5672); factory.setUsername(&quot;admin&quot;); factory.setPassword(&quot;admin&quot;); //创建连接 Connection connection = factory.newConnection(); //创建消息通道 Channel channel = connection.createChannel(); //生成一个消息队列 channel.queueDeclare(QUEUE_NAME, true, false, false, null); for (int i = 0; i &lt; 10; i++) { String message = &quot;Hello World RabbitMQ count: &quot; + i; //发布消息，第一个参数表示路由（Exchange名称），为&quot;&quot;则表示使用默认消息路由 channel.basicPublish(&quot;&quot;, QUEUE_NAME, null, message.getBytes()); System.out.println(&quot; [x] Sent '&quot; + message + &quot;'&quot;); } //关闭消息通道和连接 channel.close(); connection.close(); } @Test public void consumer() throws java.io.IOException, TimeoutException { //创建连接工厂 ConnectionFactory factory = new ConnectionFactory(); factory.setHost(&quot;127.0.0.1&quot;); factory.setPort(5672); factory.setUsername(&quot;admin&quot;); factory.setPassword(&quot;admin&quot;); //创建连接 Connection connection = factory.newConnection(); //创建消息信道 final Channel channel = connection.createChannel(); //消息队列 channel.queueDeclare(QUEUE_NAME, true, false, false, null); System.out.println(&quot;[*] Waiting for message. To exist press CTRL+C&quot;); DeliverCallback deliverCallback = (consumerTag, delivery) -&gt; { String message = new String(delivery.getBody(), &quot;UTF-8&quot;); System.out.println(&quot; [x] Received '&quot; + message + &quot;'&quot;); }; channel.basicConsume(QUEUE_NAME, true, deliverCallback, consumerTag -&gt; {}); } } 执行send()后控制台输出： [x] Sent 'Hello World RabbitMQ count: 0' [x] Sent 'Hello World RabbitMQ count: 1' [x] Sent 'Hello World RabbitMQ count: 2' [x] Sent 'Hello World RabbitMQ count: 3' [x] Sent 'Hello World RabbitMQ count: 4' [x] Sent 'Hello World RabbitMQ count: 5' [x] Sent 'Hello World RabbitMQ count: 6' [x] Sent 'Hello World RabbitMQ count: 7' [x] Sent 'Hello World RabbitMQ count: 8' [x] Sent 'Hello World RabbitMQ count: 9' 执行consumer()后： 示例中的代码讲解，可以直接参考官网：https://www.rabbitmq.com/tutorials/tutorial-one-java.html 5. 基本使用姿势 5.1 公共代码封装 封装工厂类： public class RabbitUtil { public static ConnectionFactory getConnectionFactory() { //创建连接工程，下面给出的是默认的case ConnectionFactory factory = new ConnectionFactory(); factory.setHost(&quot;127.0.0.1&quot;); factory.setPort(5672); factory.setUsername(&quot;admin&quot;); factory.setPassword(&quot;admin&quot;); factory.setVirtualHost(&quot;/&quot;); return factory; } } 封装生成者： public class MsgProducer { public static void publishMsg(String exchange, BuiltinExchangeType exchangeType, String toutingKey, String message) throws IOException, TimeoutException { ConnectionFactory factory = RabbitUtil.getConnectionFactory(); //创建连接 Connection connection = factory.newConnection(); //创建消息通道 Channel channel = connection.createChannel(); // 声明exchange中的消息为可持久化，不自动删除 channel.exchangeDeclare(exchange, exchangeType, true, false, null); // 发布消息 channel.basicPublish(exchange, toutingKey, null, message.getBytes()); System.out.println(&quot;Sent '&quot; + message + &quot;'&quot;); channel.close(); connection.close(); } } 封装消费者： public class MsgConsumer { public static void consumerMsg(String exchange, String queue, String routingKey) throws IOException, TimeoutException { ConnectionFactory factory = RabbitUtil.getConnectionFactory(); //创建连接 Connection connection = factory.newConnection(); //创建消息信道 final Channel channel = connection.createChannel(); //消息队列 channel.queueDeclare(queue, true, false, false, null); //绑定队列到交换机 channel.queueBind(queue, exchange, routingKey); System.out.println(&quot;[*] Waiting for message. To exist press CTRL+C&quot;); Consumer consumer = new DefaultConsumer(channel) { @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException { String message = new String(body, &quot;UTF-8&quot;); try { System.out.println(&quot; [x] Received '&quot; + message); } finally { System.out.println(&quot; [x] Done&quot;); channel.basicAck(envelope.getDeliveryTag(), false); } } }; // 取消自动ack channel.basicConsume(queue, false, consumer); } } 5.2 Direct方式 5.2.1 Direct示例 生产者： public class DirectProducer { private static final String EXCHANGE_NAME = &quot;direct.exchange&quot;; public void publishMsg(String routingKey, String msg) { try { MsgProducer.publishMsg(EXCHANGE_NAME, BuiltinExchangeType.DIRECT, routingKey, msg); } catch (Exception e) { e.printStackTrace(); } } public static void main(String[] args) throws InterruptedException { DirectProducer directProducer = new DirectProducer(); String[] routingKey = new String[]{&quot;aaa&quot;, &quot;bbb&quot;, &quot;ccc&quot;}; String msg = &quot;hello &gt;&gt;&gt; &quot;; for (int i = 0; i &lt; 10; i++) { directProducer.publishMsg(routingKey[i % 3], msg + i); } System.out.println(&quot;----over-------&quot;); Thread.sleep(1000 * 60 * 100); } } 执行生产者，往消息队列中放入10条消息，其中key分别为“aaa”、“bbb”和“ccc”，分别放入qa、qb、qc三个队列： 下面是qa队列的信息： 消费者： public class DirectConsumer { private static final String exchangeName = &quot;direct.exchange&quot;; public void msgConsumer(String queueName, String routingKey) { try { MsgConsumer.consumerMsg(exchangeName, queueName, routingKey); } catch (IOException e) { e.printStackTrace(); } catch (TimeoutException e) { e.printStackTrace(); } } public static void main(String[] args) throws InterruptedException { DirectConsumer consumer = new DirectConsumer(); String[] routingKey = new String[]{&quot;aaa&quot;, &quot;bbb&quot;, &quot;ccc&quot;}; String[] queueNames = new String[]{&quot;qa&quot;, &quot;qb&quot;, &quot;qc&quot;}; for (int i = 0; i &lt; 3; i++) { consumer.msgConsumer(queueNames[i], routingKey[i]); } Thread.sleep(1000 * 60 * 100); } } 执行后的输出： [*] Waiting for message. To exist press CTRL+C [x] Received 'hello &gt;&gt;&gt; 0 [x] Done [x] Received 'hello &gt;&gt;&gt; 3 [x] Done [x] Received 'hello &gt;&gt;&gt; 6 [x] Done [x] Received 'hello &gt;&gt;&gt; 9 [x] Done [*] Waiting for message. To exist press CTRL+C [x] Received 'hello &gt;&gt;&gt; 1 [x] Done [x] Received 'hello &gt;&gt;&gt; 4 [x] Done [x] Received 'hello &gt;&gt;&gt; 7 [x] Done [*] Waiting for message. To exist press CTRL+C [x] Received 'hello &gt;&gt;&gt; 2 [x] Done [x] Received 'hello &gt;&gt;&gt; 5 [x] Done [x] Received 'hello &gt;&gt;&gt; 8 [x] Done 可以看到，分别从qa、qb、qc中将不同的key的数据消费掉。 5.2.2 问题探讨 有个疑问：这个队列的名称qa、qb和qc是RabbitMQ自动生成的么，我们可以指定队列名称么？ 我做了个简单的实验，我把消费者代码修改了一下： public static void main(String[] args) throws InterruptedException { DirectConsumer consumer = new DirectConsumer(); String[] routingKey = new String[]{&quot;aaa&quot;, &quot;bbb&quot;, &quot;ccc&quot;}; String[] queueNames = new String[]{&quot;qa&quot;, &quot;qb&quot;, &quot;qc1&quot;}; // 将qc修改为qc1 for (int i = 0; i &lt; 3; i++) { consumer.msgConsumer(queueNames[i], routingKey[i]); } Thread.sleep(1000 * 60 * 100); } 执行后如下图所示： 我们可以发现，多了一个qc1，所以可以判断这个界面中的queues，是消费者执行时，会将消费者指定的队列名称和direct.exchange绑定，绑定的依据就是key。 当我们把队列中的数据全部消费掉，然后重新执行生成者后，会发现qc和qc1中都有3条待消费的数据，因为绑定的key都是“ccc”，所以两者的数据是一样的： 绑定关系如下： 注意：当没有Queue绑定到Exchange时，往Exchange中写入的消息也不会重新分发到之后绑定的queue上。 思考：不执行消费者，看不到这个Queres中信息，我其实可以把这个界面理解为消费者信息界面。不过感觉还是怪怪的，这个queues如果是消费者信息，就不应该叫queues，我理解queues应该是RabbitMQ中实际存放数据的queues，难道是我理解错了？ 5.3 Fanout方式（指定队列） 生产者封装： public class FanoutProducer { private static final String EXCHANGE_NAME = &quot;fanout.exchange&quot;; public void publishMsg(String routingKey, String msg) { try { MsgProducer.publishMsg(EXCHANGE_NAME, BuiltinExchangeType.FANOUT, routingKey, msg); } catch (Exception e) { e.printStackTrace(); } } public static void main(String[] args) { FanoutProducer directProducer = new FanoutProducer(); String msg = &quot;hello &gt;&gt;&gt; &quot;; for (int i = 0; i &lt; 10; i++) { directProducer.publishMsg(&quot;&quot;, msg + i); } } } 消费者： public class FanoutConsumer { private static final String EXCHANGE_NAME = &quot;fanout.exchange&quot;; public void msgConsumer(String queueName, String routingKey) { try { MsgConsumer.consumerMsg(EXCHANGE_NAME, queueName, routingKey); } catch (IOException e) { e.printStackTrace(); } catch (TimeoutException e) { e.printStackTrace(); } } public static void main(String[] args) { FanoutConsumer consumer = new FanoutConsumer(); String[] queueNames = new String[]{&quot;qa-2&quot;, &quot;qb-2&quot;, &quot;qc-2&quot;}; for (int i = 0; i &lt; 3; i++) { consumer.msgConsumer(queueNames[i], &quot;&quot;); } } } 执行生成者，结果如下： 我们发现，生产者生产的10条数据，在每个消费者中都可以消费，这个是和Direct不同的地方，但是使用Fanout方式时，有几个点需要注意一下： 生产者的routkey可以为空，因为生产者的所有数据，会下放到每一个队列，所以不会通过routkey去路由； 消费者需要指定queues，因为消费者需要绑定到指定的queues才能消费。 这幅图就画出了Fanout的精髓之处，exchange会和所有的queue进行绑定，不区分路由，消费者需要绑定指定的queue才能发起消费。 注意：往队列塞数据时，可能通过界面看不到消息个数的增加，可能是你之前已经开启了消费进程，导致增加的消息马上被消费了。 5.4 Fanout方式（随机获取队列） 上面我们是指定了队列，这个方式其实很不友好，比如对于Fanout，我其实根本无需关心队列的名字，如果还指定对应队列进行消费，感觉这个很冗余，所以我们这里就采用随机获取队列名字的方式，下面代码直接Copy官网。 生成者封装： public static void publishMsgV2(String exchange, BuiltinExchangeType exchangeType, String message) throws IOException, TimeoutException { ConnectionFactory factory = RabbitUtil.getConnectionFactory(); //创建连接 Connection connection = factory.newConnection(); //创建消息通道 Channel channel = connection.createChannel(); // 声明exchange中的消息 channel.exchangeDeclare(exchange, exchangeType); // 发布消息 channel.basicPublish(exchange, &quot;&quot;, null, message.getBytes(&quot;UTF-8&quot;)); System.out.println(&quot;Sent '&quot; + message + &quot;'&quot;); channel.close(); connection.close(); } 消费者封装： public static void consumerMsgV2(String exchange) throws IOException, TimeoutException { ConnectionFactory factory = RabbitUtil.getConnectionFactory(); Connection connection = factory.newConnection(); final Channel channel = connection.createChannel(); channel.exchangeDeclare(exchange, &quot;fanout&quot;); String queueName = channel.queueDeclare().getQueue(); channel.queueBind(queueName, exchange, &quot;&quot;); System.out.println(&quot; [*] Waiting for messages. To exit press CTRL+C&quot;); DeliverCallback deliverCallback = (consumerTag, delivery) -&gt; { String message = new String(delivery.getBody(), &quot;UTF-8&quot;); System.out.println(&quot; [x] Received '&quot; + message + &quot;'&quot;); }; channel.basicConsume(queueName, true, deliverCallback, consumerTag -&gt; { }); } 生产者： public class FanoutProducer { private static final String EXCHANGE_NAME = &quot;fanout.exchange.v2&quot;; public void publishMsg(String msg) { try { MsgProducer.publishMsgV2(EXCHANGE_NAME, BuiltinExchangeType.FANOUT, msg); } catch (Exception e) { e.printStackTrace(); } } public static void main(String[] args) { FanoutProducer directProducer = new FanoutProducer(); String msg = &quot;hello &gt;&gt;&gt; &quot;; for (int i = 0; i &lt; 10000; i++) { directProducer.publishMsg(msg + i); } } } 消费者： public class FanoutConsumer { private static final String EXCHANGE_NAME = &quot;fanout.exchange.v2&quot;; public void msgConsumer() { try { MsgConsumer.consumerMsgV2(EXCHANGE_NAME); } catch (IOException e) { e.printStackTrace(); } catch (TimeoutException e) { e.printStackTrace(); } } public static void main(String[] args) { FanoutConsumer consumer = new FanoutConsumer(); for (int i = 0; i &lt; 3; i++) { consumer.msgConsumer(); } } } 执行后，管理界面如下： 5.5 Topic方式 代码详见官网：https://www.rabbitmq.com/tutorials/tutorial-five-java.html 更多方式，请直接查看官网：https://www.rabbitmq.com/getstarted.html 6. RabbitMQ 进阶 6.1 durable 和 autoDeleted 在定义Queue时，可以指定这两个参数： /** * Declare an exchange. * @see com.rabbitmq.client.AMQP.Exchange.Declare * @see com.rabbitmq.client.AMQP.Exchange.DeclareOk * @param exchange the name of the exchange * @param type the exchange type * @param durable true if we are declaring a durable exchange (the exchange will survive a server restart) * @param autoDelete true if the server should delete the exchange when it is no longer in use * @param arguments other properties (construction arguments) for the exchange * @return a declaration-confirm method to indicate the exchange was successfully declared * @throws java.io.IOException if an error is encountered */ Exchange.DeclareOk exchangeDeclare(String exchange, BuiltinExchangeType type, boolean durable, boolean autoDelete, Map&lt;String, Object&gt; arguments) throws IOException; /** * Declare a queue * @see com.rabbitmq.client.AMQP.Queue.Declare * @see com.rabbitmq.client.AMQP.Queue.DeclareOk * @param queue the name of the queue * @param durable true if we are declaring a durable queue (the queue will survive a server restart) * @param exclusive true if we are declaring an exclusive queue (restricted to this connection) * @param autoDelete true if we are declaring an autodelete queue (server will delete it when no longer in use) * @param arguments other properties (construction arguments) for the queue * @return a declaration-confirm method to indicate the queue was successfully declared * @throws java.io.IOException if an error is encountered */ Queue.DeclareOk queueDeclare(String queue, boolean durable, boolean exclusive, boolean autoDelete, Map&lt;String, Object&gt; arguments) throws IOException; 6.1.1 durable 持久化，保证RabbitMQ在退出或者crash等异常情况下数据没有丢失，需要将queue，exchange和Message都持久化。 若是将queue的持久化标识durable设置为true，则代表是一个持久的队列，那么在服务重启之后，会重新读取之前被持久化的queue。 虽然队列可以被持久化，但是里面的消息是否为持久化，还要看消息的持久化设置。即重启queue，但是queue里面还没有发出去的消息，那队列里面还存在该消息么？这个取决于该消息的设置。 6.1.2 autoDeleted 自动删除，如果该队列没有任何订阅的消费者的话，该队列会被自动删除。这种队列适用于临时队列。 当一个Queue被设置为自动删除时，当消费者断掉之后，queue会被删除，这个主要针对的是一些不是特别重要的数据，不希望出现消息积累的情况。 6.1.3 小节 当一个Queue已经声明好了之后，不能更新durable或者autoDelted值；当需要修改时，需要先删除再重新声明 消费的Queue声明应该和投递的Queue声明的 durable,autoDelted属性一致，否则会报错 对于重要的数据，一般设置 durable=true, autoDeleted=false 对于设置 autoDeleted=true 的队列，当没有消费者之后，队列会自动被删除 6.4 ACK 执行一个任务可能需要花费几秒钟，你可能会担心如果一个消费者在执行任务过程中挂掉了。一旦RabbitMQ将消息分发给了消费者，就会从内存中删除。在这种情况下，如果正在执行任务的消费者宕机，会丢失正在处理的消息和分发给这个消费者但尚未处理的消息。 但是，我们不想丢失任何任务，如果有一个消费者挂掉了，那么我们应该将分发给它的任务交付给另一个消费者去处理。 为了确保消息不会丢失，RabbitMQ支持消息应答。消费者发送一个消息应答，告诉RabbitMQ这个消息已经接收并且处理完毕了。RabbitMQ就可以删除它了。 因此手动ACK的常见手段： // 接收消息之后，主动ack/nak Consumer consumer = new DefaultConsumer(channel) { @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException { String message = new String(body, &quot;UTF-8&quot;); try { System.out.println(&quot; [ &quot; + queue + &quot; ] Received '&quot; + message); channel.basicAck(envelope.getDeliveryTag(), false); } catch (Exception e) { channel.basicNack(envelope.getDeliveryTag(), false, true); } } }; // 取消自动ack channel.basicConsume(queue, false, consumer); ","link":"https://tianxiawuhao.github.io/olBu0Tj_j/"},{"title":"ExecutorCompletionService","content":"Future Future模式是多线程设计常用的一种设计模式。Future模式可以理解成：我有一个任务，提交给了Future，Future替我完成这个任务。期间我自己可以去做任何想做的事情。一段时间之后，我就便可以从Future那儿取出结果。 Future提供了三种功能： 判断任务是否完成 能够中断任务 能够获取任务执行的结果 向线程池中提交任务的submit方法不是阻塞方法，而Future.get方法是一个阻塞方法，当submit提交多个任务时，只有所有任务都完成后，才能使用get按照任务的提交顺序得到返回结果，所以一般需要使用future.isDone先判断任务是否全部执行完成，完成后再使用future.get得到结果。（也可以用get (long timeout, TimeUnit unit)方法可以设置超时时间，防止无限时间的等待） 三段式的编程：1.启动多线程任务2.处理其他事3.收集多线程任务结果，Future虽然可以实现获取异步执行结果的需求，但是它没有提供通知的机制，要么使用阻塞，在future.get()的地方等待future返回的结果，这时又变成同步操作；要么使用isDone()轮询地判断Future是否完成，这样会耗费CPU的资源。 解决方法：CompletionService和CompletableFuture（按照任务完成的先后顺序获取任务的结果） ExecutorService 创建线程池，多线程功能调用 public static void test1() throws Exception { ExecutorService executorService = Executors.newCachedThreadPool(); ArrayList&lt;Future&lt;String&gt;&gt; futureArrayList = new ArrayList&lt;&gt;(); System.out.println(&quot;公司让你通知大家聚餐 你开车去接人&quot;); Future&lt;String&gt; future10 = executorService.submit(() -&gt; { System.out.println(&quot;总裁：我在家上大号 我最近拉肚子比较慢 要蹲1个小时才能出来 你等会来接我吧&quot;); TimeUnit.SECONDS.sleep(10); System.out.println(&quot;总裁：1小时了 我上完大号了。你来接吧&quot;); return &quot;总裁上完大号了&quot;; }); futureArrayList.add(future10); Future&lt;String&gt; future6 = executorService.submit(() -&gt; { System.out.println(&quot;中层管理：我在家上大号 要蹲10分钟就可以出来 你等会来接我吧&quot;); TimeUnit.SECONDS.sleep(6); System.out.println(&quot;中层管理：10分钟 我上完大号了。你来接吧&quot;); return &quot;中层管理上完大号了&quot;; }); futureArrayList.add(future6); Future&lt;String&gt; future3 = executorService.submit(() -&gt; { System.out.println(&quot;研发：我在家上大号 我比较快 要蹲3分钟就可以出来 你等会来接我吧&quot;); TimeUnit.SECONDS.sleep(3); System.out.println(&quot;研发：3分钟 我上完大号了。你来接吧&quot;); return &quot;研发上完大号了&quot;; }); futureArrayList.add(future3); TimeUnit.SECONDS.sleep(1); System.out.println(&quot;都通知完了,等着接吧。&quot;); try { for (Future&lt;String&gt; future : futureArrayList) { String returnStr = future.get(); System.out.println(returnStr + &quot;，你去接他&quot;); } } catch (Exception e) { e.printStackTrace(); }finally { executorService.shutdown(); } } 结果 公司让你通知大家聚餐 你开车去接人 研发：我在家上大号 我比较快 要蹲3分钟就可以出来 你等会来接我吧 总裁：我在家上大号 我最近拉肚子比较慢 要蹲1个小时才能出来 你等会来接我吧 中层管理：我在家上大号 要蹲10分钟就可以出来 你等会来接我吧 都通知完了,等着接吧。 研发：3分钟 我上完大号了。你来接吧 研发上完大号了，你去接他 中层管理：10分钟 我上完大号了。你来接吧 总裁：1小时了 我上完大号了。你来接吧 总裁上完大号了，你去接他 中层管理上完大号了，你去接他 ExecutorCompletionService public static void test2() throws Exception { ExecutorService executorService = Executors.newCachedThreadPool(); ExecutorCompletionService&lt;String&gt; completionService = new ExecutorCompletionService&lt;&gt;(executorService); System.out.println(&quot;公司让你通知大家聚餐 你开车去接人&quot;); completionService.submit(() -&gt; { System.out.println(&quot;总裁：我在家上大号 我最近拉肚子比较慢 要蹲1个小时才能出来 你等会来接我吧&quot;); TimeUnit.SECONDS.sleep(10); System.out.println(&quot;总裁：1小时了 我上完大号了。你来接吧&quot;); return &quot;总裁上完大号了&quot;; }); completionService.submit(() -&gt; { System.out.println(&quot;中层管理：我在家上大号 要蹲10分钟就可以出来 你等会来接我吧&quot;); TimeUnit.SECONDS.sleep(6); System.out.println(&quot;中层管理：10分钟 我上完大号了。你来接吧&quot;); return &quot;中层管理上完大号了&quot;; }); completionService.submit(() -&gt; { System.out.println(&quot;研发：我在家上大号 我比较快 要蹲3分钟就可以出来 你等会来接我吧&quot;); TimeUnit.SECONDS.sleep(3); System.out.println(&quot;研发：3分钟 我上完大号了。你来接吧&quot;); return &quot;研发上完大号了&quot;; }); TimeUnit.SECONDS.sleep(1); System.out.println(&quot;都通知完了,等着接吧。&quot;); //提交了3个异步任务） try { for (int i = 0; i &lt; 3; i++) { String returnStr = completionService.take().get(); System.out.println(returnStr + &quot;，你去接他&quot;); } Thread.currentThread().join(); } catch (Exception e) { e.printStackTrace(); }finally { executorService.shutdown(); } } 结果 公司让你通知大家聚餐 你开车去接人 总裁：我在家上大号 我最近拉肚子比较慢 要蹲1个小时才能出来 你等会来接我吧 研发：我在家上大号 我比较快 要蹲3分钟就可以出来 你等会来接我吧 中层管理：我在家上大号 要蹲10分钟就可以出来 你等会来接我吧 都通知完了,等着接吧。 研发：3分钟 我上完大号了。你来接吧 研发上完大号了，你去接他 中层管理：10分钟 我上完大号了。你来接吧 中层管理上完大号了，你去接他 总裁：1小时了 我上完大号了。你来接吧 总裁上完大号了，你去接他 源码分析 completionService是JUC里的线程池ExecutorCompletionService 初始化线程池 同步初始化一个完成队列completionQueue public ExecutorCompletionService(Executor executor) { if (executor == null) throw new NullPointerException(); this.executor = executor; this.aes = (executor instanceof AbstractExecutorService) ? (AbstractExecutorService) executor : null; this.completionQueue = new LinkedBlockingQueue&lt;Future&lt;V&gt;&gt;(); } 执行submit方法 线程池执行的时候传入了自己的内部类QueueingFuture public Future&lt;V&gt; submit(Callable&lt;V&gt; task) { if (task == null) throw new NullPointerException(); RunnableFuture&lt;V&gt; f = newTaskFor(task); executor.execute(new QueueingFuture&lt;V&gt;(f, completionQueue)); return f; } QueueingFuture构造函数，需要关注这里重写了done方法，向阻塞队列里添加task private static class QueueingFuture&lt;V&gt; extends FutureTask&lt;Void&gt; { QueueingFuture(RunnableFuture&lt;V&gt; task, BlockingQueue&lt;Future&lt;V&gt;&gt; completionQueue) { super(task, null); this.task = task; this.completionQueue = completionQueue; } private final Future&lt;V&gt; task; private final BlockingQueue&lt;Future&lt;V&gt;&gt; completionQueue; protected void done() { completionQueue.add(task); } //往completionQueue里面存值 } 执行FutureTask的run方法 public void run() { if (state != NEW || !RUNNER.compareAndSet(this, null, Thread.currentThread())) return; try { Callable&lt;V&gt; c = callable; if (c != null &amp;&amp; state == NEW) { V result; boolean ran; try { result = c.call(); ran = true; } catch (Throwable ex) { result = null; ran = false; setException(ex); } if (ran) set(result); //设置值 } } finally { // runner must be non-null until state is settled to // prevent concurrent calls to run() runner = null; // state must be re-read after nulling runner to prevent // leaked interrupts int s = state; if (s &gt;= INTERRUPTING) handlePossibleCancellationInterrupt(s); } } set方法 protected void set(V v) { if (STATE.compareAndSet(this, NEW, COMPLETING)) { outcome = v; STATE.setRelease(this, NORMAL); // final state finishCompletion(); } } 在finishCompletion方法中执行了done方法 private void finishCompletion() { // assert state &gt; COMPLETING; for (WaitNode q; (q = waiters) != null;) { if (WAITERS.weakCompareAndSet(this, q, null)) { for (;;) { Thread t = q.thread; if (t != null) { q.thread = null; LockSupport.unpark(t); } WaitNode next = q.next; if (next == null) break; q.next = null; // unlink to help gc q = next; } break; } } done(); //执行done方法-completionQueue.add(task); callable = null; // to reduce footprint } 所以每次线程完成任务，都会往completionQueue中新增一条记录。completionQueue中的记录需要通过别的方式取：ExecutorCompletionService.take()。 ExecutorCompletionService的实际用法应该是这样的：CompletionService一边执行任务，一边处理完成的任务结果，这样可以将执行的任务与处理任务隔离开来进行处理，使用submit执行任务，使用take获取已完成的任务，先完成的任务结果先加入队列。获取的结果，跟提交给线程池的任务是无关联的。 Future&lt;String&gt; submit = completionService.submit(() -&gt; { return &quot;假装有返回&quot;; }); submit.get();//直接获取返回值 如上如果我们在使用过程中由于需要的是当前线程任务的返回值，所以没有调用take方法，而是通过get方法获取到当前线程调用的返回值，会导致task任务在队列中一直增加，从而造成OOM ","link":"https://tianxiawuhao.github.io/wT9gsiZx3/"},{"title":"消息队列选型","content":"消息队列 常用的消息队列主要这 4 种，分别为 Kafka、RabbitMQ、RocketMQ 和 ActiveMQ，主要介绍前三，不BB，上思维导图！ 消息队列基础 什么是消息队列？ 消息队列是在消息的传输过程中保存消息的容器，用于接收消息并以文件的方式存储，一个消息队列可以被一个也可以被多个消费者消费，包含以下 3 元素： Producer：消息生产者，负责产生和发送消息到 Broker； Broker：消息处理中心，负责消息存储、确认、重试等，一般其中会包含多个 Queue； Consumer：消息消费者，负责从 Broker 中获取消息，并进行相应处理。 消息队列模式 点对点模式：多个生产者可以向同一个消息队列发送消息，一个具体的消息只能由一个消费者消费。 发布/订阅模式：单个消息可以被多个订阅者并发的获取和处理。 消息队列应用场景 应用解耦：消息队列减少了服务之间的耦合性，不同的服务可以通过消息队列进行通信，而不用关心彼此的实现细节。 异步处理：消息队列本身是异步的，它允许接收者在消息发送很长时间后再取回消息。 流量削锋：当上下游系统处理能力存在差距的时候，利用消息队列做一个通用的”载体”，在下游有能力处理的时候，再进行分发与处理。 日志处理：日志处理是指将消息队列用在日志处理中，比如 Kafka 的应用，解决大量日志传输的问题。 消息通讯：消息队列一般都内置了高效的通信机制，因此也可以用在纯的消息通讯，比如实现点对点消息队列，或者聊天室等。 消息广播：如果没有消息队列，每当一个新的业务方接入，我们都要接入一次新接口。有了消息队列，我们只需要关心消息是否送达了队列，至于谁希望订阅，是下游的事情，无疑极大地减少了开发和联调的工作量。 常用消息队列 由于官方社区现在对 ActiveMQ 5.x 维护越来越少，较少在大规模吞吐的场景中使用，所以我们主要讲解 Kafka、RabbitMQ 和 RocketMQ。 Kafka Apache Kafka 最初由 LinkedIn 公司基于独特的设计实现为一个分布式的提交日志系统，之后成为 Apache 项目的一部分，号称大数据的杀手锏，在数据采集、传输、存储的过程中发挥着举足轻重的作用。 它是一个分布式的，支持多分区、多副本，基于 Zookeeper 的分布式消息流平台，它同时也是一款开源的基于发布订阅模式的消息引擎系统。 重要概念 主题（Topic）：消息的种类称为主题，可以说一个主题代表了一类消息，相当于是对消息进行分类，主题就像是数据库中的表。 分区（partition）：主题可以被分为若干个分区，同一个主题中的分区可以不在一个机器上，有可能会部署在多个机器上，由此来实现 kafka 的伸缩性。 批次：为了提高效率， 消息会分批次写入 Kafka，批次就代指的是一组消息。 消费者群组（Consumer Group）：消费者群组指的就是由一个或多个消费者组成的群体。 Broker: 一个独立的 Kafka 服务器就被称为 broker，broker 接收来自生产者的消息，为消息设置偏移量，并提交消息到磁盘保存。 Broker 集群：broker 集群由一个或多个 broker 组成。 重平衡（Rebalance）：消费者组内某个消费者实例挂掉后，其他消费者实例自动重新分配订阅主题分区的过程。 Kafka 架构 一个典型的 Kafka 集群中包含 Producer、broker、Consumer Group、Zookeeper 集群。 Kafka 通过 Zookeeper 管理集群配置，选举 leader，以及在 Consumer Group 发生变化时进行 rebalance。Producer 使用 push 模式将消息发布到 broker，Consumer 使用 pull 模式从 broker 订阅并消费消息。 Kafka 工作原理 消息经过序列化后，通过不同的分区策略，找到对应的分区。 相同主题和分区的消息，会被存放在同一个批次里，然后由一个独立的线程负责把它们发到 Kafka Broker 上。 分区的策略包括顺序轮询、随机轮询和 key hash 这 3 种方式，那什么是分区呢？ 分区是 Kafka 读写数据的最小粒度，比如主题 A 有 15 条消息，有 5 个分区，如果采用顺序轮询的方式，15 条消息会顺序分配给这 5 个分区，后续消费的时候，也是按照分区粒度消费。 由于分区可以部署在多个不同的机器上，所以可以通过分区实现 Kafka 的伸缩性，比如主题 A 的 5 个分区，分别部署在 5 台机器上，如果下线一台，分区就变为 4。 Kafka 消费是通过消费群组完成，同一个消费者群组，一个消费者可以消费多个分区，但是一个分区，只能被一个消费者消费。 如果消费者增加，会触发 Rebalance，也就是分区和消费者需要重新配对。 不同的消费群组互不干涉，比如下图的 2 个消费群组，可以分别消费这 4 个分区的消息，互不影响。 更多知识，详见Kafka 架构深入 RocketMQ RocketMQ 是阿里开源的消息中间件，它是纯 Java 开发，具有高性能、高可靠、高实时、适合大规模分布式系统应用的特点。 RocketMQ 思路起源于 Kafka，但并不是 Kafka 的一个 Copy，它对消息的可靠传输及事务性做了优化，目前在阿里集团被广泛应用于交易、充值、流计算、消息推送、日志流式处理、binlog 分发等场景。 重要概念 Name 服务器（NameServer）：充当注册中心，类似 Kafka 中的 Zookeeper。 Broker: 一个独立的 RocketMQ 服务器就被称为 broker，broker 接收来自生产者的消息，为消息设置偏移量。 主题（Topic）：消息的第一级类型，一条消息必须有一个 Topic。 子主题（Tag）：消息的第二级类型，同一业务模块不同目的的消息就可以用相同 Topic 和不同的 Tag 来标识。 分组（Group）：一个组可以订阅多个 Topic，包括生产者组（Producer Group）和消费者组（Consumer Group）。 队列（Queue）：可以类比 Kafka 的分区 Partition。 RocketMQ 工作原理 RockerMQ 中的消息模型就是按照主题模型所实现的，包括 Producer Group、Topic、Consumer Group 三个角色。 为了提高并发能力，一个 Topic 包含多个 Queue，生产者组根据主题将消息放入对应的 Topic，下图是采用轮询的方式找到里面的 Queue。 RockerMQ 中的消费群组和 Queue，可以类比 Kafka 中的消费群组和 Partition：不同的消费者组互不干扰，一个 Queue 只能被一个消费者消费，一个消费者可以消费多个 Queue。 消费 Queue 的过程中，通过偏移量记录消费的位置。 RocketMQ 架构 RocketMQ 技术架构中有四大角色 NameServer、Broker、Producer 和 Consumer，下面主要介绍 Broker。 Broker 用于存放 Queue，一个 Broker 可以配置多个 Topic，一个 Topic 中存在多个 Queue。 如果某个 Topic 消息量很大，应该给它多配置几个 Queue，并且尽量多分布在不同 broker 上，以减轻某个 broker 的压力。Topic 消息量都比较均匀的情况下，如果某个 broker 上的队列越多，则该 broker 压力越大。 简单提一下，Broker 通过集群部署，并且提供了 master/slave 的结构，slave 定时从 master 同步数据（同步刷盘或者异步刷盘），如果 master 宕机，则 slave 提供消费服务，但是不能写入消息。 看到这里，大家应该可以发现，RocketMQ 的设计和 Kafka 真的很像！ 更多知识，详见 RabbitMQ RabbitMQ 2007 年发布，是使用 Erlang 语言开发的开源消息队列系统，基于 AMQP 协议来实现。 AMQP 的主要特征是面向消息、队列、路由、可靠性、安全。AMQP 协议更多用在企业系统内，对数据一致性、稳定性和可靠性要求很高的场景，对性能和吞吐量的要求还在其次。 重要概念 信道（Channel）：消息读写等操作在信道中进行，客户端可以建立多个信道，每个信道代表一个会话任务。 交换器（Exchange）：接收消息，按照路由规则将消息路由到一个或者多个队列；如果路由不到，或者返回给生产者，或者直接丢弃。 路由键（RoutingKey）：生产者将消息发送给交换器的时候，会发送一个 RoutingKey，用来指定路由规则，这样交换器就知道把消息发送到哪个队列。 绑定（Binding）：交换器和消息队列之间的虚拟连接，绑定中可以包含一个或者多个 RoutingKey。 RabbitMQ 工作原理 AMQP 协议模型由三部分组成：生产者、消费者和服务端，执行流程如下： 生产者是连接到 Server，建立一个连接，开启一个信道。 生产者声明交换器和队列，设置相关属性，并通过路由键将交换器和队列进行绑定。 消费者也需要进行建立连接，开启信道等操作，便于接收消息。 生产者发送消息，发送到服务端中的虚拟主机。 虚拟主机中的交换器根据路由键选择路由规则，发送到不同的消息队列中。 订阅了消息队列的消费者就可以获取到消息，进行消费。 常用交换器 RabbitMQ 常用的交换器类型有 direct、topic、fanout、headers 四种 具体的使用方法，可以参考官网： 官网入口：https://www.rabbitmq.com/getstarted.html 更多知识，详见 消息队列对比&amp;选型 消息队列对比 Kafka 优点： 高吞吐、低延迟：Kafka 最大的特点就是收发消息非常快，Kafka 每秒可以处理几十万条消息，它的最低延迟只有几毫秒； 高伸缩性：每个主题（topic）包含多个分区（partition），主题中的分区可以分布在不同的主机（broker）中； 高稳定性：Kafka 是分布式的，一个数据多个副本，某个节点宕机，Kafka 集群能够正常工作； 持久性、可靠性、可回溯：Kafka 能够允许数据的持久化存储，消息被持久化到磁盘，并支持数据备份防止数据丢失，支持消息回溯； 消息有序：通过控制能够保证所有消息被消费且仅被消费一次； 有优秀的第三方 Kafka Web 管理界面 Kafka-Manager，在日志领域比较成熟，被多家公司和多个开源项目使用。 缺点： Kafka 单机超过 64 个队列/分区，Load 会发生明显的飙高现象，队列越多，load 越高，发送消息响应时间变长； 不支持消息路由，不支持延迟发送，不支持消息重试； 社区更新较慢。 RocketMQ 优点： 高吞吐：借鉴 Kafka 的设计，单一队列百万消息的堆积能力； 高伸缩性：灵活的分布式横向扩展部署架构，整体架构其实和 kafka 很像； 高容错性：通过ACK机制，保证消息一定能正常消费； 持久化、可回溯：消息可以持久化到磁盘中，支持消息回溯； 消息有序：在一个队列中可靠的先进先出（FIFO）和严格的顺序传递； 支持发布/订阅和点对点消息模型，支持拉、推两种消息模式； 提供 docker 镜像用于隔离测试和云集群部署，提供配置、指标和监控等功能丰富的 Dashboard。 缺点： 不支持消息路由，支持的客户端语言不多，目前是 java 及 c++，其中 c++ 不成熟； 部分支持消息有序：需要将同一类的消息 hash 到同一个队列 Queue 中，才能支持消息的顺序，如果同一类消息散落到不同的 Queue中，就不能支持消息的顺序。 社区活跃度一般。 RabbitMQ 优点： 支持几乎所有最受欢迎的编程语言：Java，C，C ++，C＃，Ruby，Perl，Python，PHP等等； 支持消息路由：RabbitMQ 可以通过不同的交换器支持不同种类的消息路由； 消息时序：通过延时队列，可以指定消息的延时时间，过期时间TTL等； 支持容错处理：通过交付重试和死信交换器（DLX）来处理消息处理故障； 提供了一个易用的用户界面，使得用户可以监控和管理消息 Broker； 社区活跃度高。 缺点： Erlang 开发，很难去看懂源码，不利于做二次开发和维护，基本只能依赖于开源社区的快速维护和修复 bug； RabbitMQ 吞吐量会低一些，这是因为他做的实现机制比较重； 不支持消息有序、持久化不好、不支持消息回溯、伸缩性一般。 消息队列选型 Kafka：追求高吞吐量，一开始的目的就是用于日志收集和传输，适合产生大量数据的互联网服务的数据收集业务，大型公司建议可以选用，如果有日志采集功能，肯定是首选 kafka。 RocketMQ：天生为金融互联网领域而生，对于可靠性要求很高的场景，尤其是电商里面的订单扣款，以及业务削峰，在大量交易涌入时，后端可能无法及时处理的情况。RocketMQ 在稳定性上可能更值得信赖，这些业务场景在阿里双 11 已经经历了多次考验，如果你的业务有上述并发场景，建议可以选择 RocketMQ。 RabbitMQ：结合 erlang 语言本身的并发优势，性能较好，社区活跃度也比较高，但是不利于做二次开发和维护，不过 RabbitMQ 的社区十分活跃，可以解决开发过程中遇到的 bug。如果你的数据量没有那么大，小公司优先选择功能比较完备的 RabbitMQ。 ActiveMQ：官方社区现在对 ActiveMQ 5.x 维护越来越少，较少在大规模吞吐的场景中使用。 ","link":"https://tianxiawuhao.github.io/DyzAzkEVU/"},{"title":"easyExcel","content":"读Excel 注解 使用注解很简单，只要在对应的实体类上面加上注解即可。 ExcelProperty 用于匹配excel和实体类的匹配,参数如下： 名称 默认值 描述 value 空 用于匹配excel中的头，必须全匹配,如果有多行头，会匹配最后一行头 order Integer.MAX_VALUE 优先级高于value，会根据order的顺序来匹配实体和excel中数据的顺序 index -1 优先级高于value和order，会根据index直接指定到excel中具体的哪一列 converter 自动选择 指定当前字段用什么转换器，默认会自动选择。读的情况下只要实现com.alibaba.excel.converters.Converter#convertToJavaData(com.alibaba.excel.converters.ReadConverterContext&lt;?&gt;) 方法即可 ExcelIgnore 默认所有字段都会和excel去匹配，加了这个注解会忽略该字段 ExcelIgnoreUnannotated 默认不加ExcelProperty 的注解的都会参与读写，加了不会参与 DateTimeFormat 日期转换，用String去接收excel日期格式的数据会调用这个注解,参数如下： 名称 默认值 描述 value 空 参照java.text.SimpleDateFormat书写即可 use1904windowing 自动选择 excel中时间是存储1900年起的一个双精度浮点数，但是有时候默认开始日期是1904，所以设置这个值改成默认1904年开始 NumberFormat 数字转换，用String去接收excel数字格式的数据会调用这个注解。 名称 默认值 描述 value 空 参照java.text.DecimalFormat书写即可 roundingMode RoundingMode.HALF_UP 格式化的时候设置舍入模式 参数 概念介绍 ReadWorkbook 可以理解成一个excel ReadSheet 理解成一个excel里面的一个表单 通用参数 ReadWorkbook,ReadSheet 都会有的参数，如果为空，默认使用上级。 名称 默认值 描述 converter 空 默认加载了很多转换器，这里可以加入不支持的字段 readListener 空 可以注册多个监听器，读取excel的时候会不断的回调监听器中的方法 headRowNumber 1 excel中头的行数，默认1行 head 空 与clazz二选一。读取文件头对应的列表，会根据列表匹配数据，建议使用class clazz 空 与head二选一。读取文件的头对应的class，也可以使用注解。如果两个都不指定，则会读取全部数据 autoTrim true 会对头、读取数据等进行自动trim use1904windowing false excel中时间是存储1900年起的一个双精度浮点数，但是有时候默认开始日期是1904，所以设置这个值改成默认1904年开始 useScientificFormat false 数字转文本的时候在较大的数值的是否是否采用科学计数法 ReadWorkbook 设置方法如下，找不到参数的看下通用参数里面是否存在。 EasyExcel.read(fileName, DemoData.class, new DemoDataListener()) // 在 read 方法之后， 在 sheet方法之前都是设置ReadWorkbook的参数 .sheet() .doRead(); 名称 默认值 描述 excelType 空 当前excel的类型,支持XLS、XLSX、CSV inputStream 空 与file二选一。读取文件的流，如果接收到的是流就只用，不用流建议使用file参数。因为使用了inputStream easyexcel会帮忙创建临时文件，最终还是file file 空 与inputStream二选一。读取文件的文件。 mandatoryUseInputStream false 强制使用 inputStream 来创建对象，性能会变差，但是不会创建临文件。 charset Charset#defaultCharset 只有csv文件有用，读取文件的时候使用的编码 autoCloseStream true 自动关闭读取的流。 readCache 空 默认小于5M用 内存，超过5M会使用 EhCache,这里不建议使用这个参数。 readCacheSelector SimpleReadCacheSelector 用于选择什么时候用内存去存储临时数据，什么时候用磁盘存储临时数据 ignoreEmptyRow true 忽略空的行 password 空 读取文件的密码 xlsxSAXParserFactoryName 空 指定sax读取使用的class的名称，例如：com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl useDefaultListener true @since 2.1.4 默认会加入ModelBuildEventListener 来帮忙转换成传入class的对象，设置成false后将不会协助转换对象，自定义的监听器会接收到Map&lt;Integer,CellData&gt;对象，如果还想继续接听到class对象，请调用readListener方法，加入自定义的beforeListener、 ModelBuildEventListener、 自定义的afterListener即可。 extraReadSet 空 额外需要读取内容的set，默认不读取这些数据 ReadSheet 设置方法如下，找不到参数的看下通用参数里面是否存在。 EasyExcel.read(fileName, DemoData.class, new DemoDataListener()) .sheet() // 在 sheet 方法之后， 在 doRead方法之前都是设置ReadSheet的参数 .doRead(); 名称 默认值 描述 sheetNo 0 需要读取Sheet的编码，建议使用这个来指定读取哪个Sheet sheetName 空 根据名字去匹配Sheet 写Excel 注解 使用注解很简单，只要在对应的实体类上面加上注解即可。 ExcelProperty 用于匹配excel和实体类的匹配,参数如下： 名称 默认值 描述 value 空 用于匹配excel中的头，必须全匹配,如果有多行头，会匹配最后一行头 order Integer.MAX_VALUE 优先级高于value，会根据order的顺序来匹配实体和excel中数据的顺序 index -1 优先级高于value和order，会根据index直接指定到excel中具体的哪一列 converter 自动选择 指定当前字段用什么转换器，默认会自动选择。写的情况下只要实现com.alibaba.excel.converters.Converter#convertToExcelData(com.alibaba.excel.converters.WriteConverterContext&lt;T&gt;) 方法即可 ExcelIgnore 默认所有字段都会和excel去匹配，加了这个注解会忽略该字段 ExcelIgnoreUnannotated 默认不加ExcelProperty 的注解的都会参与读写，加了不会参与 DateTimeFormat 日期转换，用String去接收excel日期格式的数据会调用这个注解,参数如下： 名称 默认值 描述 value 空 参照java.text.SimpleDateFormat书写即可 use1904windowing 自动选择 excel中时间是存储1900年起的一个双精度浮点数，但是有时候默认开始日期是1904，所以设置这个值改成默认1904年开始 NumberFormat 数字转换，用String去接收excel数字格式的数据会调用这个注解。 名称 默认值 描述 value 空 参照java.text.DecimalFormat书写即可 roundingMode RoundingMode.HALF_UP 格式化的时候设置舍入模式 参数 概念介绍 WriteWorkbook 可以理解成一个excel WriteSheet 理解成一个excel里面的一个表单 WriteTable 一个表单里面如果有多个实际用的表格，则可以用WriteTable 通用参数 WriteWorkbook,WriteSheet ,WriteTable都会有的参数，如果为空，默认使用上级。 名称 默认值 描述 converter 空 默认加载了很多转换器，这里可以加入不支持的字段 writeHandler 空 写的处理器。可以实现WorkbookWriteHandler,SheetWriteHandler,RowWriteHandler,CellWriteHandler，在写入excel的不同阶段会调用 relativeHeadRowIndex 0 写入到excel和上面空开几行 head 空 与clazz二选一。读取文件头对应的列表，会根据列表匹配数据，建议使用class clazz 空 与head二选一。读取文件的头对应的class，也可以使用注解。如果两个都不指定，则会读取全部数据 autoTrim true 会对头、读取数据等进行自动trim use1904windowing false excel中时间是存储1900年起的一个双精度浮点数，但是有时候默认开始日期是1904，所以设置这个值改成默认1904年开始 useScientificFormat false 数字转文本的时候在较大的数值的是否是否采用科学计数法 needHead true 是否需要写入头到excel useDefaultStyle true 是否使用默认的样式 automaticMergeHead true 自动合并头，头中相同的字段上下左右都会去尝试匹配 excludeColumnIndexes 空 需要排除对象中的index的数据 excludeColumnFieldNames 空 需要排除对象中的字段的数据 includeColumnIndexes 空 只要导出对象中的index的数据 includeColumnFieldNames 空 只要导出对象中的字段的数据 WriteWorkbook 设置方法如下，找不到参数的看下通用参数里面是否存在。 EasyExcel.write(fileName, DemoData.class) // 在 write 方法之后， 在 sheet方法之前都是设置WriteWorkbook的参数 .sheet(&quot;模板&quot;) .doWrite(() -&gt; { // 分页查询数据 return data(); }); 名称 默认值 描述 excelType 空 当前excel的类型,支持XLS、XLSX、CSV outputStream 空 与file二选一。写入文件的流 file 空 与outputStream二选一。写入的文件 templateInputStream 空 模板的文件流 templateFile 空 模板文件 charset Charset#defaultCharset 只有csv文件有用，写入文件的时候使用的编码 autoCloseStream true 自动关闭写入的流。 password 空 读取文件的密码 inMemory false 是否在内存处理，默认会生成临时文件以节约内存。内存模式效率会更好，但是容易OOM writeExcelOnException false 写入过程中抛出异常了，是否尝试把数据写入到excel WriteSheet 设置方法如下，找不到参数的看下通用参数里面是否存在。 EasyExcel.write(fileName, DemoData.class) .sheet(&quot;模板&quot;) // 在 sheet 方法之后， 在 doWrite方法之前都是设置WriteSheet的参数 .doWrite(() -&gt; { // 分页查询数据 return data(); }); 名称 默认值 描述 sheetNo 0 需要写入的编码 sheetName 空 需要些的Sheet名称，默认同sheetNo WriteTable 设置方法如下，找不到参数的看下通用参数里面是否存在。 EasyExcel.write(fileName, DemoData.class) .sheet(&quot;模板&quot;) .table() // 在 table 方法之后， 在 doWrite方法之前都是设置WriteTable的参数 .doWrite(() -&gt; { // 分页查询数据 return data(); }); 名称 默认值 描述 tableNo 0 需要写入的编码 &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;easyexcel&lt;/artifactId&gt; &lt;version&gt;3.1.1&lt;/version&gt; &lt;/dependency&gt; WebTest import java.io.IOException; import java.net.URLEncoder; import java.util.Date; import java.util.List; import java.util.Map; import javax.servlet.http.HttpServletResponse; import com.alibaba.excel.EasyExcel; import com.alibaba.excel.util.ListUtils; import com.alibaba.excel.util.MapUtils; import com.alibaba.fastjson.JSON; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.stereotype.Controller; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.bind.annotation.PostMapping; import org.springframework.web.bind.annotation.ResponseBody; import org.springframework.web.multipart.MultipartFile; /** * web读写案例 * **/ @Controller public class WebTest { @Autowired private UploadDAO uploadDAO; /** * 文件下载（失败了会返回一个有部分数据的Excel） * &lt;p&gt; * 1. 创建excel对应的实体对象 参照{@link DownloadData} * &lt;p&gt; * 2. 设置返回的 参数 * &lt;p&gt; * 3. 直接写，这里注意，finish的时候会自动关闭OutputStream,当然你外面再关闭流问题不大 */ @GetMapping(&quot;download&quot;) public void download(HttpServletResponse response) throws IOException { // 这里注意 有同学反应使用swagger 会导致各种问题，请直接用浏览器或者用postman response.setContentType(&quot;application/vnd.openxmlformats-officedocument.spreadsheetml.sheet&quot;); response.setCharacterEncoding(&quot;utf-8&quot;); // 这里URLEncoder.encode可以防止中文乱码 当然和easyexcel没有关系 String fileName = URLEncoder.encode(&quot;测试&quot;, &quot;UTF-8&quot;).replaceAll(&quot;\\\\+&quot;, &quot;%20&quot;); response.setHeader(&quot;Content-disposition&quot;, &quot;attachment;filename*=utf-8''&quot; + fileName + &quot;.xlsx&quot;); EasyExcel.write(response.getOutputStream(), DownloadData.class).sheet(&quot;模板&quot;).doWrite(data()); } /** * 文件下载并且失败的时候返回json（默认失败了会返回一个有部分数据的Excel） * * @since 2.1.1 */ @GetMapping(&quot;downloadFailedUsingJson&quot;) public void downloadFailedUsingJson(HttpServletResponse response) throws IOException { // 这里注意 有同学反应使用swagger 会导致各种问题，请直接用浏览器或者用postman try { response.setContentType(&quot;application/vnd.openxmlformats-officedocument.spreadsheetml.sheet&quot;); response.setCharacterEncoding(&quot;utf-8&quot;); // 这里URLEncoder.encode可以防止中文乱码 当然和easyexcel没有关系 String fileName = URLEncoder.encode(&quot;测试&quot;, &quot;UTF-8&quot;).replaceAll(&quot;\\\\+&quot;, &quot;%20&quot;); response.setHeader(&quot;Content-disposition&quot;, &quot;attachment;filename*=utf-8''&quot; + fileName + &quot;.xlsx&quot;); // 这里需要设置不关闭流 EasyExcel.write(response.getOutputStream(), DownloadData.class).autoCloseStream(Boolean.FALSE).sheet(&quot;模板&quot;) .doWrite(data()); } catch (Exception e) { // 重置response response.reset(); response.setContentType(&quot;application/json&quot;); response.setCharacterEncoding(&quot;utf-8&quot;); Map&lt;String, String&gt; map = MapUtils.newHashMap(); map.put(&quot;status&quot;, &quot;failure&quot;); map.put(&quot;message&quot;, &quot;下载文件失败&quot; + e.getMessage()); response.getWriter().println(JSON.toJSONString(map)); } } /** * 文件上传 * &lt;p&gt; * 1. 创建excel对应的实体对象 参照{@link UploadData} * &lt;p&gt; * 2. 由于默认一行行的读取excel，所以需要创建excel一行一行的回调监听器，参照{@link UploadDataListener} * &lt;p&gt; * 3. 直接读即可 */ @PostMapping(&quot;upload&quot;) @ResponseBody public String upload(MultipartFile file) throws IOException { EasyExcel.read(file.getInputStream(), UploadData.class, new UploadDataListener(uploadDAO)).sheet().doRead(); return &quot;success&quot;; } private List&lt;DownloadData&gt; data() { List&lt;DownloadData&gt; list = ListUtils.newArrayList(); for (int i = 0; i &lt; 10; i++) { DownloadData data = new DownloadData(); data.setString(&quot;字符串&quot; + 0); data.setDate(new Date()); data.setDoubleData(0.56); list.add(data); } return list; } } DownloadData import java.util.Date; import com.alibaba.excel.annotation.ExcelProperty; import lombok.EqualsAndHashCode; import lombok.Getter; import lombok.Setter; /** * 基础数据类 * **/ @Getter @Setter @EqualsAndHashCode public class DownloadData { @ExcelProperty(&quot;字符串标题&quot;) private String string; @ExcelProperty(&quot;日期标题&quot;) private Date date; @ExcelProperty(&quot;数字标题&quot;) private Double doubleData; } UploadDAO import java.util.List; import org.springframework.stereotype.Repository; /** * 假设这个是你的DAO存储。当然还要这个类让spring管理，当然你不用需要存储，也不需要这个类。 * **/ @Repository public class UploadDAO { public void save(List&lt;UploadData&gt; list) { // 如果是mybatis,尽量别直接调用多次insert,自己写一个mapper里面新增一个方法batchInsert,所有数据一次性插入 } } UploadData import java.util.Date; import lombok.EqualsAndHashCode; import lombok.Getter; import lombok.Setter; /** * 基础数据类 * **/ @Getter @Setter @EqualsAndHashCode public class UploadData { private String string; private Date date; private Double doubleData; } UploadDataListener import java.util.List; import com.alibaba.excel.context.AnalysisContext; import com.alibaba.excel.read.listener.ReadListener; import com.alibaba.excel.util.ListUtils; import com.alibaba.fastjson.JSON; import lombok.extern.slf4j.Slf4j; /** * 模板的读取类 * */ // 有个很重要的点 DemoDataListener 不能被spring管理，要每次读取excel都要new,然后里面用到spring可以构造方法传进去 @Slf4j public class UploadDataListener implements ReadListener&lt;UploadData&gt; { /** * 每隔5条存储数据库，实际使用中可以100条，然后清理list ，方便内存回收 */ private static final int BATCH_COUNT = 5; private List&lt;UploadData&gt; cachedDataList = ListUtils.newArrayListWithExpectedSize(BATCH_COUNT); /** * 假设这个是一个DAO，当然有业务逻辑这个也可以是一个service。当然如果不用存储这个对象没用。 */ private UploadDAO uploadDAO; public UploadDataListener() { // 这里是demo，所以随便new一个。实际使用如果到了spring,请使用下面的有参构造函数 uploadDAO = new UploadDAO(); } /** * 如果使用了spring,请使用这个构造方法。每次创建Listener的时候需要把spring管理的类传进来 * * @param uploadDAO */ public UploadDataListener(UploadDAO uploadDAO) { this.uploadDAO = uploadDAO; } /** * 这个每一条数据解析都会来调用 * * @param data one row value. Is is same as {@link AnalysisContext#readRowHolder()} * @param context */ @Override public void invoke(UploadData data, AnalysisContext context) { log.info(&quot;解析到一条数据:{}&quot;, JSON.toJSONString(data)); cachedDataList.add(data); // 达到BATCH_COUNT了，需要去存储一次数据库，防止数据几万条数据在内存，容易OOM if (cachedDataList.size() &gt;= BATCH_COUNT) { saveData(); // 存储完成清理 list cachedDataList = ListUtils.newArrayListWithExpectedSize(BATCH_COUNT); } } /** * 所有数据解析完成了 都会来调用 * * @param context */ @Override public void doAfterAllAnalysed(AnalysisContext context) { // 这里也要保存数据，确保最后遗留的数据也存储到数据库 saveData(); log.info(&quot;所有数据解析完成！&quot;); } /** * 加上存储数据库 */ private void saveData() { log.info(&quot;{}条数据，开始存储数据库！&quot;, cachedDataList.size()); uploadDAO.save(cachedDataList); log.info(&quot;存储数据库成功！&quot;); } } ExcelUtil import com.alibaba.excel.EasyExcelFactory; import com.alibaba.excel.ExcelWriter; import com.alibaba.excel.context.AnalysisContext; import com.alibaba.excel.event.AnalysisEventListener; import com.alibaba.excel.metadata.BaseRowModel; import com.alibaba.excel.metadata.Sheet; import lombok.Data; import lombok.Getter; import lombok.Setter; import lombok.extern.slf4j.Slf4j; import org.springframework.util.CollectionUtils; import org.springframework.util.StringUtils; import java.io.*; import java.util.ArrayList; import java.util.Collections; import java.util.List; @Slf4j public class ExcelUtil { private static Sheet initSheet; static { initSheet = new Sheet(1, 0); initSheet.setSheetName(&quot;sheet&quot;); //设置自适应宽度 initSheet.setAutoWidth(Boolean.TRUE); } /** * 读取少于1000行数据 * @param filePath 文件绝对路径 * @return */ public static List&lt;Object&gt; readLessThan1000Row(String filePath){ return readLessThan1000RowBySheet(filePath,null); } /** * 读小于1000行数据, 带样式 * filePath 文件绝对路径 * initSheet ： * sheetNo: sheet页码，默认为1 * headLineMun: 从第几行开始读取数据，默认为0, 表示从第一行开始读取 * clazz: 返回数据List&lt;Object&gt; 中Object的类名 */ public static List&lt;Object&gt; readLessThan1000RowBySheet(String filePath, Sheet sheet){ if(!StringUtils.hasText(filePath)){ return null; } sheet = sheet != null ? sheet : initSheet; InputStream fileStream = null; try { fileStream = new FileInputStream(filePath); return EasyExcelFactory.read(fileStream, sheet); } catch (FileNotFoundException e) { log.info(&quot;找不到文件或文件路径错误, 文件：{}&quot;, filePath); }finally { try { if(fileStream != null){ fileStream.close(); } } catch (IOException e) { log.info(&quot;excel文件读取失败, 失败原因：{}&quot;, e); } } return null; } /** * 读大于1000行数据 * @param filePath 文件觉得路径 * @return */ public static List&lt;Object&gt; readMoreThan1000Row(String filePath){ return readMoreThan1000RowBySheet(filePath,null); } /** * 读大于1000行数据, 带样式 * @param filePath 文件觉得路径 * @return */ public static List&lt;Object&gt; readMoreThan1000RowBySheet(String filePath, Sheet sheet){ if(!StringUtils.hasText(filePath)){ return null; } sheet = sheet != null ? sheet : initSheet; InputStream fileStream = null; try { fileStream = new FileInputStream(filePath); ExcelListener excelListener = new ExcelListener(); EasyExcelFactory.readBySax(fileStream, sheet, excelListener); return excelListener.getDatas(); } catch (FileNotFoundException e) { log.error(&quot;找不到文件或文件路径错误, 文件：{}&quot;, filePath); }finally { try { if(fileStream != null){ fileStream.close(); } } catch (IOException e) { log.error(&quot;excel文件读取失败, 失败原因：{}&quot;, e); } } return null; } /** * 生成excle * @param filePath 绝对路径, 如：/home/chenmingjian/Downloads/aaa.xlsx * @param data 数据源 * @param head 表头 */ public static void writeBySimple(String filePath, List&lt;List&lt;Object&gt;&gt; data, List&lt;String&gt; head){ writeSimpleBySheet(filePath,data,head,null); } /** * 生成excle * @param filePath 绝对路径, 如：/home/chenmingjian/Downloads/aaa.xlsx * @param data 数据源 * @param sheet excle页面样式 * @param head 表头 */ public static void writeSimpleBySheet(String filePath, List&lt;List&lt;Object&gt;&gt; data, List&lt;String&gt; head, Sheet sheet){ sheet = (sheet != null) ? sheet : initSheet; if(head != null){ List&lt;List&lt;String&gt;&gt; list = new ArrayList&lt;&gt;(); head.forEach(h -&gt; list.add(Collections.singletonList(h))); sheet.setHead(list); } OutputStream outputStream = null; ExcelWriter writer = null; try { outputStream = new FileOutputStream(filePath); writer = EasyExcelFactory.getWriter(outputStream); writer.write1(data,sheet); } catch (FileNotFoundException e) { log.error(&quot;找不到文件或文件路径错误, 文件：{}&quot;, filePath); }finally { try { if(writer != null){ writer.finish(); } if(outputStream != null){ outputStream.close(); } } catch (IOException e) { log.error(&quot;excel文件导出失败, 失败原因：{}&quot;, e); } } } /** * 生成excle * @param filePath 绝对路径, 如：/home/chenmingjian/Downloads/aaa.xlsx * @param data 数据源 */ public static void writeWithTemplate(String filePath, List&lt;? extends BaseRowModel&gt; data){ writeWithTemplateAndSheet(filePath,data,null); } /** * 生成excle * @param filePath 绝对路径, 如：/home/chenmingjian/Downloads/aaa.xlsx * @param data 数据源 * @param sheet excle页面样式 */ public static void writeWithTemplateAndSheet(String filePath, List&lt;? extends BaseRowModel&gt; data, Sheet sheet){ if(CollectionUtils.isEmpty(data)){ return; } sheet = (sheet != null) ? sheet : initSheet; sheet.setClazz(data.get(0).getClass()); OutputStream outputStream = null; ExcelWriter writer = null; try { outputStream = new FileOutputStream(filePath); writer = EasyExcelFactory.getWriter(outputStream); writer.write(data,sheet); } catch (FileNotFoundException e) { log.error(&quot;找不到文件或文件路径错误, 文件：{}&quot;, filePath); }finally { try { if(writer != null){ writer.finish(); } if(outputStream != null){ outputStream.close(); } } catch (IOException e) { log.error(&quot;excel文件导出失败, 失败原因：{}&quot;, e); } } } /** * 生成多Sheet的excle * @param filePath 绝对路径, 如：/home/chenmingjian/Downloads/aaa.xlsx * @param multipleSheelPropetys */ public static void writeWithMultipleSheel(String filePath,List&lt;MultipleSheelPropety&gt; multipleSheelPropetys){ if(CollectionUtils.isEmpty(multipleSheelPropetys)){ return; } OutputStream outputStream = null; ExcelWriter writer = null; try { outputStream = new FileOutputStream(filePath); writer = EasyExcelFactory.getWriter(outputStream); for (MultipleSheelPropety multipleSheelPropety : multipleSheelPropetys) { Sheet sheet = multipleSheelPropety.getSheet() != null ? multipleSheelPropety.getSheet() : initSheet; if(!CollectionUtils.isEmpty(multipleSheelPropety.getData())){ sheet.setClazz(multipleSheelPropety.getData().get(0).getClass()); } writer.write(multipleSheelPropety.getData(), sheet); } } catch (FileNotFoundException e) { log.error(&quot;找不到文件或文件路径错误, 文件：{}&quot;, filePath); }finally { try { if(writer != null){ writer.finish(); } if(outputStream != null){ outputStream.close(); } } catch (IOException e) { log.error(&quot;excel文件导出失败, 失败原因：{}&quot;, e); } } } /*********************匿名内部类开始，可以提取出去******************************/ @Data public static class MultipleSheelPropety{ private List&lt;? extends BaseRowModel&gt; data; private Sheet sheet; } /** * 解析监听器， * 每解析一行会回调invoke()方法。 * 整个excel解析结束会执行doAfterAllAnalysed()方法 */ @Getter @Setter public static class ExcelListener extends AnalysisEventListener { private List&lt;Object&gt; datas = new ArrayList&lt;&gt;(); /** * 逐行解析 * object : 当前行的数据 */ @Override public void invoke(Object object, AnalysisContext context) { //当前行 // context.getCurrentRowNum() if (object != null) { datas.add(object); } } /** * 解析完所有数据后会调用该方法 */ @Override public void doAfterAllAnalysed(AnalysisContext context) { //解析结束销毁不用的资源 } } /************************匿名内部类结束，可以提取出去***************************/ } ","link":"https://tianxiawuhao.github.io/T8lb37RDN/"},{"title":"ES6新语法","content":"const 与 let 变量 使用var带来的麻烦: function getClothing(isCold) { if (isCold) { var freezing = 'Grab a jacket!'; } else { var hot = 'It's a shorts kind of day.'; console.log(freezing); } } 运行getClothing(false)后输出的是undefined,这是因为执行function函数之前,所有变量都会被提升, 提升到函数作用域顶部. let与const声明的变量解决了这种问题,因为他们是块级作用域, 在代码块(用{}表示)中使用let或const声明变量, 该变量会陷入暂时性死区直到该变量的声明被处理. function getClothing(isCold) { if (isCold) { const freezing = 'Grab a jacket!'; } else { const hot = 'It's a shorts kind of day.'; console.log(freezing); } } 运行getClothing(false)后输出的是ReferenceError: freezing is not defined,因为 freezing 没有在 else 语句、函数作用域或全局作用域内声明，所以抛出 ReferenceError。 关于使用let与const规则: 使用let声明的变量可以重新赋值,但是不能在同一作用域内重新声明 使用const声明的变量必须赋值初始化,但是不能在同一作用域类重新声明也无法重新赋值. 模板字面量 在ES6之前,将字符串连接到一起的方法是+或者concat()方法,如 const student = { name: 'Richard Kalehoff', guardian: 'Mr. Kalehoff' }; const teacher = { name: 'Mrs. Wilson', room: 'N231' } let message = student.name + ' please see ' + teacher.name + ' in ' + teacher.room + ' to pick up your report card.'; 模板字面量本质上是包含嵌入式表达式的字符串字面量. 模板字面量用倒引号 ( `` )（而不是单引号 ( '' ) 或双引号( &quot;&quot; )）表示，可以包含用 ${expression} 表示的占位符 let message = `${student.name} please see ${teacher.name} in ${teacher.room} to pick up your report card.`; 解构 在ES6中,可以使用解构从数组和对象提取值并赋值给独特的变量 解构数组的值: const point = [10, 25, -34]; const [x, y, z] = point; console.log(x, y, z); Prints: 10 25 -34 []表示被解构的数组, x,y,z表示要将数组中的值存储在其中的变量, 在解构数组是, 还可以忽略值, 例如const[x,,z]=point,忽略y坐标. 解构对象中的值: const gemstone = { type: 'quartz', color: 'rose', karat: 21.29 }; const {type, color, karat} = gemstone; console.log(type, color, karat); 花括号 { } 表示被解构的对象，type、color 和 karat 表示要将对象中的属性存储到其中的变量 对象字面量简写法 let type = 'quartz'; let color = 'rose'; let carat = 21.29; const gemstone = { type: type, color: color, carat: carat }; console.log(gemstone); 使用和所分配的变量名称相同的名称初始化对象时如果属性名称和所分配的变量名称一样，那么就可以从对象属性中删掉这些重复的变量名称。 let type = 'quartz'; let color = 'rose'; let carat = 21.29; const gemstone = {type,color,carat}; console.log(gemstone); 简写方法的名称: const gemstone = { type, color, carat, calculateWorth: function() { // 将根据类型(type)，颜色(color)和克拉(carat)计算宝石(gemstone)的价值 } }; 匿名函数被分配给属性 calculateWorth，但是真的需要 function 关键字吗？在 ES6 中不需要！ let gemstone = { type, color, carat, calculateWorth() { ... } }; for...of循环 for...of循环是最新添加到 JavaScript 循环系列中的循环。 它结合了其兄弟循环形式 for 循环和 for...in 循环的优势，可以循环任何可迭代（也就是遵守可迭代协议）类型的数据。默认情况下，包含以下数据类型：String、Array、Map 和 Set，注意不包含 Object 数据类型（即 {}）。默认情况下，对象不可迭代。 for循环 const digits = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]; for (let i = 0; i &lt; digits.length; i++) { console.log(digits[i]); } for 循环的最大缺点是需要跟踪计数器和退出条件。 虽然 for 循环在循环数组时的确具有优势，但是某些数据结构不是数组，因此并非始终适合使用 loop 循环。 for...in循环 const digits = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]; for (const index in digits) { console.log(digits[index]); } 依然需要使用 index 来访问数组的值 当你需要向数组中添加额外的方法（或另一个对象）时，for...in 循环会带来很大的麻烦。因为 for...in 循环循环访问所有可枚举的属性，意味着如果向数组的原型中添加任何其他属性，这些属性也会出现在循环中。 Array.prototype.decimalfy = function() { for (let i = 0; i &lt; this.length; i++) { this[i] = this[i].toFixed(2); } }; const digits = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]; for (const index in digits) { console.log(digits[index]); } forEach 循环 forEach 循环是另一种形式的 JavaScript 循环。但是，forEach() 实际上是数组方法，因此只能用在数组中。也无法停止或退出 forEach 循环。如果希望你的循环中出现这种行为，则需要使用基本的 for 循环。 for...of循环 for...of 循环用于循环访问任何可迭代的数据类型。 for...of 循环的编写方式和 for...in 循环的基本一样，只是将 in 替换为 of，可以忽略索引。 const digits = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]; for (const digit of digits) { console.log(digit); } 建议使用复数对象名称来表示多个值的集合。这样，循环该集合时，可以使用名称的单数版本来表示集合中的单个值。例如，for (const button of buttons) {…}。 for...of 循环还具有其他优势，解决了 for 和 for...in 循环的不足之处。你可以随时停止或退出 for...of 循环。 for (const digit of digits) { if (digit % 2 === 0) { continue; } console.log(digit); } 不用担心向对象中添加新的属性。for...of 循环将只循环访问对象中的值。 Array.prototype.decimalfy = function() { for (i = 0; i &lt; this.length; i++) { this[i] = this[i].toFixed(2); } }; const digits = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]; for (const digit of digits) { console.log(digit); } 展开运算符 展开运算符（用三个连续的点 (...) 表示）是 ES6 中的新概念，使你能够将字面量对象展开为多个元素 const books = [&quot;Don Quixote&quot;, &quot;The Hobbit&quot;, &quot;Alice in Wonderland&quot;, &quot;Tale of Two Cities&quot;]; console.log(...books); Prints: Don Quixote The Hobbit Alice in Wonderland Tale of Two Cities 展开运算符的一个用途是结合数组。 如果你需要结合多个数组，在有展开运算符之前，必须使用 Array的 concat() 方法。 const fruits = [&quot;apples&quot;, &quot;bananas&quot;, &quot;pears&quot;]; const vegetables = [&quot;corn&quot;, &quot;potatoes&quot;, &quot;carrots&quot;]; const produce = fruits.concat(vegetables); console.log(produce); Prints: [&quot;apples&quot;, &quot;bananas&quot;, &quot;pears&quot;, &quot;corn&quot;, &quot;potatoes&quot;, &quot;carrots&quot;] 使用展开符来结合数组 const fruits = [&quot;apples&quot;, &quot;bananas&quot;, &quot;pears&quot;]; const vegetables = [&quot;corn&quot;, &quot;potatoes&quot;, &quot;carrots&quot;]; const produce = [...fruits,...vegetables]; console.log(produce); 剩余参数(可变参数) 使用展开运算符将数组展开为多个元素, 使用剩余参数可以将多个元素绑定到一个数组中. 剩余参数也用三个连续的点 ( ... ) 表示，使你能够将不定数量的元素表示为数组. 用途1: 将变量赋数组值时: const order = [20.17, 18.67, 1.50, &quot;cheese&quot;, &quot;eggs&quot;, &quot;milk&quot;, &quot;bread&quot;]; const [total, subtotal, tax, ...items] = order; console.log(total, subtotal, tax, items); 用途2: 可变参数函数 对于参数不固定的函数,ES6之前是使用参数对象(arguments)处理: function sum() { let total = 0; for(const argument of arguments) { total += argument; } return total; } 在ES6中使用剩余参数运算符则更为简洁,可读性提高: function sum(...nums) { let total = 0; for(const num of nums) { total += num; } return total; } ES6箭头函数 ES6之前,使用普通函数把其中每个名字转换为大写形式： const upperizedNames = ['Farrin', 'Kagure', 'Asser'].map(function(name) { return name.toUpperCase(); }); 箭头函数表示: const upperizedNames = ['Farrin', 'Kagure', 'Asser'].map( name =&gt; name.toUpperCase() ); 普通函数可以是函数声明或者函数表达式, 但是箭头函数始终都是表达式, 全程是箭头函数表达式, 因此因此仅在表达式有效时才能使用，包括： 存储在变量中， 当做参数传递给函数， 存储在对象的属性中。 const greet = name =&gt; `Hello ${name}!`; 可以如下调用: greet('Asser'); 如果函数的参数只有一个,不需要使用()包起来,但是只有一个或者多个, 则必须需要将参数列表放在圆括号内: // 空参数列表需要括号 const sayHi = () =&gt; console.log('Hello Udacity Student!'); // 多个参数需要括号 const orderIceCream = (flavor, cone) =&gt; console.log(`Here's your ${flavor} ice cream in a ${cone} cone.`); orderIceCream('chocolate', 'waffle'); 一般箭头函数都只有一个表达式作为函数主题: const upperizedNames = ['Farrin', 'Kagure', 'Asser'].map( name =&gt; name.toUpperCase() ); 这种函数表达式形式称为简写主体语法: 1,在函数主体周围没有花括号, 2,自动返回表达式 3,但是如果箭头函数的主体内需要多行代码, 则需要使用常规主体语法: 它将函数主体放在花括号内 需要使用 return 语句来返回内容。 const upperizedNames = ['Farrin', 'Kagure', 'Asser'].map( name =&gt; { name = name.toUpperCase(); return `${name} has ${name.length} characters in their name`; }); javascript标准函数this new 对象 const mySundae = new Sundae('Chocolate', ['Sprinkles', 'Hot Fudge']); sundae这个构造函数内的this的值是实例对象, 因为他使用new被调用. 指定的对象 const result = obj1.printName.call(obj2); 函数使用call/apply被调用,this的值指向指定的obj2,因为call()第一个参数明确设置this的指向 上下文对象 data.teleport(); 函数是对象的方法, this指向就是那个对象,此处this就是指向data. 全局对象或 undefined teleport(); 此处是this指向全局对象,在严格模式下,指向undefined. javascript中this是很复杂的概念, 要详细判断this,请参考this豁然开朗 箭头函数和this 对于普通函数, this的值基于函数如何被调用, 对于箭头函数,this的值基于函数周围的上下文, 换句话说,this的值和函数外面的this的值是一样的. function IceCream() { this.scoops = 0; } // 为 IceCream 添加 addScoop 方法 IceCream.prototype.addScoop = function() { setTimeout(function() { this.scoops++; console.log('scoop added!'); console.log(this.scoops); // undefined+1=NaN console.log(dessert.scoops); //0 }, 500); }; const dessert = new IceCream(); dessert.addScoop(); 传递给 setTimeout() 的函数被调用时没用到 new、call() 或 apply()，也没用到上下文对象。意味着函数内的 this 的值是全局对象，不是 dessert 对象。实际上发生的情况是，创建了新的 scoops 变量（默认值为 undefined），然后递增（undefined + 1 结果为 NaN）; 解决此问题的方式之一是使用闭包(closure): // 构造函数 function IceCream() { this.scoops = 0; } // 为 IceCream 添加 addScoop 方法 IceCream.prototype.addScoop = function() { const cone = this; // 设置 `this` 给 `cone`变量 setTimeout(function() { cone.scoops++; // 引用`cone`变量 console.log('scoop added!'); console.log(dessert.scoops);//1 }, 0.5); }; const dessert = new IceCream(); dessert.addScoop(); 箭头函数的作用正是如此, 将setTimeOut()的函数改为剪头函数: // 构造函数 function IceCream() { this.scoops = 0; } // 为 IceCream 添加 addScoop 方法 IceCream.prototype.addScoop = function() { setTimeout(() =&gt; { // 一个箭头函数被传递给setTimeout this.scoops++; console.log('scoop added!'); console.log(dessert.scoops);//1 }, 0.5); }; const dessert = new IceCream(); dessert.addScoop(); 默认参数函数 function greet(name, greeting) { name = (typeof name !== 'undefined') ? name : 'Student'; greeting = (typeof greeting !== 'undefined') ? greeting : 'Welcome'; return `${greeting} ${name}!`; } greet(); // Welcome Student! greet('James'); // Welcome James! greet('Richard', 'Howdy'); // Howdy Richard! greet() 函数中混乱的前两行的作用是什么？它们的作用是当所需的参数未提供时，为函数提供默认的值。但是看起来很麻烦, ES6引入一种新的方式创建默认值, 他叫默认函数参数: function greet(name = 'Student', greeting = 'Welcome') { return `${greeting} ${name}!`; } greet(); // Welcome Student! greet('James'); // Welcome James! greet('Richard', 'Howdy'); // Howdy Richard! 默认值与解构 默认值与解构数组 function createGrid([width = 5, height = 5]) { return `Generates a ${width} x ${height} grid`; } createGrid([]); // Generates a 5 x 5 grid createGrid([2]); // Generates a 2 x 5 grid createGrid([2, 3]); // Generates a 2 x 3 grid createGrid([undefined, 3]); // Generates a 5 x 3 grid createGrid() 函数预期传入的是数组。它通过解构将数组中的第一项设为 width，第二项设为 height。如果数组为空，或者只有一项，那么就会使用默认参数，并将缺失的参数设为默认值 5。 但是存在一个问题: createGrid(); // throws an error Uncaught TypeError: Cannot read property 'Symbol(Symbol.iterator)' of undefined 出现错误，因为 createGrid() 预期传入的是数组，然后对其进行解构。因为函数被调用时没有传入数组，所以出现问题。但是，我们可以使用默认的函数参数！ function createGrid([width = 5, height = 5] = []) { return `Generating a grid of ${width} by ${height}`; } createGrid(); // Generates a 5 x 5 grid Returns: Generates a 5 x 5 grid 默认值与解构函数 就像使用数组默认值解构数组一样，函数可以让对象成为一个默认参数，并使用对象解构： function createSundae({scoops = 1, toppings = ['Hot Fudge']}={}) { const scoopText = scoops === 1 ? 'scoop' : 'scoops'; return `Your sundae has ${scoops} ${scoopText} with ${toppings.join(' and ')} toppings.`; } createSundae({}); // Your sundae has 1 scoop with Hot Fudge toppings. createSundae({scoops: 2}); // Your sundae has 2 scoops with Hot Fudge toppings. createSundae({scoops: 2, toppings: ['Sprinkles']}); // Your sundae has 2 scoops with Sprinkles toppings. createSundae({toppings: ['Cookie Dough']}); // Your sundae has 1 scoop with Cookie Dough toppings. createSundae(); // Your sundae has 1 scoop with Hot Fudge toppings. 数组默认值与对象默认值 默认函数参数只是个简单的添加内容，但是却带来很多便利！与数组默认值相比，对象默认值具备的一个优势是能够处理跳过的选项。看看下面的代码： function createSundae({scoops = 1, toppings = ['Hot Fudge']} = {}) { … } 在 createSundae() 函数使用对象默认值进行解构时，如果你想使用 scoops 的默认值，但是更改 toppings，那么只需使用 toppings 传入一个对象： createSundae({toppings: ['Hot Fudge', 'Sprinkles', 'Caramel']}); 将上述示例与使用数组默认值进行解构的同一函数相对比。 function createSundae([scoops = 1, toppings = ['Hot Fudge']] = []) { … } 对于这个函数，如果想使用 scoops 的默认数量，但是更改 toppings，则必须以这种奇怪的方式调用你的函数： createSundae([undefined, ['Hot Fudge', 'Sprinkles', 'Caramel']]); 因为数组是基于位置的，我们需要传入 undefined 以跳过第一个参数（并使用默认值）来到达第二个参数。 Javascript类 ES5创建类: function Plane(numEngines) { this.numEngines = numEngines; this.enginesActive = false; } // 由所有实例 &quot;继承&quot; 的方法 Plane.prototype.startEngines = function () { console.log('starting engines...'); this.enginesActive = true; }; ES6创建类 ES6类只是一个语法糖,原型继续实际上在底层隐藏起来, 与传统类机制语言有些区别. class Plane { //constructor方法虽然在类中,但不是原型上的方法,只是用来生成实例的. constructor(numEngines) { this.numEngines = numEngines; this.enginesActive = false; } //原型上的方法, 由所有实例对象共享. startEngines() { console.log('starting engines…'); this.enginesActive = true; } } console.log(typeof Plane); //function javascript中类其实只是function, 方法之间不能使用,,不用逗号区分属性和方法. 静态方法 要添加静态方法，请在方法名称前面加上关键字 static class Plane { constructor(numEngines) { this.numEngines = numEngines; this.enginesActive = false; } static badWeather(planes) { for (plane of planes) { plane.enginesActive = false; } } startEngines() { console.log('starting engines…'); this.enginesActive = true; } } 关键字class带来其他基于类的语言的很多思想,但是没有向javascript中添加此功能 javascript类实际上还是原型继承 创建javascript类的新实例时必须使用new关键字 super 和 extends 使用新的super和extends关键字扩展类: class Tree { constructor(size = '10', leaves = {spring: 'green', summer: 'green', fall: 'orange', winter: null}) { this.size = size; this.leaves = leaves; this.leafColor = null; } changeSeason(season) { this.leafColor = this.leaves[season]; if (season === 'spring') { this.size += 1; } } } class Maple extends Tree { constructor(syrupQty = 15, size, leaves) { super(size, leaves); //super用作函数 this.syrupQty = syrupQty; } changeSeason(season) { super.changeSeason(season);//super用作对象 if (season === 'spring') { this.syrupQty += 1; } } gatherSyrup() { this.syrupQty -= 3; } } 使用ES5编写同样功能的类: function Tree(size, leaves) { this.size = size || 10; this.leaves = leaves || {spring: 'green', summer: 'green', fall: 'orange', winter: null}; this.leafColor; } Tree.prototype.changeSeason = function(season) { this.leafColor = this.leaves[season]; if (season === 'spring') { this.size += 1; } } function Maple (syrupQty, size, leaves) { Tree.call(this, size, leaves); this.syrupQty = syrupQty || 15; } Maple.prototype = Object.create(Tree.prototype); Maple.prototype.constructor = Maple; Maple.prototype.changeSeason = function(season) { Tree.prototype.changeSeason.call(this, season); if (season === 'spring') { this.syrupQty += 1; } } Maple.prototype.gatherSyrup = function() { this.syrupQty -= 3; } super 必须在 this 之前被调用 在子类构造函数中，在使用 this 之前，必须先调用超级类。 class Apple {} class GrannySmith extends Apple { constructor(tartnessLevel, energy) { this.tartnessLevel = tartnessLevel; // 在 'super' 之前会抛出一个错误！ super(energy); } } ","link":"https://tianxiawuhao.github.io/MKTk7DnaM/"},{"title":"springBoot参数和业务校验","content":"为什么需要参数校验 在日常的接口开发中，为了防止非法参数对业务造成影响，经常需要对接口的参数做校验，例如登录的时候需要校验用户名密码是否为空，创建用户的时候需要校验邮件、手机号码格式是否准确。靠代码对接口参数一个个校验的话就太繁琐了，代码可读性极差。 Validator框架就是为了解决开发人员在开发的时候少写代码，提升开发效率 Validator校验框架遵循了JSR-303验证规范（参数校验规范）, JSR是Java Specification Requests的缩写。 接下来我们看看在SpringbBoot中如何集成参数校验框架。 SpringBoot中集成参数校验 第一步，引入依赖 &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-validation&lt;/artifactId&gt; &lt;/dependency&gt; 注：从springboot-2.3开始，校验包被独立成了一个starter组件，所以需要引入validation和web，而springboot-2.3之前的版本只需要引入 web 依赖就可以了。 第二步，定义要参数校验的实体类 @Data public class ValidVO { private String id; @Length(min = 6,max = 12,message = &quot;appId长度必须位于6到12之间&quot;) private String appId; @NotBlank(message = &quot;名字为必填项&quot;) private String name; @Email(message = &quot;请填写正确的邮箱地址&quot;) private String email; private String sex; @NotEmpty(message = &quot;级别不能为空&quot;) private String level; } 在实际开发中对于需要校验的字段都需要设置对应的业务提示，即message属性。 常见的约束注解如下： 注解 功能 @AssertFalse 可以为null,如果不为null的话必须为false @AssertTrue 可以为null,如果不为null的话必须为true @DecimalMax 设置不能超过最大值 @DecimalMin 设置不能超过最小值 @Digits 设置必须是数字且数字整数的位数和小数的位数必须在指定范围内 @Future 日期必须在当前日期的未来 @Past 日期必须在当前日期的过去 @Max 最大不得超过此最大值 @Min 最大不得小于此最小值 @NotNull 不能为null，可以是空 @Null 必须为null @Pattern 必须满足指定的正则表达式 @Size 集合、数组、map等的size()值必须在指定范围内 @Email 必须是email格式 @Length 长度必须在指定范围内 @NotBlank 字符串不能为null,字符串trim()后也不能等于“” @NotEmpty 不能为null，集合、数组、map等size()不能为0；字符串trim()后可以等于“” @Range 值必须在指定范围内 @URL 必须是一个URL 注：此表格只是简单的对注解功能的说明，并没有对每一个注解的属性进行说明；可详见源码。 第三步，定义校验类进行测试 @RestController @Slf4j @Validated public class ValidController { @ApiOperation(&quot;RequestBody校验&quot;) @PostMapping(&quot;/valid/test1&quot;) public String test1(@Validated @RequestBody ValidVO validVO){ log.info(&quot;validEntity is {}&quot;, validVO); return &quot;test1 valid success&quot;; } @ApiOperation(&quot;Form校验&quot;) @PostMapping(value = &quot;/valid/test2&quot;) public String test2(@Validated ValidVO validVO){ log.info(&quot;validEntity is {}&quot;, validVO); return &quot;test2 valid success&quot;; } @ApiOperation(&quot;单参数校验&quot;) @PostMapping(value = &quot;/valid/test3&quot;) public String test3(@Email String email){ log.info(&quot;email is {}&quot;, email); return &quot;email valid success&quot;; } } 这里我们先定义三个方法test1，test2，test3，test1使用了@RequestBody注解，用于接受前端发送的json数据，test2模拟表单提交，test3模拟单参数提交。注意，当使用单参数校验时需要在Controller上加上@Validated注解，否则不生效。 第四步，体验效果 调用test1方法，提示的是org.springframework.web.bind.MethodArgumentNotValidException异常 POST http://localhost:8080/valid/test1 Content-Type: application/json { &quot;id&quot;: 1, &quot;level&quot;: &quot;12&quot;, &quot;email&quot;: &quot;47693899&quot;, &quot;appId&quot;: &quot;ab1c&quot; } { &quot;status&quot;: 500, &quot;message&quot;: &quot;Validation failed for argument [0] in public java.lang.String com.jianzh5.blog.valid.ValidController.test1(com.jianzh5.blog.valid.ValidVO) with 3 errors: [Field error in object 'validVO' on field 'email': rejected value [47693899]; codes [Email.validVO.email,Email.email,Email.java.lang.String,Email]; arguments [org.springframework.context.support.DefaultMessageSourceResolvable: codes [validVO.email,email]; arguments []; default message [email],[Ljavax.validation.constraints.Pattern$Flag;@26139123,.*]; default message [不是一个合法的电子邮件地址]]...&quot;, &quot;data&quot;: null, &quot;timestamp&quot;: 1628239624332 } 调用test2方法，提示的是org.springframework.validation.BindException异常 POST http://localhost:8080/valid/test2 Content-Type: application/x-www-form-urlencoded id=1&amp;level=12&amp;email=476938977&amp;appId=ab1c { &quot;status&quot;: 500, &quot;message&quot;: &quot;org.springframework.validation.BeanPropertyBindingResult: 3 errors\\nField error in object 'validVO' on field 'name': rejected value [null]; codes [NotBlank.validVO.name,NotBlank.name,NotBlank.java.lang.String,NotBlank]; arguments [org.springframework.context.support.DefaultMessageSourceResolvable: codes [validVO.name,name]; arguments []; default message [name]]; default message [名字为必填项]...&quot;, &quot;data&quot;: null, &quot;timestamp&quot;: 1628239301951 } 调用test3方法，提示的是javax.validation.ConstraintViolationException异常 POST http://localhost:8080/valid/test3 Content-Type: application/x-www-form-urlencoded email=476938977 { &quot;status&quot;: 500, &quot;message&quot;: &quot;test3.email: 不是一个合法的电子邮件地址&quot;, &quot;data&quot;: null, &quot;timestamp&quot;: 1628239281022 } 通过加入Validator校验框架可以帮助我们自动实现参数的校验。 参数异常加入全局异常处理器 Validator校验框架返回的错误提示太臃肿了，不便于阅读，为了方便前端提示，我们需要将其简化一下。 创建RestExceptionHandler，单独拦截参数校验的三个异常：javax.validation.ConstraintViolationException，org.springframework.validation.BindException，org.springframework.web.bind.MethodArgumentNotValidException，代码如下： @ExceptionHandler(value = {BindException.class, ValidationException.class, MethodArgumentNotValidException.class}) public ResponseEntity&lt;ResultData&lt;String&gt;&gt; handleValidatedException(Exception e) { ResultData&lt;String&gt; resp = null; if (e instanceof MethodArgumentNotValidException) { // BeanValidation exception MethodArgumentNotValidException ex = (MethodArgumentNotValidException) e; resp = ResultData.fail(HttpStatus.BAD_REQUEST.value(), ex.getBindingResult().getAllErrors().stream() .map(ObjectError::getDefaultMessage) .collect(Collectors.joining(&quot;; &quot;)) ); } else if (e instanceof ConstraintViolationException) { // BeanValidation GET simple param ConstraintViolationException ex = (ConstraintViolationException) e; resp = ResultData.fail(HttpStatus.BAD_REQUEST.value(), ex.getConstraintViolations().stream() .map(ConstraintViolation::getMessage) .collect(Collectors.joining(&quot;; &quot;)) ); } else if (e instanceof BindException) { // BeanValidation GET object param BindException ex = (BindException) e; resp = ResultData.fail(HttpStatus.BAD_REQUEST.value(), ex.getAllErrors().stream() .map(ObjectError::getDefaultMessage) .collect(Collectors.joining(&quot;; &quot;)) ); } return new ResponseEntity&lt;&gt;(resp,HttpStatus.BAD_REQUEST); } 体验效果 POST http://localhost:8080/valid/test1 Content-Type: application/json { &quot;id&quot;: 1, &quot;level&quot;: &quot;12&quot;, &quot;email&quot;: &quot;47693899&quot;, &quot;appId&quot;: &quot;ab1c&quot; } { &quot;status&quot;: 400, &quot;message&quot;: &quot;名字为必填项; 不是一个合法的电子邮件地址; appId长度必须位于6到12之间&quot;, &quot;data&quot;: null, &quot;timestamp&quot;: 1628435116680 } 是不是感觉清爽多了？ 自定义参数校验 虽然Spring Validation 提供的注解基本上够用，但是面对复杂的定义，我们还是需要自己定义相关注解来实现自动校验。 比如上面实体类中的sex性别属性，只允许前端传递传 M，F 这2个枚举值，如何实现呢？ 第一步，创建自定义注解 @Target({METHOD, FIELD, ANNOTATION_TYPE, CONSTRUCTOR, PARAMETER, TYPE_USE}) @Retention(RUNTIME) @Repeatable(EnumString.List.class) @Documented @Constraint(validatedBy = EnumStringValidator.class)//标明由哪个类执行校验逻辑 public @interface EnumString { String message() default &quot;value not in enum values.&quot;; Class&lt;?&gt;[] groups() default {}; Class&lt;? extends Payload&gt;[] payload() default {}; /** * @return date must in this value array */ String[] value(); /** * Defines several {@link EnumString} annotations on the same element. * * @see EnumString */ @Target({METHOD, FIELD, ANNOTATION_TYPE, CONSTRUCTOR, PARAMETER, TYPE_USE}) @Retention(RUNTIME) @Documented @interface List { EnumString[] value(); } } 第二步，自定义校验逻辑 public class EnumStringValidator implements ConstraintValidator&lt;EnumString, String&gt; { private List&lt;String&gt; enumStringList; @Override public void initialize(EnumString constraintAnnotation) { enumStringList = Arrays.asList(constraintAnnotation.value()); } @Override public boolean isValid(String value, ConstraintValidatorContext context) { if(value == null){ return true; } return enumStringList.contains(value); } } 第三步，在字段上增加注解 @ApiModelProperty(value = &quot;性别&quot;) @EnumString(value = {&quot;F&quot;,&quot;M&quot;}, message=&quot;性别只允许为F或M&quot;) private String sex; 第四步，体验效果 POST http://localhost:8080/valid/test2 Content-Type: application/x-www-form-urlencoded id=1&amp;name=javadaily&amp;level=12&amp;email=476938977@qq.com&amp;appId=ab1cdddd&amp;sex=N { &quot;status&quot;: 400, &quot;message&quot;: &quot;性别只允许为F或M&quot;, &quot;data&quot;: null, &quot;timestamp&quot;: 1628435243723 } 分组校验 一个VO对象在新增的时候某些字段为必填，在更新的时候又非必填。如上面的ValidVO中 id 和 appId 属性在新增操作时都是非必填，而在编辑操作时都为必填，name在新增操作时为必填，面对这种场景你会怎么处理呢？ 在实际开发中我见到很多同学都是建立两个VO对象，ValidCreateVO，ValidEditVO来处理这种场景，这样确实也能实现效果，但是会造成类膨胀，而且极其容易被开发老鸟们嘲笑。 其实Validator校验框架已经考虑到了这种场景并且提供了解决方案，就是分组校验，只不过很多同学不知道而已。要使用分组校验，只需要三个步骤： 第一步：定义分组接口 public interface ValidGroup extends Default { interface Crud extends ValidGroup{ interface Create extends Crud{ } interface Update extends Crud{ } interface Query extends Crud{ } interface Delete extends Crud{ } } } 这里我们定义一个分组接口ValidGroup让其继承javax.validation.groups.Default，再在分组接口中定义出多个不同的操作类型，Create，Update，Query，Delete。至于为什么需要继承Default我们稍后再说。 第二步，在模型中给参数分配分组 @Data @ApiModel(value = &quot;参数校验类&quot;) public class ValidVO { @ApiModelProperty(&quot;ID&quot;) @Null(groups = ValidGroup.Crud.Create.class) @NotNull(groups = ValidGroup.Crud.Update.class, message = &quot;应用ID不能为空&quot;) private String id; @Null(groups = ValidGroup.Crud.Create.class) @NotNull(groups = ValidGroup.Crud.Update.class, message = &quot;应用ID不能为空&quot;) @ApiModelProperty(value = &quot;应用ID&quot;,example = &quot;cloud&quot;) private String appId; @ApiModelProperty(value = &quot;名字&quot;) @NotBlank(groups = ValidGroup.Crud.Create.class,message = &quot;名字为必填项&quot;) private String name; @ApiModelProperty(value = &quot;邮箱&quot;) @Email(message = &quot;请填写正取的邮箱地址&quot;) privte String email; ... } 给参数指定分组，对于未指定分组的则使用的是默认分组。 第三步，给需要参数校验的方法指定分组 @RestController @Api(&quot;参数校验&quot;) @Slf4j @Validated public class ValidController { @ApiOperation(&quot;新增&quot;) @PostMapping(value = &quot;/valid/add&quot;) public String add(@Validated(value = ValidGroup.Crud.Create.class) ValidVO validVO){ log.info(&quot;validEntity is {}&quot;, validVO); return &quot;test3 valid success&quot;; } @ApiOperation(&quot;更新&quot;) @PostMapping(value = &quot;/valid/update&quot;) public String update(@Validated(value = ValidGroup.Crud.Update.class) ValidVO validVO){ log.info(&quot;validEntity is {}&quot;, validVO); return &quot;test4 valid success&quot;; } } 这里我们通过value属性给add()和update()方法分别指定Create和Update分组。 第四步，体验效果 POST http://localhost:8080/valid/add Content-Type: application/x-www-form-urlencoded name=javadaily&amp;level=12&amp;email=476938977@qq.com&amp;sex=F 在Create时我们没有传递id和appId参数，校验通过。 当我们使用同样的参数调用update方法时则提示参数校验错误。 { &quot;status&quot;: 400, &quot;message&quot;: &quot;ID不能为空; 应用ID不能为空&quot;, &quot;data&quot;: null, &quot;timestamp&quot;: 1628492514313 } 由于email属于默认分组，而我们的分组接口ValidGroup已经继承了Default分组，所以也是可以对email字段作参数校验的。如： POST http://localhost:8080/valid/add Content-Type: application/x-www-form-urlencoded name=javadaily&amp;level=12&amp;email=476938977&amp;sex=F { &quot;status&quot;: 400, &quot;message&quot;: &quot;请填写正取的邮箱地址&quot;, &quot;data&quot;: null, &quot;timestamp&quot;: 1628492637305 } 当然如果你的ValidGroup没有继承Default分组，那在代码属性上就需要加上@Validated(value = {ValidGroup.Crud.Create.class, Default.class}才能让email字段的校验生效。 业务规则校验 业务规则校验指接口需要满足某些特定的业务规则，举个例子：业务系统的用户需要保证其唯一性，用户属性不能与其他用户产生冲突，不允许与数据库中任何已有用户的用户名称、手机号码、邮箱产生重复。 这就要求在创建用户时需要校验用户名称、手机号码、邮箱是否被注册；编辑用户时不能将信息修改成已有用户的属性。 95%的程序员当面对这种业务规则校验时往往选择写在service逻辑中，常见的代码逻辑如下： public void create(User user) { Account account = accountDao.queryByUserNameOrPhoneOrEmail(user.getName(),user.getPhone(),user.getEmail()); if (account != null) { throw new IllegalArgumentException(&quot;用户已存在，请重新输入&quot;); } } 最优雅的实现方法应该是参考 Bean Validation 的标准方式，借助自定义校验注解完成业务规则校验。 接下来我们通过上面提到的用户接口案例，通过自定义注解完成业务规则校验。 代码实战 需求很容易理解，注册新用户时，应约束不与任何已有用户的关键信息重复；而修改自己的信息时，只能与自己的信息重复，不允许修改成已有用户的信息。 这些约束规则不仅仅为这两个方法服务，它们可能会在用户资源中的其他入口被使用到，乃至在其他分层的代码中被使用到，在 Bean 上做校验就能全部覆盖上述这些使用场景。 自定义注解 首先我们需要创建两个自定义注解，用于业务规则校验： UniqueUser:表示一个用户是唯一的，唯一性包含：用户名，手机号码、邮箱 @Documented @Retention(RUNTIME) @Target({FIELD, METHOD, PARAMETER, TYPE}) @Constraint(validatedBy = UserValidation.UniqueUserValidator.class) public @interface UniqueUser { String message() default &quot;用户名、手机号码、邮箱不允许与现存用户重复&quot;; Class&lt;?&gt;[] groups() default {}; Class&lt;? extends Payload&gt;[] payload() default {}; } NotConflictUser:表示一个用户的信息是无冲突的，无冲突是指该用户的敏感信息与其他用户不重合 @Documented @Retention(RUNTIME) @Target({FIELD, METHOD, PARAMETER, TYPE}) @Constraint(validatedBy = UserValidation.NotConflictUserValidator.class) public @interface NotConflictUser { String message() default &quot;用户名称、邮箱、手机号码与现存用户产生重复&quot;; Class&lt;?&gt;[] groups() default {}; Class&lt;? extends Payload&gt;[] payload() default {}; } 实现业务校验规则 想让自定义验证注解生效，需要实现 ConstraintValidator 接口。接口的第一个参数是 自定义注解类型，第二个参数是 被注解字段的类，因为需要校验多个参数，我们直接传入用户对象。需要提到的一点是 ConstraintValidator 接口的实现类无需添加 @Component 它在启动的时候就已经被加载到容器中了。 @Slf4j public class UserValidation&lt;T extends Annotation&gt; implements ConstraintValidator&lt;T, User&gt; { protected Predicate&lt;User&gt; predicate = c -&gt; true; @Resource protected UserRepository userRepository; @Override public boolean isValid(User user, ConstraintValidatorContext constraintValidatorContext) { return userRepository == null || predicate.test(user); } /** * 校验用户是否唯一 * 即判断数据库是否存在当前新用户的信息，如用户名，手机，邮箱 */ public static class UniqueUserValidator extends UserValidation&lt;UniqueUser&gt;{ @Override public void initialize(UniqueUser uniqueUser) { predicate = c -&gt; !userRepository.existsByUserNameOrEmailOrTelphone(c.getUserName(),c.getEmail(),c.getTelphone()); } } /** * 校验是否与其他用户冲突 * 将用户名、邮件、电话改成与现有完全不重复的，或者只与自己重复的，就不算冲突 */ public static class NotConflictUserValidator extends UserValidation&lt;NotConflictUser&gt;{ @Override public void initialize(NotConflictUser notConflictUser) { predicate = c -&gt; { log.info(&quot;user detail is {}&quot;,c); Collection&lt;User&gt; collection = userRepository.findByUserNameOrEmailOrTelphone(c.getUserName(), c.getEmail(), c.getTelphone()); // 将用户名、邮件、电话改成与现有完全不重复的，或者只与自己重复的，就不算冲突 return collection.isEmpty() || (collection.size() == 1 &amp;&amp; collection.iterator().next().getId().equals(c.getId())); }; } } } 这里使用Predicate函数式接口对业务规则进行判断。 使用 @RestController @RequestMapping(&quot;/senior/user&quot;) @Slf4j @Validated public class UserController { @Autowired private UserRepository userRepository; @PostMapping public User createUser(@UniqueUser @Valid User user){ User savedUser = userRepository.save(user); log.info(&quot;save user id is {}&quot;,savedUser.getId()); return savedUser; } @SneakyThrows @PutMapping public User updateUser(@NotConflictUser @Valid @RequestBody User user){ User editUser = userRepository.save(user); log.info(&quot;update user is {}&quot;,editUser); return editUser; } } 使用很简单，只需要在方法上加入自定义注解即可，业务逻辑中不需要添加任何业务规则的代码。 测试 调用接口后出现如下错误，说明业务规则校验生效。 { &quot;status&quot;: 400, &quot;message&quot;: &quot;用户名、手机号码、邮箱不允许与现存用户重复&quot;, &quot;data&quot;: null, &quot;timestamp&quot;: 1644309081037 } 小结 通过上面几步操作，业务校验便和业务逻辑就完全分离开来，在需要校验时用@Validated注解自动触发，或者通过代码手动触发执行，可根据你们项目的要求，将这些注解应用于控制器、服务层、持久层等任何层次的代码之中。 这种方式比任何业务规则校验的方法都优雅，推荐大家在项目中使用。在开发时可以将不带业务含义的格式校验注解放到 Bean 的类定义之上，将带业务逻辑的校验放到 Bean 的类定义的外面。这两者的区别是放在类定义中的注解能够自动运行，而放到类外面则需要像前面代码那样，明确标出注解时才会运行。 ","link":"https://tianxiawuhao.github.io/d4XUQj9K0/"},{"title":"WebFlux与WebMVC区别","content":"在构建响应式 Web 服务上，Spring 5 中引入了全新的编程框架，那就是 Spring WebFlux。作为一款新型的 Web 服务开发框架，它与传统的 WebMVC 相比具体有哪些优势呢？ Spring WebFlux 的应用场景 WebFlux 用于构建响应式 Web 服务。在详细介绍 WebFlux 之前，我们先梳理一下这个新框架的应用场景，了解应用场景才能帮助我们对所要采用的技术体系做出正确的选择。 微服务架构的兴起为 WebFlux 的应用提供了一个很好的场景。我们知道在一个微服务系统中，存在数十乃至数百个独立的微服务，它们相互通信以完成复杂的业务流程。这个过程势必会涉及大量的 I/O 操作，尤其是阻塞式 I/O 操作会整体增加系统的延迟并降低吞吐量。如果能够在复杂的流程中集成非阻塞、异步通信机制，我们就可以高效处理跨服务之间的网络请求。针对这种场景，WebFlux 是一种非常有效的解决方案。 从 WebMVC 到 WebFlux 接下来，我们将讨论 WebMVC 与 WebFlux 之间的差别，而这些差别实际上正是体现在从 WebMVC 到 WebFlux 的演进过程中。让我们先从传统的 Spring WebMVC 技术栈开始说起。 Spring WebMVC技术栈 一般而言，Web 请求处理机制都会使用“管道-过滤器（Pipe-Filter）”架构模式，而 Spring WebMVC 作为一种处理 Web 请求的典型实现方案，同样使用了 Servlet 中的过滤器链（FilterChain）来对请求进行拦截，如下图所示。 我们知道 WebMVC 运行在 Servlet 容器上，这些容器常用的包括 Tomcat、JBoss 等。当 HTTP 请求通过 Servlet 容器时就会被转换为一个 ServletRequest 对象，而最终返回一个 ServletResponse 对象，FilterChain 的定义如下所示。 public interface FilterChain { public void doFilter (ServletRequest request, ServletResponse response ) throws IOException, ServletException; } 当 ServletRequest 通过过滤器链中所包含的一系列过滤器之后，最终就会到达作为前端控制器的 DispatcherServlet。DispatcherServlet 是 WebMVC 的核心组件，扩展了 Servlet 对象，并持有一组 HandlerMapping 和 HandlerAdapter。 当 ServletRequest 请求到达时，DispatcherServlet 负责搜索 HandlerMapping 实例并使用合适的 HandlerAdapter 对其进行适配。其中，HandlerMapping 的作用是根据当前请求找到对应的处理器 Handler，它只定义了一个方法，如下所示。 public interface HandlerMapping { //找到与请求对应的 Handler，封装为一个 HandlerExecutionChain 返回 HandlerExecutionChain getHandler(HttpServletRequest request) throws Exception; } 而 HandlerAdapter 根据给定的 HttpServletRequest 和 HttpServletResponse 对象真正调用给定的 Handler，核心方法如下所示。 public interface HandlerAdapter { //针对给定的请求/响应对象调用目标 Handler ModelAndView handle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception; } 在执行过程中，DispatcherServlet 会在应用上下文中搜索所有 HandlerMapping。日常开发过程中，最常用的 HandlerMapping 包含 BeanNameUrlHandlerMapping 和 RequestMappingHandlerMapping，前者负责检测所有 Controller 并根据请求 URL 的匹配规则映射到具体的 Controller 实例上，而后者基于 @RequestMapping 注解来找到目标 Controller。 如果我们使用了 RequestMappingHandlerMapping，那么对应的 HandlerAdapter 就是 RequestMappingHandlerAdapter，它负责将传入的 ServletRequest 绑定到添加了 @RequestMapping 注解的控制器方法上，从而实现对请求的正确响应。同时， HandlerAdapter 还提供请求验证和响应转换等辅助性功能，使得 Spring WebMVC 框架在日常 Web 开发中非常实用。 作为总结，我梳理了 Spring WebMVC 的整体架构，如下图所示。 一直以来，Spring WebMVC 是我们开发 Web 服务的主流框架。但要注意的是，尽管 Servlet 本身在新版本中提供了异步非阻塞的通信机制，但 Spring WebMVC 在实现上并不允许在整个请求生命周期中都采用非阻塞式的操作方式。因此，Spring 在尽量沿用原有的开发模式以及 API 设计上提供了支持异步非阻塞的 Spring WebFlux 框架。 Spring WebFlux 技术栈 介绍完 Spring WebMVC，我们来说说 Spring WebFlux。事实上，前面介绍的 HandlerMapping、HandlerAdapter 等组件在 WebFlux 里都有同名的响应式版本，这是 WebFlux 的一种设计理念，即在既有设计的基础上，提供新的实现版本，只对部分需要增强和弱化的地方做了调整。 我们先来看第一个需要调整的地方，显然，我们应该替换掉原有的 Servlet API 以便融入响应式流。因此，在 WebFlux 中，代表请求和响应的是全新的 ServerHttpRequest 和 ServerHttpResponse 对象。 同样，WebFlux 中同样提供了一个过滤器链 WebFilterChain，定义如下。 public interface WebFilterChain { Mono&lt;Void&gt; filter(ServerWebExchange exchange); } 这里的 ServerWebExchange 相当于一个上下文容器，保存了 ServerHttpRequest、ServerHttpResponse 以及一些框架运行时状态信息。 在 WebFlux 中，和 WebMVC 中的 DispatcherServlet 相对应的组件是 DispatcherHandler。与 DispatcherServlet 类似，DispatcherHandler 同样使用了一套响应式版本的 HandlerMapping 和 HandlerAdapter 完成对请求的处理。请注意，这两个接口是定义在 org.springframework.web.reactive 包中，而不是在原有的 org.springframework.web 包中。响应式版本的 HandlerMapping 接口定义如下，可以看到这里返回的是一个 Mono 对象，从而启用了响应式行为模式。 public interface HandlerMapping { Mono&lt;Object&gt; getHandler(ServerWebExchange exchange); } 同样，我们找到响应式版本的 HandlerAdapter，如下所示。 public interface HandlerAdapter { Mono&lt;HandlerResult&gt; handle(ServerWebExchange exchange, Object handler); } 对比非响应式版本的 HandlerAdapter，这里的 ServerWebExchange 中同时包含了 ServerHttpRequest 和 ServerHttpResponse 对象，而 HandlerResult 则代表了处理结果。相比 WebMVC 中 ModelAndView 这种比较模糊的返回结果，HandlerResult 更加直接和明确。 在 WebFlux 中，同样实现了响应式版本的 RequestMappingHandlerMapping 和 RequestMappingHandlerAdapter，因此我们仍然可以采用注解的方法来构建 Controller。另一方面，WebFlux 中还提供了 RouterFunctionMapping 和 HandlerFunctionAdapter 组合，专门用来提供基于函数式编程的开发模式。这样 Spring WebFlux 的整体架构图就演变成这样。 请注意，在处理 HTTP 请求上，我们需要使用支持异步非阻塞的响应式服务器引擎，常见的包括 Netty、Undertow 以及支持 Servlet 3.1 及以上版本的 Servlet 容器。 对比 WebFlux 和 WebMVC 的处理模型 现在我们已经明确了 WebMVC 到 WebFlux 的演进过程，但你可能会问，新的 WebFlux 要比传统 WebMVC 好在哪里呢？从两者的处理模型上入手可以帮助你很好地理解这个问题，我们一起来看一下。 WebFlux 和 Web MVC 中的处理模型 通过前面的讨论你已经知道 Servlet 是阻塞式的，所以 WebMVC 建立在阻塞 I/O 之上，我们来分析这种模型下线程处理请求的过程。假设有一个工作线程会处理来自客户端的请求，所有请求构成一个请求队列，并由一个线程按顺序进行处理。针对一个请求，线程需要执行两部分工作，首先是接受请求，然后再对其进行处理，如下图所示。 在前面的示例中，正如你可能注意到的，工作线程的实际处理时间远小于花费在阻塞操作上的时间。这意味着工作线程会被 I/O 读取或写入数据这一操作所阻塞。从这个简单的图中，我们可以得出结论，线程效率低下。同时，因为所有请求是排队的，相当于一个请求队列，所以接受请求和处理请求这两部分操作实际上是可以共享等待时间的。 相比之下，WebFlux 构建在非阻塞 API 之上，这意味着没有操作需要与 I/O 阻塞线程进行交互。接受和处理请求的效率很高，如下图所示。 将上图中所展示的异步非阻塞请求处理与前面的阻塞过程进行比较，我们会注意到，现在没有在读取请求数据时发生等待，工作线程高效接受新连接。然后，提供了非阻塞 I/O 机制的底层操作系统会告诉我们请求数据是否已经接收完成，并且处理器可以在不阻塞的情况下进行处理。 类似的，写入响应结果时同样不需要阻塞，操作系统会在准备好将一部分数据非阻塞地写入 I/O 时通知我们。这样，我们就拥有了最佳的 CPU 利用率。 前面的示例展示了 WebFlux 比 WebMVC 更有效地利用一个工作线程，因此可以在相同的时间内处理更多的请求。那么，如果是在多线程的场景下会发生什么呢？我们来看下面这张图。 从上图中可以看出，多线程模型允许更快地处理排队请求，能够同时接受、处理和响应几乎相同数量的请求。当然，我们明白多线程技术有利有弊。当处理用户请求涉及太多的线程实例时，相互之间就需要协调资源，这是由于它们之间的不一致性会导致性能下降。 ","link":"https://tianxiawuhao.github.io/OoSJ8O7Jf/"},{"title":"Spring Boot 自带 Buff 工具类","content":"断言 断言是一个逻辑判断，用于检查不应该发生的情况 Assert 关键字在 JDK1.4 中引入，可通过 JVM 参数-enableassertions开启 SpringBoot 中提供了 Assert 断言工具类，通常用于数据合法性检查 // 要求参数 object 必须为非空（Not Null），否则抛出异常，不予放行 // 参数 message 参数用于定制异常信息。 void notNull(Object object, String message) // 要求参数必须空（Null），否则抛出异常，不予『放行』。 // 和 notNull() 方法断言规则相反 void isNull(Object object, String message) // 要求参数必须为真（True），否则抛出异常，不予『放行』。 void isTrue(boolean expression, String message) // 要求参数（List/Set）必须非空（Not Empty），否则抛出异常，不予放行 void notEmpty(Collection collection, String message) // 要求参数（String）必须有长度（即，Not Empty），否则抛出异常，不予放行 void hasLength(String text, String message) // 要求参数（String）必须有内容（即，Not Blank），否则抛出异常，不予放行 void hasText(String text, String message) // 要求参数是指定类型的实例，否则抛出异常，不予放行 void isInstanceOf(Class type, Object obj, String message) // 要求参数 `subType` 必须是参数 superType 的子类或实现类，否则抛出异常，不予放行 void isAssignable(Class superType, Class subType, String message) 对象、数组、集合 ObjectUtils 获取对象的基本信息 // 获取对象的类名。参数为 null 时，返回字符串：&quot;null&quot; String nullSafeClassName(Object obj) // 参数为 null 时，返回 0 int nullSafeHashCode(Object object) // 参数为 null 时，返回字符串：&quot;null&quot; String nullSafeToString(boolean[] array) // 获取对象 HashCode（十六进制形式字符串）。参数为 null 时，返回 0 String getIdentityHexString(Object obj) // 获取对象的类名和 HashCode。 参数为 null 时，返回字符串：&quot;&quot; String identityToString(Object obj) // 相当于 toString()方法，但参数为 null 时，返回字符串：&quot;&quot; String getDisplayString(Object obj) 判断工具 // 判断数组是否为空 boolean isEmpty(Object[] array) // 判断参数对象是否是数组 boolean isArray(Object obj) // 判断数组中是否包含指定元素 boolean containsElement(Object[] array, Object element) // 相等，或同为 null时，返回 true boolean nullSafeEquals(Object o1, Object o2) /* 判断参数对象是否为空，判断标准为： Optional: Optional.empty() Array: length == 0 CharSequence: length == 0 Collection: Collection.isEmpty() Map: Map.isEmpty() */ boolean isEmpty(Object obj) 其他工具方法 // 向参数数组的末尾追加新元素，并返回一个新数组 &lt;A, O extends A&gt; A[] addObjectToArray(A[] array, O obj) // 原生基础类型数组 --&gt; 包装类数组 Object[] toObjectArray(Object source) StringUtils 字符串判断工具 // 判断字符串是否为 null，或 &quot;&quot;。注意，包含空白符的字符串为非空 boolean isEmpty(Object str) // 判断字符串是否是以指定内容结束。忽略大小写 boolean endsWithIgnoreCase(String str, String suffix) // 判断字符串是否已指定内容开头。忽略大小写 boolean startsWithIgnoreCase(String str, String prefix) // 是否包含空白符 boolean containsWhitespace(String str) // 判断字符串非空且长度不为 0，即，Not Empty boolean hasLength(CharSequence str) // 判断字符串是否包含实际内容，即非仅包含空白符，也就是 Not Blank boolean hasText(CharSequence str) // 判断字符串指定索引处是否包含一个子串。 boolean substringMatch(CharSequence str, int index, CharSequence substring) // 计算一个字符串中指定子串的出现次数 int countOccurrencesOf(String str, String sub) 字符串操作工具 // 查找并替换指定子串 String replace(String inString, String oldPattern, String newPattern) // 去除尾部的特定字符 String trimTrailingCharacter(String str, char trailingCharacter) // 去除头部的特定字符 String trimLeadingCharacter(String str, char leadingCharacter) // 去除头部的空白符 String trimLeadingWhitespace(String str) // 去除头部的空白符 String trimTrailingWhitespace(String str) // 去除头部和尾部的空白符 String trimWhitespace(String str) // 删除开头、结尾和中间的空白符 String trimAllWhitespace(String str) // 删除指定子串 String delete(String inString, String pattern) // 删除指定字符（可以是多个） String deleteAny(String inString, String charsToDelete) // 对数组的每一项执行 trim() 方法 String[] trimArrayElements(String[] array) // 将 URL 字符串进行解码 String uriDecode(String source, Charset charset) 路径相关工具方法 // 解析路径字符串，优化其中的 “..” String cleanPath(String path) // 解析路径字符串，解析出文件名部分 String getFilename(String path) // 解析路径字符串，解析出文件后缀名 String getFilenameExtension(String path) // 比较两个两个字符串，判断是否是同一个路径。会自动处理路径中的 “..” boolean pathEquals(String path1, String path2) // 删除文件路径名中的后缀部分 String stripFilenameExtension(String path) // 以 “. 作为分隔符，获取其最后一部分 String unqualify(String qualifiedName) // 以指定字符作为分隔符，获取其最后一部分 String unqualify(String qualifiedName, char separator) CollectionUtils 集合判断工具 // 判断 List/Set 是否为空 boolean isEmpty(Collection&lt;?&gt; collection) // 判断 Map 是否为空 boolean isEmpty(Map&lt;?,?&gt; map) // 判断 List/Set 中是否包含某个对象 boolean containsInstance(Collection&lt;?&gt; collection, Object element) // 以迭代器的方式，判断 List/Set 中是否包含某个对象 boolean contains(Iterator&lt;?&gt; iterator, Object element) // 判断 List/Set 是否包含某些对象中的任意一个 boolean containsAny(Collection&lt;?&gt; source, Collection&lt;?&gt; candidates) // 判断 List/Set 中的每个元素是否唯一。即 List/Set 中不存在重复元素 boolean hasUniqueObject(Collection&lt;?&gt; collection) 集合操作工具 // 将 Array 中的元素都添加到 List/Set 中 &lt;E&gt; void mergeArrayIntoCollection(Object array, Collection&lt;E&gt; collection) // 将 Properties 中的键值对都添加到 Map 中 &lt;K,V&gt; void mergePropertiesIntoMap(Properties props, Map&lt;K,V&gt; map) // 返回 List 中最后一个元素 &lt;T&gt; T lastElement(List&lt;T&gt; list) // 返回 Set 中最后一个元素 &lt;T&gt; T lastElement(Set&lt;T&gt; set) // 返回参数 candidates 中第一个存在于参数 source 中的元素 &lt;E&gt; E findFirstMatch(Collection&lt;?&gt; source, Collection&lt;E&gt; candidates) // 返回 List/Set 中指定类型的元素。 &lt;T&gt; T findValueOfType(Collection&lt;?&gt; collection, Class&lt;T&gt; type) // 返回 List/Set 中指定类型的元素。如果第一种类型未找到，则查找第二种类型，以此类推 Object findValueOfType(Collection&lt;?&gt; collection, Class&lt;?&gt;[] types) // 返回 List/Set 中元素的类型 Class&lt;?&gt; findCommonElementType(Collection&lt;?&gt; collection) 文件、资源、IO 流 FileCopyUtils 输入 // 从文件中读入到字节数组中 byte[] copyToByteArray(File in) // 从输入流中读入到字节数组中 byte[] copyToByteArray(InputStream in) // 从输入流中读入到字符串中 String copyToString(Reader in) 输出 // 从字节数组到文件 void copy(byte[] in, File out) // 从文件到文件 int copy(File in, File out) // 从字节数组到输出流 void copy(byte[] in, OutputStream out) // 从输入流到输出流 int copy(InputStream in, OutputStream out) // 从输入流到输出流 int copy(Reader in, Writer out) // 从字符串到输出流 void copy(String in, Writer out) ResourceUtils 从资源路径获取文件 // 判断字符串是否是一个合法的 URL 字符串。 static boolean isUrl(String resourceLocation) // 获取 URL static URL getURL(String resourceLocation) // 获取文件（在 JAR 包内无法正常使用，需要是一个独立的文件） static File getFile(String resourceLocation) Resource // 文件系统资源 D:\\... FileSystemResource // URL 资源，如 file://... http://... UrlResource // 类路径下的资源，classpth:... ClassPathResource // Web 容器上下文中的资源（jar 包、war 包） ServletContextResource 复制代码 // 判断资源是否存在 boolean exists() // 从资源中获得 File 对象 File getFile() // 从资源中获得 URI 对象 URI getURI() // 从资源中获得 URI 对象 URL getURL() // 获得资源的 InputStream InputStream getInputStream() // 获得资源的描述信息 String getDescription() StreamUtils 输入 void copy(byte[] in, OutputStream out) int copy(InputStream in, OutputStream out) void copy(String in, Charset charset, OutputStream out) long copyRange(InputStream in, OutputStream out, long start, long end) 输出 byte[] copyToByteArray(InputStream in) String copyToString(InputStream in, Charset charset) // 舍弃输入流中的内容 int drain(InputStream in) 反射、AOP ReflectionUtils 获取方法 // 在类中查找指定方法 Method findMethod(Class&lt;?&gt; clazz, String name) // 同上，额外提供方法参数类型作查找条件 Method findMethod(Class&lt;?&gt; clazz, String name, Class&lt;?&gt;... paramTypes) // 获得类中所有方法，包括继承而来的 Method[] getAllDeclaredMethods(Class&lt;?&gt; leafClass) // 在类中查找指定构造方法 Constructor&lt;T&gt; accessibleConstructor(Class&lt;T&gt; clazz, Class&lt;?&gt;... parameterTypes) // 是否是 equals() 方法 boolean isEqualsMethod(Method method) // 是否是 hashCode() 方法 boolean isHashCodeMethod(Method method) // 是否是 toString() 方法 boolean isToStringMethod(Method method) // 是否是从 Object 类继承而来的方法 boolean isObjectMethod(Method method) // 检查一个方法是否声明抛出指定异常 boolean declaresException(Method method, Class&lt;?&gt; exceptionType) 执行方法 // 执行方法 Object invokeMethod(Method method, Object target) // 同上，提供方法参数 Object invokeMethod(Method method, Object target, Object... args) // 取消 Java 权限检查。以便后续执行该私有方法 void makeAccessible(Method method) // 取消 Java 权限检查。以便后续执行私有构造方法 void makeAccessible(Constructor&lt;?&gt; ctor) 获取字段 // 在类中查找指定属性 Field findField(Class&lt;?&gt; clazz, String name) // 同上，多提供了属性的类型 Field findField(Class&lt;?&gt; clazz, String name, Class&lt;?&gt; type) // 是否为一个 &quot;public static final&quot; 属性 boolean isPublicStaticFinal(Field field) 设置字段 // 获取 target 对象的 field 属性值 Object getField(Field field, Object target) // 设置 target 对象的 field 属性值，值为 value void setField(Field field, Object target, Object value) // 同类对象属性对等赋值 void shallowCopyFieldState(Object src, Object dest) // 取消 Java 的权限控制检查。以便后续读写该私有属性 void makeAccessible(Field field) // 对类的每个属性执行 callback void doWithFields(Class&lt;?&gt; clazz, ReflectionUtils.FieldCallback fc) // 同上，多了个属性过滤功能。 void doWithFields(Class&lt;?&gt; clazz, ReflectionUtils.FieldCallback fc, ReflectionUtils.FieldFilter ff) // 同上，但不包括继承而来的属性 void doWithLocalFields(Class&lt;?&gt; clazz, ReflectionUtils.FieldCallback fc) AopUtils 判断代理类型 // 判断是不是 Spring 代理对象 boolean isAopProxy() // 判断是不是 jdk 动态代理对象 isJdkDynamicProxy() // 判断是不是 CGLIB 代理对象 boolean isCglibProxy() 获取被代理对象的 class // 获取被代理的目标 class Class&lt;?&gt; getTargetClass() AopContext 获取当前对象的代理对象 Object currentProxy() ","link":"https://tianxiawuhao.github.io/I1j63ouUN/"},{"title":"nginx.conf中文详解","content":"#定义Nginx运行的用户和用户组 user www www; #nginx进程数，建议设置为等于CPU总核心数。 worker_processes 8; #全局错误日志定义类型，[ debug | info | notice | warn | error | crit ] error_log /usr/local/nginx/logs/error.log info; #进程pid文件 pid /usr/local/nginx/logs/nginx.pid; #指定进程可以打开的最大描述符：数目 #工作模式与连接数上限 #这个指令是指当一个nginx进程打开的最多文件描述符数目，理论值应该是最多打开文件数（ulimit -n）与nginx进程数相除，但是nginx分配请求并不是那么均匀，所以最好与ulimit -n 的值保持一致。 #现在在linux 2.6内核下开启文件打开数为65535，worker_rlimit_nofile就相应应该填写65535。 #这是因为nginx调度时分配请求到进程并不是那么的均衡，所以假如填写10240，总并发量达到3-4万时就有进程可能超过10240了，这时会返回502错误。 worker_rlimit_nofile 65535; events{ ​ #参考事件模型，use [ kqueue | rtsig | epoll | /dev/poll | select | poll ]; epoll模型 ​ #是Linux 2.6以上版本内核中的高性能网络I/O模型，linux建议epoll，如果跑在FreeBSD上面，就用kqueue模型。 ​ #补充说明： ​ #与apache相类，nginx针对不同的操作系统，有不同的事件模型 ​ #A）标准事件模型 ​ #Select、poll属于标准事件模型，如果当前系统不存在更有效的方法，nginx会选择select或poll ​ #B）高效事件模型 ​ #Kqueue：使用于FreeBSD 4.1+, OpenBSD 2.9+, NetBSD 2.0 和 MacOS X.使用双处理器的MacOS X系统使用kqueue可能会造成内核崩溃。 ​ #Epoll：使用于Linux内核2.6版本及以后的系统。 ​ #/dev/poll：使用于Solaris 7 11/99+，HP/UX 11.22+ (eventport)，IRIX 6.5.15+ 和 Tru64 UNIX 5.1A+。 ​ #Eventport：使用于Solaris 10。 为了防止出现内核崩溃的问题， 有必要安装安全补丁。 ​ use epoll; ​ #单个进程最大连接数（最大连接数=连接数*进程数） ​ #根据硬件调整，和前面工作进程配合起来用，尽量大，但是别把cpu跑到100%就行。每个进程允许的最多连接数，理论上每台nginx服务器的最大连接数为。 ​ worker_connections 65535; ​ #keepalive超时时间。 ​ keepalive_timeout 60; ​ #客户端请求头部的缓冲区大小。这个可以根据你的系统分页大小来设置，一般一个请求头的大小不会超过1k，不过由于一般系统分页都要大于1k，所以这里设置为分页大小。 ​ #分页大小可以用命令getconf PAGESIZE 取得。 ​ #[root@web001 ~]# getconf PAGESIZE ​ #4096 ​ #但也有client_header_buffer_size超过4k的情况，但是client_header_buffer_size该值必须设置为“系统分页大小”的整倍数。 ​ client_header_buffer_size 4k; ​ #这个将为打开文件指定缓存，默认是没有启用的，max指定缓存数量，建议和打开文件数一致，inactive是指经过多长时间文件没被请求后删除缓存。 ​ open_file_cache max=65535 inactive=60s; ​ #这个是指多长时间检查一次缓存的有效信息。 ​ #语法:open_file_cache_valid time 默认值:open_file_cache_valid 60 使用字段:http, server, location 这个指令指定了何时需要检查open_file_cache中缓存项目的有效信息. ​ open_file_cache_valid 80s; ​ #open_file_cache指令中的inactive参数时间内文件的最少使用次数，如果超过这个数字，文件描述符一直是在缓存中打开的，如上例，如果有一个文件在inactive时间内一次没被使用，它将被移除。 ​ #语法:open_file_cache_min_uses number 默认值:open_file_cache_min_uses 1 使用字段:http, server, location 这个指令指定了在open_file_cache指令无效的参数中一定的时间范围内可以使用的最小文件数,如果使用更大的值,文件描述符在cache中总是打开状态. ​ open_file_cache_min_uses 1; ​ #语法:open_file_cache_errors on | off 默认值:open_file_cache_errors off 使用字段:http, server, location 这个指令指定是否在搜索一个文件时记录cache错误. ​ open_file_cache_errors on; } #设定http服务器，利用它的反向代理功能提供负载均衡支持 http{ ​ #文件扩展名与文件类型映射表 ​ include mime.types; ​ #默认文件类型 ​ default_type application/octet-stream; ​ #默认编码 ​ #charset utf-8; ​ #服务器名字的hash表大小 ​ #保存服务器名字的hash表是由指令server_names_hash_max_size 和server_names_hash_bucket_size所控制的。参数hash bucket size总是等于hash表的大小，并且是一路处理器缓存大小的倍数。在减少了在内存中的存取次数后，使在处理器中加速查找hash表键值成为可能。如果hash bucket size等于一路处理器缓存的大小，那么在查找键的时候，最坏的情况下在内存中查找的次数为2。第一次是确定存储单元的地址，第二次是在存储单元中查找键 值。因此，如果Nginx给出需要增大hash max size 或 hash bucket size的提示，那么首要的是增大前一个参数的大小. ​ server_names_hash_bucket_size 128; ​ #客户端请求头部的缓冲区大小。这个可以根据你的系统分页大小来设置，一般一个请求的头部大小不会超过1k，不过由于一般系统分页都要大于1k，所以这里设置为分页大小。分页大小可以用命令getconf PAGESIZE取得。 ​ client_header_buffer_size 32k; ​ #客户请求头缓冲大小。nginx默认会用client_header_buffer_size这个buffer来读取header值，如果header过大，它会使用large_client_header_buffers来读取。 ​ large_client_header_buffers 4 64k; ​ #设定通过nginx上传文件的大小 ​ client_max_body_size 8m; ​ #开启高效文件传输模式，sendfile指令指定nginx是否调用sendfile函数来输出文件，对于普通应用设为 on，如果用来进行下载等应用磁盘IO重负载应用，可设置为off，以平衡磁盘与网络I/O处理速度，降低系统的负载。注意：如果图片显示不正常把这个改成off。 ​ #sendfile指令指定 nginx 是否调用sendfile 函数（zero copy 方式）来输出文件，对于普通应用，必须设为on。如果用来进行下载等应用磁盘IO重负载应用，可设置为off，以平衡磁盘与网络IO处理速度，降低系统uptime。 ​ sendfile on; ​ #开启目录列表访问，合适下载服务器，默认关闭。 ​ autoindex on; ​ #此选项允许或禁止使用socke的TCP_CORK的选项，此选项仅在使用sendfile的时候使用 ​ tcp_nopush on; ​ tcp_nodelay on; ​ #长连接超时时间，单位是秒 ​ keepalive_timeout 120; ​ #FastCGI相关参数是为了改善网站的性能：减少资源占用，提高访问速度。下面参数看字面意思都能理解。 ​ fastcgi_connect_timeout 300; ​ fastcgi_send_timeout 300; ​ fastcgi_read_timeout 300; ​ fastcgi_buffer_size 64k; ​ fastcgi_buffers 4 64k; ​ fastcgi_busy_buffers_size 128k; ​ fastcgi_temp_file_write_size 128k; ​ #gzip模块设置 ​ gzip on; #开启gzip压缩输出 ​ gzip_min_length 1k; #最小压缩文件大小 ​ gzip_buffers 4 16k; #压缩缓冲区 ​ gzip_http_version 1.0; #压缩版本（默认1.1，前端如果是squid2.5请使用1.0） ​ gzip_comp_level 2; #压缩等级 ​ gzip_types text/plain application/x-javascript text/css application/xml; #压缩类型，默认就已经包含textml，所以下面就不用再写了，写上去也不会有问题，但是会有一个warn。 ​ gzip_vary on; ​ #开启限制IP连接数的时候需要使用 ​ #limit_zone crawler $binary_remote_addr 10m; ​ #负载均衡配置 ​ upstream jh.w3cschool.cn { ​ #upstream的负载均衡，weight是权重，可以根据机器配置定义权重。weigth参数表示权值，权值越高被分配到的几率越大。 ​ server 192.168.80.121:80 weight=3; ​ server 192.168.80.122:80 weight=2; ​ server 192.168.80.123:80 weight=3; ​ #nginx的upstream目前支持4种方式的分配 ​ #1、轮询（默认） ​ #每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器down掉，能自动剔除。 ​ #2、weight ​ #指定轮询几率，weight和访问比率成正比，用于后端服务器性能不均的情况。 ​ #例如： ​ #upstream bakend { ​ # server 192.168.0.14 weight=10; ​ # server 192.168.0.15 weight=10; ​ #} ​ #2、ip_hash ​ #每个请求按访问ip的hash结果分配，这样每个访客固定访问一个后端服务器，可以解决session的问题。 ​ #例如： ​ #upstream bakend { ​ # ip_hash; ​ # server 192.168.0.14:88; ​ # server 192.168.0.15:80; ​ #} ​ #3、fair（第三方） ​ #按后端服务器的响应时间来分配请求，响应时间短的优先分配。 ​ #upstream backend { ​ # server server1; ​ # server server2; ​ # fair; ​ #} ​ #4、url_hash（第三方） ​ #按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，后端服务器为缓存时比较有效。 ​ #例：在upstream中加入hash语句，server语句中不能写入weight等其他的参数，hash_method是使用的hash算法 ​ #upstream backend { ​ # server squid1:3128; ​ # server squid2:3128; ​ # hash $request_uri; ​ # hash_method crc32; ​ #} ​ #tips: ​ #upstream bakend{#定义负载均衡设备的Ip及设备状态}{ ​ # ip_hash; ​ # server 127.0.0.1:9090 down; ​ # server 127.0.0.1:8080 weight=2; ​ # server 127.0.0.1:6060; ​ # server 127.0.0.1:7070 backup; ​ #} ​ #在需要使用负载均衡的server中增加 proxy_pass http://bakend/; ​ #每个设备的状态设置为: ​ #1.down表示单前的server暂时不参与负载 ​ #2.weight为weight越大，负载的权重就越大。 ​ #3.max_fails：允许请求失败的次数默认为1.当超过最大次数时，返回proxy_next_upstream模块定义的错误 ​ #4.fail_timeout:max_fails次失败后，暂停的时间。 ​ #5.backup： 其它所有的非backup机器down或者忙的时候，请求backup机器。所以这台机器压力会最轻。 ​ #nginx支持同时设置多组的负载均衡，用来给不用的server来使用。 ​ #client_body_in_file_only设置为On 可以讲client post过来的数据记录到文件中用来做debug ​ #client_body_temp_path设置记录文件的目录 可以设置最多3层目录 ​ #location对URL进行匹配.可以进行重定向或者进行新的代理 负载均衡 ​ } ​ #虚拟主机的配置 ​ server{ ​ #监听端口 ​ listen 80; ​ #域名可以有多个，用空格隔开 ​ server_name www.w3cschool.cn w3cschool.cn; ​ index index.html index.htm index.php; ​ root /data/www/w3cschool; ​ #对******进行负载均衡 ​ location ~ .*.(php|php5)?${ ​ fastcgi_pass 127.0.0.1:9000; ​ fastcgi_index index.php; ​ include fastcgi.conf; ​ } ​ #图片缓存时间设置 ​ location ~ .*.(gif|jpg|jpeg|png|bmp|swf)${ ​ expires 10d; ​ } ​ #JS和CSS缓存时间设置 ​ location ~ .*.(js|css)?${ ​ expires 1h; ​ } ​ #日志格式设定 ​ #$remote_addr与$http_x_forwarded_for用以记录客户端的ip地址； ​ #$remote_user：用来记录客户端用户名称； ​ #$time_local： 用来记录访问时间与时区； ​ #$request： 用来记录请求的url与http协议； ​ #$status： 用来记录请求状态；成功是200， ​ #$body_bytes_sent ：记录发送给客户端文件主体内容大小； ​ #$http_referer：用来记录从那个页面链接访问过来的； ​ #$http_user_agent：记录客户浏览器的相关信息； ​ #通常web服务器放在反向代理的后面，这样就不能获取到客户的IP地址了，通过$remote_add拿到的IP地址是反向代理服务器的iP地址。反向代理服务器在转发请求的http头信息中，可以增加x_forwarded_for信息，用以记录原有客户端的IP地址和原来客户端的请求的服务器地址。 ​ log_format access '$remote_addr - $remote_user [$time_local] &quot;$request&quot; ' ​ '$status $body_bytes_sent &quot;$http_referer&quot; ' ​ '&quot;$http_user_agent&quot; $http_x_forwarded_for'; ​ #定义本虚拟主机的访问日志 ​ access_log /usr/local/nginx/logs/host.access.log main; ​ access_log /usr/local/nginx/logs/host.access.404.log log404; ​ #对 &quot;/&quot; 启用反向代理 ​ location / { ​ proxy_pass http://127.0.0.1:88; ​ proxy_redirect off; ​ proxy_set_header X-Real-IP $remote_addr; ​ #后端的Web服务器可以通过X-Forwarded-For获取用户真实IP ​ proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; ​ #以下是一些反向代理的配置，可选。 ​ proxy_set_header Host $host; ​ #允许客户端请求的最大单文件字节数 ​ client_max_body_size 10m; ​ #缓冲区代理缓冲用户端请求的最大字节数， ​ #如果把它设置为比较大的数值，例如256k，那么，无论使用firefox还是IE浏览器，来提交任意小于256k的图片，都很正常。如果注释该指令，使用默认的client_body_buffer_size设置，也就是操作系统页面大小的两倍，8k或者16k，问题就出现了。 ​ #无论使用firefox4.0还是IE8.0，提交一个比较大，200k左右的图片，都返回500 Internal Server Error错误 ​ client_body_buffer_size 128k; ​ #表示使nginx阻止HTTP应答代码为400或者更高的应答。 ​ proxy_intercept_errors on; ​ #后端服务器连接的超时时间_发起握手等候响应超时时间 ​ #nginx跟后端服务器连接超时时间(代理连接超时) ​ proxy_connect_timeout 90; ​ #后端服务器数据回传时间(代理发送超时) ​ #后端服务器数据回传时间_就是在规定时间之内后端服务器必须传完所有的数据 ​ proxy_send_timeout 90; ​ #连接成功后，后端服务器响应时间(代理接收超时) ​ #连接成功后_等候后端服务器响应时间_其实已经进入后端的排队之中等候处理（也可以说是后端服务器处理请求的时间） ​ proxy_read_timeout 90; ​ #设置代理服务器（nginx）保存用户头信息的缓冲区大小 ​ #设置从被代理服务器读取的第一部分应答的缓冲区大小，通常情况下这部分应答中包含一个小的应答头，默认情况下这个值的大小为指令proxy_buffers中指定的一个缓冲区的大小，不过可以将其设置为更小 ​ proxy_buffer_size 4k; ​ #proxy_buffers缓冲区，网页平均在32k以下的设置 ​ #设置用于读取应答（来自被代理服务器）的缓冲区数目和大小，默认情况也为分页大小，根据操作系统的不同可能是4k或者8k ​ proxy_buffers 4 32k; ​ #高负荷下缓冲大小（proxy_buffers*2） ​ proxy_busy_buffers_size 64k; ​ #设置在写入proxy_temp_path时数据的大小，预防一个工作进程在传递文件时阻塞太长 ​ #设定缓存文件夹大小，大于这个值，将从upstream服务器传 ​ proxy_temp_file_write_size 64k; ​ } ​ #设定查看Nginx状态的地址 ​ location /NginxStatus { ​ stub_status on; ​ access_log on; ​ auth_basic &quot;NginxStatus&quot;; ​ auth_basic_user_file confpasswd; ​ #htpasswd文件的内容可以用apache提供的htpasswd工具来产生。 ​ } ​ #本地动静分离反向代理配置 ​ #所有jsp的页面均交由tomcat或resin处理 ​ location ~ .(jsp|jspx|do)?$ { ​ proxy_set_header Host $host; ​ proxy_set_header X-Real-IP $remote_addr; ​ proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; ​ proxy_pass http://127.0.0.1:8080; ​ } ​ #所有静态文件由nginx直接读取不经过tomcat或resin ​ location ~ .*.(htm|html|gif|jpg|jpeg|png|bmp|swf|ioc|rar|zip|txt|flv|mid|doc|ppt| ​ pdf|xls|mp3|wma)${ ​ expires 15d; ​ } ​ location ~ .*.(js|css)?${ ​ expires 1h; ​ } ​ } } ","link":"https://tianxiawuhao.github.io/yRJDaVAsJ/"},{"title":"docker 宿主机定时清除容器的运行日志","content":"一般docker容器都是最小化安装，不仅如此系统定时器相关的服务也不存在，自己去安装也很麻烦，故此直接使用宿主机的定时器即可。 一、在容器中编写清除日志脚本 这一部分不论你是把定时器加在宿主机或者是容器都必须要去做的 ； 网上随意一搜就可以看到如下的删除模板： find 对应目录 -mtime +天数 -name &quot;文件名&quot; -exec rm -rf {} \\; 因为本人的日志目录层级比较深 所以改良了如下： 1. -- /opt/auto-del-log.sh 2. \\#!/bin/sh 3. find /home/schedule_log/ -mtime -5 -type f -iname &quot;*.log&quot; -exec rm -rf {} \\; 一定记得加可执行权限 chmod +777 /opt/auto-del-log.sh 后面经过验证 其实效果是一样的! 重点就是你要去验证你的脚本有无效！ 你可以这样直接输入验证 1. find /home/schedule_log/ -type f -iname &quot;*.log&quot; 2. 或者 3. find /home/schedule_log/ -name &quot;*.log&quot; 如果能查出你想删除的文件那么后面就可以开始套模板了。 -mtime：标准语句写法； +30：查找30天前的文件，这里用数字代表天数； &quot;*.log&quot;：希望查找的数据类型，&quot;*.jpg&quot;表示查找扩展名为jpg的所有文件，&quot;*&quot;表示查找所有文件，这个可以灵活运用，举一反三； -exec：固定写法； rm -rf：强制删除文件，包括目录； {} \\; ：固定写法，一对大括号+空格+\\+; 二、宿主机加入定时器 使用docker exec 命令校验之前写的脚本是否有效 如下： docker exec -it tomcat8002 /opt/auto-del-log.sh tomcat8002 ： 容器名称或者ID /opt/auto-del-log.sh :脚本在容器中的位置 如果此命令有效那么就可以编辑定时器了 本人采用的是centos7 具体可以参看网上介绍的挺全的一篇博客如下： centos7 linux定时任务详解 crontab -e 02 4 * * * docker exec -it tomcat8002 /opt/auto-del-log.sh 接下来就OK啦！ ","link":"https://tianxiawuhao.github.io/KW_02xshc/"},{"title":"docker安装elasticsearch","content":"docker search elasticsearch 选择一个版本，拉取镜像 docker pull elasticsearch:2.4.4 查看镜像 docker images 通过镜像，启动一个容器，并将9200和9300端口映射到本机 docker run -d -p 9200:9200 -p 9300:9300 -e ES_JAVA_OPTS=&quot;-Xms512m -Xmx512m&quot; --name elasticsearch elasticsearch:2.4.4 查看已启动容器 docker ps 验证是否安装成功？访问： http://localhost:9200/ 安装插件，先进入容器： docker exec -it 4d34fbf944a5 /bin/bash 进入容器bin目录，并执行安装插件命令： cd bin ls plugin install mobz/elasticsearch-head /**（低版本执行命令有所不同）**/ plugin -install mobz/elasticsearch-head 访问： http://localhost:9200/_plugin/head/ 插件安装成功 ","link":"https://tianxiawuhao.github.io/2K9TL16e1/"},{"title":"使用docker搭建FastDFS文件系统","content":"1.首先下载FastDFS文件系统的docker镜像 查询镜像 [root@localhost /]# docker search fastdfs 安装镜像 [root@localhost ~]# docker pull season/fastdfs [root@localhost ~]# docker images 2.使用docker镜像构建tracker容器（跟踪服务器，起到调度的作用）： 创建tracker容器 [root@localhost /]# docker run -ti -d --name trakcer -v ~/tracker_data:/fastdfs/tracker/data --net=host season/fastdfs tracker Tracker服务器的端口默认是22122，你可以查看是否启用端口 [root@localhost /]# netstat -aon | grep 22122 3.使用docker镜像构建storage容器（存储服务器，提供容量和备份服务）： docker run -tid --name storage -v ~/storage_data:/fastdfs/storage/data -v ~/store_path:/fastdfs/store_path --net=host -e TRACKER_SERVER:192.168.115.130:22122 -e GROUP_NAME=group1 season/fastdfs storage 4.此时两个服务都以启动，进行服务的配置。 进入storage容器，到storage的配置文件中配置http访问的端口，配置文件在fdfs_conf目录下的storage.conf。 [root@localhost /]# docker exec -it storage bash root@localhost:/# cd fdfs_conf root@localhost:/fdfs_conf# more storage.conf 往下拉，你会发现storage容器的ip不是你linux的ip，如下： 接下来，退出storage容器，并将配置文件拷贝一份出来： [root@localhost ~]# docker cp storage:/fdfs_conf/storage.conf ~/ [root@localhost ~]# vi ~/storage.conf 将修改后的配置文件拷贝到storagee的配置目录下： [root@localhost ~]# docker cp ~/storage.conf storage:/fdfs_conf/ 重新启动storage容器 [root@localhost ~]# docker stop storage [root@localhost ~]# docker start storage 查看tracker容器和storage容器的关联 [root@localhost ~]# docker exec -it storage bash root@localhost:/# cd fdfs_conf root@localhost:/fdfs_conf# fdfs_monitor storage.conf 5.在docker模拟客户端上传文件到storage容器 开启一个客户端 [root@localhost 00]# docker run -tid --name fdfs_sh --net=host season/fastdfs sh 更改配置文件，因为之前已经改过一次了，所以现在直接拷贝 [root@localhost 00]# docker cp ~/storage.conf fdfs_sh:/fdfs_conf/ 创建一个txt文件 [root@localhost 00]# docker exec -it fdfs_sh bash root@localhost:/# echo hello&gt;a.txt 进入fdfs_conf目录，并将文件上传到storage容器 root@localhost:/# cd fdfs_conf root@localhost:/fdfs_conf# fdfs_upload_file storage.conf /a.txt /a.txt：指要上传的文件 上传之后，根据返回的路径去找a.txt 退出去查看上传的txt文件 [root@localhost ~]# cd ~/store_path/data/00/00 [root@localhost 00]# ls 查看是否和输入的值是否相同 [root@localhost 00]# more wKhzg1wGsieAL-3RAAAABncc3SA337.txt ","link":"https://tianxiawuhao.github.io/yIe_yl-14/"},{"title":"Dockerfile详解","content":"Dockerfile详解 环境介绍 Dockerfile中所用的所有文件一定要和Dockerfile文件在同一级父目录下，可以为Dockerfile父目录的子目录 Dockerfile中相对路径默认都是Dockerfile所在的目录 Dockerfile中一定要惜字如金，能写到一行的指令，一定要写到一行，原因是分层构建，联合挂载这个特性。 Dockerfile中每一条指令被视为一层 Dockerfile中指明大写（约定俗成） 指令介绍 FROM 功能为指定基础镜像，并且必须是第一条指令。 如果不以任何镜像为基础，那么写法为：FROM scratch。 同时意味着接下来所写的指令将作为镜像的第一层开始 语法： FROM &lt;image&gt; FROM &lt;image&gt;:&lt;tag&gt; FROM &lt;image&gt;:&lt;digest&gt; 三种写法，其中&lt;tag&gt;和&lt;digest&gt; 是可选项，如果没有选择，那么默认值为latest MAINTAINER 指定作者 语法： MAINTAINER &lt;name&gt; 新版docker中使用LABEL指明 LABEL 功能是为镜像指定标签 语法： LABEL &lt;key&gt;=&lt;value&gt; &lt;key&gt;=&lt;value&gt; &lt;key&gt;=&lt;value&gt; ... 一个Dockerfile种可以有多个LABEL，如下： LABEL &quot;com.example.vendor&quot;=&quot;ACME Incorporated&quot; LABEL com.example.label-with-value=&quot;foo&quot; LABEL version=&quot;1.0&quot; LABEL description=&quot;This text illustrates that label-values can span multiple lines.&quot; 但是并不建议这样写，最好就写成一行，如太长需要换行的话则使用\\符号 如下： LABEL multi.label1=&quot;value1&quot; multi.label2=&quot;value2&quot; other=&quot;value3&quot; 说明：LABEL会继承基础镜像种的LABEL，如遇到key相同，则值覆盖 ADD 一个复制命令，把文件复制到镜像中。 如果把虚拟机与容器想象成两台linux服务器的话，那么这个命令就类似于scp，只是scp需要加用户名和密码的权限验证，而ADD不用。 语法如下： 1. ADD &lt;src&gt;... &lt;dest&gt; 2. ADD [&quot;&lt;src&gt;&quot;,... &quot;&lt;dest&gt;&quot;] 路径的填写可以是容器内的绝对路径，也可以是相对于工作目录的相对路径，推荐写成绝对路径 可以是一个本地文件或者是一个本地压缩文件，还可以是一个url 如果把写成一个url，那么ADD就类似于wget命令 示例 ADD test relativeDir/ ADD test /relativeDir ADD http://example.com/foobar/ 注意事项 src为一个目录的时候，会自动把目录下的文件复制过去，目录本身不会复制 如果src为多个文件，dest一定要是一个目录 COPY 看这个名字就知道，又是一个复制命令 语法如下： COPY &lt;src&gt;... &lt;dest&gt; COPY [&quot;&lt;src&gt;&quot;,... &quot;&lt;dest&gt;&quot;] 与ADD的区别 COPY的只能是本地文件，其他用法一致 EXPOSE 功能为暴漏容器运行时的监听端口给外部 但是EXPOSE并不会使容器访问主机的端口 如果想使得容器与主机的端口有映射关系，必须在容器启动的时候加上 -P参数 语法： EXPOSE &lt;port&gt;/&lt;tcp/udp&gt; ENV 功能为设置环境变量 语法有两种 ENV &lt;key&gt; &lt;value&gt; ENV &lt;key&gt;=&lt;value&gt; ... 两者的区别就是第一种是一次设置一个，第二种是一次设置多个 在Dockerfile中使用变量的方式 $varname ${varname} ${varname:-default value} $(varname:+default value} 第一种和第二种相同 第三种表示当变量不存在使用-号后面的值 第四种表示当变量存在时使用+号后面的值（当然不存在也是使用后面的值） RUN 功能为运行指定的命令 RUN命令有两种格式 1. RUN &lt;command&gt; 2. RUN [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;] 第一种后边直接跟shell命令 在linux操作系统上默认 /bin/sh -c 在windows操作系统上默认 cmd /S /C 第二种是类似于函数调用。 可将executable理解成为可执行文件，后面就是两个参数。 CMD 功能为容器启动时默认命令或参数 语法有三种写法 CMD [&quot;executable&quot;,&quot;param1&quot;,&quot;param2&quot;] CMD [&quot;param1&quot;,&quot;param2&quot;] CMD command param1 param2 第三种比较好理解了，就时shell这种执行方式和写法 第一种和第二种其实都是可执行文件加上参数的形式 举例说明两种写法： CMD [ &quot;sh&quot;, &quot;-c&quot;, &quot;echo $HOME&quot; CMD [ &quot;echo&quot;, &quot;$HOME&quot; ] 补充细节：这里边包括参数的一定要用双引号，就是&quot;,不能是单引号。千万不能写成单引号。 原因是参数传递后，docker解析的是一个JSON array RUN&amp;&amp;CMD 不要把RUN和CMD搞混了。 RUN是构件容器时就运行的命令以及提交运行结果 CMD是容器启动时执行的命令，在构件时并不运行，构件时紧紧指定了这个命令到底是个什么样子 ENTRYPOINT 功能是：容器启动时运行得启动命令 语法如下： ENTRYPOINT [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;] ENTRYPOINT command param1 param2 如果从上到下看到这里的话，那么你应该对这两种语法很熟悉啦。 第二种就是写shell 第一种就是可执行文件加参数 与CMD比较说明（这俩命令太像了，而且还可以配合使用）： 相同点： 只能写一条，如果写了多条，那么只有最后一条生效 容器启动时才运行，运行时机相同 不同点： ENTRYPOINT不会被运行的command覆盖，而CMD则会被覆盖 如果我们在Dockerfile种同时写了ENTRYPOINT和CMD，并且CMD指令不是一个完整的可执行命令，那么CMD指定的内容将会作为ENTRYPOINT的参数 如下： FROM ubuntu ENTRYPOINT [&quot;top&quot;, &quot;-b&quot;] CMD [&quot;-c&quot;] 如果我们在Dockerfile种同时写了ENTRYPOINT和CMD，并且CMD是一个完整的指令，那么它们两个会互相覆盖，谁在最后谁生效 如下： FROM ubuntu ENTRYPOINT [&quot;top&quot;, &quot;-b&quot;] CMD ls -al 那么将执行ls -al ,top -b不会执行。 Docker官方使用一张表格来展示了ENTRYPOINT 和 CMD不同组合的执行情况 VOLUME 可实现挂载功能，可以将宿主机目录挂载到容器中 说的这里大家都懂了，可用专用的文件存储当作Docker容器的数据存储部分 语法如下： VOLUME [&quot;/data&quot;] 说明： [&quot;/data&quot;]可以是一个JsonArray ，也可以是多个值。所以如下几种写法都是正确的 VOLUME [&quot;/var/log/&quot;] VOLUME /var/log VOLUME /var/log /var/db 一般的使用场景为需要持久化存储数据时 容器使用的是AUFS，这种文件系统不能持久化数据，当容器关闭后，所有的更改都会丢失。 USER 设置启动容器的用户，可以是用户名或UID，所以，只有下面的两种写法是正确的 USER daemo USER UID 注意：如果设置了容器以daemon用户去运行，那么RUN, CMD 和 ENTRYPOINT 都会以这个用户去运行, 使用这个命令一定要确认容器中拥有这个用户，并且拥有足够权限 WORKDIR 设置工作目录 语法： WORKDIR /path/to/workdir 设置工作目录，对RUN,CMD,ENTRYPOINT,COPY,ADD生效。如果不存在则会创建，也可以设置多次。 如： WORKDIR /a WORKDIR b WORKDIR c RUN pwd pwd执行的结果是/a/b/c WORKDIR也可以解析环境变量 如： ENV DIRPATH /path WORKDIR $DIRPATH/$DIRNAME RUN pwd pwd的执行结果是/path/$DIRNAME ARG 设置变量命令 语法： ARG &lt;name&gt;[=&lt;default value&gt;] 设置变量命令，ARG命令定义了一个变量，在docker build创建镜像的时候，使用 --build-arg =来指定参数 如果用户在build镜像时指定了一个参数没有定义在Dockerfile种，那么将有一个Warning 提示如下： [Warning] One or more build-args [foo] were not consumed. 我们可以定义一个或多个参数，如下： FROM busybox ARG user1 ARG buildno 也可以给参数一个默认值： FROM busybox ARG user1=someuser ARG buildno=1 如果我们给了ARG定义的参数默认值，那么当build镜像时没有指定参数值，将会使用这个默认值 ONBUILD 语法： ONBUILD [INSTRUCTION] 这个命令只对当前镜像的子镜像生效。 比如当前镜像为A，在Dockerfile种添加： ONBUILD RUN ls -al 这个 ls -al 命令不会在A镜像构建或启动的时候执行 此时有一个镜像B是基于A镜像构建的，那么这个ls -al 命令会在B镜像构建的时候被执行。 STOPSIGNAL 语法： STOPSIGNAL signal STOPSIGNAL命令是的作用是当容器停止时给系统发送什么样的指令，默认是15 HEALTHCHECK 容器健康状况检查命令 语法有两种： HEALTHCHECK [OPTIONS] CMD command HEALTHCHECK NONE 第一个的功能是在容器内部运行一个命令来检查容器的健康状况 第二个的功能是在基础镜像中取消健康检查命令 [OPTIONS]的选项支持以下三中选项： –interval=DURATION 两次检查默认的时间间隔为30秒 –timeout=DURATION 健康检查命令运行超时时长，默认30秒 –retries=N 当连续失败指定次数后，则容器被认为是不健康的，状态为unhealthy，默认次数是3 注意： HEALTHCHECK命令只能出现一次，如果出现了多次，只有最后一个生效。 CMD后边的命令的返回值决定了本次健康检查是否成功，具体的返回值如下： 0: success - 表示容器是健康的 1: unhealthy - 表示容器已经不能工作了 2: reserved - 保留值 例子： HEALTHCHECK --interval=5m --timeout=3s CMD curl -f http://localhost/ || exit 1 健康检查命令是：curl -f http://localhost/ || exit 1 两次检查的间隔时间是5秒 命令超时时间为3秒 ","link":"https://tianxiawuhao.github.io/jw7hb2R8R/"},{"title":"Docker ","content":"Docker 学习目标： 掌握Docker基础知识，能够理解Docker镜像与容器的概念 完成Docker安装与启动 掌握Docker镜像与容器相关命令 掌握Tomcat Nginx 等软件的常用应用的安装 掌握docker迁移与备份相关命令 能够运用Dockerfile编写创建容器的脚本 能够搭建与使用docker私有仓库 1 Docker简介 1.1 什么是虚拟化 ​ 在计算机中，虚拟化（英语：Virtualization）是一种资源管理技术，是将计算机的各种实体资源，如服务器、网络、内存及存储等，予以抽象、转换后呈现出来，打破实体结构间的不可切割的障碍，使用户可以比原本的组态更好的方式来应用这些资源。这些资源的新虚拟部份是不受现有资源的架设方式，地域或物理组态所限制。一般所指的虚拟化资源包括计算能力和资料存储。 ​ 在实际的生产环境中，虚拟化技术主要用来解决高性能的物理硬件产能过剩和老的旧的硬件产能过低的重组重用，透明化底层物理硬件，从而最大化的利用物理硬件 对资源充分利用 ​ 虚拟化技术种类很多，例如：软件虚拟化、硬件虚拟化、内存虚拟化、网络虚拟化(vip)、桌面虚拟化、服务虚拟化、虚拟机等等。 1.2 什么是Docker ​ Docker 是一个开源项目，诞生于 2013 年初，最初是 dotCloud 公司内部的一个业余项目。它基于 Google 公司推出的 Go 语言实现。 项目后来加入了 Linux 基金会，遵从了 Apache 2.0 协议，项目代码在 GitHub 上进行维护。 ​ ​ Docker 自开源后受到广泛的关注和讨论，以至于 dotCloud 公司后来都改名为 Docker Inc。Redhat 已经在其 RHEL6.5 中集中支持 Docker；Google 也在其 PaaS 产品中广泛应用。 ​ Docker 项目的目标是实现轻量级的操作系统虚拟化解决方案。 Docker 的基础是 Linux 容器（LXC）等技术。 ​ 在 LXC 的基础上 Docker 进行了进一步的封装，让用户不需要去关心容器的管理，使得操作更为简便。用户操作 Docker 的容器就像操作一个快速轻量级的虚拟机一样简单。 为什么选择Docker? （1）上手快。 ​ 用户只需要几分钟，就可以把自己的程序“Docker化”。Docker依赖于“写时复制”（copy-on-write）模型，使修改应用程序也非常迅速，可以说达到“随心所致，代码即改”的境界。 随后，就可以创建容器来运行应用程序了。大多数Docker容器只需要不到1秒中即可启动。由于去除了管理程序的开销，Docker容器拥有很高的性能，同时同一台宿主机中也可以运行更多的容器，使用户尽可能的充分利用系统资源。 （2）职责的逻辑分类 ​ 使用Docker，开发人员只需要关心容器中运行的应用程序，而运维人员只需要关心如何管理容器。Docker设计的目的就是要加强开发人员写代码的开发环境与应用程序要部署的生产环境一致性。从而降低那种“开发时一切正常，肯定是运维的问题（测试环境都是正常的，上线后出了问题就归结为肯定是运维的问题）” （3）快速高效的开发生命周期 ​ Docker的目标之一就是缩短代码从开发、测试到部署、上线运行的周期，让你的应用程序具备可移植性，易于构建，并易于协作。（通俗一点说，Docker就像一个盒子，里面可以装很多物件，如果需要这些物件的可以直接将该大盒子拿走，而不需要从该盒子中一件件的取。） （4）鼓励使用面向服务的架构 ​ Docker还鼓励面向服务的体系结构和微服务架构。Docker推荐单个容器只运行一个应用程序或进程，这样就形成了一个分布式的应用程序模型，在这种模型下，应用程序或者服务都可以表示为一系列内部互联的容器，从而使分布式部署应用程序，扩展或调试应用程序都变得非常简单，同时也提高了程序的内省性。（当然，可以在一个容器中运行多个应用程序） 1.3 容器与虚拟机比较 ​ 下面的图片比较了 Docker 和传统虚拟化方式的不同之处，可见容器是在操作系统层面上实现虚拟化，直接复用本地主机的操作系统，而传统方式则是在硬件层面实现。 与传统的虚拟机相比，Docker优势体现为启动速度快、占用体积小。 1.4 Docker 组件 1.4.1 Docker服务器与客户端 ​ Docker是一个客户端-服务器（C/S）架构程序。Docker客户端只需要向Docker服务器或者守护进程发出请求，服务器或者守护进程将完成所有工作并返回结果。Docker提供了一个命令行工具Docker以及一整套RESTful API。你可以在同一台宿主机上运行Docker守护进程和客户端，也可以从本地的Docker客户端连接到运行在另一台宿主机上的远程Docker守护进程。 1.4.2 Docker镜像与容器 ​ 镜像是构建Docker的基石。用户基于镜像来运行自己的容器。镜像也是Docker生命周期中的“构建”部分。镜像是基于联合文件系统的一种层式结构，由一系列指令一步一步构建出来。例如： 添加一个文件； 执行一个命令； 打开一个窗口。 也可以将镜像当作容器的“源代码”。镜像体积很小，非常“便携”，易于分享、存储和更新。 ​ Docker可以帮助你构建和部署容器，你只需要把自己的应用程序或者服务打包放进容器即可。容器是基于镜像启动起来的，容器中可以运行一个或多个进程。我们可以认为，镜像是Docker生命周期中的构建或者打包阶段，而容器则是启动或者执行阶段。 容器基于镜像启动，一旦容器启动完成后，我们就可以登录到容器中安装自己需要的软件或者服务。 所以Docker容器就是： ​ 一个镜像格式； ​ 一些列标准操作； ​ 一个执行环境。 ​ Docker借鉴了标准集装箱的概念。标准集装箱将货物运往世界各地，Docker将这个模型运用到自己的设计中，唯一不同的是：集装箱运输货物，而Docker运输软件。 和集装箱一样，Docker在执行上述操作时，并不关心容器中到底装了什么，它不管是web服务器，还是数据库，或者是应用程序服务器什么的。所有的容器都按照相同的方式将内容“装载”进去。 Docker也不关心你要把容器运到何方：我们可以在自己的笔记本中构建容器，上传到Registry，然后下载到一个物理的或者虚拟的服务器来测试，在把容器部署到具体的主机中。像标准集装箱一样，Docker容器方便替换，可以叠加，易于分发，并且尽量通用。 1.4.3 Registry（注册中心） ​ Docker用Registry来保存用户构建的镜像。Registry分为公共和私有两种。Docker公司运营公共的Registry叫做Docker Hub。用户可以在Docker Hub注册账号，分享并保存自己的镜像（说明：在Docker Hub下载镜像巨慢，可以自己构建私有的Registry）。 ​ https://hub.docker.com/ 2 Docker安装与启动 2.1 安装Docker ​ Docker官方建议在Ubuntu中安装，因为Docker是基于Ubuntu发布的，而且一般Docker出现的问题Ubuntu是最先更新或者打补丁的。在很多版本的CentOS中是不支持更新最新的一些补丁包的。 ​ 由于我们学习的环境都使用的是CentOS，因此这里我们将Docker安装到CentOS上。注意：这里建议安装在CentOS7.x以上的版本，在CentOS6.x的版本中，安装前需要安装其他很多的环境而且Docker很多补丁不支持更新。 ​ 请直接挂载课程配套的Centos7.x镜像 （1）yum 包更新到最新 sudo yum update （2）安装需要的软件包， yum-util 提供yum-config-manager功能，另外两个是devicemapper驱动依赖的 sudo yum install -y yum-utils device-mapper-persistent-data lvm2 （3）设置yum源为阿里云 sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo （4）安装docker sudo yum install docker-ce （5）安装后查看docker版本 docker -v 2.2 设置ustc的镜像 ustc是老牌的linux镜像服务提供者了，还在遥远的ubuntu 5.04版本的时候就在用。ustc的docker镜像加速器速度很快。ustc docker mirror的优势之一就是不需要注册，是真正的公共服务。 https://lug.ustc.edu.cn/wiki/mirrors/help/docker 编辑该文件： vi /etc/docker/daemon.json 在该文件中输入如下内容： { &quot;registry-mirrors&quot;: [&quot;https://docker.mirrors.ustc.edu.cn&quot;] } 2.3 Docker的启动与停止 systemctl命令是系统服务管理器指令 启动docker： systemctl start docker 停止docker： systemctl stop docker 重启docker： systemctl restart docker 查看docker状态： systemctl status docker 开机启动： systemctl enable docker 查看docker概要信息 docker info 查看docker帮助文档 docker --help 3 常用命令 3.1 镜像相关命令 3.1.1 查看镜像 docker images REPOSITORY：镜像名称 TAG：镜像标签 IMAGE ID：镜像ID CREATED：镜像的创建日期（不是获取该镜像的日期） SIZE：镜像大小 这些镜像都是存储在Docker宿主机的/var/lib/docker目录下 3.1.2 搜索镜像 如果你需要从网络中查找需要的镜像，可以通过以下命令搜索 docker search 镜像名称 NAME：仓库名称 DESCRIPTION：镜像描述 STARS：用户评价，反应一个镜像的受欢迎程度 OFFICIAL：是否官方 AUTOMATED：自动构建，表示该镜像由Docker Hub自动构建流程创建的 3.1.3 拉取镜像 拉取镜像就是从中央仓库中下载镜像到本地 docker pull 镜像名称 例如，我要下载centos7镜像 docker pull centos:7 Docker采用联合文件系统，不同镜像的相同文件无需再次下载： 4 删除镜像 按镜像ID删除镜像 docker rmi 镜像ID 删除所有镜像 docker rmi `docker images -q` 3.2 容器相关命令 3.2.1 查看容器 查看正在运行的容器 docker ps 查看所有容器 docker ps –a 查看最后一次运行的容器 docker ps –l 查看停止的容器 docker ps -f status=exited 3.2.2 创建与启动容器 创建容器常用的参数说明： 创建容器命令：docker run -i：表示运行容器 -t：表示容器启动后会进入其命令行。加入这两个参数后，容器创建就能登录进去。即分配一个伪终端。 --name :为创建的容器命名。 -v：表示目录映射关系（前者是宿主机目录，后者是映射到宿主机上的目录），可以使用多个－v做多个目录或文件映射。注意：最好做目录映射，在宿主机上做修改，然后共享到容器上。 -d：在run后面加上-d参数,则会创建一个守护式容器在后台运行（这样创建容器后不会自动登录容器，如果只加-i -t两个参数，创建后就会自动进去容器）。 -p：表示端口映射，前者是宿主机端口，后者是容器内的映射端口。可以使用多个-p做多个端口映射 （1）交互式方式创建容器 docker run -it --name=容器名称 镜像名称:标签 /bin/bash 这时我们通过ps命令查看，发现可以看到启动的容器，状态为启动状态 退出当前容器 exit （2）守护式方式创建容器： docker run -di --name=容器名称 镜像名称:标签 登录守护式容器方式： docker exec -it 容器名称 (或者容器ID) /bin/bash Exit # 从容器中退回主机 CTRL+Q+P # 容器不停止退出 3.2.3 停止与启动容器 停止容器： docker stop 容器名称（或者容器ID） 启动容器： docker start 容器名称（或者容器ID） 3.2.4 文件拷贝 如果我们需要将文件拷贝到容器内可以使用cp命令 docker cp 需要拷贝的文件或目录 容器名称:容器目录 也可以将文件从容器内拷贝出来 docker cp 容器名称:容器目录 需要拷贝的文件或目录 3.2.5 目录挂载 我们可以在创建容器的时候，将宿主机的目录与容器内的目录进行映射，这样我们就可以通过修改宿主机某个目录的文件从而去影响容器。 创建容器 添加-v参数 后边为 宿主机目录:容器目录，例如： docker run -di -v /usr/local/myhtml:/usr/local/myhtml --name=mycentos3 centos:7 如果你共享的是多级的目录，可能会出现权限不足的提示。 这是因为CentOS7中的安全模块selinux把权限禁掉了，我们需要添加参数 --privileged=true 来解决挂载的目录没有权限的问题 匿名挂载 docker run -d -v 容器内目录 镜像名/id # 匿名挂载 匿名挂载后，使用docker volume ls命令查看所有挂载的卷： 每一个VOLUME NAME对应一个挂载的卷，由于挂载时未指定主机目录，因此无法直接找到目录。 具名挂载 docker run -d -v 卷名：容器内目录 镜像名/id # 具名挂载 可以发现挂载的卷：volume01，并通过docker volume inspect 卷名 命令找到主机内目录： 所有docker容器内的卷，在未指定主机内目录时，都在：/var/lib/docker/volumes/卷名/_data 下，可通过具名挂载可以方便的找到卷，因此广泛使用这种方式进行挂载。 数据卷容器 docker run -it --name container02 --volumes from container01 镜像名/id # 将两个容器进行挂载 3.2.6 查看容器IP地址 我们可以通过以下命令查看容器运行的各种数据 docker inspect 容器名称（容器ID） 也可以直接执行下面的命令直接输出IP地址 docker inspect --format='{{.NetworkSettings.IPAddress}}' 容器名称（容器ID） 3.2.7 删除容器 删除指定的容器： docker rm 容器名称（容器ID） 3.2.8 其他命令 docker start/restart/stop/kill 容器名/id docker logs -tf --tail 显示的日志条数 容器名/id # 查看日志 docker top 容器名/id # 查看容器中的进程信息 docker inspect 容器名/id # 查看镜像的元数据 docker exec -it 容器名/id /bin/bash # 通常容器以后台方式运行，需要进入其中修改配置：进入容器后开启一个新终端 docker attach 容器名/id # 进入容器正在执行的终端 docker cp 容器名/id:容器内路径 主机文件路径 # 从容器内拷贝文件到主机上 4 Docker镜像详解 UnionFS（联合文件系统） 联合文件系统（UnionFS）是一种分层、轻量级并且高性能的文件系统，它支持对文件系统的修改作为一次提交来一层层的叠加，同时可以将不同目录挂载到同一个虚拟文件系统下。联合文件系统是 Docker 镜像的基础。镜像可以通过分层来进行继承，基于基础镜像（没有父镜像），可以制作各种具体的应用镜像。 特性：一次同时加载多个文件系统，但从外面看起来只能看到一个文件系统。联合加载会把各层文件系统叠加起来，这样最终的文件系统会包含所有底层的文件和目录。 镜像加载原理 Docker的镜像实际由一层一层的文件系统组成： bootfs（boot file system）主要包含bootloader和kernel。bootloader主要是引导加载kernel，完成后整个内核就都在内存中了。此时内存的使用权已由bootfs转交给内核，系统卸载bootfs。可以被不同的Linux发行版公用。 rootfs（root file system），包含典型Linux系统中的/dev，/proc，/bin，/etc等标准目录和文件。rootfs就是各种不同操作系统发行版（Ubuntu，Centos等）。因为底层直接用Host的kernel，rootfs只包含最基本的命令，工具和程序就可以了。 分层理解 所有的Docker镜像都起始于一个基础镜像层，当进行修改或增加新的内容时，就会在当前镜像层之上，创建新的容器层。 容器在启动时会在镜像最外层上建立一层可读写的容器层（R/W），而镜像层是只读的（R/O）。 docker commit -m=&quot;描述信息&quot; -a=&quot;作者&quot; 容器id 目标镜像名:[tag] # 编辑容器后提交容器成为一个新镜像 4 应用部署 4.1 MySQL部署 （1）拉取mysql镜像 docker pull centos/mysql-57-centos7 （2）创建容器 docker run -di --name=tensquare_mysql -p 33306:3306 -e MYSQL_ROOT_PASSWORD=123456 mysql -p 代表端口映射，格式为 宿主机映射端口:容器运行端口 -e 代表添加环境变量 MYSQL_ROOT_PASSWORD 是root用户的登陆密码 （3）远程登录mysql 连接宿主机的IP ,指定端口为33306 4.2 tomcat部署 （1）拉取镜像 docker pull tomcat:7-jre7 （2）创建容器 创建容器 -p表示地址映射 docker run -di --name=mytomcat -p 9000:8080 -v /usr/local/webapps:/usr/local/tomcat/webapps tomcat:7-jre7 4.3 Nginx部署 （1）拉取镜像 docker pull nginx （2）创建Nginx容器 docker run -di --name=mynginx -p 80:80 nginx 4.4 Redis部署 （1）拉取镜像 docker pull redis （2）创建容器 docker run -di --name=myredis -p 6379:6379 redis 4.5 Redis集群部署 # 创建网卡 docker network create redis --subnet 172.38.0.0/16 # 通过脚本创建六个redis配置 for port in $(seq 1 6);\\ do \\ mkdir -p /mydata/redis/node-${port}/conf touch /mydata/redis/node-${port}/conf/redis.conf cat &lt;&lt; EOF &gt;&gt; /mydata/redis/node-${port}/conf/redis.conf port 6379 bind 0.0.0.0 cluster-enabled yes cluster-config-file nodes.conf cluster-node-timeout 5000 cluster-announce-ip 172.38.0.1${port} cluster-announce-port 6379 cluster-announce-bus-port 16379 appendonly yes EOF done # 通过脚本运行六个redis for port in $(seq 1 6);\\ docker run -p 637${port}:6379 -p 1667${port}:16379 --name redis-${port} \\ -v /mydata/redis/node-${port}/data:/data \\ -v /mydata/redis/node-${port}/conf/redis.conf:/etc/redis/redis.conf \\ -d --net redis --ip 172.38.0.1${port} redis:5.0.9-alpine3.11 redis-server /etc/redis/redis.conf docker exec -it redis-1 /bin/sh #redis默认没有bash redis-cli --cluster create 172.38.0.11:6379 172.38.0.12:6379 172.38.0.13:6379 172.38.0.14:6379 172.38.0.15:6379 172.38.0.16:6379 --cluster-replicas 1 5 迁移与备份 5.1 容器保存为镜像 我们可以通过以下命令将容器保存为镜像 docker commit mynginx mynginx_i 5.2 镜像备份 我们可以通过以下命令将镜像保存为tar 文件 docker save -o mynginx.tar mynginx_i 5.3 镜像恢复与迁移 首先我们先删除掉mynginx_img镜像 然后执行此命令进行恢复 docker load -i mynginx.tar -i 输入的文件 执行后再次查看镜像，可以看到镜像已经恢复 6 Dockerfile 6.1 什么是Dockerfile Dockerfile是由一系列命令和参数构成的脚本，这些命令应用于基础镜像并最终创建一个新的镜像。 1、对于开发人员：可以为开发团队提供一个完全一致的开发环境； 2、对于测试人员：可以直接拿开发时所构建的镜像或者通过Dockerfile文件构建一个新的镜像开始工作了； 3、对于运维人员：在部署时，可以实现应用的无缝移植。 6.2 常用命令 命令 作用 FROM image_name:tag 定义了使用哪个基础镜像启动构建流程 MAINTAINER user_name 声明镜像的创建者 ENV key value 设置环境变量 (可以写多条) RUN command 是Dockerfile的核心部分(可以写多条) ADD source_dir/file dest_dir/file 将宿主机的文件复制到容器内，如果是一个压缩文件，将会在复制后自动解压 COPY source_dir/file dest_dir/file 和ADD相似，但是如果有压缩文件并不能解压 WORKDIR path_dir 设置工作目录 6.3 使用脚本创建镜像 步骤： （1）创建目录 mkdir –p /usr/local/dockerjdk8 （2）下载jdk-8u171-linux-x64.tar.gz并上传到服务器（虚拟机）中的/usr/local/dockerjdk8目录 （3）创建文件Dockerfile vi Dockerfile #依赖镜像名称和ID FROM centos:7 #指定镜像创建者信息 MAINTAINER ITCAST #切换工作目录 WORKDIR /usr RUN mkdir /usr/local/java #ADD 是相对路径jar,把java添加到容器中 ADD jdk-8u171-linux-x64.tar.gz /usr/local/java/ #配置java环境变量 ENV JAVA_HOME /usr/local/java/jdk1.8.0_171 ENV JRE_HOME $JAVA_HOME/jre ENV CLASSPATH $JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib:$CLASSPATH ENV PATH $JAVA_HOME/bin:$PATH （4）执行命令构建镜像 docker build -t='jdk1.8' . 注意后边的空格和点，不要省略 （5）查看镜像是否建立完成 docker images 7 Docker私有仓库 7.1 私有仓库搭建与配置 （1）拉取私有仓库镜像（此步省略） docker pull registry （2）启动私有仓库容器 docker run -di --name=registry -p 5000:5000 registry （3）打开浏览器 输入地址http://192.168.184.141:5000/v2/_catalog看到{&quot;repositories&quot;:[]} 表示私有仓库搭建成功并且内容为空 （4）修改daemon.json vi /etc/docker/daemon.json 添加以下内容，保存退出。 {&quot;insecure-registries&quot;:[&quot;192.168.184.141:5000&quot;]} 此步用于让 docker信任私有仓库地址 （5）重启docker 服务 systemctl restart docker 7.2 镜像上传至私有仓库 （1）标记此镜像为私有仓库的镜像 docker tag jdk1.8 192.168.184.141:5000/jdk1.8 （2）再次启动私服容器 docker start registry （3）上传标记的镜像 docker push 192.168.184.141:5000/jdk1.8 8 Docker网络 8.1 理解Doker0 通过命令ip addr查看本地ip地址，我们发现除了本机回环地址和埃里远的内网地址外，还多了一个网卡：Docker0，这是Docker服务启动后自动生成的。 而如果进入一个正在后台运行的tomcat容器，同样使用ip addr命令，发现容器得到了一个新的网络：12: eth@if13，ip地址：172.17.0.2。这是Docker在容器启动时为其分配的。 思考一个问题：此时我们的linux主机可以ping通容器内部（172.17.0.2）吗？（注意与容器暴露端口相区分) linux可以ping通docker容器内部，因为docker0的ip地址为172.17.0.1，容器为172.17.0.2。 原理：我们每启动一个docker容器，docker就会给容器分配一个默认的可用ip，我们只要安装了docker，就会有一个网卡docker0(bridge)。网卡采用桥接模式，并使用veth-pair技术（veth-pair就是一堆虚拟设备接口，成对出现，一段连着协议，一段彼此相连，充当一个桥梁。）。 这时我们退出容器，回到主机再次观察主机的ip地址： 我们惊奇地发现了一个新网络13: vethda1df4b@if12，对应容器内网络地址的12: eth@if13。 容器和容器之间是可以互相ping通的：容器1→Docker0→容器2 docker中的所有网络接口都是虚拟的 ，转发效率高。删除容器后，对应的网桥也随之删除。 8.2 --link 若编写一个微服务并连接数据库，如果数据库ip改变，如何根据容器名而不是ip访问容器？显然，直接使用容器名是无法ping通容器内部的： 这时我们可以在容器启动命令中加入一个选项：--link，使得我们可以根据容器名来访问容器。 docker run -d -P --link 容器名/id 镜像名/id 然而反向就不可以ping通，这是因为--link的本质是把需要连接的容器名/id写入启动容器的配置文件中，即增加了一个ip和容器名/id的映射： 目前已经不建议使用这种方式。 8.3 自定义网络 我们使用命令： docker network ls # 查看所有的docker网络 docker中的网络模式有： bridge：桥接（docker默认）/ none：不配置网络 / host：和宿主机共享网络 docker run 命令默认带有一个参数--net bridge，此处的bridge指的就是docker0。如果我们不想使用docker0，那如何创建一个新的网络呢？ docker network create --driver 网络模式 --subnet 子网ip --gateway 网关 网络名 我们不仅在docker network ls命令下发现了这个新创建的网络newnet，还可以使用docker network inspect命令查看其详细信息，包括了我们创建时定义的子网ip和网关： 只要两个容器启动时都通过 --net，选用了同一个已创建的网络，不同容器间即可通过ip地址或容器名/id连通: 8.4 网络连通 对于建立在不同网络下(docker0, newnet)的两个容器tomcat01和tomcat02，他们的网段不同，因此是无法彼此ping通容器内部的： 这时我们需要通过docker network connect命令打通容器与网络之间的连接： docker network connect 网络名 容器名/id 这个功能类似于将一个容器赋予多个ip地址，同样可以用docker network inspect命令查看网络连通后，该网络的变化： 原本newnet网络中只含有tomcat02，现在增加了tomcat01，因此可以连通。 ","link":"https://tianxiawuhao.github.io/VSWRPm3xo/"},{"title":"Git 操作命令清单","content":"git工作流程图 下面是常用 的Git 命令清单。几个专用名词的译名如下： Workspace：工作区 Index / Stage：暂存区 Repository：仓库区（或本地仓库） Remote：远程仓库 一、新建代码库 # 在当前目录新建一个Git代码库 $ git init # 新建一个目录，将其初始化为Git代码库 $ git init [project-name] # 下载一个项目和它的整个代码历史 $ git clone [url] 二、配置 Git的设置文件为.gitconfig，它可以在用户主目录下（全局配置），也可以在项目目录下（项目配置）。 # 显示当前的Git配置 $ git config --list # 编辑Git配置文件 $ git config -e [--global] # 设置提交代码时的用户信息 $ git config [--global] user.name &quot;[name]&quot; $ git config [--global] user.email &quot;[email address]&quot; 三、增加/删除文件 # 添加指定文件到暂存区 $ git add [file1] [file2] ... # 添加指定目录到暂存区，包括子目录 $ git add [dir] # 添加当前目录的所有文件到暂存区 $ git add . # 添加每个变化前，都会要求确认 # 对于同一个文件的多处变化，可以实现分次提交 $ git add -p # 删除工作区文件，并且将这次删除放入暂存区 $ git rm [file1] [file2] ... # 停止追踪指定文件，但该文件会保留在工作区 $ git rm --cached [file] # 改名文件，并且将这个改名放入暂存区 $ git mv [file-original] [file-renamed] 四、代码提交 # 提交暂存区到仓库区 $ git commit -m [message] # 提交暂存区的指定文件到仓库区 $ git commit [file1] [file2] ... -m [message] # 提交工作区自上次commit之后的变化，直接到仓库区 $ git commit -a # 提交时显示所有diff信息 $ git commit -v # 使用一次新的commit，替代上一次提交 # 如果代码没有任何新变化，则用来改写上一次commit的提交信息 $ git commit --amend -m [message] # 重做上一次commit，并包括指定文件的新变化 $ git commit --amend [file1] [file2] ... 五、分支 # 列出所有本地分支 $ git branch # 列出所有远程分支 $ git branch -r # 列出所有本地分支和远程分支 $ git branch -a # 新建一个分支，但依然停留在当前分支 $ git branch [branch-name] # 新建一个分支，并切换到该分支 $ git checkout -b [branch] # 新建一个分支，指向指定commit $ git branch [branch] [commit] # 新建一个分支，与指定的远程分支建立追踪关系 $ git branch --track [branch] [remote-branch] # 切换到指定分支，并更新工作区 $ git checkout [branch-name] # 切换到上一个分支 $ git checkout - # 建立追踪关系，在现有分支与指定的远程分支之间 $ git branch --set-upstream [branch] [remote-branch] # 合并指定分支到当前分支 $ git merge [branch] # 选择一个commit，合并进当前分支 $ git cherry-pick [commit] # 删除分支 $ git branch -d [branch-name] # 删除远程分支 $ git push origin --delete [branch-name] $ git branch -dr [remote/branch] 六、标签 # 列出所有tag $ git tag # 新建一个tag在当前commit $ git tag [tag] # 新建一个tag在指定commit $ git tag [tag] [commit] # 删除本地tag $ git tag -d [tag] # 删除远程tag $ git push origin :refs/tags/[tagName] # 查看tag信息 $ git show [tag] # 提交指定tag $ git push [remote] [tag] # 提交所有tag $ git push [remote] --tags # 新建一个分支，指向某个tag $ git checkout -b [branch] [tag] 七、查看信息 # 显示有变更的文件 $ git status # 显示当前分支的版本历史 $ git log # 显示commit历史，以及每次commit发生变更的文件 $ git log --stat # 搜索提交历史，根据关键词 $ git log -S [keyword] # 显示某个commit之后的所有变动，每个commit占据一行 $ git log [tag] HEAD --pretty=format:%s # 显示某个commit之后的所有变动，其&quot;提交说明&quot;必须符合搜索条件 $ git log [tag] HEAD --grep feature # 显示某个文件的版本历史，包括文件改名 $ git log --follow [file] $ git whatchanged [file] # 显示指定文件相关的每一次diff $ git log -p [file] # 显示过去5次提交 $ git log -5 --pretty --oneline # 显示所有提交过的用户，按提交次数排序 $ git shortlog -sn # 显示指定文件是什么人在什么时间修改过 $ git blame [file] # 显示暂存区和工作区的差异 $ git diff # 显示暂存区和上一个commit的差异 $ git diff --cached [file] # 显示工作区与当前分支最新commit之间的差异 $ git diff HEAD # 显示两次提交之间的差异 $ git diff [first-branch]...[second-branch] # 显示今天你写了多少行代码 $ git diff --shortstat &quot;@{0 day ago}&quot; # 显示某次提交的元数据和内容变化 $ git show [commit] # 显示某次提交发生变化的文件 $ git show --name-only [commit] # 显示某次提交时，某个文件的内容 $ git show [commit]:[filename] # 显示当前分支的最近几次提交 $ git reflog 八、远程同步 # 下载远程仓库的所有变动 $ git fetch [remote] # 显示所有远程仓库 $ git remote -v # 显示某个远程仓库的信息 $ git remote show [remote] # 增加一个新的远程仓库，并命名 $ git remote add [shortname] [url] # 取回远程仓库的变化，并与本地分支合并 $ git pull [remote] [branch] # 上传本地指定分支到远程仓库 $ git push [remote] [branch] # 强行推送当前分支到远程仓库，即使有冲突 $ git push [remote] --force # 推送所有分支到远程仓库 $ git push [remote] --all 九、撤销 # 恢复暂存区的指定文件到工作区 $ git checkout [file] # 恢复某个commit的指定文件到暂存区和工作区 $ git checkout [commit] [file] # 恢复暂存区的所有文件到工作区 $ git checkout . # 重置暂存区的指定文件，与上一次commit保持一致，但工作区不变 $ git reset [file] # 重置暂存区与工作区，与上一次commit保持一致 $ git reset --hard # 重置当前分支的指针为指定commit，同时重置暂存区，但工作区不变 $ git reset [commit] # 重置当前分支的HEAD为指定commit，同时重置暂存区和工作区，与指定commit一致 $ git reset --hard [commit] # 重置当前HEAD为指定commit，但保持暂存区和工作区不变 $ git reset --keep [commit] # 新建一个commit，用来撤销指定commit # 后者的所有变化都将被前者抵消，并且应用到当前分支 $ git revert [commit] # 暂时将未提交的变化移除，稍后再移入 $ git stash $ git stash pop 十、其他 # 生成一个可供发布的压缩包 $ git archive ","link":"https://tianxiawuhao.github.io/awhwcpAEp/"},{"title":"jar包内类动态加载","content":"实体对象 public class CmptDef { private String idCmptDef; //组件在DB里的唯一ID号 private String name;//组件的唯一名称，只能是字母、数字、下划线组成，大于1小雨50，通常为Jar的名字 private CmptCategory cmptCategory; //种类 private String fullQualifiedName; //组件入口类全限定名 private String extraParas; //组件额外参数 private String formUrl; //组件自定义表单 private Float version = 1.0f; //组件版本 private CmptExecutePos executePos = CmptExecutePos.PRE; //组件在网关中执行的位置 private Integer priority = 100; //组件在相同位置的执行优先级，数字越小越高 private Integer timeout = 1000;//组件执行超时时间，单位毫秒 private String description; //组件描述 private Boolean defaultVersion; //是否是默认有效版本 private CmptStatus cmptStatus = CmptStatus.editing; // 组件状态 private String remarks; // 组件发布时需要填写备注信息 private String code; // 组件code，同一个组件的多个版本的code是一样的 private Date releaseTime; // 组件发布时间 private CustomFormCode customFormCode; // 自定义表单里类型 private String cmptType; //组件类型（特殊组件|普通组件|默认组件） } /** * 组件类型。 */ public enum CmptCategory { AUTHENTICATION, //认证 AUTHORIZATION, //鉴权 FLOW_MANAGEMENT, //流量管理 REQUEST_COUNT_MANAGEMENT, //请求次数管理 CACHE, //缓存 ROUTER, //路由 TRANSFORM, //数据转换 LOGGER, //日志 OTHER//其它 } /** * 组件执行的位置。 */ public enum CmptExecutePos { PRE, //调用上游服务前 ROUTING, //调用中 AFTER //调用后 } public enum CmptStatus { editing, //编辑中 published, //已经发布 offline //已经下线 } /** * 组件风格。 */ public enum CustomFormCode { RESTFUL_FORM, SQL_FORM, DUBBO_FORM, OTHER, MQ_FORM, WEBSERVICE_FORM } 方法接口 public interface ICmptService { /** * 根据组件类名和版本从缓存中获取组件,如果不存在则尝试动态加载组件类，实例化并刷新缓存后再获取，还是不存在则返回null； * 另外配置更新会有单独的线程刷新缓存。 * * @return */ ICmpt getCmptInstance(final CmptDef cmptDef); /** * 刷新API关联的组件配置信息 * * @param apis * @param ignoreRefreshTime * @return */ boolean refreshCmptInstanceCache(List&lt;Api&gt; apis, boolean ignoreRefreshTime); /** * 删除组件实例缓存 * * @param fullQualifiedName * @param code */ void removeCmpt(final String fullQualifiedName, final Float version, final String code); } 方法实现 @Service public class CmptServiceImpl implements ICmptService { private static Logger logger = LoggerFactory.getLogger(CmptServiceImpl.class); @Value(&quot;${cmpt.dynamicLoadCmptClass}&quot;) private boolean dynamicLoadCmptClass; @Autowired private ICmptDefService cmptDefService; @Override public synchronized ICmpt getCmptInstance(final CmptDef cmptDef) { ICmpt cmpt = CmptInstanceHolder.getInstance().getCmpt(cmptDef.getFullQualifiedName(), cmptDef.getVersion(), cmptDef.getCode()); if (null == cmpt) { if (logger.isDebugEnabled()) { logger.debug(&quot;反射拿到实例&gt;&gt;&gt; &quot; + cmptDef.getFullQualifiedName()); } if (this.dynamicLoadCmptClass) { cmpt = CmptClassLoaderUtil.newInstance(cmptDef); } else { try { Class clazz = Class.forName(cmptDef.getFullQualifiedName()); cmpt = (ICmpt) clazz.newInstance(); } catch (Exception e) { e.printStackTrace(); } } if (null != cmpt) { if (cmpt instanceof AbstractCmpt) { //设置版本号 ((AbstractCmpt) cmpt).setVersion(cmptDef.getVersion()); ((AbstractCmpt) cmpt).setIdCmptDef(cmptDef.getIdCmptDef()); } CmptInstanceHolder.getInstance().addEntry(cmpt, cmptDef.getFullQualifiedName(), cmptDef.getVersion(), cmptDef.getCode()); } } else { if (logger.isDebugEnabled()) { logger.debug(&quot;缓存拿到实例&gt;&gt;&gt; &quot; + cmptDef.getFullQualifiedName()); } } if (null != cmpt) { ApplicationContext context = SpringContextHolder.getContext(); if (null != context) { ZkClient zkClient = context.getBean(ZkClient.class); if (null != zkClient) { BaseListener listener = zkClient.getListener(); listener.addObserver((AbstractCmpt) cmpt); } } } return cmpt; } @Override public boolean refreshCmptInstanceCache(List&lt;Api&gt; apis, boolean ignoreRefreshTime) { return false; } @Override public synchronized void removeCmpt(final String fullQualifiedName, final Float version, final String code) { ICmpt cmpt = CmptInstanceHolder.getInstance().getCmpt(fullQualifiedName, version, code); if (null != cmpt) { ApplicationContext context = SpringContextHolder.getContext(); if (null != context) { ZkClient zkClient = context.getBean(ZkClient.class); if (null != zkClient) { BaseListener listener = zkClient.getListener(); listener.deleteObserver((AbstractCmpt) cmpt); } } cmpt.destroy(); CmptDef cmptDef = CmptDefHolder.getInstance().getCmptDef(fullQualifiedName, version, code); if (null == cmptDef) { cmptDef = new CmptDef(); cmptDef.setFullQualifiedName(fullQualifiedName); cmptDef.setVersion(version); cmptDef.setCode(code); } //删除引用实体 CmptInstanceHolder.getInstance().removeEntry(fullQualifiedName, version, code); cmpt = null; String jarPath = CmptClassLoaderUtil.getJarPath(cmptDef); if (logger.isDebugEnabled()) { logger.debug(&quot;尝试卸载jar包: &quot; + jarPath); } CmptClassLoaderManager.unLoadJar(jarPath); } CmptInstanceHolder.getInstance().removeEntry(fullQualifiedName, version, code); } } 工具类 /** * 组件配置缓存持有者 */ public class CmptInstanceHolder { private static CmptInstanceHolder cmptInstanceHolder = new CmptInstanceHolder(); //API所关联的组件配置缓存 key: fullQualifiedName value: ICmpt private Map&lt;String, ICmpt&gt; cmpts = new ConcurrentHashMap&lt;&gt;(); private CmptInstanceHolder() { } public static CmptInstanceHolder getInstance() { return CmptInstanceHolder.cmptInstanceHolder; } /** * 根据类名加版本从缓存中获取组件实例,如果不存在直接返回null； * * @return */ public ICmpt getCmpt(final String fullQualifiedName, final Float version, String code) { final String key = buildKey(fullQualifiedName, version, code); return this.cmpts.get(key); } /** * 添加对象 */ public void addEntry(final ICmpt cmpt, final String fullQualifiedName, final Float version, final String code) { if (cmpt == null || StringUtils.isEmpty(fullQualifiedName) || StringUtils.isEmpty(code)) { return; } final String key = buildKey(fullQualifiedName, version, code); removeEntry(fullQualifiedName, version, code); this.cmpts.put(key, cmpt); } private String buildKey(final String fullQualifiedName, Float version, final String code) { return fullQualifiedName + &quot;.&quot; + version + code; } public void removeEntry(final String fullQualifiedName, final Float version, final String code) { ICmpt remove = this.cmpts.remove(buildKey(fullQualifiedName, version, code)); if (null != remove) { remove.destroy(); } } /** * 获取所有的组件实例 * @return */ public Map&lt;String, ICmpt&gt; getAllCmpts(){ return this.cmpts; } } public class CmptClassLoaderUtil { private static Logger logger = LoggerFactory.getLogger(CmptClassLoaderUtil.class); public static ICmpt newInstance(final CmptDef cmptDef) { ICmpt cmpt = null; try { final String jarPath = getJarPath(cmptDef); logger.info(&quot;尝试载入jar包,jar包路径: &quot; + jarPath); //加载依赖jar CmptClassLoader cmptClassLoader = CmptClassLoaderManager.loadJar(cmptDef.getIdCmptDef(), jarPath, true); // 创建实例 if (null != cmptClassLoader) { cmpt = LoadClassUtil.newObject(cmptDef, ICmpt.class, cmptClassLoader); } else { logger.error(&quot;加载组件jar包失败! jarPath: &quot; + jarPath); } } catch (Exception e) { logger.error(&quot;组件类加载失败，请检查类名和版本是否正确。ClassName=&quot; + cmptDef.getFullQualifiedName() + &quot;, Version=&quot; + cmptDef.getVersion()); e.printStackTrace(); } return cmpt; } public static String getJarPath(final CmptDef cmptDef) { StringBuffer sb = new StringBuffer(); sb.append(AppConfigUtil.getValue(&quot;app.home&quot;)); sb.append(AppConfigUtil.getValue(&quot;cmpt.location&quot;)); sb.append(&quot;/&quot;); //开发中关闭多级目录,不好部署 sb.append(cmptDef.getCode()); sb.append(&quot;/&quot;); sb.append(cmptDef.getVersion()); sb.append(&quot;/&quot;); String[] split = cmptDef.getFullQualifiedName().split(&quot;\\\\.&quot;); sb.append(split[split.length - 1]); sb.append(&quot;_&quot;); sb.append(cmptDef.getVersion()); sb.append(&quot;.jar&quot;); if (logger.isDebugEnabled()) { logger.debug(&quot;构建jar包路径: &quot; + sb.toString()); } return sb.toString(); } } #linux版home目录 app.home=/work/sharestore #组件资源存放路径，&lt;home&gt;/cmpt/&lt;code&gt;/&lt;version&gt;/&lt;name&gt;_&lt;version&gt;.jar,html cmpt.location=/cmpt /** * 类加载器管理, 加载, 卸载jar包 */ public class CmptClassLoaderManager { private static final Logger logger = Logger.getLogger(CmptClassLoaderManager.class); private static Map&lt;String, CmptClassLoader&gt; classLoaderMap = new ConcurrentHashMap&lt;&gt;(); private CmptClassLoaderManager() { } /** * 载入Jar包, 判断是否重新载入Jar包 * * @param fileName * @param isReloadJar * @return */ public static CmptClassLoader loadJar(String IdCmptDef, String fileName, boolean isReloadJar) { fileName = FileUtil.fixFileName(fileName); CmptClassLoader cmptClassLoader = classLoaderMap.get(IdCmptDef); if (isReloadJar || null == cmptClassLoader) { if (logger.isDebugEnabled()) { logger.debug(&quot;从文件载入jar组件&quot;); } return loadJar(IdCmptDef, fileName); } else { if (logger.isDebugEnabled()) { logger.debug(&quot;从缓存载入jar组件&quot;); } return cmptClassLoader; } } /** * 载入Jar包, 以新Jar包载入 * * @param fileName * @return 返回一个类加载器 */ private static CmptClassLoader loadJar(String IdCmptDef, String fileName) { CmptClassLoader loader; try { boolean exists = new File(fileName).exists(); if (exists) { if (null == classLoaderMap.get(IdCmptDef) || unLoadJar(IdCmptDef)) { loader = new CmptClassLoader(); boolean loadJar = loader.addURL(fileName); if (loadJar) { classLoaderMap.put(IdCmptDef, loader); return loader; } else { loader.close(); } } } else { throw new IllegalArgumentException(&quot;传入参数错误,文件不存在! file: &quot; + fileName); } } catch (Exception e) { logger.error(&quot;&quot;,e); } return null; } public static boolean unLoadJar(String IdCmptDef) { boolean unLoadJar = false; if (logger.isDebugEnabled()) { logger.debug(&quot;请求卸载jar包: &quot; + IdCmptDef); } try { CmptClassLoader loader = classLoaderMap.remove(IdCmptDef); if (null != loader) { loader.close(); } unLoadJar = true; } catch (Exception e) { logger.error(&quot;&quot;,e); } return unLoadJar; } /** * 获取组件定义对应的实例的类加载器 * * @param idCmptDef * @return */ public static ClassLoader getCmptClassLoader(String idCmptDef) { return classLoaderMap.get(idCmptDef); } } public class FileUtil { public static String fixFileName(String fileName) { if (null == fileName) fileName = &quot;&quot;; fileName = fileName.replaceAll(&quot;\\\\\\\\&quot;, &quot;/&quot;); fileName = fileName.replaceAll(&quot;//&quot;, &quot;/&quot;); return fileName; } } /** * 类加载器终极版, 加载, 卸载jar包 */ public class CmptClassLoader extends URLClassLoader { private static final Logger logger = Logger.getLogger(CmptClassLoader.class); CmptClassLoader(URL[] urls, ClassLoader parent) { super(urls, parent); } CmptClassLoader(URL[] urls) { super(urls); } CmptClassLoader(URL[] urls, ClassLoader parent, URLStreamHandlerFactory factory) { super(urls, parent, factory); } CmptClassLoader() { super(new URL[]{}, findParentClassLoader()); } /** * 载入Jar * * @param fileName * @return */ boolean addURL(String fileName) { try { URL url = new File(fileName).toURL(); URLClassPath ucp = this.getUCP(); ucp.addURL(url); } catch (Exception e) { e.printStackTrace(); return false; } return true; } /** * 定位当前父类加载器 * * @return */ private static ClassLoader findParentClassLoader() { ClassLoader parent = logger.getClass().getClassLoader(); if (parent == null) { throw new RuntimeException(&quot;无法获取当前父加载器!&quot;); } return parent; } private URLClassPath getUCP() { URLClassPath ucp = null; try { Field declaredField = URLClassLoader.class.getDeclaredField(&quot;ucp&quot;); declaredField.setAccessible(true); Object o = declaredField.get(this); ucp = (URLClassPath) o; } catch (IllegalAccessException e) { e.printStackTrace(); } catch (NoSuchFieldException e) { e.printStackTrace(); } return ucp; } @Override protected void finalize() throws Throwable { super.finalize(); this.close(); } } /** * 类加载器工具类 */ public class LoadClassUtil { private final static LoadClassUtil LOAD_JAR_UTIL = new LoadClassUtil(); private static Logger logger = LoggerFactory.getLogger(LoadClassUtil.class); private LoadClassUtil() { } /** * 载入jar包 * 将jar包路径添加到系统类加载器扫描类和资源的文件列表里 * * @param fileName jar绝对路径 * @return */ public static boolean loadJar(String fileName) { try { if (strNotNull(fileName) &amp;&amp; fileExists(fileName)) {//(ClassLoader要与当前程序同一个loader) getMethod().invoke(LOAD_JAR_UTIL.getClass().getClassLoader(), getURL(fileName));//添加路径URL //getMethod().invoke(Launcher.getLauncher().getClassLoader(), getURL(fileName));//添加路径URL return true; } else { throw new IllegalArgumentException(&quot;传入参数错误,文件或不存在! file: &quot; + fileName); } } catch (Exception e) { logger.error(&quot;&quot;,e); } return false; } /** * 判断字符串不为空 * * @param str * @return */ private static boolean strNotNull(String str) { return null != str &amp;&amp; !str.equals(&quot;&quot;); } private static boolean fileExists(String fileName) { return new File(fileName).exists(); } /** * 拿到添加扫描类和资源的路径URL的方法 * * @return * @throws NoSuchMethodException */ private static Method getMethod() throws NoSuchMethodException { Method method = URLClassLoader.class.getDeclaredMethod(&quot;addURL&quot;, URL.class); // 破解方法的访问权限 method.setAccessible(true); return method; } /** * 得到一个URL * * @param fileName * @return * @throws MalformedURLException */ private static URL getURL(String fileName) throws MalformedURLException { return new File(fileName).toURI().toURL(); } /** * 创建对象实例 * * @param className 全限定名 * @return */ public static Object newObject(String className) { try { return Class.forName(className).newInstance(); } catch (Exception e) { logger.error(&quot;&quot;,e); } return null; } /** * 创建转换类型后的对象实例 * * @param className 全限定名 * @param tClass 返回类型 * @param &lt;T&gt; * @return */ public static &lt;T&gt; T newObject(String className, Class&lt;T&gt; tClass) { try { return (T) Class.forName(className).newInstance(); } catch (Exception e) { logger.error(&quot;&quot;,e); } return null; } /** * 创建转换类型后的对象实例 * * @param cmptDef 组件定义-&gt;全限定名 * @param tClass 返回类型 * @param loader 类加载器 * @param &lt;T&gt; * @return */ public static &lt;T&gt; T newObject(CmptDef cmptDef, Class&lt;T&gt; tClass, ClassLoader loader) { try { String className = cmptDef.getFullQualifiedName(); Object newInstance = Class.forName(className, true, loader).newInstance(); return (T) newInstance; } catch (Exception e) { String jarPath = CmptClassLoaderUtil.getJarPath(cmptDef); logger.error(&quot;创建组件实例失败! className=&quot; + cmptDef.getFullQualifiedName() + &quot; jarPath=&quot; + jarPath); logger.error(&quot;创建出错组件:&quot; + cmptDef.getName() + cmptDef.getVersion() + &quot; ,文件信息:&quot; + FileUtil.fileInfo(jarPath)); try { Class&lt;?&gt; aClass = loader.loadClass(cmptDef.getFullQualifiedName()); } catch (ClassNotFoundException e1) { e1.printStackTrace(); logger.error(&quot;类未载入:&quot; + cmptDef.getFullQualifiedName()); } } return null; } /** * 执行对象的方法 * * @param o 实例对象 * @param methodName 方法名称 * @param args 形参 * @return */ public static Object invokeMethod(Object o, String methodName, Object... args) { try { Object invoke; if (null == args || args.length == 0) {//有缺陷,如果形参是任意Object,但传入参数是null,无法得到形参类型,得到有参的方法. Method method = o.getClass().getMethod(methodName); invoke = method.invoke(o); } else { Method method = o.getClass().getMethod(methodName, args.getClass()); invoke = method.invoke(o, args); } return invoke; } catch (Exception e) { logger.error(&quot;&quot;,e); } return null; } } 组件实现公共接口类 public interface ICmpt { /** * 组件执行入口 * * @param request * @param config，组件实例的参数配置 * @param actionNode, 当前执行的流程节点 * @param procContextDTO, 流程引擎实例上下文 * @return */ CmptResult execute(CmptRequest request, Map&lt;String, FieldDTO&gt; config, ActionNode actionNode, ProcContextDTO procContextDTO); /** * 销毁组件持有的特殊资源，比如线程。 */ void destroy(); } ","link":"https://tianxiawuhao.github.io/E2kaihQ5a/"},{"title":"mybatis级联查询","content":"ModelMapper.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt; &lt;mapper namespace=&quot;com.haier.biz.mapper.ModelMapper&quot;&gt; &lt;resultMap id=&quot;BaseResultMap&quot; type=&quot;com.haier.biz.entity.Model&quot;&gt; &lt;id column=&quot;model_id&quot; jdbcType=&quot;BIGINT&quot; property=&quot;modelId&quot;/&gt; &lt;result column=&quot;equipment_model_mark&quot; jdbcType=&quot;VARCHAR&quot; property=&quot;equipmentModelMark&quot;/&gt; &lt;result column=&quot;equipment_name&quot; jdbcType=&quot;VARCHAR&quot; property=&quot;equipmentName&quot;/&gt; &lt;result column=&quot;specification_model&quot; jdbcType=&quot;VARCHAR&quot; property=&quot;specificationModel&quot;/&gt; &lt;result column=&quot;model_description&quot; jdbcType=&quot;VARCHAR&quot; property=&quot;modelDescription&quot;/&gt; &lt;result column=&quot;model_sort_key&quot; jdbcType=&quot;VARCHAR&quot; property=&quot;modelSortKey&quot;/&gt; &lt;result column=&quot;model_classification_label&quot; jdbcType=&quot;VARCHAR&quot; property=&quot;modelClassificationLabel&quot;/&gt; &lt;result column=&quot;tenant_product_id&quot; jdbcType=&quot;BIGINT&quot; property=&quot;tenantProductId&quot;/&gt; &lt;result column=&quot;tenant_product_name&quot; jdbcType=&quot;VARCHAR&quot; property=&quot;tenantProductName&quot;/&gt; &lt;result column=&quot;manufacturer&quot; jdbcType=&quot;VARCHAR&quot; property=&quot;manufacturer&quot;/&gt; &lt;result column=&quot;create_by&quot; jdbcType=&quot;BIGINT&quot; property=&quot;createBy&quot;/&gt; &lt;result column=&quot;create_time&quot; jdbcType=&quot;TIMESTAMP&quot; property=&quot;createTime&quot;/&gt; &lt;result column=&quot;update_by&quot; jdbcType=&quot;BIGINT&quot; property=&quot;updateBy&quot;/&gt; &lt;result column=&quot;update_time&quot; jdbcType=&quot;TIMESTAMP&quot; property=&quot;updateTime&quot;/&gt; &lt;/resultMap&gt; &lt;!--级联查询--&gt; &lt;resultMap id=&quot;BaseResultInstanceMap&quot; type=&quot;com.haier.biz.entity.DTO.ModelDTO&quot; extends=&quot;BaseResultMap&quot;&gt; &lt;result column=&quot;publishNumber&quot; jdbcType=&quot;BIGINT&quot; property=&quot;publishNumber&quot;/&gt; &lt;result column=&quot;customer_id&quot; jdbcType=&quot;BIGINT&quot; property=&quot;customerId&quot;/&gt; &lt;result column=&quot;topic&quot; jdbcType=&quot;VARCHAR&quot; property=&quot;topic&quot;/&gt; &lt;!--property来源ModelDTO属性，column来源于selectInstancelList查询--&gt; &lt;!--查询单条--&gt; &lt;association property=&quot;equipmentPicture&quot; column=&quot;equipment_model_mark&quot; select=&quot;getEquipmentPicture&quot;&gt;&lt;/association&gt; &lt;association property=&quot;instanceNumber&quot; column=&quot;{equipmentModelMark=equipment_model_mark,customerId=customer_id}&quot; select=&quot;getCustomerInstanceNumber&quot;&gt;&lt;/association&gt; &lt;!--查询列表--&gt; &lt;collection property=&quot;equipmentPictures&quot; column=&quot;equipment_model_mark&quot; select=&quot;getEquipmentPictures&quot;&gt;&lt;/collection&gt; &lt;/resultMap&gt; &lt;select id=&quot;selectInstancelList&quot; parameterType=&quot;com.haier.biz.entity.VO.InstanceSelectVo&quot; resultMap=&quot;BaseResultInstanceMap&quot;&gt; SELECT distinct model.model_id, equipment_model_mark, equipment_name, specification_model, model_description, model_sort_key, model_classification_label, tenant_product_id, tenant_product_name, manufacturer, model.create_by, model.create_time, model.update_by, model.update_time, 0 as publishNumber, relation_model_customer.customer_id, relation_model_customer.topic FROM model LEFT JOIN relation_model_customer on relation_model_customer.model_id=model.model_id &lt;where&gt; &lt;if test=&quot;customerId != null&quot;&gt; and relation_model_customer.customer_id = #{customerId} &lt;/if&gt; &lt;if test=&quot;modelSortKey != null and modelSortKey!=''&quot;&gt; and `model`.model_sort_key = #{modelSortKey} &lt;/if&gt; &lt;if test=&quot;equipmentName != null and equipmentName!=''&quot;&gt; and `model`.equipment_name like CONCAT('%', #{equipmentName,jdbcType=VARCHAR},'%') &lt;/if&gt; &lt;/where&gt; &lt;/select&gt; &lt;select id=&quot;getEquipmentPicture&quot; parameterType=&quot;java.lang.String&quot; resultType=&quot;java.lang.String&quot;&gt; SELECT model_instance_file_associate.file_object_key as equipmentPicture from model_instance_file_associate where model_instance_file_associate.file_type=4 and model_instance_file_associate.type=0 and model_instance_file_associate.del_flag=0 and model_instance_file_associate.model_or_instance_mark = #{equipmentModelMark,jdbcType=VARCHAR} limit 1 &lt;/select&gt; &lt;select id=&quot;getCustomerInstanceNumber&quot; parameterType=&quot;java.util.Map&quot; resultType=&quot;java.lang.Long&quot;&gt; SELECT count(*) from model_instance_associate left join `instance` on model_instance_associate.instance_mark=`instance`.instance_mark where model_instance_associate.equipment_model_mark = #{equipmentModelMark,jdbcType=VARCHAR} and `instance`.customer_id=#{customerId} &lt;/select&gt; &lt;select id=&quot;getEquipmentPictures&quot; parameterType=&quot;java.lang.String&quot; resultType=&quot;java.lang.String&quot;&gt; SELECT model_instance_file_associate.file_object_key as equipmentPictures from model_instance_file_associate where model_instance_file_associate.file_type=4 and model_instance_file_associate.type=0 and model_instance_file_associate.del_flag=0 and model_instance_file_associate.model_or_instance_mark = #{equipmentModelMark,jdbcType=VARCHAR} &lt;/select&gt; &lt;/mapper&gt; ModelMapper @Repository public interface ModelMapper extends BaseMapper&lt;Model&gt; { List&lt;ModelDTO&gt; selectInstancelList(InstanceSelectVo instanceSelectVo); List&lt;String&gt; getEquipmentPictures(@Param(&quot;equipmentModelMark&quot;) String equipmentModelMark); } ModelDTO @Data @EqualsAndHashCode(callSuper = false) @Accessors(chain = true) @AllArgsConstructor @NoArgsConstructor public class ModelDTO extends Model { @ApiModelProperty(value = &quot;产品(模型)图片(取第一张)&quot;) private String equipmentPicture; @ApiModelProperty(value = &quot;产品(模型)图片&quot;) private List&lt;String&gt; equipmentPictures; @ApiModelProperty(value = &quot;发布客户数&quot;) private Long publishNumber; @ApiModelProperty(value = &quot;实例设备数&quot;) private Long instanceNumber; @ApiModelProperty(value = &quot;使用方Id&quot;) private Long customerId; @ApiModelProperty(value = &quot;实例设备运行情况&quot;) private List&lt;NumberDTO&gt; equipmentList; @ApiModelProperty(value = &quot;kafka实时推送的topic&quot;) private String topic; } ","link":"https://tianxiawuhao.github.io/hIydIZSN1/"},{"title":"mysql 简单导入 clickhouse","content":"数据迁移需要从 mysql 导入 clickhouse, clickhouse 自身支持的三种方式 。 create table engin mysql CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1] [TTL expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2] [TTL expr2], ... INDEX index_name1 expr1 TYPE type1(...) GRANULARITY value1, INDEX index_name2 expr2 TYPE type2(...) GRANULARITY value2 ) ENGINE = MySQL('host:port', 'database', 'table', 'user', 'password'[, replace_query, 'on_duplicate_clause']); 官方文档: https://clickhouse.yandex/docs/en/operations/table_engines/mysql/ 注意，实际数据存储在远端 mysql 数据库中，可以理解成外表。 可以通过在 mysql 增删数据进行验证。 insert into select from -- 先建表 CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2], ... ) ENGINE = engine -- 导入数据 INSERT INTO [db.]table [(c1, c2, c3)] select 列或者* from mysql('host:port', 'db', 'table_name', 'user', 'password') 可以自定义列类型，列数，使用 clickhouse 函数对数据进行处理，比如 select toDate(xx) from mysql(&quot;host:port&quot;,&quot;db&quot;,&quot;table_name&quot;,&quot;user_name&quot;,&quot;password&quot;) create table as select from CREATE TABLE [IF NOT EXISTS] [db.]table_name ENGINE =Log AS SELECT * FROM mysql('host:port', 'db', 'article_clientuser_sum', 'user', 'password') 网友文章: http://jackpgao.github.io/2018/02/04/ClickHouse-Use-MySQL-Data/ 不支持自定义列，参考资料里的博主写的 ENGIN=MergeTree 测试失败。 可以理解成 create table 和 insert into select 的组合 ","link":"https://tianxiawuhao.github.io/BHjbgcFm-/"},{"title":"Thingsboard源码编译","content":"环境安装 开发环境要求： Jdk 1.11 版本 Postgresql 9 以上 Node.js Npm Maven 3.6 以上 Git 工具 Idea 开发工具 Redis JDK 下载安装 JDK 官方下载地址： Java Downloads | Oracle JDK 版本选择 JDK11，我本地环境是 Windos10 64 位，所以选择 jdk-11.0.13-windows-x64.exe 下载好了之后直接默认安装就行 免安装版本 下载jdk11 http://openjdk.java.net/install/index.html 这个页面大部分都是linux系统的； 然后我们点击jdk.java.net ，接着我们选择下载Java SE 11； 下好了后，我们得到这样的文件：openjdk-11+28_windows-x64_bin.zip，解压后得到：jdk-11这样的文件夹；将该文件夹放到你习惯的地方； 配置环境变量 步骤 1： 在 JAVA_HOME 中增加 JDK 的安装地址：C:\\Program Files\\Java\\jdk1.8.0_221 步骤 2： 在 CLASSPATH 中增加 JDK 的安装地址中的文件：.;%JAVA_HOME%\\lib;%JAVA_HOME%\\lib\\dt.jar;%JAVA_HOME%\\lib\\tools.jar 步骤 3： 在 Path 中增加 JDK 的地址：%JAVA_HOME%\\bin;%JAVA_HOME%\\jre\\bin; 步骤 4 输入以下命令 java -version 如果能出现以下的提示信息，就算安装成功了 安装 IDEA 参考：IDEA 安装教程 安装 Maven 步骤 1：下载 maven，进入地址：http://maven.apache.org/download.cgi 步骤 2：下载到本地 步骤 3：配置环境变量 增加 MAVEN_HOME，即 maven 的地址：D:\\tb\\apache-maven-3.6.1-bin，请注意，如果直接解压，有可能会有两个 apache-maven-3.6.1-bin MAVEN_OPTS，参数是 -Xms128m -Xmx1024m 修改 Path，增加 Maven 的地址%MAVEN_HOME%\\bin; 测试 Maven 安装，打开命令行工具。使用命令 mvn -v，如果能出现以下提示，即安装成功 Nodejs 安装 步骤 1：下载 Nodejs 安装包，Nodejs 官网地址：https://nodejs.org/en/download/ 步骤 2：安装完成后，使用命令查看 Nodejs 是否已经安装完成，能出现以下提示说明已经安装成功 ! 安装 git 步骤 1：下载 git 安装包，git 官网地址是：https://git-scm.com/download/win 步骤 2：安装完成后，使用命令行测试 git 安装 npm 全局依赖 步骤 1：使用管理员 CMD 命令行，执行下面命令 #npm 环境读取环境变量包 npm install -g cross-env #webpack打包工具 npm install -g webpack 安装 redis Redis 安装参考：https://www.iotschool.com/wiki/redis 环境安装到此结束，接下来是通过 Git 拉取代码。 克隆 thingsboard 代码 确定代码存放位置 在本地创建代码存放位置的文件目录，然后进入当前目录点击鼠标右键，选择 Git Bash Here 输入 git 命令克隆源代码 git clone https://github.com/thingsboard/thingsboard.git 耐心等待一段时间后，看到以下界面就算下载成功 切换 git 分支 默认下载的代码是 master 主分支的，我们开发需要切换到最新版本的分支。 查看项目源码的所有分支，下载源码后，需要进入到 thingsboard 文件夹 发现最新发布的版本是 2.4，所以我这里选择 2.4，当然你可以根据自己的情况进行分支选择 输入命令以下，即可切换至 2.4 的分支 git checkout release-2.4 看到下图这样，即切换成成功 准备工作 外网连接 因为 TB 在编译过程中需要依赖很多国外的包，那么需要外网才能连接，有连接外网支持，可以到社区求助：https://www.iotschool.com/topics/node8 设置 Maven 为淘宝镜像 工程是基于 Maven 管理，直接通过 idea open，之后会自动下载各种依赖包。依赖包的默认存储地址为：C:\\Users\\用户名.m2\\repository，内容如下： $tree ~/.m2 -L 2 /home/jay/.m2 └── repository ├── antlr ├── aopalliance ├── asm ├── backport-util-concurrent ├── ch ... 一般情况下，使用官方镜像更新依赖包，网速不稳定，可将 Maven 镜像源设置为淘宝的，在 maven 安装包目录下找到 settings.xml 设置 大概位置截图： 把 settings.xml 里面内容设置成以下： &lt;mirrors&gt; &lt;mirror&gt; &lt;!--This sends everything else to /public --&gt; &lt;id&gt;aliyun_nexus&lt;/id&gt; &lt;mirrorOf&gt;*,!maven_nexus_201&lt;/mirrorOf&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;/mirror&gt; &lt;/mirrors&gt; 不会设置的，可以参考这个文件：https://cdn.iotschool.com/iotschool/settings.xml thingsboard QQ 群也有这个资源：121202538 设置 npm 为淘宝镜像 同上，网速不好 npm 过程中也会下载失败，这是导致很多同学 thingsboard 编译失败的主要原因，所以我们在进行编译之前，也将 npm 替换为淘宝镜像： npm install -g mirror-config-china --registry=http://registry.npm.taobao.org #使用淘宝镜像 npm config get registry #查询当前镜像 npm config rm registry #删除自定义镜像，使用官方镜像 npm info express 设置 IDEA 管理员启动 我本地开发环境编译项目使用 IDEA 工具进行编译，所以需要设置管理员启动，这样才有所有的权限执行编译命令。 步骤 1：点击 IDEA 图标右键，选择属性。 步骤 2：点击兼容性 - 更改所有用户设置 - 以管理员身份运行此程序 开始编译 编译项目跟网速有关，最好连接上外网进行编译，一般 5~30 分钟都有可能，超过 30 分钟要检查你的网络。 清理项目编译文件 使用 IDEA Maven 工具进行清理 输入编译命令开始编译 在 IDEA 控制台（左下方）Terminal 输入以下命令进行编译： mvn clean install -DskipTests 等一段时间后，看到下面这张图就算编译成功，如果没有编译成功请按照本教程最后的常见问题进行排查，一般都是网络问题。如果还有问题，请到社区thingsboard 专题中提问。 常见问题 pom包pkg.name等标签未定义 &lt;properties&gt; &lt;pkg.name&gt;thingsboard&lt;/pkg.name&gt; &lt;main.dir&gt;${basedir}&lt;/main.dir&gt; &lt;pkg.type&gt;java&lt;/pkg.type&gt; &lt;pkg.mainClass&gt;org.thingsboard.server.ThingsboardServerApplication&lt;/pkg.mainClass&gt; &lt;pkg.copyInstallScripts&gt;true&lt;/pkg.copyInstallScripts&gt; &lt;main.dir&gt;${basedir}&lt;/main.dir&gt; &lt;pkg.disabled&gt;true&lt;/pkg.disabled&gt; &lt;pkg.process-resources.phase&gt;none&lt;/pkg.process-resources.phase&gt; &lt;pkg.package.phase&gt;none&lt;/pkg.package.phase&gt; &lt;pkg.user&gt;thingsboard&lt;/pkg.user&gt; &lt;pkg.implementationTitle&gt;${project.name}&lt;/pkg.implementationTitle&gt; &lt;pkg.unixLogFolder&gt;/var/log/${pkg.name}&lt;/pkg.unixLogFolder&gt; &lt;pkg.installFolder&gt;/usr/share/${pkg.name}&lt;/pkg.installFolder&gt; &lt;/properties&gt; 缓存导致编译失败 每次编译失败进行二次编译时，要清理缓存，并杀死遗留进程 步骤 1：执行下面命令，杀死遗留进程 taskkill /f /im java.exe 步骤 2：使用 IDEA Maven 工具进行清理 温馨提示：要进行二次编译前，最好重启一次电脑！ Server UI 编译失败 [ERROR] Failed to execute goal com.github.eirslett:frontend-maven-plugin:1.0:npm (npm install) on project ui: Failed to run task: 'npm install' failed. (error code 1) -&gt; [Help 1] 如果遇到这个问题，可从以下几个原因进行分析： 原因 1：node、npm 版本号问题 本地环境安装的 node、npm 版本号与源码中 pom.xml 文件配置的版本号不一致。 解决方案： 步骤 1：使用 node -v、npm -v 查看安装的 node 和 npm 版本号 步骤 2：修改源码中 pom.xml 文件中的版本号 &lt;configuration&gt; &lt;nodeVersion&gt;v12.13.1&lt;/nodeVersion&gt; &lt;npmVersion&gt;6.12.1&lt;/npmVersion&gt; &lt;/configuration&gt; 需要修改的文件有三处，位置如下： 原因 2：node-sass 下载失败 编译 Server UI 时，会下载 node-sass 依赖，如果因为网络原因没有下载成功，也会编译失败。如果你是按照本本教材一步一步来的，应该不会有问题，上面准备工作中，将 npm 镜像源切换为淘宝，那么下载会很快的。 [INFO] Downloading binary from https://github.com/sass/node-sass/releases/download/v4.12.0/win32-x64-72_binding.node [ERROR] Cannot download &quot;https://github.com/sass/node-sass/releases/download/v4.12.0/win32-x64-72_binding.node&quot;: [ERROR] [ERROR] ESOCKETTIMEDOUT [ERROR] [ERROR] Hint: If github.com is not accessible in your location [ERROR] try setting a proxy via HTTP_PROXY, e.g. [ERROR] [ERROR] export HTTP_PROXY=http://example.com:1234 [ERROR] [ERROR] or configure npm proxy via [ERROR] [ERROR] npm config set proxy http://example.com:8080 [INFO] [INFO] &gt; node-sass@4.12.0 postinstall F:\\workspace\\thingsboard\\thingsboard\\ui\\node_modules\\node-sass [INFO] &gt; node scripts/build.js [INFO] 解决方案：切换镜像源为淘宝 解决方案：重启电脑，清理缓存 原因 3：Thingsboard 3.0 版本编译遇到的问题 亲测：2.4 版本也可以通过这种方式来解决 Failed to execute goal com.github.eirslett:frontend-maven-plugin:1.7.5:npm (npm install) on project ui-ngx: Failed to run task: 'npm install' failed. org.apache.commons.exec.ExecuteException: Process exited with an error: -4048 (Exit value: -4048) -&gt; [Help 1] 解决方案：https://www.iotschool.com/topics/84 原因 4：二次编译导致残留进程 报错： [ERROR] Failed to execute goal org.apache.maven.plugins:maven-clean-plugin:2.5:clean (default-clean) on project ui: Failed to clean project: Failed to delete F:\\workspace\\thingsboard\\thingsboard\\ui\\target\\node\\node.exe -&gt; [Help 1] Server Tool 编译失败 [ERROR] Failed to execute goal on project tools: Could not resolve dependencies for project org.thingsboard:tools:jar:2.4.3: Failed to collect dependencies at org.eclipse.paho:org.eclipse.paho.client.mqttv3:jar:1.1.0: Failed to read artifact descriptor for org.eclipse.paho:org.eclipse.paho.clien t.mqttv3:jar:1.1.0: Could not transfer artifact org.eclipse.paho:org.eclipse.paho.client.mqttv3:pom:1.1.0 from/to aliyun_nexus (http://maven.aliyun.com/nexus/content/groups/public/): Failed to transfer file http://maven.aliyun.com/nexus/content/groups/public/org/eclipse/paho/org.eclipse.paho.cli ent.mqttv3/1.1.0/org.eclipse.paho.client.mqttv3-1.1.0.pom with status code 502 -&gt; [Help 1] 一般由于网络原因，IoTSchool 小编至少编译了 3 次才成功，每次编译都重启电脑，并清理环境。 解决方案：如果使用的是 mvn clean install -DskipTests 命令进行编译，那么就多尝试几次，每次编译前，要清理环境。 参考：https://github.com/thingsboard/performance-tests/issues/10 JavaScript Executor 编译失败 JavaScript Executor Microservice 编译失败 [ERROR] Failed to execute goal com.github.eirslett:frontend-maven-plugin:1.0:npm (npm install) on project js-executor: Failed to run task: 'npm install' failed. (error code 2) -&gt; [Help 1] [ERROR] [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch. [ERROR] Re-run Maven using the -X switch to enable full debug logging. [ERROR] [ERROR] For more information about the errors and possible solutions, please read the following articles: [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException [ERROR] [ERROR] After correcting the problems, you can resume the build with the command [ERROR] mvn &lt;goals&gt; -rf :js-executor 原因：本地缓存缺少 fetched-v10.15.3-linux-x64 和 fetched-v10.15.3-win-x64 这两个文件。 解决方案： 步骤 1：下载这两个文件到本地，下载后记得重命名，下载地址：https://github.com/zeit/pkg-fetch/releases 步骤 2: 将下载的两个文件放到：放到：C:\\Users\\你的用户名 \\ .pkg-cache\\v2.6。并将名字分别修改为：fetched-v10.15.3-linux-x64 和 fetched-v10.15.3-win-x64 参考：https://github.com/thingsboard/thingsboard/issues/2084 License 检查不通过 [ERROR] Failed to execute goal com.mycila:license-maven-plugin:3.0:check (default) on project thingsboard: Some files do not have the expected license header -&gt; [Help 1] 解决方案：在根目录 pom.xml 中屏蔽 license-maven-plugin 搜索 license-maven-plugin，将整个 plugin 都注释掉 Web UI 编译失败 Web UI 编译失败请参考[Server UI 编译失败第一个原因](https://www.iotschool.com/wiki/tbinstall#Server Tool编译失败) maven:Could not resolve dependencies for project org.thingsboard:application: 错误信息 [ERROR] Failed to execute goal on project application: Could not resolve dependencies for project org.thingsboard:application:jar:2.4.1: The following artifacts could not be resolved: org.thingsboard.rule-engine:rule-engine-components:jar:2.4.1, org.thingsboard:dao:jar:2.4.1: Could not find artifact org.thingsboard.rule-engine:rule-engine-components:jar:2.4.1 in jenkins (http://repo.jenkins-ci.org/releases) -&gt; [Help 1] 解决方案：根目录下去 maven 编译，不要在每个单独编译，否则不能自动解决依赖，如果你已经在子模块进行了编译，请回到根目录先 clean 一下，再重新编译。 maven:Failed to delete tb-http-transport.rpm 错误信息： [ERROR] Failed to execute goal org.apache.maven.plugins:maven-clean-plugin:2.5:clean (default-clean) on project http: Failed to clean project: Failed to delete D:\\my_project\\thingsboard\\transport\\http\\target\\tb-http-transport.rpm -&gt; [Help 1] 解决方案：第一次编译失败，再次编译可能会提示该错误，可以手动到报错路径删除，如果提示文件正在使用，需要在任务管理器杀死 java 进程后再手动删除。 npm:npm:cb() never called! 错误信息： npm ERR! cb() never called! npm ERR! This is an error with npm itself. Please report this error at: npm ERR! &lt;https://npm.community&gt; npm ERR! A complete log of this run can be found in: npm ERR! C:\\Users\\yuren\\AppData\\Roaming\\npm-cache\\_logs\\2019-11-06T10_55_28_258Z-debug.log 解决方案： 尝试 npm cache clean --force 后再次 npm install 无果； 尝试更换淘宝镜像源后再次 npm install 无果； 怀疑有些包下载需要翻墙，全局代理翻墙后问题依然存在； 参考网上关闭所有代理后问题依然存在； 通过 log 日志分析最后一个解包报错的地方，屏蔽需要的 material-design-icons，新 modules rxjs 仍然报错； extract material-design-icons@3.0.1 extracted to node_modules\\.staging\\material-design-icons-61b4d55e (72881ms) extract rxjs@6.5.2 extracted to node_modules\\.staging\\rxjs-e901ba4c (24280ms) 参考 npm ERR cb() never called 执行 npm install --no-package-lock 之后提示 npm ERR! path git，添加 git 到环境变量后正常。 npm:npm ERR! path git 错误信息 npm ERR! path git npm ERR! code ENOENT npm ERR! errno ENOENT npm ERR! syscall spawn git npm ERR! enoent Error while executing: npm ERR! enoent undefined ls-remote -h -t git://github.com/fabiobiondi/angular- 解决方案：添加 git 到环境变量。 No compiler is provided in this environment 错误信息： [ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3. 1:compile (default-compile) on project netty-mqtt: Compilation failure [ERROR] No compiler is provided in this environment. Perhaps you are running on a JRE rather than a JDK? 需要在环境变量中设置 java，包含%JAVA_HOME%bin;%JAVA_HOME%lib; Failed to execute goal org.thingsboard:gradle-maven-plugin:1.0.10:invoke (default) on project http: org.gradle.tooling.BuildException: Could not execute build using Gradle distribution 'https://services.gradle.org/distributions/gradle-6.3-bin.zip'. Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.8.1:compile (default-compile) on project rest-client: Compilation failure An unknown compilation problem occurred 这个问题主要是jdk版本跟项目不一致导致的，如果项目的版本是大于(不含！)3.2.1，则需要JDK11，反之JDK8 ","link":"https://tianxiawuhao.github.io/DseKIIHw3/"},{"title":"ClickHouse-8分片集群","content":"ClickHouse-分片集群 副本虽然能够提高数据的可用性，降低丢失风险，但是每台服务器实际上必须容纳全量数据，对数据的横向扩容没有解决。 要解决数据水平切分的问题，需要引入分片的概念。通过分片把一份完整的数据进行切分，不同的分片分布到不同的节点上，再通过 Distributed 表引擎把数据拼接起来一同使用。 Distributed 表引擎本身不存储数据，有点类似于 MyCat 之于 MySql，成为一种中间件，通过分布式逻辑表来写入、分发、路由来操作多台节点不同分片的分布式数据。 注意：ClickHouse 的集群是表级别的，实际企业中，大部分做了高可用，但是没有用分片，避免降低查询性能以及操作集群的复杂性。 1.集群写入流程（3 分片 2 副本共 6 个节点） internal_replication:内部副本同步 true：由分片自己同步 false：由distribute表同步，压力大 2.集群读取流程（3 分片 2 副本共 6 个节点） 3 分片 2 副本共 6 个节点集群配置（供参考） 配置的位置还是在之前的/etc/clickhouse-server/config.d/metrika.xml，内容如下 注：也可以不创建外部文件，直接在 config.xml 的&lt;remote_servers&gt;中指定 &lt;yandex&gt; &lt;remote_servers&gt; &lt;fz_cluster&gt; &lt;!-- 集群名称--&gt; &lt;shard&gt; &lt;!--集群的第一个分片--&gt; &lt;internal_replication&gt;true&lt;/internal_replication&gt; &lt;!--该分片的第一个副本--&gt; &lt;replica&gt; &lt;host&gt;node01&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;/replica&gt; &lt;!--该分片的第二个副本--&gt; &lt;replica&gt; &lt;host&gt;node02&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;/replica&gt; &lt;/shard&gt; &lt;shard&gt; &lt;!--集群的第二个分片--&gt; &lt;internal_replication&gt;true&lt;/internal_replication&gt; &lt;replica&gt; &lt;!--该分片的第一个副本--&gt; &lt;host&gt;node03&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;/replica&gt; &lt;replica&gt; &lt;!--该分片的第二个副本--&gt; &lt;host&gt;node04&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;/replica&gt; &lt;/shard&gt; &lt;shard&gt; &lt;!--集群的第三个分片--&gt; &lt;internal_replication&gt;true&lt;/internal_replication&gt; &lt;replica&gt; &lt;!--该分片的第一个副本--&gt; &lt;host&gt;node05&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;/replica&gt; &lt;replica&gt; &lt;!--该分片的第二个副本--&gt; &lt;host&gt;node06&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;/replica&gt; &lt;/shard&gt; &lt;/fz_cluster&gt; &lt;/remote_servers&gt; &lt;/yandex&gt; 4 .配置三节点版本集群及副本 4.1 集群及副本规划（2 个分片，只有第一个分片有副本） 4.2 配置步骤 1）在 Node01 的/etc/clickhouse-server/config.d 目录下创建 metrika-shard.xml 文件 vim /etc/clickhouse-server/config.d/metrika-shard.xml 注：也可以不创建外部文件，直接在 config.xml 的&lt;remote_servers&gt;中指定 &lt;?xml version=&quot;1.0&quot;?&gt; &lt;yandex&gt; &lt;remote_servers&gt; &lt;gmall_cluster&gt; &lt;!-- 集群名称--&gt; &lt;shard&gt; &lt;!--集群的第一个分片--&gt; &lt;internal_replication&gt;true&lt;/internal_replication&gt; &lt;replica&gt; &lt;!--该分片的第一个副本--&gt; &lt;host&gt;Node01&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;user&gt;default&lt;/user&gt; &lt;password&gt;1234qwer&lt;/password&gt; &lt;/replica&gt; &lt;replica&gt; &lt;!--该分片的第二个副本--&gt; &lt;host&gt;Node02&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;user&gt;default&lt;/user&gt; &lt;password&gt;1234qwer&lt;/password&gt; &lt;/replica&gt; &lt;/shard&gt; &lt;shard&gt; &lt;!--集群的第二个分片--&gt; &lt;internal_replication&gt;true&lt;/internal_replication&gt; &lt;replica&gt; &lt;!--该分片的第一个副本--&gt; &lt;host&gt;Node03&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;user&gt;default&lt;/user&gt; &lt;password&gt;1234qwer&lt;/password&gt; &lt;/replica&gt; &lt;/shard&gt; &lt;/gmall_cluster&gt; &lt;/remote_servers&gt; &lt;zookeeper-servers&gt; &lt;node index=&quot;1&quot;&gt; &lt;host&gt;Node01&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;node index=&quot;2&quot;&gt; &lt;host&gt;Node02&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;node index=&quot;3&quot;&gt; &lt;host&gt;Node03&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;/zookeeper-servers&gt; &lt;macros&gt; &lt;shard&gt;01&lt;/shard&gt; &lt;!--不同机器放的分片数不一样--&gt; &lt;replica&gt;rep_1_1&lt;/replica&gt; &lt;!--不同机器放的副本数不一样--&gt; &lt;/macros&gt; &lt;/yandex&gt; 2）将 Node01 的 metrika-shard.xml 同步到 Node02 和 Node03 scp /etc/clickhouse-server/config.d/metrika-shard.xml root@Node02:/etc/clickhouse-server/config.d/ scp /etc/clickhouse-server/config.d/metrika-shard.xml root@Node03:/etc/clickhouse-server/config.d/ 3）修改 Node02 和 Node03 中 metrika-shard.xml 宏的配置 （1）Node02 &lt;macros&gt; &lt;shard&gt;01&lt;/shard&gt; &lt;!--不同机器放的分片数不一样--&gt; &lt;replica&gt;rep_1_2&lt;/replica&gt; &lt;!--不同机器放的副本数不一样--&gt; &lt;/macros&gt;1.2.3.4. （2）Node03 &lt;macros&gt; &lt;shard&gt;02&lt;/shard&gt; &lt;!--不同机器放的分片数不一样--&gt; &lt;replica&gt;rep_2_1&lt;/replica&gt; &lt;!--不同机器放的副本数不一样--&gt; &lt;/macros&gt;1.2.3.4. 4）在 Node01 上修改/etc/clickhouse-server/config.xml vim /etc/clickhouse-server/config.xml &lt;zookeeper incl=&quot;zookeeper-servers&quot; optional=&quot;true&quot; /&gt; &lt;include_from&gt;/etc/clickhouse-server/config.d/metrika-shard.xml&lt;/include_from&gt; 5）同步/etc/clickhouse-server/config.xml 到 Node02 和 Node03 scp /etc/clickhouse-server/config.xml root@Node02:/etc/clickhouse-server/ scp /etc/clickhouse-server/config.xml root@Node03:/etc/clickhouse-server/ 6）重启三台服务器上的 ClickHouse 服务 sudo clickhouse restart 查看集群 superset-BI :) show clusters; SHOW CLUSTERS Query id: 391735d2-bf74-43f5-aa86-b6d203c357cd ┌─cluster─────────────────────────────────────────┐ │ gmall_cluster │ │ test_cluster_one_shard_three_replicas_localhost │ │ test_cluster_two_shards │ │ test_cluster_two_shards_internal_replication │ │ test_cluster_two_shards_localhost │ │ test_shard_localhost │ │ test_shard_localhost_secure │ │ test_unavailable_shard │ └─────────────────────────────────────────────────┘ 8 rows in set. Elapsed: 0.002 sec. 7）在 Node01 上执行建表语句 ➢ 会自动同步到 Node02 和 Node03 上 ➢ 集群名字要和配置文件中的一致 ➢ 分片和副本名称从配置文件的宏定义中获取 create table st_fz_order_mt_01 on cluster gmall_cluster ( id UInt32, sku_id String, total_amount Decimal(16,2), create_time Datetime ) engine =ReplicatedMergeTree('/clickhouse/tables/{shard}/st_fz_order_mt_01','{replica}') partition by toYYYYMMDD(create_time) primary key (id) order by (id,sku_id); create table st_fz_order_mt_01 on cluster gmall_cluster ( id UInt32, sku_id String, total_amount Decimal(16,2), create_time Datetime ) engine =ReplicatedMergeTree('/clickhouse/tables/{shard}/st_fz_order_mt_01','{replica}') partition by toYYYYMMDD(create_time) primary key (id) order by (id,sku_id); CREATE TABLE st_fz_order_mt_01 ON CLUSTER gmall_cluster ( `id` UInt32, `sku_id` String, `total_amount` Decimal(16, 2), `create_time` Datetime ) ENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/st_fz_order_mt_01', '{replica}') PARTITION BY toYYYYMMDD(create_time) PRIMARY KEY id ORDER BY (id, sku_id) 在Node02和Node03上查看表是否创建成功 show tables; 8）在 Node02 上创建 Distribute 分布式表 create table st_fz_order_mt_all2 on cluster gmall_cluster ( id UInt32, sku_id String, total_amount Decimal(16,2), create_time Datetime )engine = Distributed(gmall_cluster,default, st_fz_order_mt_01,hiveHash(sku_id)); CREATE TABLE st_fz_order_mt_all2 ON CLUSTER gmall_cluster ( `id` UInt32, `sku_id` String, `total_amount` Decimal(16, 2), `create_time` Datetime ) ENGINE = Distributed(gmall_cluster, default, st_fz_order_mt_01, hiveHash(sku_id)) 参数含义： Distributed（集群名称，库名，本地表名，分片键） 分片键必须是整型数字，所以用 hiveHash 函数转换，也可以 rand() 9）在 Node01 上插入测试数据 insert into st_order_mt_all2 values (201,'sku_001',1000.00,'2020-06-01 12:00:00') , (202,'sku_002',2000.00,'2020-06-01 12:00:00'), (203,'sku_004',2500.00,'2020-06-01 12:00:00'), (204,'sku_002',2000.00,'2020-06-01 12:00:00'), (205,'sku_003',600.00,'2020-06-02 12:00:00');1.2.3.4.5.6. 10）通过查询分布式表和本地表观察输出结果 （1）分布式表 select * From st_fz_order_mt_all2; Query id: d8b676e9-c119-4483-8ca2-f0b5cd150a61 ┌──id─┬─sku_id──┬─total_amount─┬─────────create_time─┐ │ 202 │ sku_002 │ 2000 │ 2020-06-01 12:00:00 │ │ 203 │ sku_004 │ 2500 │ 2020-06-01 12:00:00 │ │ 204 │ sku_002 │ 2000 │ 2020-06-01 12:00:00 │ └─────┴─────────┴──────────────┴─────────────────────┘ ┌──id─┬─sku_id──┬─total_amount─┬─────────create_time─┐ │ 205 │ sku_003 │ 600 │ 2020-06-02 12:00:00 │ └─────┴─────────┴──────────────┴─────────────────────┘ ┌──id─┬─sku_id──┬─total_amount─┬─────────create_time─┐ │ 201 │ sku_001 │ 1000 │ 2020-06-01 12:00:00 │ └─────┴─────────┴──────────────┴─────────────────────┘ （2）本地表 Node1: SELECT * FROM st_fz_order_mt_01 Query id: ddcb5176-e443-4253-9877-57fec8f57311 ┌──id─┬─sku_id──┬─total_amount─┬─────────create_time─┐ │ 202 │ sku_002 │ 2000 │ 2020-06-01 12:00:00 │ │ 203 │ sku_004 │ 2500 │ 2020-06-01 12:00:00 │ │ 204 │ sku_002 │ 2000 │ 2020-06-01 12:00:00 │ └─────┴─────────┴──────────────┴─────────────────────┘ 3 rows in set. Elapsed: 0.002 sec. Node3: SELECT * FROM st_fz_order_mt_01 Query id: 7a336004-7040-4098-948e-1e7c5d983edb ┌──id─┬─sku_id──┬─total_amount─┬─────────create_time─┐ │ 205 │ sku_003 │ 600 │ 2020-06-02 12:00:00 │ └─────┴─────────┴──────────────┴─────────────────────┘ ┌──id─┬─sku_id──┬─total_amount─┬─────────create_time─┐ │ 201 │ sku_001 │ 1000 │ 2020-06-01 12:00:00 │ └─────┴─────────┴──────────────┴─────────────────────┘ 2 rows in set. Elapsed: 0.002 sec. 可以看到数据分布在Node1和Node3两个节点上。 ","link":"https://tianxiawuhao.github.io/vuilLU8qj/"},{"title":"ClickHouse-7副本","content":"1 副本写入流程 2 配置步骤 启动 zookeeper 集群 在 hadoop102 的/etc/clickhouse-server/config.d 目录下创建一个名为 metrika.xml 的配置文件,内容如下： 注：也可以不创建外部文件，直接在 config.xml 中指定 &lt;?xml version=&quot;1.0&quot;?&gt; &lt;yandex&gt; &lt;zookeeper-servers&gt; &lt;node index=&quot;1&quot;&gt; &lt;host&gt;hadoop102&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;node index=&quot;2&quot;&gt; &lt;host&gt;hadoop103&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;node index=&quot;3&quot;&gt; &lt;host&gt;hadoop104&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;/zookeeper-servers&gt; &lt;/yandex&gt; 同步到 hadoop103 和 hadoop104 上 sudo /home/atguigu/bin/xsync /etc/clickhouse-server/config.d/metrika.xml 在 hadoop102 的/etc/clickhouse-server/config.xml 中增加 &lt;zookeeper incl=&quot;zookeeper-servers&quot; optional=&quot;true&quot; /&gt; &lt;include_from&gt;/etc/clickhouse-server/config.d/metrika.xml&lt;/include_from&gt; 同步到 hadoop103 和 hadoop104 上 sudo /home/atguigu/bin/xsync /etc/clickhouse-server/config.xml 分别在 hadoop102 和 hadoop103 上启动 ClickHouse 服务 注意：因为修改了配置文件，如果以前启动了服务需要重启 sudo clickhouse restart 注意：我们演示副本操作只需要在 hadoop102 和 hadoop103 两台服务器即可，上面的 操作，我们 hadoop104 可以你不用同步，我们这里为了保证集群中资源的一致性，做了同 步。 在 hadoop102 和 hadoop103 上分别建表 副本只能同步数据，不能同步表结构，所以我们需要在每台机器上自己手动建表 create table t_order_rep2 ( id UInt32, sku_id String, total_amount Decimal(16,2), create_time Datetime ) engine = ReplicatedMergeTree('/clickhouse/table/01/t_order_rep','rep_102') partition by toYYYYMMDD(create_time) primary key (id) order by (id,sku_id); ReplicatedMergeTree(‘/clickhouse/table/01/t_order_rep’,‘rep_102’)中， 第一个参数是分片的 zk_path ， 一般按照：/clickhouse/table/{shard}/{table_name} 的格式写，如果只有一个分片就写 01 即可。 第二个参数是副本名称，相同的分片副本名称不能相同。 在 hadoop102 上执行 insert 语句 insert into t_order_rep2 values (101,'sku_001',1000.00,'2020-06-01 12:00:00'), (102,'sku_002',2000.00,'2020-06-01 12:00:00'), (103,'sku_004',2500.00,'2020-06-01 12:00:00'), (104,'sku_002',2000.00,'2020-06-01 12:00:00'), (105,'sku_003',600.00,'2020-06-02 12:00:00'); 在 hadoop103 上执行 select，可以查询出结果，说明副本配置正确 ","link":"https://tianxiawuhao.github.io/bps7a3f5p/"},{"title":"ClickHouse-6sql操作","content":"1 Insert 基本与标准 SQL（MySQL）基本一致 标准 insert into [table_name] values(…),(….) 从表到表的插入 insert into [table_name] select a,b,c from [table_name_2] 2 Update 和 Delete ClickHouse 提供了 Delete 和 Update 的能力，这类操作被称为 Mutation 查询，它可以看做 Alter 的一种。 虽然可以实现修改和删除，但是和一般的 OLTP 数据库不一样，Mutation 语句是一种很“重”的操作，而且不支持事务。 “重”的原因主要是每次修改或者删除都会导致放弃目标数据的原有分区，重建新分区。所以尽量做批量的变更，不要进行频繁小数据的操作。 删除操作 alter table t_order_smt delete where sku_id ='sku_001'; 修改操作 alter table t_order_smt update total_amount=toDecimal32(2000.00,2) where id =102; 由于操作比较“重”，所以 Mutation 语句分两步执行，同步执行的部分其实只是进行新增数据新增分区和并把旧分区打上逻辑上的失效标记。直到触发分区合并的时候，才会删除旧数据释放磁盘空间，一般不会开放这样的功能给用户，由管理员完成。 3 查询操作 ClickHouse 基本上与标准 SQL 差别不大 支持子查询 支持 CTE(Common Table Expression 公用表表达式 with 子句) 支持各种 JOIN，但是 JOIN 操作无法使用缓存, 尽量避免使用 JOIN，所以即使是两次相同的 JOIN 语句，ClickHouse 也会视为两条新 SQL 窗口函数 不支持自定义函数 GROUP BY 操作增加了 with rollup \\ with cube \\ with total 用来计算小计和总计。 插入数据 alter table t_order_mt delete where 1=1; insert into t_order_mt values (101,'sku_001',1000.00,'2020-06-01 12:00:00'), (101,'sku_002',2000.00,'2020-06-01 12:00:00'), (103,'sku_004',2500.00,'2020-06-01 12:00:00'), (104,'sku_002',2000.00,'2020-06-01 12:00:00'), (105,'sku_003',600.00,'2020-06-02 12:00:00'), (106,'sku_001',1000.00,'2020-06-04 12:00:00'), (107,'sku_002',2000.00,'2020-06-04 12:00:00'), (108,'sku_004',2500.00,'2020-06-04 12:00:00'), (109,'sku_002',2000.00,'2020-06-04 12:00:00'), (110,'sku_003',600.00,'2020-06-01 12:00:00'); with rollup：上卷，从右至左去掉维度进行小计 select id , sku_id,sum(total_amount) from t_order_mt group by id,sku_id with rollup; with cube : 多维分析， 从右至左去掉维度进行小计，再从左至右去掉维度进行小计 select id , sku_id,sum(total_amount) from t_order_mt group by id,sku_id with cube; with totals: 只计算合计 select id , sku_id,sum(total_amount) from t_order_mt group by id,sku_id with totals; 4 alter 操作 同 MySQL 的修改字段基本一致 新增字段 alter table tableName add column newcolname String after col1; 修改字段类型 alter table tableName modify column newcolname String; 删除字段 alter table tableName drop column newcolname; 5 导出数据 这个用的比较少 clickhouse-client --query &quot;select * from t_order_mt where create_time='2020-06-01 12:00:00'&quot; --format CSVWithNames&gt; /opt/module/data/rs1.csv 更多支持格式参照：https://clickhouse.com/docs/en/interfaces/formats/ ","link":"https://tianxiawuhao.github.io/3wkSNwWQy/"},{"title":"Clickhouse-5数据分区","content":"MergeTree 数据分区规则 创建按照月份为分区条件的表 tab_partition CREATE TABLE tab_partition(`dt` Date, `v` UInt8) ENGINE = MergeTree PARTITION BY toYYYYMM(dt) ORDER BY v; insert into tab_partition(dt,v) values ('2020-02-11',1),('2020-02-13',2); insert into tab_partition(dt,v) values ('2020-04-11',3),('2020-04-13',4); insert into tab_partition(dt,v) values ('2020-09-11',5),('2020-09-10',6); insert into tab_partition(dt,v) values ('2020-10-12',7),('2020-10-09',8); insert into tab_partition(dt,v) values ('2020-02-14',9),('2020-02-15',10); insert into tab_partition(dt,v) values ('2020-02-11',23),('2020-02-13',45); MergeTree 存储引擎在写入数据之后生成对应的分区文件为： MergeTree 的分区目录是在写入数据的过程中被创建出来，每 insert 一次，就会创建一批次分区目录。也就是说如果仅创建表结构，是不会创建分区目录的，因为木有数据。 MergeTree 数据分区目录命名规则其规则为：PartitionID_MinBlockNum_MaxBlockNum_Level 比如 202002_4_4_0 其中 202002 是分区ID ，4_4 对应的是 最小的数据块编号和最大的数据块编号，最后的 _0 表示目前分区合并的层级。 各部分的含义及命名规则如下： PartitionID：该值由 insert 数据时分区键的值来决定。分区键支持使用任何一个或者多个字段组合表达式，针对取值数据类型的不同，分区 ID 的生成逻辑目前有四种规则： 不指定分区键：如果建表时未指定分区键，则分区 ID 默认使用 all，所有数据都被写入 all 分区中。 整型字段：如果分区键取值是整型字段，并且无法转换为 YYYYMMDD 的格式，则会按照该整型字段的字符形式输出，作为分区 ID 取值。 日期类型：如果分区键属于日期格式，或可以转换为 YYYYMMDD 格式的整型，则按照 YYYYMMDD 格式化后的字符形式输出，作为分区 ID 取值。 其他类型：如果使用其他类似 Float、String 等类型作为分区键，会通过对其插入数据的 128 位 Hash 值作为分区 ID 的取值。 MinBlockNum 和 MaxBlockNum：BlockNum 是一个整型的自增长型编号，该编号在单张 MergeTree 表中从 1 开始全局累加，当有新的分区目录创建后，该值就加 1，对新的分区目录来讲，MinBlockNum 和 MaxBlockNum 取值相同。例如上面示例数据为 202002_1_1_0 202002_1_5_1，但当分区目录进行合并后，取值规则会发生变化，MinBlockNum 取同一分区所欲目录中最新的 MinBlockNum 值。MaxBlockNum 取同一分区内所有目录中的最大值。 Level：表示合并的层级。相当于某个分区被合并的次数，它不是以表全局累加，而是以分区为单位，初始创建的分区，初始值为 0，相同分区 ID 发生合并动作时，在相应分区内累计加 1。 MergeTree 数据分区合并规则 随着数据的写入 MergeTree 存储引擎会很多分区目录。如果分区目录数太多怎么办？因为 Clickhouse 的 MergeTree 存储引擎是基于 LSM 实现的。MergeTree 可以通过分区合并将属于相同分区的多个目录合并为一个新的目录（官方描述在 10 到 15 分钟内会进行合并，也可直接执行 optimize 语句），已经存在的旧目录（也即 system.parts 表中 activie 为 0 的分区）在之后某个时刻通过后台任务删除（默认 8 分钟）。 分区合并 我们回顾之前创建的表的分区目录 # ls 202002_1_1_0 202004_2_2_0 202009_3_3_0 202002_4_4_0 202002_5_5_0 手工触发分区合并 qabb-qa-ch00 :) optimize table tab_partition; OPTIMIZE TABLE tab_partition Ok. 0 rows in set. Elapsed: 0.003 sec. qabb-qa-ch00 :) select partition,name,part_type, active from system.parts where table ='tab_partition'; ┌─partition─┬─name─────────┬─part_type─┬─active─┐ │ 202002 │ 202002_1_1_0 │ Wide │ 0 │ │ 202002 │ 202002_1_5_1 │ Wide │ 1 │ │ 202002 │ 202002_4_4_0 │ Wide │ 0 │ │ 202002 │ 202002_5_5_0 │ Wide │ 0 │ │ 202004 │ 202004_2_2_0 │ Wide │ 1 │ │ 202009 │ 202009_3_3_0 │ Wide │ 1 │ └───────────┴──────────────┴───────────┴────────┘ 6 rows in set. Elapsed: 0.003 sec. 其中 active 为 1 表示经过合并之后的最新分区，为 0 则表示旧分区，查询时会自动过滤 active=0 的分区。 我们通过分区 202002 最新的分区目录 202002_1_5_1 看到合并分区新目录的命名规则如下： PartitionID：分区 ID 保持不变 MinBlockNum：取同一个分区内所有目录中最小的 MinBlockNum 值 MaxBlockNUm：取同一个分区内所有目录中最大的 MaxBlockNum 值 Level：取同一个分区内最大 Level 值并加 1 合并之后的目录结构如下： ","link":"https://tianxiawuhao.github.io/hle1ZXFqv/"},{"title":"ClickHouse-4表引擎","content":"1 表引擎的使用 表引擎是 ClickHouse 的一大特色。可以说，表引擎决定了如何存储表的数据。包括： 数据的存储方式和位置，写到哪里以及从哪里读取数据。(默认是在安装路径下的 data 路径) 支持哪些查询以及如何支持。（有些语法只有在特定的引擎下才能用） 并发数据访问。 索引的使用（如果存在）。 是否可以执行多线程请求。 数据复制参数。 表引擎的使用方式就是必须显式在创建表时定义该表使用的引擎，以及引擎使用的相关参数。 特别注意：引擎的名称大小写敏感, 驼峰命名 2 TinyLog 以列文件的形式保存在磁盘上，不支持索引，没有并发控制。一般保存少量数据的小表，生产环境上作用有限。可以用于平时练习测试用。 如： create table t_tinylog ( id String, name String) engine=TinyLog; 3 Memory 内存引擎，数据以未压缩的原始形式直接保存在内存当中，服务器重启数据就会消失。读写操作不会相互阻塞，不支持索引。简单查询下有非常非常高的性能表现（超过 10G/s）。 一般用到它的地方不多，除了用来测试，就是在需要非常高的性能，同时数据量又不太大（上限大概 1 亿行）的场景。 4 MergeTree ClickHouse 中最强大的表引擎当属 MergeTree（合并树）引擎及该系列（*MergeTree）中的其他引擎，支持索引和分区，地位可以相当于 innodb 之于 Mysql。而且基于 MergeTree，还衍生除了很多小弟，也是非常有特色的引擎。 建表语句 create table t_order_mt( id UInt32, sku_id String, total_amount Decimal(16,2), create_time Datetime ) engine =MergeTree partition by toYYYYMMDD(create_time) primary key (id) order by (id,sku_id); 插入数据 insert into t_order_mt values (101,'sku_001',1000.00,'2020-06-01 12:00:00') , (102,'sku_002',2000.00,'2020-06-01 11:00:00'), (102,'sku_004',2500.00,'2020-06-01 12:00:00'), (102,'sku_002',2000.00,'2020-06-01 13:00:00'), (102,'sku_002',12000.00,'2020-06-01 13:00:00'), (102,'sku_002',600.00,'2020-06-02 12:00:00'); MergeTree 其实还有很多参数(绝大多数用默认值即可)，但是三个参数是更加重要的，也涉及了关于 MergeTree 的很多概念。 4.1 partition by 分区(可选) 作用 学过 hive 的应该都不陌生，分区的目的主要是降低扫描的范围，优化查询速度，如果不填,只会使用一个分区 —— all。 分区目录 MergeTree 是以列文件+索引文件+表定义文件组成的，但是如果设定了分区那么这些文件就会保存到不同的分区目录中。目录保存在本地磁盘 并行 分区后，面对涉及跨分区的查询统计，ClickHouse 会以分区为单位并行处理， 即一个线程处理一个分区内的数据。 数据写入与分区合并 任何一个批次的数据写入都会产生一个临时分区，不会纳入任何一个已有的分区。写入后的某个时刻（大概 10-15 分钟后），ClickHouse 会自动执行合并操作（等不及也可以手动通过 optimize 执行），把临时分区的数据，合并到已有分区中。 optimize table xxxx final; 例如 再次执行上面的插入操作 insert into t_order_mt values (101,'sku_001',1000.00,'2020-06-01 12:00:00') , (102,'sku_002',2000.00,'2020-06-01 11:00:00'), (102,'sku_004',2500.00,'2020-06-01 12:00:00'), (102,'sku_002',2000.00,'2020-06-01 13:00:00'), (102,'sku_002',12000.00,'2020-06-01 13:00:00'), (102,'sku_002',600.00,'2020-06-02 12:00:00'); 查看数据并没有纳入任何分区 手动 optimize 之后 optimize table t_order_mt final; 4.2 primary key 主键(可选) ClickHouse 中的主键，和其他数据库不太一样，它只提供了数据的一级索引，但是却不是唯一约束。这就意味着是可以存在相同 primary key 的数据的。 主键的设定主要依据是查询语句中的 where 条件。 根据条件通过对主键进行某种形式的二分查找，能够定位到对应的 index granularity,避免了全表扫描。 index granularity： 直接翻译的话就是索引粒度，指在稀疏索引中两个相邻索引对应数据的间隔。ClickHouse 中的 MergeTree 默认是 8192。官方不建议修改这个值，除非该列存在大量重复值，比如在一个分区中几万行才有一个不同数据。 稀疏索引： 稀疏索引的好处就是可以用很少的索引数据，定位更多的数据，代价就是只能定位到索引粒度的第一行，然后再进行进行一点扫描。 4.3 order by（必选） order by 设定了分区内的数据按照哪些字段顺序进行有序保存。 order by 是 MergeTree 中唯一一个必填项，甚至比 primary key 还重要，因为当用户不设置主键的情况，很多处理会依照 order by 的字段进行处理（比如后面会讲的去重和汇总）。 要求：主键必须是 order by 字段的前缀字段。有点类似 SQL 索引中的最左前缀。 比如 order by 字段是 (id,sku_id) 那么主键必须是 id 或者(id,sku_id) 4.4 二级索引(跳数索引) 目前在 ClickHouse 的官网上二级索引的功能在 v20.1.2.4 之前是被标注为实验性的，在这个版本之后默认是开启的。 老版本使用二级索引前需要增加设置 是否允许使用实验性的二级索引（v20.1.2.4 开始，这个参数已被删除，默认开启） set allow_experimental_data_skipping_indices=1; 创建测试表 create table t_order_mt2( id UInt32, sku_id String, total_amount Decimal(16,2), create_time Datetime, INDEX a total_amount TYPE minmax GRANULARITY 5 ) engine =MergeTree partition by toYYYYMMDD(create_time) primary key (id) order by (id, sku_id); 最主要的是需要加上这一句：INDEX a total_amount TYPE minmax GRANULARITY 5 其中： INDEX a： 定义一个索引，并命名为 a total_amount： 索引字段名称 TYPE minmax： 定义类型， 保存最大最小值 GRANULARITY N：是设定二级索引对于一级索引粒度的粒度。一级索引会记录每个分区的最大最小值，二级索引就是在一级索引的基础上，取每 N 个分区合在一起的最大最小值。 插入数据 insert into t_order_mt2 values (101,'sku_001',1000.00,'2020-06-01 12:00:00') , (102,'sku_002',2000.00,'2020-06-01 11:00:00'), (102,'sku_004',2500.00,'2020-06-01 12:00:00'), (102,'sku_002',2000.00,'2020-06-01 13:00:00'), (102,'sku_002',12000.00,'2020-06-01 13:00:00'), (102,'sku_002',600.00,'2020-06-02 12:00:00'); 对比效果 那么在使用下面语句进行测试，可以看出二级索引能够为非主键字段的查询发挥作用。 clickhouse-client --send_logs_level=trace &lt;&lt;&lt; 'select * from t_order_mt2 where total_amount &gt; toDecimal32(900., 2)'; 4.5 数据 TTL（数据存活时间） TTL 即 Time To Live，MergeTree 提供了可以管理数据表或者列的生命周期的功能。过期的数据不会立马处理，只是会做标记，等到合并时一起处理。 列级别 TTL （1）创建测试表 create table t_order_mt3( id UInt32, sku_id String, total_amount Decimal(16,2) TTL create_time+interval 10 SECOND, create_time Datetime ) engine =MergeTree partition by toYYYYMMDD(create_time) primary key (id) order by (id, sku_id); 在列后加上： TTL create_time+interval 10 SECOND 其中： TTL：定义一个过期时间 create_time：使用了当前表中的一个 Datetime 类型的列，TTL 中引用的字段不能是主键字段，且类型必须是 日期类型 ± interval 10 SECOND：需要增加/减少的 时间长度 时间单位 注：如果在建表时没有加 TTL 配置，需要在建表之后加，那么则需要使用如下语法： ALTER TABLE 表名 MODIFY COLUMN 列名 列数据类型 TTL d + INTERVAL 10 SECOND; （2）插入数据（注意：根据实际时间改变） insert into t_order_mt3 values (106,'sku_001',1000.00,'2020-06-12 22:52:30'), (107,'sku_002',2000.00,'2020-06-12 22:52:30'), (110,'sku_003',600.00,'2020-06-13 12:00:00'); （3）手动合并，查看效果,到期后，指定的字段数据归 0；如果执行合并后还是可以看到数据，可能是需要设置时区，或者需要重启。 表级 TTL 与列级相同，在建表时与建表之后都有对应的语法进行设置 （1）在建表时，写在 Order By 的后面 CREATE TABLE example_table ( d DateTime, a Int ) ENGINE = MergeTree PARTITION BY toYYYYMM(d) ORDER BY d TTL d + INTERVAL 1 MONTH [DELETE], d + INTERVAL 1 WEEK TO VOLUME 'aaa', d + INTERVAL 2 WEEK TO DISK 'bbb'; 该示例中的 [DELETE]、TO VOLUME……在下文&quot;注意&quot;中说明 （2）在建表之后，使用 alter alter table t_order_mt3 MODIFY TTL create_time + INTERVAL 10 SECOND; 表级的 TTL ，如果是按照表中字段值判断是否过期，那么不会整表删除，只会是某一行数据到期删某一行。 注意 涉及判断的字段必须是 Date 或者 Datetime 类型，推荐使用分区的日期字段。 能够使用的时间单位： SECOND MINUTE HOUR DAY WEEK MONTH QUARTER YEAR 上文提到的 [DELETE]、TO VOLUME…… 表示的是过期之后，需要做的操作。可选的配置如下 DELETE - 删除数据 （默认，不配置则删除）; RECOMPRESS codec_name - 使用codec_name重新压缩数据部分； TO DISK ‘aaa’ - 将数据移动到磁盘 aaa 上 TO VOLUME ‘bbb’ - 将数据移动到磁盘 bbb 上 GROUP BY - 聚合过期行 5 ReplacingMergeTree ReplacingMergeTree 是 MergeTree 的一个变种，它存储特性完全继承 MergeTree，只是多了一个去重的功能。 尽管 MergeTree 可以设置主键，但是 primary key 其实没有唯一约束的功能。如果你想处理掉重复的数据，可以借助这个 ReplacingMergeTree。去重按照order by的字段去重 去重时机 数据的去重只会在合并的过程中出现。合并会在未知的时间在后台进行，所以你无法预先作出计划。有一些数据可能仍未被处理。 去重范围 如果表经过了分区，去重只会在分区内部进行去重，不能执行跨分区的去重。 所以 ReplacingMergeTree 能力有限， ReplacingMergeTree 适用于在后台清除重复的数据以节省空间，但是它不保证没有重复的数据出现。 案例演示 （1）创建表 create table t_order_rmt( id UInt32, sku_id String, total_amount Decimal(16,2) , create_time Datetime ) engine =ReplacingMergeTree(create_time) partition by toYYYYMMDD(create_time) primary key (id) order by (id, sku_id); ReplacingMergeTree() 填入的参数为版本字段，重复数据保留版本字段值最大的。 如果不填版本字段，默认按照插入顺序保留最后一条。 （2）向表中插入数据 insert into t_order_rmt values (101,'sku_001',1000.00,'2020-06-01 12:00:00') , (102,'sku_002',2000.00,'2020-06-01 11:00:00'), (102,'sku_004',2500.00,'2020-06-01 12:00:00'), (102,'sku_002',2000.00,'2020-06-01 13:00:00'), (102,'sku_002',12000.00,'2020-06-01 13:00:00'), (102,'sku_002',600.00,'2020-06-02 12:00:00'); （3）执行第一次查询 select * from t_order_rmt; （4）手动合并 OPTIMIZE TABLE t_order_rmt FINAL; （5）再执行一次查询 select * from t_order_rmt; 通过测试得到结论 实际上是使用 order by 字段作为唯一键 去重不能跨分区 只有同一批插入（新版本）或合并分区时才会进行去重 认定重复的数据保留，版本字段值最大的 如果版本字段相同则按插入顺序保留最后一笔 6 SummingMergeTree 对于不查询明细，只关心以维度进行汇总聚合结果的场景。如果只使用普通的MergeTree的话，无论是存储空间的开销，还是查询时临时聚合的开销都比较大。 ClickHouse 为了这种场景，提供了一种能够“预聚合”的引擎 SummingMergeTree。该引擎聚合的依据依然是 order by 的字段，相当于按照 order by 的字段做了一次 Group By。 案例演示 （1）创建表 create table t_order_smt( id UInt32, sku_id String, total_amount Decimal(16,2) , create_time Datetime ) engine =SummingMergeTree(total_amount) partition by toYYYYMMDD(create_time) primary key (id) order by (id,sku_id ); （2）插入数据 insert into t_order_smt values (101,'sku_001',1000.00,'2020-06-01 12:00:00'), (102,'sku_002',2000.00,'2020-06-01 11:00:00'), (102,'sku_004',2500.00,'2020-06-01 12:00:00'), (102,'sku_002',2000.00,'2020-06-01 13:00:00'), (102,'sku_002',12000.00,'2020-06-01 13:00:00'), (102,'sku_002',600.00,'2020-06-02 12:00:00'); （3）执行第一次查询 select * from t_order_smt; （4）手动合并 OPTIMIZE TABLE t_order_smt FINAL; （5）再执行一次查询 select * from t_order_smt; 通过结果可以得到以下结论 以 SummingMergeTree（）中指定的列作为汇总数据列 可以填写多列必须数字列，如果不填，以所有非维度列且为数字列的字段为汇总数据列 以 order by 的列为准，作为维度列 其他的列按插入顺序保留第一行 不在一个分区的数据不会被聚合 只有在同一批次插入(新版本)或分片合并时才会进行聚合 开发建议 设计聚合表的话，唯一键值、流水号可以去掉，所有字段全部是维度、度量或者时间戳。 问题 能不能直接执行以下 SQL 得到汇总值 select total_amount from XXX where province_name=’’ and create_date=’xxx’ 答： 不行，可能会包含一些还没来得及聚合的临时明细 如果要是获取汇总值，还是需要使用 sum 进行聚合，这样效率会有一定的提高，但本身 ClickHouse 是列式存储的，效率提升有限，不会特别明显。 select sum(total_amount) from province_name=’’ and create_date=‘xxx’ ","link":"https://tianxiawuhao.github.io/XvNkKhwMg/"},{"title":"ClickHouse-3数据类型","content":"1. 整型 固定长度的整型，包括 有符号整型(有正有负) 或 无符号整型。 类比 Java 类型： CH类型 Java类型 整型范围（-2n-1~2n-1-1） Int8 - [-128 : 127] Byte Int16 - [-32768 : 32767] Short Int32 - [-2147483648 : 2147483647] Int Int64 - [-9223372036854775808 : 9223372036854775807] Long 无符号整型范围（0~2n-1） UInt8 - [0 : 255] UInt16 - [0 : 65535] UInt32 - [0 : 4294967295] UInt64 - [0 : 18446744073709551615] 后面的数组就代表位数 使用场景： 个数、数量、也可以存储型 id。 2. 浮点型 类比 Java 类型： CH类型 Java类型 Float32 float Float64 double 建议尽可能以整数形式存储数据。例如，将固定精度的数字转换为整数值，如时间用毫秒为单位表示，因为浮点型进行计算时可能引起四舍五入的误差。 使用场景：一般数据值比较小，不涉及大量的统计计算，精度要求不高的时候。比如保存商品的重量。 3. 布尔型 没有单独的类型来存储布尔值。可以使用 UInt8 类型，取值限制为 0 或 1。 4. Decimal 型 有符号的浮点数，可在加、减和乘法运算过程中保持精度。对于除法，最低有效数字会被丢弃（不四舍五入）。 有三种声明 (s 标识小数位)： Decimal32(s)， 即 一共有 8 位，其中小数部分占 s 位，相当于 Mysql - Decimal(9-s,s) Decimal64(s)， 即 一共有 18 位，其中小数部分占 s 位，相当于 Decimal(18-s,s) Decimal128(s)， 即 一共有 38位，其中小数部分占 s 位，相当于 Decimal(38-s,s) 使用场景： 一般金额字段、汇率、利率等字段为了保证小数点精度，都使用 Decimal进行存储。 5. 字符串 String - 对应 Mysql - Varchar 字符串可以任意长度的。它可以包含任意的字节集，包含空字节。 FixedString(N) 固定长度 N 的字符串，N 必须是严格的正自然数。当服务端读取长度小于 N 的字符串时候，通过在字符串末尾添加空字节来达到 N 字节长度。 当服务端读取长度大于 N 的字符串时候，将返回错误消息。 与 String 相比，极少会使用 FixedString，因为使用起来不是很方便。 使用场景：名称、文字描述、字符型编码。 固定长度的可以保存一些定长的内容，比如一些编码，性别等但是考虑到一定的变化风险，带来收益不够明显，所以定长字符串使用意义有限。 6. 枚举类型 包括 Enum8 和 Enum16 类型。Enum 保存 ‘string’= integer 的对应关系。 Enum8 用 ‘String’= Int8 对描述。 Enum16 用 ‘String’= Int16 对描述。 创建一个带有一个枚举 Enum8(‘hello’ = 1, ‘world’ = 2) 类型的列 CREATE TABLE t_enum ( x Enum8('hello' = 1, 'world' = 2) ) ENGINE = TinyLog; 这个 x 列只能存储类型定义中列出的值：‘hello’或’world’ INSERT INTO t_enum VALUES ('hello'), ('world'), ('hello'); 如果尝试保存任何其他值，ClickHouse 抛出异常 insert into t_enum values('a') 如果需要看到对应行的数值，则必须将 Enum 值转换为整数类型 SELECT CAST(x, 'Int8') FROM t_enum; 使用场景：对一些状态、类型的字段算是一种空间优化，也算是一种数据约束。但是实际使用中往往因为一些数据内容的变化增加一定的维护成本，甚至是数据丢失问题。所以谨慎使用。 7. 时间类型 目前 ClickHouse 有三种时间类型 Date 接受年-月-日的字符串比如 ‘2019-12-16’ Datetime 接受年-月-日 时:分:秒的字符串比如 ‘2019-12-16 20:50:10’ Datetime64 接受年-月-日 时:分:秒.亚秒的字符串比如‘2019-12-16 20:50:10.66’ 日期类型，用两个字节存储，表示从 1970-01-01 (无符号) 到当前的日期值。 8. 数组 Array(T)：由 T 类型元素组成的数组。 T 可以是任意类型，包含数组类型。 但不推荐使用多维数组，ClickHouse 对多维数组的支持有限。例如，不能在 MergeTree 表中存储多维数组。 创建数组方式 1，使用 array 函数array(T) SELECT array(1, 2) AS x, toTypeName(x) ; 创建数组方式 2：使用方括号[] SELECT [1, 2] AS x, toTypeName(x); 9. 其他 还有很多数据结构，可以参考官方文档：https://clickhouse.com/docs/zh/sql-reference/data-types/ ","link":"https://tianxiawuhao.github.io/1h169zkFb/"},{"title":"Docker安装clickhouse","content":"ClickHouse 很多大厂都在用，本篇主要使用Docker进行安装 安装配置 创建目录并更改权限 mkdir -p /app/cloud/clickhouse/data mkdir -p /app/cloud/clickhouse/conf mkdir -p /app/cloud/clickhouse/log chmod -R 777 /app/cloud/clickhouse/data chmod -R 777 /app/cloud/clickhouse/conf chmod -R 777 /app/cloud/clickhouse/log 拉取镜像 docker pull yandex/clickhouse-server:20.3.5.21 docker pull yandex/clickhouse-client:20.3.5.21 查看 https://hub.docker.com/r/yandex/clickhouse-server/dockerfile 文件，EXPOSE 9000 8123 9009 了三个端口 创建临时容器 docker run --rm -d --name=clickhouse-server --ulimit nofile=262144:262144 -p 8123:8123 -p 9009:9009 -p 9000:9000 yandex/clickhouse-server:20.3.5.21 复制临时容器内配置文件到宿主机 docker cp clickhouse-server:/etc/clickhouse-server/config.xml D:/clickhouse/conf/config.xml docker cp clickhouse-server:/etc/clickhouse-server/users.xml D:/clickhouse/conf/users.xml 停掉临时容器 docker stop clickhouse-server 创建default账号密码 PASSWORD=$(base64 &lt; /dev/urandom | head -c8); echo &quot;$PASSWORD&quot;; echo -n &quot;$PASSWORD&quot; | sha256sum | tr -d '-' SEGByR98 211371f5bc54970907173acf6facb35f0acbc17913e1b71b814117667c01d96d 会输出明码和SHA256密码 创建root账号密码 PASSWORD=$(base64 &lt; /dev/urandom | head -c8); echo &quot;$PASSWORD&quot;; echo -n &quot;$PASSWORD&quot; | sha256sum | tr -d '-' 092j3AnV 35542ded44184b1b4b6cd621e052662578025b58b4187176a3ad2b9548c8356e 会输出明码和SHA256密码 修改 D:/clickhouse/conf/users.xml 把default账号设为只读权限，并设置密码yandex--&gt;users--&gt;default--&gt;profile节点设为 readonly 注释掉 yandex--&gt;users--&gt;default--&gt;password 节点 新增 yandex--&gt;users--&gt;default--&gt;password_sha256_hex 节点，填入生成的密码 修改default账号 &lt;?xml version=&quot;1.0&quot;?&gt; &lt;yandex&gt; &lt;users&gt; &lt;default&gt; &lt;password_sha256_hex&gt;211371f5bc54970907173acf6facb35f0acbc17913e1b71b814117667c01d96d&lt;/password_sha256_hex&gt; &lt;networks incl=&quot;networks&quot; replace=&quot;replace&quot;&gt; &lt;ip&gt;::/0&lt;/ip&gt; &lt;/networks&gt; &lt;profile&gt;readonly&lt;/profile&gt; &lt;quota&gt;default&lt;/quota&gt; &lt;/default&gt; &lt;/users&gt; &lt;/yandex&gt; 新增root账号 &lt;?xml version=&quot;1.0&quot;?&gt; &lt;yandex&gt; &lt;users&gt; &lt;root&gt; &lt;password_sha256_hex&gt;35542ded44184b1b4b6cd621e052662578025b58b4187176a3ad2b9548c8356e&lt;/password_sha256_hex&gt; &lt;networks incl=&quot;networks&quot; replace=&quot;replace&quot;&gt; &lt;ip&gt;::/0&lt;/ip&gt; &lt;/networks&gt; &lt;profile&gt;default&lt;/profile&gt; &lt;quota&gt;default&lt;/quota&gt; &lt;/root&gt; &lt;/users&gt; &lt;/yandex&gt; 创建容器 docker run -d --name=clickhouse-server -p 8123:8123 -p 9009:9009 -p 9000:9000 --ulimit nofile=262144:262144 -v D:/clickhouse/data:/var/lib/clickhouse:rw -v D:/clickhouse/conf/config.xml:/etc/clickhouse-server/config.xml -v D:/clickhouse/conf/users.xml:/etc/clickhouse-server/users.xml -v D:/clickhouse/log:/var/log/clickhouse-server:rw yandex/clickhouse-server 操作 docker exec -it docker-clickhouse /bin/bash 进入容器 clickhouse-client 进入clickhouse命令行 ","link":"https://tianxiawuhao.github.io/BBgY7nkHa/"},{"title":"ClickHouse-2初识","content":"一、简介 1.1 ClickHouse 是什么？ ClickHouse 是 Yandex（俄罗斯最大的搜索引擎）开源的一个用于实时数据分析的基于列存储的数据库，其处理数据的速度比传统方法快 100-1000 倍。ClickHouse 的性能超过了目前市场上可比的面向列的 DBMS，每秒钟每台服务器每秒处理数亿至十亿多行和数十千兆字节的数据。 1.2 ClickHouse的一些特性： 快速：ClickHouse 会充分利用所有可用的硬件，以尽可能快地处理每个查询。单个查询的峰值处理性能超过每秒 2 TB（解压缩后，仅使用的列）。在分布式设置中，读取是在健康副本之间自动平衡的，以避免增加延迟。 容错：ClickHouse 支持多主机异步复制，并且可以跨多个数据中心进行部署。所有节点都相等，这可以避免出现单点故障。单个节点或整个数据中心的停机时间不会影响系统的读写可用性。 可伸缩：ClickHouse 可以在垂直和水平方向上很好地缩放。ClickHouse 易于调整以在具有数百或数千个节点的群集上或在单个服务器上，甚至在小型虚拟机上执行。当前，每个单节点安装的数据量超过数万亿行或数百兆兆字节。 易用：ClickHouse 简单易用，开箱即用。它简化了所有数据处理：将所有结构化数据吸收到系统中，并且立即可用于构建报告。SQL 允许表达期望的结果，而无需涉及某些 DBMS 中可以找到的任何自定义非标准 API。 充分利用硬件：ClickHouse 与具有相同的可用 I/O 吞吐量和 CPU 容量的传统的面向行的系统相比，其处理典型的分析查询要快两到三个数量级。列式存储格式允许在 RAM 中容纳更多热数据，从而缩短了响应时间。 提高 CPU 效率：向量化查询执行涉及相关的 SIMD 处理器指令和运行时代码生成。处理列中的数据会提高 CPU 行缓存的命中率。 优化磁盘访问：ClickHouse 可以最大程度地减少范围查询的次数，从而提高了使用旋转磁盘驱动器的效率，因为它可以保持连续存储数据。 最小化数据传输：ClickHouse 使公司无需使用专门针对高性能计算的专用网络即可管理其数据。 何时使用 ClickHouse： 用于分析结构良好且不可变的事件或日志流，建议将每个此类流放入具有预连接维度的单个宽表中。 何时不使用 ClickHouse： 不适合事务性工作负载（OLTP）、高价值的键值请求、Blob 或文档存储。 1.3 为什么 ClickHouse 速度这么快？ 首先我们了解一下 OLAP 场景的特点： 读多于写。 大宽表，读大量行但是少量列，结果集较小。 数据批量写入，且数据不更新或少更新。 针对分析类查询，通常只需要读取表的一小部分列。在列式数据库中你可以只读取你需要的数据。例如，如果只需要读取 100 列中的 5 列，这将帮助你最少减少 20 倍的 I/O 消耗。 由于数据总是打包成批量读取的，所以压缩是非常容易的。同时数据按列分别存储这也更容易压缩。这进一步降低了 I/O 的体积。由于 I/O 的降低，这将帮助更多的数据被系统缓存。 例如，查询《统计每个广告平台的记录数量》需要读取《广告平台 ID》这一列，它在未压缩的情况下需要 1 个字节进行存储。如果大部分流量不是来自广告平台，那么这一列至少可以以十倍的压缩率被压缩。当采用快速压缩算法，它的解压速度最少在十亿字节（未压缩数据）每秒。换句话说，这个查询可以在单个服务器上以每秒大约几十亿行的速度进行处理。这实际上是当前实现的速度。 ClickHouse 从 OLAP 场景需求出发，定制开发了一套全新的高效列式存储引擎 column-oriented 图片来源见水印相比于行式存储，列式存储在分析场景下有着许多优良的特性。 如前所述，分析场景中往往需要读大量行但是少数几个列。在行存模式下，数据按行连续存储，所有列的数据都存储在一个 block 中，不参与计算的列在 IO 时也要全部读出，读取操作被严重放大。而列存模式下，只需要读取参与计算的列即可，极大的减低了 IO cost，加速了查询。 同一列中的数据属于同一类型，压缩效果显著。列存往往有着高达十倍甚至更高的压缩比，节省了大量的存储空间，降低了存储成本。 更高的压缩比意味着更小的 data size，从磁盘中读取相应数据耗时更短。 自由的压缩算法选择。不同列的数据具有不同的数据类型，适用的压缩算法也就不尽相同。可以针对不同列类型，选择最合适的压缩算法。 高压缩比，意味着同等大小的内存能够存放更多数据，系统 cache 效果更好。 ","link":"https://tianxiawuhao.github.io/GwxNFXsxh/"},{"title":"Clickhouse-1安装","content":"1.ClickHouse的安装 1.1 准备工作 下载RPM包: https://repo.yandex.ru/clickhouse/rpm/stable/x86_64/ 下载完毕如下: ​ 1.1.1 确定防火墙处于关闭状态 相关命令: sudo systemctl status firewalld sudo systemctl start firewalld sudo systemctl stop firewalld sudo systemctl restart firewalld 1.1.2 CentOS取消打开文件数限制 Ø 在 /etc/security/limits.conf文件的末尾加入以下内容 vim /etc/security/limits.conf * soft nofile 65536 * hard nofile 65536 * soft nproc 131072 * hard nproc 131072 Ø 在/etc/security/limits.d/20-nproc.conf文件的末尾加入以下内容 vim /etc/security/limits.d/20-nproc.conf * soft nofile 65536 * hard nofile 65536 * soft nproc 131072 * hard nproc 131072 Ø 执行同步操作(同步到集群其他机器) xsync /etc/security/limits.conf xsync /etc/security/limits.d/20-nproc.conf 1.1.3 安装依赖 sudo yum install -y libtool sudo yum install -y *unixODBC* 同样在集群其他机器上执行以上操作 1.1.4 CentOS取消SELINUX SELINUX(美国开源的linux安全增强功能) Ø 修改/etc/selinux/config中的SELINUX=disabled sudo vim /etc/selinux/config SELINUX=disabled Ø 执行同步操作 sudo /home/atguigu/bin/xsync /etc/selinux/config Ø 重启三台服务器 1.1.5 将安装文件同步到其他两个服务器 1.1.6 分别在三台机子上安装这4个rpm文件 sudo rpm -ivh *.rpm(提前把四个*.rpm放在一个目录下) sudo rpm -qa|grep clickhouse查看安装情况 1.1.7 修改配置文件 vim /etc/clickhouse-server/config.xml 把 &lt;listen_host&gt;::&lt;/listen_host&gt; 的注释打开，这样的话才能让ClickHouse被除本机以外的服务器访问 #分发配置文件 xsync /etc/clickhouse-server/config.xml 1.1.8 启动Server sudo systemctl start clickhouse-server 1.1.9 三台机器上关闭开机自启 systemctl disable clickhouse-server 1.1.10 使用client连接server clickhouse-client -m Clickhouse常用端口号: 第2章 副本 副本的目的主要是保障数据的高可用性，即使一台ClickHouse节点宕机，那么也可以从其他服务器获得相同的数据。 2.1 副本写入流程 2.2 配置步骤 Ø 启动zookeeper集群 Ø 注意:服务器的hostname需要改成对应的ck101、ck102、ck103 Ø 在/etc/clickhouse-server/config.d目录下创建一个名为metrika.xml的配置文件,内容如下： &lt;?xml version=&quot;1.0&quot;?&gt; &lt;yandex&gt; &lt;zookeeper-servers&gt; &lt;node index=&quot;1&quot;&gt; &lt;host&gt;ck101&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;node index=&quot;2&quot;&gt; &lt;host&gt;ck102&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;node index=&quot;3&quot;&gt; &lt;host&gt;ck103&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;/zookeeper-servers&gt; &lt;/yandex&gt; Ø 同步到另外两个机器上 xsync /etc/clickhouse-server/config.d/metrika.xml Ø 在 /etc/clickhouse-server/config.xml中增加 &lt;zookeeper incl=&quot;zookeeper-servers&quot; optional=&quot;true&quot; /&gt; &lt;include_from&gt;/etc/clickhouse-server/config.d/metrika.xml&lt;/include_from&gt; Ø 同步到另外两个机器上 xsync /etc/clickhouse-server/config.xml Ø 分别在另外两个机器上启动ClickHouse服务 注意：因为修改了配置文件，如果以前启动了服务需要重启 systemctl start clickhouse-server Ø 在另外两个机器上分别建表 副本只能同步数据，不能同步表结构，所以我们需要在每台机器上自己手动建表 Ck101 create table t_order_rep ( id UInt32, sku_id String, total_amount Decimal(16,2), create_time Datetime ) engine =ReplicatedMergeTree('/clickhouse/table/01/t_order_rep','rep_102') partition by toYYYYMMDD(create_time) primary key (id) order by (id,sku_id); Ck102 create table t_order_rep ( id UInt32, sku_id String, total_amount Decimal(16,2), create_time Datetime ) engine =ReplicatedMergeTree('/clickhouse/table/01/t_order_rep','rep_103') partition by toYYYYMMDD(create_time) primary key (id) order by (id,sku_id); 参数解释 ReplicatedMergeTree 中， 第一个参数是分片的zk_path一般按照： /clickhouse/table/{shard}/{table_name} 的格式写，如果只有一个分片就写01即可。 第二个参数是副本名称，相同的分片副本名称不能相同。 Ø 在ck101上执行insert语句 insert into t_order_rep values (101,'sku_001',1000.00,'2020-06-01 12:00:00'), (102,'sku_002',2000.00,'2020-06-01 12:00:00'), (103,'sku_004',2500.00,'2020-06-01 12:00:00'), (104,'sku_002',2000.00,'2020-06-01 12:00:00'), (105,'sku_003',600.00,'2020-06-02 12:00:00'); Ø 在ck102上执行select，可以查询出结果，说明副本配置正确 第3章 分片集群 副本虽然能够提高数据的可用性，降低丢失风险，但是每台服务器实际上必须容纳全量数据，对数据的横向扩容没有解决。 要解决数据水平切分的问题，需要引入分片的概念。通过分片把一份完整的数据进行切分，不同的分片分布到不同的节点上，再通过Distributed表引擎把数据拼接起来一同使用。 Distributed表引擎本身不存储数据，有点类似于MyCat之于MySql，成为一种中间件，通过分布式逻辑表来写入、分发、路由来操作多台节点不同分片的分布式数据。 注意：ClickHouse的集群是表级别的，实际企业中，大部分做了高可用，但是没有用分片，避免降低查询性能以及操作集群的复杂性。 3.1 集群写入流程（3分片2副本共6个节点） 3.2 集群读取流程（3分片2副本共6个节点） 3.3 配置三节点版本集群及副本 3.3.1 集群及副本规划（2个分片，只有第一个分片有副本） ck101 ck102 ck103 01 rep_1_1 01 rep_1_2 02 rep_2_1 3.3.2 配置步骤 (1) 在ck101的/etc/clickhouse-server/config.d目录下创建metrika-shard.xml文件 &lt;?xml version=&quot;1.0&quot;?&gt; &lt;yandex&gt; &lt;remote_servers&gt; &lt;my_cluster&gt; &lt;!-- 集群名称--&gt; &lt;shard&gt; &lt;!--集群的第一个分片--&gt; &lt;internal_replication&gt;true&lt;/internal_replication&gt; &lt;replica&gt; &lt;!--该分片的第一个副本--&gt; &lt;host&gt;ck101&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;/replica&gt; &lt;replica&gt; &lt;!--该分片的第二个副本--&gt; &lt;host&gt;ck102&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;/replica&gt; &lt;/shard&gt; &lt;shard&gt; &lt;!--集群的第二个分片--&gt; &lt;internal_replication&gt;true&lt;/internal_replication&gt; &lt;replica&gt; &lt;!--该分片的第一个副本--&gt; &lt;host&gt;ck103&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;/replica&gt; &lt;/shard&gt; &lt;/my_cluster&gt; &lt;/remote_servers&gt; &lt;zookeeper-servers&gt; &lt;node index=&quot;1&quot;&gt; &lt;host&gt; ck101&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;node index=&quot;2&quot;&gt; &lt;host&gt; ck102&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;node index=&quot;3&quot;&gt; &lt;host&gt; ck103&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;/zookeeper-servers&gt; &lt;macros&gt; &lt;shard&gt;01&lt;/shard&gt; &lt;!--不同机器放的分片数不一样--&gt; &lt;replica&gt;rep_1_1&lt;/replica&gt; &lt;!--不同机器放的副本数不一样--&gt; &lt;/macros&gt; &lt;/yandex&gt; (2) 将ck101的metrika-shard.xml同步到102和103 xsync /etc/clickhouse-server/config.d/metrika-shard.xml (3) 修改102和103中metrika-shard.xml宏的配置 Ø 102 vim /etc/clickhouse-server/config.d/metrika-shard.xml Ø 103 vim /etc/clickhouse-server/config.d/metrika-shard.xml (4) 在hadoop102上修改/etc/clickhouse-server/config.xml (5) 同步/etc/clickhouse-server/config.xml到103和104 sudo /home/atguigu/bin/xsync /etc/clickhouse-server/config.xml (6) 重启三台服务器上的ClickHouse服务 systemctl stop clickhouse-server systemctl start clickhouse-server systemctl status clickhouse-server (7) 在ck101上执行建表语句 Ø 会自动同步到ck102和ck103上 Ø 集群名字要和配置文件中的一致 Ø 分片和副本名称从配置文件的宏定义中获取 create table st_order_mt on cluster my_cluster ( id UInt32, sku_id String, total_amount Decimal(16,2), create_time Datetime ) engine =ReplicatedMergeTree('/clickhouse/tables/{shard}/st_order_mt','{replica}') partition by toYYYYMMDD(create_time) primary key (id) order by (id,sku_id); 可以到ck102和ck103上查看表是否创建成功 (8) 在ck101上创建Distribute 分布式表 create table st_order_mt_all on cluster my_cluster ( id UInt32, sku_id String, total_amount Decimal(16,2), create_time Datetime )engine = Distributed(my_cluster,default, st_order_mt,hiveHash(sku_id)); 参数含义 ​ Distributed(集群名称，库名，本地表名，分片键) 分片键必须是整型数字，所以用hiveHash函数转换，也可以rand() (9) 在ck101上插入测试数据 insert into st_order_mt_all values (201,'sku_001',1000.00,'2020-06-01 12:00:00') , (202,'sku_002',2000.00,'2020-06-01 12:00:00'), (203,'sku_004',2500.00,'2020-06-01 12:00:00'), (204,'sku_002',2000.00,'2020-06-01 12:00:00'), (205,'sku_003',600.00,'2020-06-02 12:00:00'); (10) 通过查询分布式表和本地表观察输出结果 Ø 分布式表 SELECT * FROM st_order_mt_all; Ø 本地表 select * from st_order_mt; Ø 观察数据的分布 st_order_mt_all Ck101: st_order_mt Ck102: st_order_mt Ck103: st_order_mt 第4章 版本信息 Clickhouse: clickhouse-21.9.4.35 Zookeeper: zookeeper-3.4.6 ","link":"https://tianxiawuhao.github.io/r2VmiGgor/"},{"title":"Kafka 架构深入","content":"3.1 Kafka 工作流程及文件存储机制 Kafka 中消息是以 topic 进行分类的， 生产者生产消息，消费者消费消息，都是面向 topic 的。 topic 是逻辑上的概念，而 partition 是物理上的概念，每个 partition 对应于一个 log 文件，该 log 文件中存储的就是 producer 生产的数据。 Producer 生产的数据会被不断追加到该 log 文件末端，且每条数据都有自己的 offset。 消费者组中的每个消费者， 都会实时记录自己消费到了哪个 offset，以便出错恢复时，从上次的位置继续消费 。 由于生产者生产的消息会不断追加到 log 文件末尾， 为防止 log 文件过大导致数据定位效率低下， Kafka 采取了分片和索引机制，将每个 partition 分为多个 segment。 每个 segment 对应两个文件——“.index”文件和“.log”文件。 这些文件位于一个文件夹下， 该文件夹的命名规则为： topic名称+分区序号。例如， first 这个 topic 有三个分区，则其对应的文件夹为 first-0,first-1,first-2。 00000000000000000000.index 00000000000000000000.log 00000000000000170410.index 00000000000000170410.log 00000000000000239430.index 00000000000000239430.log index 和 log 文件以当前 segment 的第一条消息的 offset 命名。下图为 index 文件和 log文件的结构示意图。 “.index”文件存储大量的索引信息，“.log”文件存储大量的数据，索引文件中的元数据指向对应数据文件中 message 的物理偏移地址。 3.2 Kafka 生产者 3.2.1 分区策略 分区的原因 （1） 方便在集群中扩展，每个 Partition 可以通过调整以适应它所在的机器，而一个 topic 又可以有多个 Partition 组成，因此整个集群就可以适应任意大小的数据了； （2） 可以提高并发，因为可以以 Partition 为单位读写了。 分区的原则 我们需要将 producer 发送的数据封装成一个 ProducerRecord 对象。 （1） 指明 partition 的情况下，直接将指明的值直接作为 partiton 值； （2）没有指明 partition 值但有 key 的情况下，将 key 的 hash 值与 topic 的 partition 数进行取余得到 partition 值； （3） 既没有 partition 值又没有 key 值的情况下，第一次调用时随机生成一个整数（后 面每次调用在这个整数上自增），将这个值与 topic 可用的 partition 总数取余得到 partition 值，也就是常说的 round-robin 算法。 3.2.2 数据可靠性保证 为保证 producer 发送的数据，能可靠的发送到指定的 topic， topic 的每个 partition 收到 producer 发送的数据后， 都需要向 producer 发送 ack（acknowledgement 确认收到） ，如果 producer 收到 ack， 就会进行下一轮的发送，否则重新发送数据。 1） 副本数据同步策略 方案 优点 缺点 半数以上完成同步， 就发 送 ack 延迟低 选举新的 leader 时， 容忍 n 台 节点的故障，需要 2n+1 个副 本 全部完成同步，才发送 ack 选举新的 leader 时， 容忍 n 台 节点的故障，需要 n+1 个副 本 延迟高 Kafka 选择了第二种方案，原因如下： 同样为了容忍 n 台节点的故障，第一种方案需要 2n+1 个副本，而第二种方案只需要 n+1 个副本，而 Kafka 的每个分区都有大量的数据， 第一种方案会造成大量数据的冗余。 虽然第二种方案的网络延迟会比较高，但网络延迟对 Kafka 的影响较小。 2） ISR 采用第二种方案之后，设想以下情景： leader 收到数据，所有 follower 都开始同步数据， 但有一个 follower，因为某种故障，迟迟不能与 leader 进行同步，那 leader 就要一直等下去， 直到它完成同步，才能发送 ack。这个问题怎么解决呢？ Leader 维护了一个动态的 in-sync replica set (ISR)，意为和 leader 保持同步的 follower 集 合。当 ISR 中的 follower 完成数据的同步之后， leader 就会给 follower 发送 ack。如果 follower 长 时 间 未 向 leader 同 步 数 据 ， 则 该 follower 将 被 踢 出 ISR ， 该 时 间 阈 值 由replica.lag.time.max.ms 参数设定。 Leader 发生故障之后，就会从 ISR 中选举新的 leader。 3） ack 应答机制 对于某些不太重要的数据，对数据的可靠性要求不是很高，能够容忍数据的少量丢失， 所以没必要等 ISR 中的 follower 全部接收成功。 所以 Kafka 为用户提供了三种可靠性级别，用户根据对可靠性和延迟的要求进行权衡， 选择以下的配置。 acks 参数配置： acks： 0： producer 不等待 broker 的 ack，这一操作提供了一个最低的延迟， broker 一接收到还 没有写入磁盘就已经返回，当 broker 故障时有可能丢失数据； 1： producer 等待 broker 的 ack， partition 的 leader 落盘成功后返回 ack，如果在 follower 同步成功之前 leader 故障，那么将会丢失数据； -1（all） ： producer 等待 broker 的 ack， partition 的 leader 和 follower 全部落盘成功后才 返回 ack。但是如果在 follower 同步完成后， broker 发送 ack 之前， leader 发生故障，那么会 造成数据重复。 4） 故障处理细节 LEO：指的是每个副本最大的 offset； HW：指的是消费者能见到的最大的 offset， ISR 队列中最小的 LEO。 （1） follower 故障 follower 发生故障后会被临时踢出 ISR，待该 follower 恢复后， follower 会读取本地磁盘记录的上次的 HW，并将 log 文件高于 HW 的部分截取掉，从 HW 开始向 leader 进行同步。等该 follower 的 LEO 大于等于该 Partition 的 HW，即 follower 追上 leader 之后，就可以重新加入 ISR 了。 （2） leader 故障 leader 发生故障之后，会从 ISR 中选出一个新的 leader，之后，为保证多个副本之间的 数据一致性， 其余的 follower 会先将各自的 log 文件高于 HW 的部分截掉，然后从新的 leader同步数据。 注意： 这只能保证副本之间的数据一致性，并不能保证数据不丢失或者不重复。 3.2.3 Exactly Once 语义 将服务器的 ACK 级别设置为-1，可以保证 Producer 到 Server 之间不会丢失数据，即 At Least Once 语义。相对的，将服务器 ACK 级别设置为 0，可以保证生产者每条消息只会被 发送一次，即 At Most Once 语义。 At Least Once 可以保证数据不丢失，但是不能保证数据不重复；相对的， At Most Once 可以保证数据不重复，但是不能保证数据不丢失。 但是，对于一些非常重要的信息，比如说 交易数据，下游数据消费者要求数据既不重复也不丢失，即 Exactly Once 语义。 在 0.11 版 本以前的 Kafka，对此是无能为力的，只能保证数据不丢失，再在下游消费者对数据做全局 去重。对于多个下游应用的情况，每个都需要单独做全局去重，这就对性能造成了很大影响。 0.11 版本的 Kafka，引入了一项重大特性：幂等性。所谓的幂等性就是指 Producer 不论向 Server 发送多少次重复数据， Server 端都只会持久化一条。幂等性结合 At Least Once 语义，就构成了 Kafka 的 Exactly Once 语义。即： At Least Once + 幂等性 = Exactly Once 要启用幂等性，只需要将 Producer 的参数中 enable.idompotence 设置为 true 即可。 Kafka 的幂等性实现其实就是将原来下游需要做的去重放在了数据上游。开启幂等性的 Producer 在 初始化的时候会被分配一个 PID，发往同一 Partition 的消息会附带 Sequence Number。而 Broker 端会对&lt;PID, Partition, SeqNumber&gt;做缓存，当具有相同主键的消息提交时， Broker 只 会持久化一条。 但是 PID 重启就会变化，同时不同的 Partition 也具有不同主键，所以幂等性无法保证跨 分区跨会话的 Exactly Once。 3.3 Kafka 消费者 3.3.1 消费方式 consumer 采用 pull（拉） 模式从 broker 中读取数据。 push（推）模式很难适应消费速率不同的消费者，因为消息发送速率是由 broker 决定的。 它的目标是尽可能以最快速度传递消息，但是这样很容易造成 consumer 来不及处理消息， 典型的表现就是拒绝服务以及网络拥塞。而 pull 模式则可以根据 consumer 的消费能力以适 当的速率消费消息。 pull 模式不足之处是，如果 kafka 没有数据，消费者可能会陷入循环中， 一直返回空数 据。 针对这一点， Kafka 的消费者在消费数据时会传入一个时长参数 timeout，如果当前没有 数据可供消费， consumer 会等待一段时间之后再返回，这段时长即为 timeout。 3.3.2 分区分配策略 一个 consumer group 中有多个 consumer，一个 topic 有多个 partition，所以必然会涉及 到 partition 的分配问题，即确定那个 partition 由哪个 consumer 来消费。 Kafka 有两种分配策略，一是 RoundRobin，一是 Range。 当消费者个数改变时，会用到分区分配策略 1） RoundRobin RoundRobinAssignor策略的原理是将消费组内所有消费者以及消费者所订阅的所有topic的partition按照字典序排序，然后通过轮询方式逐个将分区以此分配给每个消费者。RoundRobinAssignor策略对应的partition.assignment.strategy参数值为：org.apache.kafka.clients.consumer.RoundRobinAssignor。 使用RoundRobin策略有两个前提条件必须满足： 同一个消费者组里面的所有消费者的num.streams（消费者消费线程数）必须相等； 每个消费者订阅的主题必须相同。 所以这里假设前面提到的2个消费者的num.streams = 2。RoundRobin策略的工作原理：将所有主题的分区组成 TopicAndPartition 列表，然后对 TopicAndPartition 列表按照 hashCode 进行排序,在我们的例子里面，按照 hashCode 排序完的topic-partitions组依次为T1-5, T1-3, T1-0, T1-8, T1-2, T1-1, T1-4, T1-7, T1-6, T1-9，我们的消费者线程排序为C1-0, C1-1, C2-0, C2-1，最后分区分配的结果为： C1-0 将消费 T1-5, T1-2, T1-6 分区； C1-1 将消费 T1-3, T1-1, T1-9 分区； C2-0 将消费 T1-0, T1-4 分区； C2-1 将消费 T1-8, T1-7 分区； 2） Range（默认策略） Range是对每个Topic而言的（即一个Topic一个Topic分），首先对同一个Topic里面的分区按照序号进行排序，并对消费者按照字母顺序进行排序。然后用Partitions分区的个数除以消费者线程的总数来决定每个消费者线程消费几个分区。如果除不尽，那么前面几个消费者线程将会多消费一个分区。 假设n=分区数/消费者数量，m=分区数%消费者数量，那么前m个消费者每个分配n+1个分区，后面的（消费者数量-m）个消费者每个分配n个分区。 假如有10个分区，3个消费者线程，把分区按照序号排列0，1，2，3，4，5，6，7，8，9；消费者线程为C1-0，C2-0，C2-1，那么用partition数除以消费者线程的总数来决定每个消费者线程消费几个partition，如果除不尽，前面几个消费者将会多消费一个分区。在我们的例子里面，我们有10个分区，3个消费者线程，10/3 = 3，而且除除不尽，那么消费者线程C1-0将会多消费一个分区，所以最后分区分配的结果看起来是这样的： C1-0：0，1，2，3 C2-0：4，5，6 C2-1：7，8，9 3.3.3 offset 的维护 由于 consumer 在消费过程中可能会出现断电宕机等故障， consumer 恢复后，需要从故 障前的位置的继续消费，所以 consumer 需要实时记录自己消费到了哪个 offset，以便故障恢 复后继续消费。 Kafka 0.9 版本之前， consumer 默认将 offset 保存在 Zookeeper 中，从 0.9 版本开始， consumer 默认将 offset 保存在 Kafka 一个内置的 topic 中，该 topic 为__consumer_offsets。 1）修改配置文件 consumer.properties exclude.internal.topics=false 2）读取 offset 0.11.0.0 之前版本: bin/kafka-console-consumer.sh --topic __consumer_offsets --zookeeper hadoop102:2181 --formatter &quot;kafka.coordinator.GroupMetadataManager\\$OffsetsMessageFormatter&quot; --consumer.config config/consumer.properties --from-beginning 0.11.0.0 之后版本(含): bin/kafka-console-consumer.sh --topic __consumer_offsets -- zookeeper hadoop102:2181 --formatter &quot;kafka.coordinator.group.GroupMetadataManager\\$OffsetsMessageForm atter&quot; --consumer.config config/consumer.properties --frombeginning 3.3.4 消费者组案例 1） 需求：测试同一个消费者组中的消费者， 同一时刻只能有一个消费者消费。 2） 案例实操 （1）在 hadoop102、 hadoop103 上修改/opt/module/kafka/config/consumer.properties 配置 文件中的 group.id 属性为任意组名。 [atguigu@hadoop103 config]$ vi consumer.properties group.id=atguigu （2）在 hadoop102、 hadoop103 上分别启动消费者 [atguigu@hadoop102 kafka]$ bin/kafka-console-consumer.sh \\ --zookeeper hadoop102:2181 --topic first --consumer.config config/consumer.properties [atguigu@hadoop103 kafka]$ bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic first --consumer.config config/consumer.properties （3）在 hadoop104 上启动生产者 [atguigu@hadoop104 kafka]$ bin/kafka-console-producer.sh \\ --broker-list hadoop102:9092 --topic first &gt;hello world （4）查看 hadoop102 和 hadoop103 的接收者。 同一时刻消费组内只有一个消费者接收到消息。 3.4 Kafka 高效读写数据 1）顺序写磁盘 Kafka 的 producer 生产数据，要写入到 log 文件中，写的过程是一直追加到文件末端， 为顺序写。 官网有数据表明，同样的磁盘，顺序写能到 600M/s，而随机写只有 100K/s。这 与磁盘的机械机构有关，顺序写之所以快，是因为其省去了大量磁头寻址的时间。 2）零复制技术 3）分布式 3.5 Zookeeper 在 Kafka 中的作用 Kafka 集群中有一个 broker 会被选举为 Controller，负责管理集群 broker 的上下线，所有 topic 的分区副本分配和 leader 选举等工作。Controller 的管理工作都是依赖于 Zookeeper 的。 以下为 partition 的 leader 选举过程： 3.6 Kafka 事务 Kafka 从 0.11 版本开始引入了事务支持。事务可以保证 Kafka 在 Exactly Once 语义的基 础上，生产和消费可以跨分区和会话，要么全部成功，要么全部失败。 3.6.1 Producer 事务 为了实现跨分区跨会话的事务，需要引入一个全局唯一的 Transaction ID，并将 Producer 获得的PID 和Transaction ID 绑定。这样当Producer 重启后就可以通过正在进行的 Transaction ID 获得原来的 PID。 为了管理 Transaction， Kafka 引入了一个新的组件 Transaction Coordinator。 Producer 就是通过和 Transaction Coordinator 交互获得 Transaction ID 对应的任务状态。 Transaction Coordinator 还负责将事务所有写入 Kafka 的一个内部 Topic，这样即使整个服务重启，由于事务状态得到保存，进行中的事务状态可以得到恢复，从而继续进行。 3.6.2 Consumer 事务 上述事务机制主要是从 Producer 方面考虑，对于 Consumer 而言，事务的保证就会相对 较弱，尤其时无法保证 Commit 的信息被精确消费。这是由于 Consumer 可以通过 offset 访 问任意信息，而且不同的 Segment File 生命周期不同，同一事务的消息可能会出现重启后被 删除的情况。 ","link":"https://tianxiawuhao.github.io/KM-0yw6ab/"},{"title":"kafka常用命令","content":"管理 ## 创建topic（4个分区，2个副本） bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 2 --partitions 4 --topic test ### kafka版本 &gt;= 2.2 bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic test ## 分区扩容 ### kafka版本 &lt; 2.2 bin/kafka-topics.sh --zookeeper localhost:2181 --alter --topic topic1 --partitions 2 ### kafka版本 &gt;= 2.2 bin/kafka-topics.sh --bootstrap-server broker_host:port --alter --topic topic1 --partitions 2 ## 删除topic bin/kafka-topics.sh --zookeeper localhost:2181 --delete --topic test 查询 ## 查询集群描述 bin/kafka-topics.sh --describe --zookeeper 127.0.0.1:2181 ## 查询集群描述（新） bin/kafka-topics.sh --bootstrap-server localhost:9092 --topic foo --describe ## topic列表查询 bin/kafka-topics.sh --zookeeper 127.0.0.1:2181 --list ## topic列表查询（支持0.9版本+） bin/kafka-topics.sh --list --bootstrap-server localhost:9092 ## 消费者列表查询（存储在zk中的） bin/kafka-consumer-groups.sh --zookeeper localhost:2181 --list ## 消费者列表查询（支持0.9版本+） bin/kafka-consumer-groups.sh --new-consumer --bootstrap-server localhost:9092 --list ## 消费者列表查询（支持0.10版本+） bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --list ## 显示某个消费组的消费详情（仅支持offset存储在zookeeper上的） bin/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker --zookeeper localhost:2181 --group test ## 显示某个消费组的消费详情（0.9版本 - 0.10.1.0 之前） bin/kafka-consumer-groups.sh --new-consumer --bootstrap-server localhost:9092 --describe --group test-consumer-group ## 显示某个消费组的消费详情（0.10.1.0版本+） bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group my-group 发送和消费 ## 生产者 bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test ## 消费者（已失效） bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic test ## 生产者（支持0.9版本+） bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test --producer.config config/producer.properties ## 消费者（支持0.9版本+，已失效） bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --new-consumer --from-beginning --consumer.config config/consumer.properties ## 消费者（最新） bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning --consumer.config config/consumer.properties ## kafka-verifiable-consumer.sh（消费者事件，例如：offset提交等） bin/kafka-verifiable-consumer.sh --broker-list localhost:9092 --topic test --group-id groupName ## 高级点的用法 bin/kafka-simple-consumer-shell.sh --brist localhost:9092 --topic test --partition 0 --offset 1234 --max-messages 10 切换leader ## kafka版本 &lt;= 2.4 bin/kafka-preferred-replica-election.sh --zookeeper zk_host:port/chroot ## kafka新版本 bin/kafka-preferred-replica-election.sh --bootstrap-server broker_host:port kafka自带压测命令 bin/kafka-producer-perf-test.sh --topic test --num-records 100 --record-size 1 --throughput 100 --producer-props bootstrap.servers=localhost:9092 kafka持续发送消息 持续发送消息到指定的topic中，且每条发送的消息都会有响应信息： kafka-verifiable-producer.sh --broker-list $(hostname -i):9092 --topic test --max-messages 100000 zookeeper-shell.sh 如果kafka集群的zk配置了chroot路径，那么需要加上/path。 bin/zookeeper-shell.sh localhost:2181[/path] ls /brokers/ids get /brokers/ids/0 迁移分区 创建规则json cat &gt; increase-replication-factor.json &lt;&lt;EOF {&quot;version&quot;:1, &quot;partitions&quot;:[ {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:3,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:4,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:5,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:6,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:7,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:8,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:9,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:10,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:11,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:12,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:13,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:14,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:15,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:16,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:17,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:18,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:19,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:20,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:21,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:22,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:23,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:24,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:25,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:26,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:27,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:28,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:29,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:30,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:31,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:32,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:33,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:34,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:35,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:36,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:37,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:38,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:39,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:40,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:41,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:42,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:43,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:44,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:45,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:46,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:47,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:48,&quot;replicas&quot;:[0,1]}, {&quot;topic&quot;:&quot;__consumer_offsets&quot;,&quot;partition&quot;:49,&quot;replicas&quot;:[0,1]}] } EOF 执行 bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file increase-replication-factor.json --execute 验证 bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file increase-replication-factor.json --verify 删除消费者组 查询消费者组列表： kafka-consumer-groups.sh --bootstrap-server 172.31.1.245:9092 --list 查询消费者组明细： kafka-consumer-groups.sh --bootstrap-server {Kafka instance connection address} --describe --group {consumer group name} 删除消费者组： kafka-consumer-groups.sh --bootstrap-server {Kafka instance connection address} --delete --group {consumer group name} ","link":"https://tianxiawuhao.github.io/cvzf4T4Wn/"},{"title":"zookeeper选举过程","content":"初始化选举 运行期间选举 ","link":"https://tianxiawuhao.github.io/Rz0qPOGXr/"},{"title":"ReentrantLock源码详解","content":"ReentrantLock重入锁，是实现Lock接口的一个类，也是在实际编程中使用频率很高的一个锁，支持重入性，表示能够对共享资源能够重复加锁，即当前线程获取该锁再次获取不会被阻塞。 ReentrantLock还支持公平锁和非公平锁两种方式。那么，要想完完全全的弄懂ReentrantLock的话，主要也就是ReentrantLock同步语义的学习： 重入性的实现原理； 公平锁和非公平锁 ReentrantLock源码解析 加锁 //使用案例 class Bank{ /** * volatile实现 */ private int count=0; /** * 使用可重入锁 */ private Lock lock=new ReentrantLock(); public void getCount(){ System.out.println(&quot;账户余额为：&quot;+count); } /** * 同步方法实现存钱 * @param money */ public void save(int money){ lock.lock(); try { count+=money; System.out.println(System.currentTimeMillis()+&quot;存进：&quot;+money); } catch (Exception e) { // TODO Auto-generated catch block e.printStackTrace(); }finally { lock.unlock();//释放锁 } } /** * 同步代码块实现取钱 * @param money */ public void remove(int money){ if (count-money&lt;0) { System.err.println(&quot;余额不足。&quot;); return; } lock.lock(); try { count-=money; System.err.println(System.currentTimeMillis()+&quot;取出：&quot;+money); } catch (Exception e) { // TODO Auto-generated catch block e.printStackTrace(); }finally { lock.unlock(); } } } /** * Creates an instance of {@code ReentrantLock}. * This is equivalent to using {@code ReentrantLock(false)}. */ public ReentrantLock() { sync = new NonfairSync(); } /** * Creates an instance of {@code ReentrantLock} with the * given fairness policy. * * @param fair {@code true} if this lock should use a fair ordering policy */ public ReentrantLock(boolean fair) { sync = fair ? new FairSync() : new NonfairSync(); } 初始化默认使用非公平锁 公平锁和非公平锁继承AbstractQueuedSynchronizer接口（抽象的队列式的同步器，AQS定义了一套多线程访问共享资源的同步器框架） AQS框架 NonfairSync的类继承关系 FairSync的类继承关系 下面以非公平锁为例 /** * Performs lock. Try immediate barge, backing up to normal * acquire on failure. */ //加锁流程真正意义上的入口 final void lock() { //以cas方式尝试将AQS中的state从0更新为1 if (compareAndSetState(0, 1)) setExclusiveOwnerThread(Thread.currentThread());//获取锁成功则将当前线程标记为持有锁的线程,然后直接返回 else acquire(1);//获取锁失败则执行该方法 } 加锁时调用lock方法，首先判断AQS中的sate参数是否被标记,尝试以cas方式尝试将AQS中的state从0更新为1，成功将当前线程赋予AQS 失败则调用AQS类的acquire(1)方法 //非公平模式下尝试获取锁的方法 protected final boolean tryAcquire(int acquires) { return nonfairTryAcquire(acquires); } /** * Performs non-fair tryLock. tryAcquire is implemented in * subclasses, but both need nonfair try for trylock method. */ final boolean nonfairTryAcquire(int acquires) { final Thread current = Thread.currentThread();//获取当前线程实例 int c = getState();//获取state变量的值,即当前锁被重入的次数 if (c == 0) { //state为0,说明当前锁未被任何线程持有 if (compareAndSetState(0, acquires)) { //以cas方式获取锁 setExclusiveOwnerThread(current); //将当前线程标记为持有锁的线程 return true;//获取锁成功,非重入 } } else if (current == getExclusiveOwnerThread()) { //当前线程就是持有锁的线程,说明该锁被重入了 int nextc = c + acquires;//计算state变量要更新的值 if (nextc &lt; 0) // overflow throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc);//非同步方式更新state值 return true; //获取锁成功,重入 } return false; //走到这里说明尝试获取锁失败 } 这是非公平模式下获取锁的通用方法。它囊括了当前线程在尝试获取锁时的所有可能情况： 1.当前锁未被任何线程持有(state=0),则以cas方式获取锁,若获取成功则设置exclusiveOwnerThread为当前线程,然后返回成功的结果；若cas失败,说明在得到state=0和cas获取锁之间有其他线程已经获取了锁,返回失败结果。 2.若锁已经被当前线程获取(state&gt;0,exclusiveOwnerThread为当前线程),则将锁的重入次数加1(state+1),然后返回成功结果。因为该线程之前已经获得了锁,所以这个累加操作不用同步。 3.若当前锁已经被其他线程持有(state&gt;0,exclusiveOwnerThread不为当前线程),则直接返回失败结果 因为我们用state来统计锁被线程重入的次数,所以当前线程尝试获取锁的操作是否成功可以简化为:state值是否成功累加1,是则尝试获取锁成功,否则尝试获取锁失败。 其实这里还可以思考一个问题:nonfairTryAcquire已经实现了一个囊括所有可能情况的尝试获取锁的方式,为何在刚进入lock方法时还要通过compareAndSetState(0, 1)去获取锁,毕竟后者只有在锁未被任何线程持有时才能执行成功,我们完全可以把compareAndSetState(0, 1)去掉,对最后的结果不会有任何影响。这种在进行通用逻辑处理之前针对某些特殊情况提前进行处理的方式在后面还会看到,一个直观的想法就是它能提升性能，而代价是牺牲一定的代码简洁性。 退回到上层的acquire方法, public final void acquire(int arg) { if (!tryAcquire(arg) &amp;&amp; //当前线程尝试获取锁,若获取成功返回true,否则false acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) //只有当前线程获取锁失败才会执行者这部分代码 selfInterrupt(); } tryAcquire(arg)返回成功,则说明当前线程成功获取了锁(第一次获取或者重入),由取反和&amp;&amp;可知,整个流程到这结束，只有当前线程获取锁失败才会执行后面的判断。先来看addWaiter(Node.EXCLUSIVE)部分,这部分代码描述了当线程获取锁失败时如何安全的加入同步等待队列。 这部分逻辑在addWaiter()方法中 /** * Creates and enqueues node for current thread and given mode. * * @param mode Node.EXCLUSIVE for exclusive, Node.SHARED for shared * @return the new node */ private Node addWaiter(Node mode) { Node node = new Node(Thread.currentThread(), mode);//首先创建一个新节点,并将当前线程实例封装在内部,mode这里为null // Try the fast path of enq; backup to full enq on failure Node pred = tail; if (pred != null) { node.prev = pred; if (compareAndSetTail(pred, node)) { pred.next = node; return node; } } enq(node);//入队的逻辑这里都有 return node; } 首先创建了一个新节点,并将当前线程实例封装在其内部,之后我们直接看enq(node)方法就可以了,中间这部分逻辑在enq(node)中都有,之所以加上这部分“重复代码”和尝试获取锁时的“重复代码”一样,对某些特殊情况 进行提前处理,牺牲一定的代码可读性换取性能提升。 /** * Inserts node into queue, initializing if necessary. See picture above. * @param node the node to insert * @return node's predecessor */ private Node enq(final Node node) { for (;;) { Node t = tail;//t指向当前队列的最后一个节点,队列为空则为null if (t == null) { // Must initialize //队列为空 if (compareAndSetHead(new Node())) //构造新结点,CAS方式设置为队列首元素,当head==null时更新成功 tail = head;//尾指针指向首结点 } else { //队列不为空 node.prev = t; if (compareAndSetTail(t, node)) { //CAS将尾指针指向当前结点,当t(原来的尾指针)==tail(当前真实的尾指针)时执行成功 t.next = node; //原尾结点的next指针指向当前结点 return t; } } } } 这里有两个CAS操作: compareAndSetHead(new Node()),CAS方式更新head指针,仅当原值为null时更新成功 /** * CAS head field. Used only by enq. */ private final boolean compareAndSetHead(Node update) { return unsafe.compareAndSwapObject(this, headOffset, null, update); } compareAndSetTail(t, node),CAS方式更新tial指针,仅当原值为t时更新成功 /** * CAS tail field. Used only by enq. */ private final boolean compareAndSetTail(Node expect, Node update) { return unsafe.compareAndSwapObject(this, tailOffset, expect, update); } 外层的for循环保证了所有获取锁失败的线程经过失败重试后最后都能加入同步队列。因为AQS的同步队列是不带哨兵结点的,故当队列为空时要进行特殊处理,这部分在if分句中。注意当前线程所在的结点不能直接插入 空队列,因为阻塞的线程是由前驱结点进行唤醒的。故先要插入一个结点作为队列首元素,当锁释放时由它来唤醒后面被阻塞的线程,从逻辑上这个队列首元素也可以表示当前正获取锁的线程,虽然并不一定真实持有其线程实例。 首先通过new Node()创建一个空结点，然后以CAS方式让头指针指向该结点(该结点并非当前线程所在的结点),若该操作成功,则将尾指针也指向该结点。这部分的操作流程可以用下图表示 当队列不为空,则执行通用的入队逻辑,这部分在else分句中 else { //队列不为空 node.prev = t; if (compareAndSetTail(t, node)) { //CAS将尾指针指向当前结点,当t(原来的尾指针)==tail(当前真实的尾指针)时执行成功 t.next = node; //原尾结点的next指针指向当前结点 return t; } 首先当前线程所在的结点的前向指针pre指向当前线程认为的尾结点,源码中用t表示。然后以CAS的方式将尾指针指向当前结点,该操作仅当tail=t,即尾指针在进行CAS前未改变时成功。若CAS执行成功,则将原尾结点的后向指针next指向新的尾结点。整个过程如下图所示 整个入队的过程并不复杂,是典型的CAS加失败重试的乐观锁策略。其中只有更新头指针和更新尾指针这两步进行了CAS同步,可以预见高并发场景下性能是非常好的。但是本着质疑精神我们不禁会思考下这么做真的线程安全吗？ 1.队列为空的情况: 因为队列为空,故head=tail=null,假设线程执行2成功,则在其执行3之前,因为tail=null,其他进入该方法的线程因为head不为null将在2处不停的失败,所以3即使没有同步也不会有线程安全问题。 2.队列不为空的情况: 假设线程执行5成功,则此时4的操作必然也是正确的(当前结点的prev指针确实指向了队列尾结点,换句话说tail指针没有改变,如若不然5必然执行失败),又因为4执行成功,当前节点在队列中的次序已经确定了,所以6何时执行对线程安全不会有任何影响,比如下面这种情况 为了确保真的理解了它,可以思考这个问题:把enq方法图中的4放到5之后,整个入队的过程还线程安全吗？ 到这为止,获取锁失败的线程加入同步队列的逻辑就结束了。但是线程加入同步队列后会做什么我们并不清楚,这部分在acquireQueued方法中 acquireQueued方法的源码 /** * Acquires in exclusive uninterruptible mode for thread already in * queue. Used by condition wait methods as well as acquire. * * @param node the node * @param arg the acquire argument * @return {@code true} if interrupted while waiting */ final boolean acquireQueued(final Node node, int arg) { boolean failed = true; try { boolean interrupted = false; //死循环,正常情况下线程只有获得锁才能跳出循环 for (;;) { final Node p = node.predecessor();//获得当前线程所在结点的前驱结点 //第一个if分句 if (p == head &amp;&amp; tryAcquire(arg)) { setHead(node); //将当前结点设置为队列头结点 p.next = null; // help GC failed = false; return interrupted;//正常情况下死循环唯一的出口 } //第二个if分句 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; //判断是否要阻塞当前线程 parkAndCheckInterrupt()) //阻塞当前线程 interrupted = true; } } finally { if (failed) cancelAcquire(node); } } 这段代码主要的内容都在for循环中,这是一个死循环,主要有两个if分句构成。第一个if分句中,当前线程首先会判断前驱结点是否是头结点,如果是则尝试获取锁,获取锁成功则会设置当前结点为头结点(更新头指针)。为什么必须前驱结点为头结点才尝试去获取锁？因为头结点表示当前正占有锁的线程,正常情况下该线程释放锁后会通知后面结点中阻塞的线程,阻塞线程被唤醒后去获取锁,这是我们希望看到的。然而还有一种情况,就是前驱结点取消了等待,此时当前线程也会被唤醒,这时候就不应该去获取锁,而是往前回溯一直找到一个没有取消等待的结点,然后将自身连接在它后面。一旦我们成功获取了锁并成功将自身设置为头结点,就会跳出for循环。否则就会执行第二个if分句:确保前驱结点的状态为SIGNAL,然后阻塞当前线程。 先来看shouldParkAfterFailedAcquire(p, node)，从方法名上我们可以大概猜出这是判断是否要阻塞当前线程的,方法内容如下 /** * Checks and updates status for a node that failed to acquire. * Returns true if thread should block. This is the main signal * control in all acquire loops. Requires that pred == node.prev. * * @param pred node's predecessor holding status * @param node the node * @return {@code true} if thread should block */ private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) { int ws = pred.waitStatus; if (ws == Node.SIGNAL) //状态为SIGNAL /* * This node has already set status asking a release * to signal it, so it can safely park. */ return true; if (ws &gt; 0) { //状态为CANCELLED, /* * Predecessor was cancelled. Skip over predecessors and * indicate retry. */ do { node.prev = pred = pred.prev; } while (pred.waitStatus &gt; 0); pred.next = node; } else { //状态为初始化状态(ReentrentLock语境下) /* * waitStatus must be 0 or PROPAGATE. Indicate that we * need a signal, but don't park yet. Caller will need to * retry to make sure it cannot acquire before parking. */ compareAndSetWaitStatus(pred, ws, Node.SIGNAL); } return false; } 可以看到针对前驱结点pred的状态会进行不同的处理 1.pred状态为SIGNAL,则返回true,表示要阻塞当前线程。 2.pred状态为CANCELLED,则一直往队列头部回溯直到找到一个状态不为CANCELLED的结点,将当前节点node挂在这个结点的后面。 3.pred的状态为初始化状态,此时通过compareAndSetWaitStatus(pred, ws, Node.SIGNAL)方法将pred的状态改为SIGNAL。 其实这个方法的含义很简单,就是确保当前结点的前驱结点的状态为SIGNAL,SIGNAL意味着线程释放锁后会唤醒后面阻塞的线程。毕竟,只有确保能够被唤醒，当前线程才能放心的阻塞。 但是要注意只有在前驱结点已经是SIGNAL状态后才会执行后面的方法立即阻塞,对应上面的第一种情况。其他两种情况则因为返回false而重新执行一遍 for循环。这种延迟阻塞其实也是一种高并发场景下的优化,试想我如果在重新执行循环的时候成功获取了锁,是不是线程阻塞唤醒的开销就省了呢？ 最后我们来看看阻塞线程的方法parkAndCheckInterrupt shouldParkAfterFailedAcquire返回true表示应该阻塞当前线程,则会执行parkAndCheckInterrupt方法,这个方法比较简单,底层调用了LockSupport来阻塞当前线程,源码如下: /** * Convenience method to park and then check if interrupted * * @return {@code true} if interrupted */ private final boolean parkAndCheckInterrupt() { LockSupport.park(this); return Thread.interrupted(); } 该方法内部通过调用LockSupport的park方法来阻塞当前线程 LockSupport就是通过控制变量_counter来对线程阻塞唤醒进行控制的。原理有点类似于信号量机制。 当调用park()方法时，会将_counter置为0，同时判断前值，小于1说明前面被unpark过,则直接退出，否则将使该线程阻塞。 当调用unpark()方法时，会将_counter置为1，同时判断前值，小于1会进行线程唤醒，否则直接退出。 形象的理解，线程阻塞需要消耗凭证(permit)，这个凭证最多只有1个。当调用park方法时，如果有凭证，则会直接消耗掉这个凭证然后正常退出；但是如果没有凭证，就必须阻塞等待凭证可用；而unpark则相反，它会增加一个凭证，但凭证最多只能有1个。 为什么可以先唤醒线程后阻塞线程？ 因为unpark获得了一个凭证,之后调用park因为有凭证消费，故不会阻塞。 为什么唤醒两次后阻塞两次会阻塞线程。 因为凭证的数量最多为1，连续调用两次unpark和调用一次unpark效果一样，只会增加一个凭证；而调用两次park却需要消费两个凭证。 下面通过一张流程图来说明线程从加入同步队列到成功获取锁的过程 概括的说,线程在同步队列中会尝试获取锁,失败则被阻塞,被唤醒后会不停的重复这个过程,直到线程真正持有了锁,并将自身结点置于队列头部。 ReentrantLock非公平模式下的加锁流程如下 解锁 解锁源码如下： public void unlock() { sync.release(1); } public final boolean release(int arg) { if (tryRelease(arg)) { //释放锁(state-1),若释放后锁可被其他线程获取(state=0),返回true Node h = head; //当前队列不为空且头结点状态不为初始化状态(0) if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h); //唤醒同步队列中被阻塞的线程 return true; } return false; } 正确找到sync的实现类,找到真正的入口方法,主要内容都在一个if语句中,先看下判断条件tryRelease方法 protected final boolean tryRelease(int releases) { int c = getState() - releases; //计算待更新的state值 if (Thread.currentThread() != getExclusiveOwnerThread()) throw new IllegalMonitorStateException(); boolean free = false; if (c == 0) { //待更新的state值为0,说明持有锁的线程未重入,一旦释放锁其他线程将能获取 free = true; setExclusiveOwnerThread(null);//清除锁的持有线程标记 } setState(c);//更新state值 return free; } tryRelease其实只是将线程持有锁的次数减1,即将state值减1,若减少后线程将完全释放锁(state值为0),则该方法将返回true,否则返回false。由于执行该方法的线程必然持有锁,故该方法不需要任何同步操作。 若当前线程已经完全释放锁,即锁可被其他线程使用,则还应该唤醒后续等待线程。不过在此之前需要进行两个条件的判断： h!=null是为了防止队列为空,即没有任何线程处于等待队列中,那么也就不需要进行唤醒的操作 h.waitStatus != 0是为了防止队列中虽有线程,但该线程还未阻塞,由前面的分析知,线程在阻塞自己前必须设置前驱结点的状态为SIGNAL,否则它不会阻塞自己。 接下来就是唤醒线程的操作,unparkSuccessor(h)源码如下 private void unparkSuccessor(Node node) { /* * If status is negative (i.e., possibly needing signal) try * to clear in anticipation of signalling. It is OK if this * fails or if status is changed by waiting thread. */ int ws = node.waitStatus; if (ws &lt; 0) compareAndSetWaitStatus(node, ws, 0); /* * Thread to unpark is held in successor, which is normally * just the next node. But if cancelled or apparently null, * traverse backwards from tail to find the actual * non-cancelled successor. */ Node s = node.next; if (s == null || s.waitStatus &gt; 0) { s = null; for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev) if (t.waitStatus &lt;= 0) s = t; } if (s != null) LockSupport.unpark(s.thread); } 一般情况下只要唤醒后继结点的线程就行了,但是后继结点可能已经取消等待,所以从队列尾部往前回溯,找到离头结点最近的正常结点,并唤醒其线程。 解锁流程源码总结 公平锁相比非公平锁的不同 公平锁模式下,对锁的获取有严格的条件限制。在同步队列有线程等待的情况下,所有线程在获取锁前必须先加入同步队列。队列中的线程按加入队列的先后次序获得锁。 从公平锁加锁的入口开始, 对比非公平锁,少了非重入式获取锁的方法,这是第一个不同点 接着看获取锁的通用方法tryAcquire(),该方法在线程未进入队列,加入队列阻塞前和阻塞后被唤醒时都会执行。 在真正CAS获取锁之前加了判断,内容如下 public final boolean hasQueuedPredecessors() { // The correctness of this depends on head being initialized // before tail and on head.next being accurate if the current // thread is first in queue. Node t = tail; // Read fields in reverse initialization order Node h = head; Node s; return h != t &amp;&amp; ((s = h.next) == null || s.thread != Thread.currentThread()); } 从方法名我们就可知道这是判断队列中是否有优先级更高的等待线程,队列中哪个线程优先级最高？由于头结点是当前获取锁的线程,队列中的第二个结点代表的线程优先级最高。 那么我们只要判断队列中第二个结点是否存在以及这个结点是否代表当前线程就行了。这里分了两种情况进行探讨: 第二个结点已经完全插入,但是这个结点是否就是当前线程所在结点还未知,所以通过s.thread != Thread.currentThread()进行判断,如果为true,说明第二个结点代表其他线程。 第二个结点并未完全插入,我们知道结点入队一共分三步： 1.待插入结点的pre指针指向原尾结点 2.CAS更新尾指针 3.原尾结点的next指针指向新插入结点 所以(s = h.next) == null 就是用来判断2刚执行成功但还未执行3这种情况的。这种情况第二个结点必然属于其他线程。 以上两种情况都会使该方法返回true,即当前有优先级更高的线程在队列中等待,那么当前线程将不会执行CAS操作去获取锁,保证了线程获取锁的顺序与加入同步队列的顺序一致，很好的保证了公平性,但也增加了获取锁的成本。 一些疑问的解答 为什么基于FIFO的同步队列可以实现非公平锁？ 由FIFO队列的特性知,先加入同步队列等待的线程会比后加入的线程更靠近队列的头部,那么它将比后者更早的被唤醒,它也就能更早的得到锁。从这个意义上,对于在同步队列中等待的线程而言,它们获得锁的顺序和加入同步队列的顺序一致，这显然是一种公平模式。然而,线程并非只有在加入队列后才有机会获得锁,哪怕同步队列中已有线程在等待,非公平锁的不公平之处就在于此。回看下非公平锁的加锁流程,线程在进入同步队列等待之前有两次抢占锁的机会: 第一次是非重入式的获取锁,只有在当前锁未被任何线程占有(包括自身)时才能成功; 第二次是在进入同步队列前,包含所有情况的获取锁的方式。 只有这两次获取锁都失败后,线程才会构造结点并加入同步队列等待。而线程释放锁时是先释放锁(修改state值),然后才唤醒后继结点的线程的。试想下这种情况,线程A已经释放锁,但还没来得及唤醒后继线程C,而这时另一个线程B刚好尝试获取锁,此时锁恰好不被任何线程持有,它将成功获取锁而不用加入队列等待。线程C被唤醒尝试获取锁,而此时锁已经被线程B抢占,故而其获取失败并继续在队列中等待。整个过程如下图所示 如果以线程第一次尝试获取锁到最后成功获取锁的次序来看,非公平锁确实很不公平。因为在队列中等待很久的线程相比还未进入队列等待的线程并没有优先权,甚至竞争也处于劣势:在队列中的线程要等待其他线程唤醒,在获取锁之前还要检查前驱结点是否为头结点。在锁竞争激烈的情况下,在队列中等待的线程可能迟迟竞争不到锁。这也就非公平在高并发情况下会出现的饥饿问题。那我们再开发中为什么大多使用会导致饥饿的非公平锁？很简单,因为它性能好啊。 为什么非公平锁性能好 非公平锁对锁的竞争是抢占式的(队列中线程除外),线程在进入等待队列前可以进行两次尝试,这大大增加了获取锁的机会。这种好处体现在两个方面: 1.线程不必加入等待队列就可以获得锁,不仅免去了构造结点并加入队列的繁琐操作,同时也节省了线程阻塞唤醒的开销,线程阻塞和唤醒涉及到线程上下文的切换和操作系统的系统调用,是非常耗时的。在高并发情况下,如果线程持有锁的时间非常短,短到线程入队阻塞的过程超过线程持有并释放锁的时间开销,那么这种抢占式特性对并发性能的提升会更加明显。 2.减少CAS竞争。如果线程必须要加入阻塞队列才能获取锁,那入队时CAS竞争将变得异常激烈,CAS操作虽然不会导致失败线程挂起,但不断失败重试导致的对CPU的浪费也不能忽视。除此之外,加锁流程中至少有两处通过将某些特殊情况提前来减少CAS操作的竞争,增加并发情况下的性能。一处就是获取锁时将非重入的情况提前,如下图所示 另一处就是入队的操作,将同步队列非空的情况提前处理 这两部分的代码在之后的通用逻辑处理中都有,很显然属于重复代码,但因为避免了执行无意义的流程代码,比如for循环,获取同步状态等,高并发场景下也能减少CAS竞争失败的可能。 读写锁ReentrantReadWriteLock 首先明确一下，不是说 ReentrantLock 不好，只是 ReentrantLock 某些时候有局限。如果使用 ReentrantLock，可能本身是为了防止线程 A 在写数据、线程 B 在读数据造成的数据不一致，但这样，如果线程 C 在读数据、线程 D 也在读数据，读数据是不会改变数据的，没有必要加锁，但是还是加锁了，降低了程序的性能。因为这个，才诞生了读写锁 ReadWriteLock。 ReadWriteLock 是一个读写锁接口，读写锁是用来提升并发程序性能的锁分离技术，ReentrantReadWriteLock 是 ReadWriteLock 接口的一个具体实现，实现了读写的分离，读锁是共享的，写锁是独占的，读和读之间不会互斥，读和写、写和读、写和写之间才会互斥，提升了读写的性能。 而读写锁有以下三个重要的特性： （1）公平选择性：支持非公平（默认）和公平的锁获取方式，吞吐量还是非公平优于公平。 （2）重进入：读锁和写锁都支持线程重进入。 ","link":"https://tianxiawuhao.github.io/CxCcIUkNX/"},{"title":"netty零拷贝","content":"零拷贝的应用程序要求内核（kernel）直接将数据从磁盘文件拷贝到套接字（Socket），而无须通过应用程序。零拷贝不仅提高了应用程序的性能，而且减少了内核和用户模式见上下文切换。 数据传输：传统方法 从文件中读取数据，并将数据传输到网络上的另一个程序的场景：从下图可以看出，拷贝的操作需要4次用户模式和内核模式之间的上下文切换，而且在操作完成前数据被复制了4次。(DMA：直接内存拷贝) 从磁盘中copy放到一个内存buf中，然后将buf通过socket传输给用户,下面是伪代码实现： read(file, tmp_buf, len); write(socket, tmp_buf, len); 从图中可以看出文件经历了4次copy过程： 1.首先，调用read方法，文件从user模式拷贝到了kernel模式；（用户模式-&gt;内核模式的上下文切换，在内部发送sys_read() 从文件中读取数据，存储到一个内核地址空间缓存区中） 2.之后CPU控制将kernel模式数据拷贝到user模式下；（内核模式-&gt; 用户模式的上下文切换，read()调用返回，数据被存储到用户地址空间的缓存区中） 3.调用write时候，先将user模式下的内容copy到kernel模式下的socket的buffer中（用户模式-&gt;内核模式，数据再次被放置在内核缓存区中，send（）套接字调用） 4.最后将kernel模式下的socket buffer的数据copy到网卡设备中；（send套接字调用返回） 从图中看2，3两次copy是多余的，数据从kernel模式到user模式走了一圈，浪费了2次copy。 数据传输：mmap 优化 mmap 通过内存映射，将文件映射到内核缓冲区，同时，用户空间可以共享内核空间的数据。这样，在进行网络传输时，就可以减少内核空间到用户空间的拷贝次数。如下图： 如上图，user buffer 和 kernel buffer 共享 index.html。如果你想把硬盘的 index.html 传输到网络中，再也不用拷贝到用户空间，再从用户空间拷贝到 Socket 缓冲区。 现在，你只需要从内核缓冲区拷贝到 Socket 缓冲区即可，这将减少一次内存拷贝（从 4 次变成了 3 次），但不减少上下文切换次数。 数据传输：零拷贝方法 从传统的场景看，会注意到上图，第2次和第3次拷贝根本就是多余的。应用程序只是起到缓存数据被将传回到套接字的作用而已，别无他用。 应用程序使用zero-copy来请求kernel直接把disk的数据传输到socket中，而不是通过应用程序传输。zero-copy大大提高了应用程序的性能，并且减少了kernel和user模式的上下文切换。 数据可以直接从read buffer 读缓存区传输到套接字缓冲区，也就是省去了将操作系统的read buffer 拷贝到程序的buffer，以及从程序buffer拷贝到socket buffer的步骤，直接将read buffer拷贝到socket buffer。JDK NIO中的的transferTo() 方法就能够让您实现这个操作，这个实现依赖于操作系统底层的sendFile（）实现的： public void transferTo(long position, long count, WritableByteChannel target); 底层调用sendFile方法： #include &lt;sys/socket.h&gt; ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count); 使用了zero-copy技术后，整个过程如下： 1.transferTo()方法使得文件的内容直接copy到了一个read buffer（kernel buffer）中 2.然后数据（kernel buffer）copy到socket buffer中 3.最后将socket buffer中的数据copy到网卡设备（protocol engine）中传输； 这个显然是一个伟大的进步：这里上下文切换从4次减少到2次，同时把数据copy的次数从4次降低到3次； 但是这是zero-copy么，答案是否定的； linux 2.1 内核开始引入了sendfile函数，用于将文件通过socket传输。 sendfile(socket, file, len); 该函数通过一次调用完成了文件的传输。 该函数通过一次系统调用完成了文件的传输，减少了原来read/write方式的模式切换。此外更是减少了数据的copy，sendfile的详细过程如图： 通过sendfile传送文件只需要一次系统调用，当调用sendfile时： 1.首先通过DMA将数据从磁盘读取到kernel buffer中 2.然后将kernel buffer数据拷贝到socket buffer中 3.最后将socket buffer中的数据copy到网卡设备中（protocol buffer）发送； sendfile与read/write模式相比，少了一次copy。但是从上述过程中发现从kernel buffer中将数据copy到socket buffer是没有必要的； Linux2.4 内核对sendfile做了改进，如图： 改进后的处理过程如下： 将文件拷贝到kernel buffer中；(DMA引擎将文件内容copy到内核缓存区) 向socket buffer中追加当前要发生的数据在kernel buffer中的位置和偏移量； 根据socket buffer中的位置和偏移量直接将kernel buffer的数据copy到网卡设备（protocol engine）中； 从图中看到，linux 2.1内核中的 “数据被copy到socket buffer”的动作，在Linux2.4 内核做了优化，取而代之的是只包含关于数据的位置和长度的信息的描述符被追加到了socket buffer 缓冲区中。DMA引擎直接把数据从内核缓冲区传输到协议引擎（protocol engine），从而消除了最后一次CPU copy。经过上述过程，数据只经过了2次copy就从磁盘传送出去了。这个才是真正的Zero-Copy(这里的零拷贝是针对kernel来讲的，数据在kernel模式下是Zero-Copy)。 正是Linux2.4的内核做了改进，Java中的TransferTo()实现了Zero-Copy,如下图： Zero-Copy技术的使用场景有很多，比如Kafka, 又或者是Netty等，可以大大提升程序的性能。 首先我们说零拷贝，是从操作系统的角度来说的。因为内核缓冲区之间，没有数据是重复的（只有 kernel buffer 有一份数据，sendFile 2.1 版本实际上有 2 份数据，算不上零拷贝）。例如我们刚开始的例子，内核缓存区和 Socket 缓冲区的数据就是重复的。 而零拷贝不仅仅带来更少的数据复制，还能带来其他的性能优势，例如更少的上下文切换，更少的 CPU 缓存伪共享以及无 CPU 校验和计算。 mmap 和 sendFile 的区别。 mmap 适合小数据量读写，sendFile 适合大文件传输。 mmap 需要 4 次上下文切换，3 次数据拷贝；sendFile 需要 3 次上下文切换，最少 2 次数据拷贝。 sendFile 可以利用 DMA 方式，减少 CPU 拷贝，mmap 则不能（必须从内核拷贝到 Socket 缓冲区）。 在这个选择上：rocketMQ 在消费消息时，使用了 mmap。kafka 使用了 sendFile。 ","link":"https://tianxiawuhao.github.io/Ts_mpMa6r/"},{"title":"netty异步任务调度 ( TaskQueue | ScheduleTaskQueue | SocketChannel 管理 )","content":" 一、 任务队列 任务队列的任务 Task 应用场景 : 自定义任务 : 自己开发的任务 , 然后将该任务提交到任务队列中 ; 自定义定时任务 : 自己开发的任务 , 然后将该任务提交到任务队列中 , 同时可以指定任务的执行时间 ; 其它线程调度任务 : 上面的任务都是在当前的 NioEventLoop ( 反应器 Reactor 线程 ) 中的任务队列中排队执行 , 在其它线程中也可以调度本线程的 Channel 通道与该线程对应的客户端进行数据读写 ; 二、 处理器 Handler 同步异步操作 在之前的 Netty 服务器与客户端项目中 , 用户自定义的 Handler 处理器 , 该处理器继承了 ChannelInboundHandlerAdapter 类 , 在重写的 public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception 方法中 , 执行的业务逻辑要注意以下两点 : 同步操作 : 如果在该业务逻辑中只执行一个短时间的操作 , 那么可以直接执行 ; 异步操作 : 如果在该业务逻辑中执行访问数据库 , 访问网络 , 读写本地文件 , 执行一系列复杂计算等耗时操作 , 肯定不能在该方法中处理 , 这样会阻塞整个线程 ; 正确的做法是将耗时的操作放入任务队列 TaskQueue , 异步执行 ; 在 ChannelInboundHandlerAdapter 的 channelRead 方法执行时 , 客户端与服务器端的反应器 Reactor 线程 NioEventLoop 是处于阻塞状态的 , 此时服务器端与客户端同时都处于阻塞状态 , 这样肯定不行 , 因为 NioEventLoop 需要为多个客户端服务 , 不能因为与单一客户端交互而产生阻塞 ; 三、 异步任务 ( 用户自定义任务 ) 用户自定义任务流程 : ① 获取通道 : 首先获取 通道 Channel ; ② 获取线程 : 获取通道对应的 EventLoop 线程 , 就是 NioEventLoop , 该 NioEventLoop 中封装了任务队列 TaskQueue ; ③ 任务入队 : 向任务队列 TaskQueue 中放入异步任务 Runnable , 调用 NioEventLoop 线程的 execute 方法 , 即可将上述 Runnable 异步任务放入任务队列 TaskQueue ; 多任务执行 : 如果用户连续向任务队列中放入了多个任务 , NioEventLoop 会按照顺序先后执行这些任务 , 注意任务队列中的任务 是先后执行 , 不是同时执行 ; 顺序执行任务 ( 不是并发 ) : 任务队列任务执行机制是顺序执行的 ; 先执行第一个 , 执行完毕后 , 从任务队列中获取第二个任务 , 执行完毕之后 , 依次从任务队列中取出任务执行 , 前一个任务执行完毕后 , 才从任务队列中取出下一个任务执行 ; 代码示例 : 监听到客户端上传数据后 , channelRead 回调 , 执行 获取通道 -&gt; 获取线程 -&gt; 异步任务调度 流程 ; /** * Handler 处理者, 是 NioEventLoop 线程中处理业务逻辑的类 * * 继承 : 该业务逻辑处理者 ( Handler ) 必须继承 Netty 中的 ChannelInboundHandlerAdapter 类 * 才可以设置给 NioEventLoop 线程 * * 规范 : 该 Handler 类中需要按照业务逻辑处理规范进行开发 */ public class ServerHandr extends ChannelInboundHandlerAdapter { /** * 读取数据 : 在服务器端读取客户端发送的数据 * @param ctx * 通道处理者上下文对象 : 封装了 管道 ( Pipeline ) , 通道 ( Channel ), 客户端地址信息 * 管道 ( Pipeline ) : 注重业务逻辑处理 , 可以关联很多 Handler * 通道 ( Channel ) : 注重数据读写 * @param msg * 客户端上传的数据 * @throws Exception */ @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception { // 1 . 从 ChannelHandlerContext ctx 中获取通道 Channel channel = ctx.channel(); // 2 . 获取通道对应的事件循环 EventLoop eventLoop = channel.eventLoop(); // 3 . 在 Runnable 中用户自定义耗时操作, 异步执行该操作, 该操作不能阻塞在此处执行 eventLoop.execute(new Runnable() { @Override public void run() { //执行耗时操作 } }); } } 四、 异步任务 ( 用户自定义定时任务 ) 用户自定义定时任务 与 用户自定义任务流程基本类似 , 有以下两个不同之处 : ① 调度方法 : 定时异步任务使用 schedule 方法进行调度 ; 普通异步任务使用 execute 方法进行调度 ; ② 任务队列 : 定时异步任务提交到 ScheduleTaskQueue 任务队列中 ; 普通异步任务提交到 TaskQueue 任务队列中 ; 用户自定义定时任务流程 : ① 获取通道 : 首先获取 通道 Channel ; ② 获取线程 : 获取通道对应的 EventLoop 线程 , 就是 NioEventLoop , 该 NioEventLoop 中封装了任务队列 TaskQueue ; ③ 任务入队 : 向任务队列 ScheduleTaskQueue 中放入异步任务 Runnable , 调用 NioEventLoop 线程的 schedule 方法 , 即可将上述 Runnable 异步任务放入任务队列 ScheduleTaskQueue ; 代码示例 : 监听到客户端上传数据后 , channelRead 回调 , 执行 获取通道 -&gt; 获取线程 -&gt; 异步任务调度 流程 ; /** * Handler 处理者, 是 NioEventLoop 线程中处理业务逻辑的类 * * 继承 : 该业务逻辑处理者 ( Handler ) 必须继承 Netty 中的 ChannelInboundHandlerAdapter 类 * 才可以设置给 NioEventLoop 线程 * * 规范 : 该 Handler 类中需要按照业务逻辑处理规范进行开发 */ public class ServerHandr extends ChannelInboundHandlerAdapter { /** * 读取数据 : 在服务器端读取客户端发送的数据 * @param ctx * 通道处理者上下文对象 : 封装了 管道 ( Pipeline ) , 通道 ( Channel ), 客户端地址信息 * 管道 ( Pipeline ) : 注重业务逻辑处理 , 可以关联很多 Handler * 通道 ( Channel ) : 注重数据读写 * @param msg * 客户端上传的数据 * @throws Exception */ @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception { // 1 . 从 ChannelHandlerContext ctx 中获取通道 Channel channel = ctx.channel(); // 2 . 获取通道对应的事件循环 EventLoop eventLoop = channel.eventLoop(); // 3 . 在 Runnable 中用户自定义耗时操作, 异步执行该操作, 该操作不能阻塞在此处执行 // schedule(Runnable command, long delay, TimeUnit unit) // Runnable command 参数 : 异步任务 // long delay 参数 : 延迟执行时间 // TimeUnit unit参数 : 延迟时间单位, 秒, 毫秒, 分钟 eventLoop.schedule(new Runnable() { @Override public void run() { //执行耗时操作 } }, 100, TimeUnit.MILLISECONDS); } } 五、 异步任务 ( 其它线程向本线程调度任务 ) 通过EventExecutorGroup线程池获取不同线程执行异步耗时任务 代码示例（一） // 服务器启动对象, 需要为该对象配置各种参数 ServerBootstrap bootstrap = new ServerBootstrap(); // 添加线程组异步执行耗时任务 final EventExecutorGroup businessGroup = new DefaultEventExecutorGroup(16); bootstrap.group(bossGroup, workerGroup) // 设置 主从 线程组 , 分别对应 主 Reactor 和 从 Reactor .channel(NioServerSocketChannel.class) // 设置 NIO 网络套接字通道类型 .option(ChannelOption.SO_BACKLOG, 128) // 设置线程队列维护的连接个数 .childOption(ChannelOption.SO_KEEPALIVE, true) // 设置连接状态行为, 保持连接状态 .childHandler( // 为 WorkerGroup 线程池对应的 NioEventLoop 设置对应的事件 处理器 Handler new ChannelInitializer&lt;SocketChannel&gt;() {// 创建通道初始化对象 @Override protected void initChannel(SocketChannel ch) throws Exception { // 该方法在服务器与客户端连接建立成功后会回调 // 为 管道 Pipeline 设置处理器 Hanedler ch.pipeline().addLast(businessGroup,new NettyServerHandler()); } } ); 代码示例（二） public class NettyServerHandler extends SimpleChannelInboundHandler&lt;Object&gt; { final EventExecutorGroup businessGroup = new DefaultEventExecutorGroup(16); @Override protected void channelRead0(ChannelHandlerContext ctx, Object obj) throws Exception { businessGroup.submit(new Callable&lt;Object&gt;() { @Override public Object call() throws Exception { //业务处理 return null; } }); } } ","link":"https://tianxiawuhao.github.io/b3vVP1a3A/"},{"title":"netty_nio_Reactor模式","content":"Reactor有三种模式： 单reactor单线程工作原理图 dispatch与handler在同一个线程中处理. redis就是采用这种模式 单reactor多线程工作原理图 （1） reactor对象通过select监控客户端请求事件，收到事件后，通过dispatch进行分发 （2）如果建立连接请求，则由Acceptor通过accept处理连接请求，然后创建一个Handler对象处理完成连接后的各种事件 （3）如果不是连接请求，则由reactor分发调用连接对应的Handler来处理 （4）Handler只负责响应事件，不做具体的业务处理， 通过read读取数据后分发给后面的work线程池中的某个线程。 （5）work线程池会分配一个独立的线程完成真正的业务 ，并将处理完的业务结果返回给Handler （6）Handler收到响应结果后，通过send将结果返回给client 优点： （1） 可以充分利用多核cpu的处理能力 （2） 多线程数据共享和访问比较复杂 ，reactor处理了所有的事件监听和响应，而且是在单线程中运行，在高并发场景容易出现性能瓶颈 主从reactor多线程工作原理图 （1）Reactor主线程MainReactor对象通过select监听连接事件，收到连接事件后，通过Acceptor处理连接事件 （2）当Acceptor处理连接事件后，MainReactor将连接分配给SubReactor, （3）SubReactor将连接加入到连接监听队列进行监听，并创建Handler进行各种事件处理 （4）当有新的事件发生时， subreactor就会调用对应的handler处理， （5）handler通过read读取数据后，分发给后面的worker线程处理 （6）worker线程池会分配独立的worker线程进行业务处理，并返回结果 （7）handler收到响应结果后，再通过send将结果返回给client 实现Reactor模式我们需要实现以下几个类： InputSource: 外部输入类，用来表示需要reactor去处理的原始对象 Event: reactor模式的事件类，可以理解为将输入原始对象根据不同状态包装成一个事件类，reactor模式里处理的斗士event事件对象 EventType: 枚举类型表示事件的不同类型 EventHandler: 处理事件的抽象类，里面包含了不同事件处理器的公共逻辑和公共对象 AcceptEventHandler\\ReadEventhandler等: 继承自EventHandler的具体事件处理器的实现类，一般根据事件不同的状态来定义不同的处理器 Dispatcher: 事件分发器，整个reactor模式解决的主要问题就是在接收到任务后根据分发器快速进行分发给相应的事件处理器，不需要从开始状态就阻塞 Selector: 事件轮循选择器，selector主要实现了轮循队列中的事件状态，取出当前能够处理的状态 Acceptor:reactor的事件接收类，负责初始化selector和接收缓冲队列 Server:负责启动reactor服务并启动相关服务接收请求 InputSource.java import lombok.AllArgsConstructor; import lombok.Data; @Data @AllArgsConstructor public class InputSource { private final Object data; private final long id; } Event.java import lombok.Getter; import lombok.Setter; @Getter @Setter public class Event { private InputSource source; private EventType type; } EventType public enum EventType { ACCEPT, READ, WRITE; } EventHandler.java @Getter @Setter public abstract class EventHandler { private InputSource source; public abstract void handle(Event event); } AcceptEventHandler.java public class AcceptEventHandler extends EventHandler { private Selector selector; public AcceptEventHandler(Selector selector) { this.selector = selector; } @Override public void handle(Event event) { //处理Accept的event事件 if (event.getType() == EventType.ACCEPT) { //TODO 处理ACCEPT状态的事件 //将事件状态改为下一个READ状态，并放入selector的缓冲队列中 Event readEvent = new Event(); readEvent.setSource(event.getSource()); readEvent.setType(EventType.READ); selector.addEvent(readEvent); } } } Dispatcher.java import java.util.List; import java.util.Map; import java.util.concurrent.ConcurrentHashMap; public class Dispatcher { //通过ConcurrentHashMap来维护不同事件处理器 Map&lt;EventType, EventHandler&gt; eventHandlerMap = new ConcurrentHashMap&lt;EventType, EventHandler&gt;(); //本例只维护一个selector负责事件选择，netty为了保证性能实现了多个selector来保证循环处理性能，不同事件加入不同的selector的事件缓冲队列 Selector selector; Dispatcher(Selector selector) { this.selector = selector; } //在Dispatcher中注册eventHandler public void registEventHandler(EventType eventType, EventHandler eventHandler) { eventHandlerMap.put(eventType, eventHandler); } public void removeEventHandler(EventType eventType) { eventHandlerMap.remove(eventType); } public void handleEvents() { dispatch(); } //此例只是实现了简单的事件分发给相应的处理器处理，例子中的处理器都是同步，在reactor模式的典型实现NIO中都是在handle异步处理，来保证非阻塞 private void dispatch() { while (true) { List&lt;Event&gt; events = selector.select(); for (Event event : events) { EventHandler eventHandler = eventHandlerMap.get(event.getType()); eventHandler.handle(event); } } } } Selector.java import java.util.ArrayList; import java.util.List; import java.util.concurrent.BlockingQueue; import java.util.concurrent.LinkedBlockingQueue; public class Selector { //定义一个链表阻塞queue实现缓冲队列，用于保证线程安全 private final BlockingQueue&lt;Event&gt; eventQueue = new LinkedBlockingQueue&lt;Event&gt;(); //定义一个object用于synchronize方法块上锁 private final Object lock = new Object(); List&lt;Event&gt; select() { return select(0); } // List&lt;Event&gt; select(long timeout) { if (timeout &gt; 0) { if (eventQueue.isEmpty()) { synchronized (lock) { if (eventQueue.isEmpty()) { try { lock.wait(timeout); } catch (InterruptedException ignored) { } } } } } //TODO 例子中只是简单的将event列表全部返回，可以在此处增加业务逻辑，选出符合条件的event进行返回 List&lt;Event&gt; events = new ArrayList&lt;Event&gt;(); eventQueue.drainTo(events); return events; } public void addEvent(Event e) { //将event事件加入队列 boolean success = eventQueue.offer(e); if (success) { synchronized (lock) { //如果有新增事件则对lock对象解锁 lock.notify(); } } } } Acceptor.java import java.util.concurrent.BlockingQueue; import java.util.concurrent.LinkedBlockingQueue; public class Acceptor implements Runnable{ private final int port; // server socket port private final Selector selector; // 代表 serversocket，通过LinkedBlockingQueue来模拟外部输入请求队列 private final BlockingQueue&lt;InputSource&gt; sourceQueue = new LinkedBlockingQueue&lt;InputSource&gt;(); Acceptor(Selector selector, int port) { this.selector = selector; this.port = port; } //外部有输入请求后，需要加入到请求队列中 public void addNewConnection(InputSource source) { sourceQueue.offer(source); } public int getPort() { return this.port; } public void run() { while (true) { InputSource source = null; try { // 相当于 serversocket.accept()，接收输入请求，该例从请求队列中获取输入请求 source = sourceQueue.take(); } catch (InterruptedException e) { // ignore it; } //接收到InputSource后将接收到event设置type为ACCEPT，并将source赋值给event if (source != null) { Event acceptEvent = new Event(); acceptEvent.setSource(source); acceptEvent.setType(EventType.ACCEPT); selector.addEvent(acceptEvent); } } } } Server.java public class Server { Selector selector = new Selector(); Dispatcher eventLooper = new Dispatcher(selector); Acceptor acceptor; Server(int port) { acceptor = new Acceptor(selector, port); } public void start() { eventLooper.registEventHandler(EventType.ACCEPT, new AcceptEventHandler(selector)); new Thread(acceptor, &quot;Acceptor-&quot; + acceptor.getPort()).start(); eventLooper.handleEvents(); } } ","link":"https://tianxiawuhao.github.io/wvy4g9Zbn/"},{"title":"saiku自动对接kylin","content":"saiku通过添加schema和datasource的形式管理对接入系统的数据源，然后提供界面作为直观的分析数据方式，界面产生mdx，由mondrian连接数据源，解析mdx和执行查询 kylin提供大规模数据的olap能力，通过saiku与kylin的对接，利用saiku的友好界面来很方面的查询 如上的整合，需要手动配置数据源，编写schema的操作，感觉比较繁琐，可以通过修改saiku的代码，到kylin中获取project和cube的各种信息，根据一定规则转换生成schema并作为数据源管理起来，这样就很直接将saiku与kylin无缝对接起来。 代码案例 saiku-wabapp #saiku-beans.properties sylin.user=ADMIN sylin.password=ADMIN sylin.cube.url=http://localhost:7070/kylin/api/cubes sylin.cubedesc.url=http://localhost:7070/kylin/api/cube_desc sylin.model.url=http://localhost:7070/kylin/api/model sylin.url=localhost:7070 //saiku-beans.xml &lt;bean id=&quot;repositoryDsManager&quot; class=&quot;org.saiku.service.datasource.RepositoryDatasourceManager&quot; init-method=&quot;load&quot; destroy-method=&quot;unload&quot;&gt; &lt;!--aop:scoped-proxy/--&gt; ...... &lt;property name=&quot;sylinUser&quot; value=&quot;${sylin.user}&quot;/&gt; &lt;property name=&quot;sylinPassWord&quot; value=&quot;${sylin.password}&quot;/&gt; &lt;property name=&quot;sylinCubeUrl&quot; value=&quot;${sylin.cube.url}&quot;/&gt; &lt;property name=&quot;sylinCubeDescUrl&quot; value=&quot;${sylin.cubedesc.url}&quot;/&gt; &lt;property name=&quot;sylinModelUrl&quot; value=&quot;${sylin.model.url}&quot;/&gt; &lt;property name=&quot;sylinUrl&quot; value=&quot;${sylin.url}&quot;/&gt; &lt;/bean&gt; saiku-service public class RepositoryDatasourceManager implements IDatasourceManager, ApplicationListener&lt;HttpSessionCreatedEvent&gt; { private String sylinUser; private String sylinPassWord; private String sylinCubeUrl; private String sylinCubeDescUrl; private String sylinModelUrl; private String sylinUrl; //get.set省略 ...... private void loadDatasources(Properties ext) { datasources.clear(); List&lt;DataSource&gt; exporteddatasources = null; try { String result = execute(sylinCubeUrl); JSONArray ja = JSON.parseArray(result); for (int i = 0; i &lt; ja.size(); i++) { JSONObject js = JSONObject.parseObject(ja.getString(i)); String newCubeName = js.getString(&quot;project&quot;) + &quot;#&quot; + js.getString(&quot;name&quot;); datasources.put(js.getString(&quot;name&quot;), getSaikuDatasource(newCubeName)); } } catch (Exception e) { log.error(&quot;Failed add sylin cube to datasource&quot;, e); e.printStackTrace(); } ...... } private SaikuDatasource getSaikuDatasource(String datasourceName) throws Exception { if (datasourceName.contains(&quot;#&quot;)) { String cubeName = datasourceName.split(&quot;#&quot;)[1].trim(); String cubeDescString = execute(sylinCubeDescUrl + &quot;/&quot; + cubeName); CubeDesc cubeDesc = OBJECT_MAPPER.readValue(JSON.parseArray(cubeDescString).getString(0), CubeDesc.class); //CubeDesc cubeDesc = JSONObject.parseObject(JSON.parseArray(cubeDescString).getString(0), CubeDesc.class); String modelName = cubeDesc.getModelName(); String cubeModelString = execute(sylinModelUrl + &quot;/&quot; + modelName); DataModelDesc modelDesc = OBJECT_MAPPER.readValue(cubeModelString, DataModelDesc.class); //DataModelDesc modelDesc = JSONObject.parseObject(cubeModelString, DataModelDesc.class); if (cubeDesc != null &amp;&amp; modelDesc != null) { addSchema(SchemaUtil.createSchema(datasourceName, cubeDesc, modelDesc), &quot;/datasources/&quot; + datasourceName.replace(&quot;#&quot;, &quot;.&quot;) + &quot;.xml&quot;, datasourceName); } String project = new String(); if (datasourceName.contains(&quot;#&quot;)) { project = datasourceName.split(&quot;#&quot;)[0].trim(); } else { project = datasourceName; } Properties properties = new Properties(); properties.put(&quot;location&quot;, &quot;jdbc:mondrian:Jdbc=jdbc:kylin://&quot; + sylinUrl + &quot;/&quot; + project + &quot;;JdbcDrivers=org.apache.kylin.jdbc.Driver&quot; + &quot;;Catalog=mondrian:///datasources/&quot; + datasourceName.replace(&quot;#&quot;, &quot;.&quot;) + &quot;.xml&quot;); properties.put(&quot;driver&quot;, &quot;mondrian.olap4j.MondrianOlap4jDriver&quot;); properties.put(&quot;username&quot;, sylinUser); properties.put(&quot;password&quot;, sylinPassWord); properties.put(&quot;security.enabled&quot;, false); properties.put(&quot;advanced&quot;, false); return new SaikuDatasource(cubeName, SaikuDatasource.Type.OLAP, properties); } return null; } private String execute(String url) throws URISyntaxException, IOException { int httpConnectionTimeoutMs = 30000; int httpSocketTimeoutMs = 120000; final HttpParams httpParams = new BasicHttpParams(); final PoolingClientConnectionManager cm = new PoolingClientConnectionManager(); HttpConnectionParams.setSoTimeout(httpParams, httpSocketTimeoutMs); HttpConnectionParams.setConnectionTimeout(httpParams, httpConnectionTimeoutMs); cm.setDefaultMaxPerRoute(20); cm.setMaxTotal(200); HttpGet get = newGet(url); get.setURI(new URI(url)); DefaultHttpClient client = new DefaultHttpClient(cm, httpParams); if (sylinUser != null &amp;&amp; sylinPassWord != null) { CredentialsProvider provider = new BasicCredentialsProvider(); UsernamePasswordCredentials credentials = new UsernamePasswordCredentials(sylinUser, sylinPassWord); provider.setCredentials(AuthScope.ANY, credentials); client.setCredentialsProvider(provider); } HttpResponse response = client.execute(get); if (response.getStatusLine().getStatusCode() != 200) { throw new IOException(&quot;Invalid response &quot; + response.getStatusLine().getStatusCode()); } String result = getContent(response); return result; } private HttpGet newGet(String url) { HttpGet get = new HttpGet(); addHttpHeaders(get); return get; } private void addHttpHeaders(HttpRequestBase method) { method.addHeader(&quot;Accept&quot;, &quot;application/json, text/plain, */*&quot;); method.addHeader(&quot;Content-Type&quot;, &quot;application/json&quot;); String basicAuth = DatatypeConverter .printBase64Binary((sylinUser + &quot;:&quot; + sylinPassWord).getBytes(StandardCharsets.UTF_8)); method.addHeader(&quot;Authorization&quot;, &quot;Basic &quot; + basicAuth); } private String getContent(HttpResponse response) throws IOException { InputStreamReader reader = null; BufferedReader rd = null; StringBuffer result = new StringBuffer(); try { reader = new InputStreamReader(response.getEntity().getContent(), StandardCharsets.UTF_8); rd = new BufferedReader(reader); String line = null; while ((line = rd.readLine()) != null) { result.append(line); } } finally { IOUtils.closeQuietly(reader); IOUtils.closeQuietly(rd); } return result.toString(); } } mondrian3.0语法工具类 package org.saiku.service.util; import org.apache.kylin.cube.model.CubeDesc; import org.apache.kylin.cube.model.DimensionDesc; import org.apache.kylin.metadata.model.*; import java.util.*; public class SchemaUtil1 { private static String newLine = &quot;\\r\\n&quot;; private static Set&lt;String&gt; aggSet = new HashSet&lt;String&gt;(){ { add(&quot;sum&quot;); add(&quot;min&quot;); add(&quot;max&quot;); add(&quot;count&quot;); add(&quot;count_distinct&quot;); } }; public static String createSchema(String dataSourceName, CubeDesc cubeDesc, DataModelDesc modelDesc) { StringBuffer sb = new StringBuffer(); sb = appendSchema(sb, dataSourceName, cubeDesc, modelDesc); return sb.toString(); } public static StringBuffer appendSchema(StringBuffer sb, String dataSourceName, CubeDesc cubeDesc, DataModelDesc modelDesc) { sb.append(&quot;&lt;?xml version='1.0'?&gt;&quot;).append(newLine) .append(&quot;&lt;Schema name='&quot; + dataSourceName.split(&quot;#&quot;)[0].trim()+&quot;'&gt;&quot;) .append(newLine); sb = appendCube(sb, dataSourceName, cubeDesc, modelDesc); sb.append(&quot;&lt;/Schema&gt;&quot;).append(newLine); return sb; } public static StringBuffer appendCube(StringBuffer sb, String cubeName, CubeDesc cubeDesc, DataModelDesc modelDesc) { sb.append(&quot;&lt;Cube name='&quot; + cubeName.split(&quot;#&quot;)[1] + &quot;'&gt;&quot;).append(newLine); sb.append(&quot;&lt;Table name='&quot;+dealTableName(modelDesc.getRootFactTableName())+&quot;'/&gt;&quot;).append(newLine); HashMap&lt;String,String&gt; aliasTableMap = new HashMap&lt;String,String&gt;(); HashMap&lt;String,String&gt; aliasTableJoinMap = new HashMap&lt;String,String&gt;(); aliasTableMap.put(dealTableName(modelDesc.getRootFactTableName()),dealTableName(modelDesc.getRootFactTableName())); for(JoinTableDesc joinTableDesc:modelDesc.getJoinTables()) { aliasTableMap.put(joinTableDesc.getAlias(),dealTableName(joinTableDesc.getTable())); if(joinTableDesc.getJoin().getPrimaryKey().length == 1){ aliasTableJoinMap.put(dealTableName(joinTableDesc.getAlias()),joinTableDesc.getJoin().getPrimaryKey()[0].concat(&quot;#&quot;).concat(joinTableDesc.getJoin().getForeignKey()[0])); } } for(DimensionDesc dimensionDesc: cubeDesc.getDimensions()){ // tableSet.add(dealTableName(dimensionDesc.getTable())); if(aliasTableMap.get(dealTableName(dimensionDesc.getTable())).equals(dealTableName(modelDesc.getRootFactTableName()))){ sb.append(&quot;&lt;Dimension name='&quot;+dimensionDesc.getName()+&quot;'&gt;&quot;).append(newLine); sb.append(&quot;&lt;Hierarchy name='&quot;+dimensionDesc.getName()+&quot;' hasAll='true' allMemberName='All &quot;+dimensionDesc.getName()+&quot;'&gt;&quot;).append(newLine); }else if(aliasTableJoinMap.get(dealTableName(dimensionDesc.getTable()))==null){ continue; } else if(aliasTableJoinMap.get(dealTableName(dimensionDesc.getTable()))!=null &amp;&amp; aliasTableJoinMap.get(dealTableName(dimensionDesc.getTable())).split(&quot;#&quot;)[1].split(&quot;\\\\.&quot;)[0] .equals(dealTableName(modelDesc.getRootFactTableName()))){ sb.append(&quot;&lt;Dimension name='&quot;+dimensionDesc.getName()+&quot;' foreignKey='&quot;+aliasTableJoinMap.get(dealTableName(dimensionDesc.getTable())) .split(&quot;#&quot;)[1].split(&quot;\\\\.&quot;)[1]+&quot;'&gt;&quot;).append(newLine); sb.append(&quot;&lt;Hierarchy name='&quot;+dimensionDesc.getName()+&quot;' hasAll='true' allMemberName='All &quot;+dimensionDesc.getName()+&quot;' primaryKey='&quot;+ aliasTableJoinMap.get(dealTableName(dimensionDesc.getTable())).split(&quot;#&quot;)[0].split(&quot;\\\\.&quot;)[1]+&quot;'&gt;&quot;).append(newLine); sb.append(&quot;&lt;Table name='&quot;+aliasTableMap.get(dealTableName(dimensionDesc.getTable()))+&quot;'/&gt;&quot;).append(newLine); } else if(aliasTableJoinMap.get(aliasTableJoinMap.get(dealTableName(dimensionDesc.getTable())).split(&quot;#&quot;)[1].split(&quot;\\\\.&quot;)[0]) .split(&quot;#&quot;)[1].split(&quot;\\\\.&quot;)[0].equals(dealTableName(modelDesc.getRootFactTableName()))){ sb.append(&quot;&lt;Dimension name='&quot;+dimensionDesc.getName()+&quot;' foreignKey='&quot;+aliasTableJoinMap.get(aliasTableJoinMap.get(dealTableName(dimensionDesc.getTable())) .split(&quot;#&quot;)[1].split(&quot;\\\\.&quot;)[0]).split(&quot;#&quot;)[1].split(&quot;\\\\.&quot;)[1]+&quot;'&gt;&quot;).append(newLine); sb.append(&quot;&lt;Hierarchy name='&quot;+dimensionDesc.getName()+&quot;' hasAll='true' allMemberName='All &quot;+dimensionDesc.getName()+&quot;' primaryKey='&quot;+ aliasTableJoinMap.get(aliasTableJoinMap.get(dealTableName(dimensionDesc.getTable())).split(&quot;#&quot;)[1].split(&quot;\\\\.&quot;)[0]) .split(&quot;#&quot;)[0].split(&quot;\\\\.&quot;)[1]+&quot;' primaryKeyTable='&quot;+aliasTableMap.get(aliasTableJoinMap.get(dealTableName(dimensionDesc.getTable())) .split(&quot;#&quot;)[1].split(&quot;\\\\.&quot;)[0])+&quot;'&gt;&quot;).append(newLine); sb.append(&quot;&lt;Join leftKey='&quot;+aliasTableJoinMap.get(dealTableName(dimensionDesc.getTable())).split(&quot;#&quot;)[1].split(&quot;\\\\.&quot;)[1]+&quot;' rightAlias='&quot;+aliasTableMap.get(dimensionDesc.getTable())+ &quot;' rightKey='&quot;+aliasTableJoinMap.get(dealTableName(dimensionDesc.getTable())).split(&quot;#&quot;)[0].split(&quot;\\\\.&quot;)[1]+&quot;'&gt;&quot;).append(newLine); sb.append(&quot;&lt;Table name='&quot;+aliasTableMap.get(aliasTableJoinMap.get(dealTableName(dimensionDesc.getTable())).split(&quot;#&quot;)[1].split(&quot;\\\\.&quot;)[0])+&quot;'/&gt;&quot;).append(newLine); sb.append(&quot;&lt;Table name='&quot;+aliasTableMap.get(dimensionDesc.getTable())+&quot;'/&gt;&quot;).append(newLine); sb.append(&quot;&lt;/Join&gt;&quot;).append(newLine); } else { continue; } Set&lt;String&gt; columns = getColumns(dimensionDesc); for(String column:columns){ sb.append(&quot;&lt;Level name='&quot;+column+&quot;' column='&quot;+column+&quot;' table='&quot;+aliasTableMap.get(dimensionDesc.getTable())+&quot;'/&gt;&quot;).append(newLine); } sb.append(&quot;&lt;/Hierarchy&gt;&quot;).append(newLine); sb.append(&quot;&lt;/Dimension&gt;&quot;).append(newLine); } for (MeasureDesc measureDesc : cubeDesc.getMeasures()) { int i=0; String table = measureDesc.getFunction().getParameter().getValue().split(&quot;\\\\.&quot;)[0]; final boolean flag = Arrays.stream(modelDesc.getJoinTables()).anyMatch(item -&gt; table.equals(dealTableName(item.getTable()))); if(table.equals(dealTableName(modelDesc.getRootFactTableName()))||flag||table.equals(&quot;1&quot;)){ addMeasure(sb,measureDesc,getColumn(cubeDesc,modelDesc,dealTableName(modelDesc.getRootFactTableName()))); } } sb.append(&quot;&lt;/Cube&gt;&quot;).append(newLine); return sb; } public static String dealTableName(String tableName){ if(tableName.contains(&quot;.&quot;)) return tableName.split(&quot;\\\\.&quot;)[1]; else return tableName; } public static StringBuffer addMeasure(StringBuffer sb, MeasureDesc measureDesc, String defaultColumn) { FunctionDesc funtionDesc = measureDesc.getFunction(); String aggregator = funtionDesc.getExpression().trim().toLowerCase(); if(aggSet.contains(aggregator.toLowerCase())){ //mondrian only have distinct-count if(aggregator.equals(&quot;count_distinct&quot;)){ aggregator = &quot;distinct-count&quot;; } if(funtionDesc.getParameter().getValue().equals(&quot;1&quot;)) { sb.append(&quot;&lt;Measure aggregator='&quot; + aggregator + &quot;' column='&quot; + defaultColumn + &quot;' name='&quot; + measureDesc.getName() + &quot;' visible='true'/&gt;&quot;) .append(newLine); } else sb.append(&quot;&lt;Measure aggregator='&quot; + aggregator + &quot;' column='&quot; + funtionDesc.getParameter().getValue().split(&quot;\\\\.&quot;)[1] + &quot;' name='&quot; + measureDesc.getName() + &quot;' visible='true'/&gt;&quot;) .append(newLine); return sb; } return sb; } public static String getColumn(CubeDesc cubeDesc,DataModelDesc dataModelDesc,String tableName){ List&lt;MeasureDesc&gt; measureDescList = cubeDesc.getMeasures(); for(MeasureDesc measureDesc:measureDescList){ if(measureDesc.getFunction().getParameter().getValue().split(&quot;\\\\.&quot;)[0].equals(tableName)){ return measureDesc.getFunction().getParameter().getValue().split(&quot;\\\\.&quot;)[1]; } } if(dataModelDesc.getMetrics().length&gt;0){ return dataModelDesc.getMetrics()[0]; } for(ModelDimensionDesc modelDimensionDesc:dataModelDesc.getDimensions()){ if(modelDimensionDesc.getTable().equals(tableName)){ return modelDimensionDesc.getColumns()[0]; } } return null; } public static Set&lt;String&gt; getColumns(DimensionDesc dimensionDesc){ Set&lt;String&gt; columns = new HashSet&lt;String&gt;(); if (dimensionDesc.getColumn() != null || dimensionDesc.getDerived() != null) { if(dimensionDesc.getColumn() != null) { columns.add(dimensionDesc.getColumn()); } if (dimensionDesc.getDerived() != null) { for (String derived : dimensionDesc.getDerived()) { columns.add(derived); } } } else { columns.add(dimensionDesc.getName()); } return columns; } } &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;Schema name=&quot;Mondrian&quot;&gt; &lt;!--模型定义--&gt; &lt;Cube name=&quot;Person&quot;&gt; &lt;!--立方体 ，一个立方体有多个维度--&gt; &lt;Table name=&quot;PERSON&quot; /&gt; &lt;!--立方体对应的表 --&gt; &lt;Dimension name=&quot;部门&quot; foreignKey=&quot;USERID&quot; &gt; &lt;!--定义维度 --&gt; &lt;Hierarchy hasAll=&quot;true&quot; primaryKey=&quot;USERID&quot; allMemberName=&quot;所有部门&quot; &gt; &lt;!--定义维度下面的层次，层次包含很多层 --&gt; &lt;Table name=&quot;PERSON&quot; alias=&quot;a&quot;/&gt; &lt;!--定义维度获取数据的来源-表 --&gt; &lt;Level name=&quot;部门&quot; column=&quot;DEPARTMENT&quot; uniqueMembers=&quot;true&quot; /&gt; &lt;!--定义层次的层，每个层对应数据库中对应的字段 --&gt; &lt;Level name=&quot;姓名&quot; column=&quot;USERNAME&quot; uniqueMembers=&quot;true&quot; /&gt; &lt;/Hierarchy&gt; &lt;/Dimension&gt; &lt;Dimension name=&quot;性别&quot; foreignKey=&quot;USERID&quot; &gt; &lt;Hierarchy hasAll=&quot;true&quot; primaryKey=&quot;USERID&quot; allMemberName=&quot;所有性别&quot;&gt; &lt;Table name=&quot;PERSON&quot; alias=&quot;b&quot; /&gt; &lt;Level name=&quot;性别&quot; column=&quot;SEX&quot; uniqueMembers=&quot;true&quot; /&gt; &lt;/Hierarchy&gt; &lt;/Dimension&gt; &lt;Dimension name=&quot;专业技术资格类别&quot; foreignKey=&quot;USERID&quot; &gt; &lt;Hierarchy hasAll=&quot;true&quot; primaryKey=&quot;USERID&quot; allMemberName=&quot;所有专业技术资格类别&quot;&gt; &lt;Table name=&quot;PERSON&quot; alias=&quot;c&quot; /&gt; &lt;Level name=&quot;资格类别&quot; column=&quot;ZYJSLB&quot; uniqueMembers=&quot;true&quot; /&gt; &lt;/Hierarchy&gt; &lt;/Dimension&gt; &lt;Dimension name=&quot;专业技术资格等级&quot; foreignKey=&quot;USERID&quot; &gt; &lt;Hierarchy hasAll=&quot;true&quot; primaryKey=&quot;USERID&quot; allMemberName=&quot;所有专业技术资格等级&quot;&gt; &lt;Table name=&quot;PERSON&quot; alias=&quot;d&quot; /&gt; &lt;Level name=&quot;资格等级&quot; column=&quot;ZYJSDJ&quot; uniqueMembers=&quot;true&quot; /&gt; &lt;/Hierarchy&gt; &lt;/Dimension&gt; &lt;Dimension name=&quot;职系&quot; foreignKey=&quot;USERID&quot; &gt; &lt;Hierarchy hasAll=&quot;true&quot; primaryKey=&quot;USERID&quot; allMemberName=&quot;所有职系&quot;&gt; &lt;Table name=&quot;PERSON&quot; alias=&quot;e&quot; /&gt; &lt;Level name=&quot;职系&quot; column=&quot;ZHIXI&quot; uniqueMembers=&quot;true&quot; /&gt; &lt;/Hierarchy&gt; &lt;/Dimension&gt; &lt;Dimension name=&quot;民族&quot; foreignKey=&quot;USERID&quot; &gt; &lt;Hierarchy hasAll=&quot;true&quot; primaryKey=&quot;USERID&quot; allMemberName=&quot;所有民族&quot;&gt; &lt;Table name=&quot;PERSON&quot; alias=&quot;f&quot; /&gt; &lt;Level name=&quot;民族&quot; column=&quot;NATIONALITY&quot; uniqueMembers=&quot;true&quot; /&gt; &lt;/Hierarchy&gt; &lt;/Dimension&gt; &lt;Dimension name=&quot;学历&quot; foreignKey=&quot;USERID&quot; &gt; &lt;Hierarchy hasAll=&quot;true&quot; primaryKey=&quot;USERID&quot; allMemberName=&quot;所有学历&quot;&gt; &lt;Table name=&quot;PERSON&quot; alias=&quot;g&quot; /&gt; &lt;Level name=&quot;学历&quot; column=&quot;XUELI&quot; uniqueMembers=&quot;true&quot; /&gt; &lt;/Hierarchy&gt; &lt;/Dimension&gt; &lt;Measure name=&quot;人数&quot; column=&quot;USERID&quot; aggregator=&quot;distinct count&quot; /&gt; &lt;!--指标/度量，采用distinct count聚合 --&gt; &lt;/Cube&gt; &lt;/Schema&gt; mondrian4.0语法工具类 package org.saiku.service.util; import org.apache.kylin.cube.model.CubeDesc; import org.apache.kylin.cube.model.DimensionDesc; import org.apache.kylin.cube.model.RowKeyDesc; import org.apache.kylin.metadata.model.*; import java.util.*; public class SchemaUtil { private static final String newLine = &quot;\\r\\n&quot;; private static Map&lt;String, String&gt; map; public static String createSchema(String dataSourceName, CubeDesc cubeDesc, DataModelDesc modelDesc) { StringBuffer sb = new StringBuffer(); sb = appendSchema(sb, dataSourceName, cubeDesc, modelDesc); // System.out.println(&quot;********************************&quot; + sb.toString()); return sb.toString(); } public static StringBuffer appendSchema(StringBuffer sb, String dataSourceName, CubeDesc cubeDesc, DataModelDesc modelDesc) { sb.append(&quot;&lt;?xml version='1.0'?&gt;&quot;).append(newLine) .append(&quot;&lt;Schema name='&quot;).append(dataSourceName.split(&quot;#&quot;)[0].trim()).append(&quot;' metamodelVersion='4.0'&gt;&quot;) // .append(&quot;&lt;Schema name='&quot; + dataSourceName + &quot;' metamodelVersion='4.0'&gt;&quot;) .append(newLine); appendTable(sb, modelDesc); appendDimension(sb, modelDesc); appendCube(sb, dataSourceName, cubeDesc, modelDesc); sb.append(&quot;&lt;/Schema&gt;&quot;).append(newLine); return sb; } public static StringBuffer appendTable(StringBuffer sb, DataModelDesc modelDesc) { StringBuilder linkSb = new StringBuilder(); //获取所有关系表 final List&lt;ModelDimensionDesc&gt; tables = modelDesc.getDimensions(); sb.append(&quot;&lt;PhysicalSchema&gt;&quot;).append(newLine); //添加表关联关系 final JoinTableDesc[] joinTables = modelDesc.getJoinTables(); for (JoinTableDesc joinTableDesc : joinTables) { String factTablename = dealModelTableName(joinTableDesc.getJoin().getForeignKey()[0]); String Column = dealTableName(joinTableDesc.getJoin().getForeignKey()[0]); String joinTablename = dealModelTableName(joinTableDesc.getJoin().getPrimaryKey()[0]); String joinColumn = dealTableName(joinTableDesc.getJoin().getPrimaryKey()[0]); linkSb.append(&quot;&lt;Link source='&quot;).append(factTablename).append(&quot;' target='&quot;).append(joinTablename).append(&quot;'&gt;&quot;).append(newLine) .append(&quot;&lt;ForeignKey&gt;&quot;).append(newLine) .append(&quot;&lt;Column name='&quot;).append(Column).append(&quot;'/&gt;&quot;).append(newLine) .append(&quot;&lt;/ForeignKey&gt;&quot;).append(newLine) .append(&quot;&lt;/Link&gt;&quot;).append(newLine); //清空map map = new HashMap&lt;&gt;(); tables.forEach(item -&gt; { if (item.getTable().equals(factTablename)) { map.put(factTablename, Column); } if (item.getTable().equals(joinTablename)) { map.put(joinTablename, joinColumn); } }); } //添加表 for (String tableName : map.keySet()) { sb.append(&quot;&lt;Table name='&quot;).append(tableName).append(&quot;'&gt;&quot;).append(newLine) .append(&quot;&lt;Key&gt;&quot;).append(newLine).append(&quot;&lt;Column name='&quot;).append(map.get(tableName)).append(&quot;'/&gt;&quot;).append(newLine) .append(&quot;&lt;/Key&gt;&quot;).append(newLine) .append(&quot;&lt;/Table&gt;&quot;).append(newLine); } sb.append(linkSb); linkSb.delete(0, linkSb.length()); sb.append(&quot;&lt;/PhysicalSchema&gt;&quot;).append(newLine); return sb; } public static Map&lt;String, JoinDesc&gt; getJoinDesc(DataModelDesc modelDesc) { Map&lt;String, JoinDesc&gt; joinDescMap = new HashMap&lt;String, JoinDesc&gt;(); for (JoinTableDesc lookupDesc : modelDesc.getJoinTables()) { if (!joinDescMap.containsKey(dealTableName(lookupDesc.getTable()))) joinDescMap.put(dealTableName(lookupDesc.getTable()), lookupDesc.getJoin()); } return joinDescMap; } public static void appendDimension(StringBuffer sb, DataModelDesc modelDesc) { StringBuilder hierSb = new StringBuilder(); for (ModelDimensionDesc dimensionDesc : modelDesc.getDimensions()) { sb.append(&quot;&lt;Dimension name='&quot;).append(dimensionDesc.getTable()).append(&quot;' key='&quot;).append(map.get(dimensionDesc.getTable())).append(&quot;' table='&quot;).append(dimensionDesc.getTable()).append(&quot;'&gt;&quot;).append(newLine); sb.append(&quot;&lt;Attributes&gt;&quot;).append(newLine); hierSb.append(&quot;&lt;Hierarchies&gt;&quot;).append(newLine); hierSb.append(&quot;&lt;Hierarchy name='&quot;).append(dimensionDesc.getTable()).append(&quot;' hasAll='true'&gt;&quot;).append(newLine); for (String column : dimensionDesc.getColumns()) { // add Attributes to stringbuffer addAttribute(sb, column); addHierarchy(hierSb, column); } sb.append(&quot;&lt;/Attributes&gt;&quot;).append(newLine); hierSb.append(&quot;&lt;/Hierarchy&gt;&quot;).append(newLine); hierSb.append(&quot;&lt;/Hierarchies&gt;&quot;).append(newLine); sb.append(hierSb); hierSb.delete(0, hierSb.length()); sb.append(&quot;&lt;/Dimension&gt;&quot;).append(newLine); } } public static Set&lt;String&gt; getColumns(DimensionDesc dimensionDesc) { Set&lt;String&gt; columns = new HashSet&lt;String&gt;(); if (dimensionDesc.getColumn() != null || dimensionDesc.getDerived() != null) { if (dimensionDesc.getColumn() != null) { // for (String column : dimensionDesc.getColumn()) { columns.add(dimensionDesc.getColumn()); // } } if (dimensionDesc.getDerived() != null) { columns.addAll(Arrays.asList(dimensionDesc.getDerived())); } } else { columns.add(dimensionDesc.getName()); } return columns; } public static StringBuffer addAttribute(StringBuffer sb, String attr) { sb.append(&quot;&lt;Attribute name='&quot;).append(attr).append(&quot;' keyColumn='&quot;).append(attr).append(&quot;' hasHierarchy='false'/&gt;&quot;).append(newLine); return sb; } public static void addHierarchy(StringBuilder sb, String attr) { sb.append(&quot;&lt;Level attribute='&quot;).append(attr).append(&quot;'/&gt;&quot;).append(newLine); } public static String dealTableName(String tableName) { if (tableName.contains(&quot;.&quot;)) return tableName.split(&quot;\\\\.&quot;)[1]; else return tableName; } public static String dealModelTableName(String tableName) { if (tableName.contains(&quot;.&quot;)) return tableName.split(&quot;\\\\.&quot;)[0]; else return tableName; } public static StringBuffer appendCube(StringBuffer sb, String cubeName, CubeDesc cubeDesc, DataModelDesc modelDesc) { sb.append(&quot;&lt;Cube name='&quot;).append(cubeName.split(&quot;#&quot;)[1].trim()).append(&quot;'&gt;&quot;).append(newLine); addCubeDimension(sb, modelDesc.getDimensions()); sb.append(&quot;&lt;MeasureGroups&gt;&quot;).append(newLine); Map&lt;String, Map&lt;String, MeasureDesc&gt;&gt; allMap = new HashMap(); MeasureDesc one = new MeasureDesc(); for (MeasureDesc measureDesc : cubeDesc.getMeasures()) { final String tableName = dealModelTableName(measureDesc.getFunction().getParameter().getValue()); final String columnName = dealTableName(measureDesc.getFunction().getParameter().getValue()); if (&quot;1&quot;.equals(tableName)) { one = measureDesc; } else { if (Objects.isNull(allMap.get(tableName))) { Map&lt;String, MeasureDesc&gt; map = new HashMap(); if (Objects.nonNull(one)) { map.put(columnName, one); one = null; } map.put(columnName, measureDesc); allMap.put(tableName, map); } else { allMap.get(tableName).put(columnName, measureDesc); } } } allMap.forEach((tableName, value) -&gt; { sb.append(&quot;&lt;MeasureGroup table='&quot;).append(dealTableName(tableName)).append(&quot;'&gt;&quot;).append(newLine); addDimensionLink(sb, modelDesc); sb.append(&quot;&lt;Measures&gt;&quot;).append(newLine); value.forEach((columnName, measureDesc) -&gt; { addMeasure(sb, measureDesc, getColumn(cubeDesc)); }); sb.append(&quot;&lt;/Measures&gt;&quot;).append(newLine); sb.append(&quot;&lt;/MeasureGroup&gt;&quot;).append(newLine); }); sb.append(&quot;&lt;/MeasureGroups&gt;&quot;).append(newLine); sb.append(&quot;&lt;/Cube&gt;&quot;).append(newLine); return sb; } public static void addCubeDimension(StringBuffer sb, List&lt;ModelDimensionDesc&gt; dimensionDescs) { sb.append(&quot;&lt;Dimensions&gt;&quot;).append(newLine); for (ModelDimensionDesc dimensionDesc : dimensionDescs) { sb.append(&quot;&lt;Dimension source='&quot;).append(dimensionDesc.getTable()).append(&quot;' visible='true'/&gt;&quot;).append(newLine); } sb.append(&quot;&lt;/Dimensions&gt;&quot;).append(newLine); } public static void addDimensionLink(StringBuffer sb, DataModelDesc modelDesc) { sb.append(&quot;&lt;DimensionLinks&gt;&quot;).append(newLine); for(ModelDimensionDesc dimensionDesc : modelDesc.getDimensions()) { if(dimensionDesc.getTable().contains(dealTableName(modelDesc.getRootFactTableName()))) { sb.append(&quot;&lt;FactLink dimension='&quot;).append(dimensionDesc.getTable()).append(&quot;'/&gt;&quot;).append(newLine); }else{ sb.append(&quot;&lt;ForeignKeyLink dimension='&quot;).append(dimensionDesc.getTable()).append(&quot;' foreignKeyColumn='&quot;).append(map.get(dimensionDesc.getTable())).append(&quot;'/&gt;&quot;).append(newLine); } } sb.append(&quot; &lt;/DimensionLinks&gt;&quot;).append(newLine); } public static StringBuffer addMeasure(StringBuffer sb, MeasureDesc measureDesc, String defaultColumn) { FunctionDesc funtionDesc = measureDesc.getFunction(); String aggregator = funtionDesc.getExpression().trim().toLowerCase(); //mondrian only have distinct-count if (aggregator.equals(&quot;count_distinct&quot;)) { aggregator = &quot;distinct-count&quot;; } if (funtionDesc.getParameter().getValue().equals(&quot;1&quot;)) { sb.append(&quot;&lt;Measure aggregator='&quot;).append(aggregator).append(&quot;' column='&quot;).append(dealTableName(defaultColumn)).append(&quot;' name='&quot;).append(measureDesc.getName()).append(&quot;' visible='true'/&gt;&quot;) .append(newLine); } else sb.append(&quot;&lt;Measure aggregator='&quot;).append(aggregator).append(&quot;' column='&quot;).append(dealTableName(funtionDesc.getParameter().getValue())).append(&quot;' name='&quot;).append(measureDesc.getName()).append(&quot;' visible='true'/&gt;&quot;) .append(newLine); return sb; } public static Set&lt;String&gt; getTables(List&lt;DimensionDesc&gt; dimensionDescList) { Set&lt;String&gt; tables = new HashSet&lt;String&gt;(); for (DimensionDesc dimensionDesc : dimensionDescList) { String table = dealTableName(dimensionDesc.getTable()); tables.add(table); } return tables; } public static String getColumn(CubeDesc cubeDesc) { RowKeyDesc rowKey = cubeDesc.getRowkey(); return rowKey.getRowKeyColumns()[0].getColumn(); } } &lt;?xml version='1.0'?&gt; &lt;Schema name='xiaowei' metamodelVersion='4.0'&gt; &lt;PhysicalSchema&gt; &lt;Table name='TBL_FARM_INCOME_STATICS'&gt; &lt;Key&gt; &lt;Column name='COMPANY_ID'/&gt; &lt;/Key&gt; &lt;/Table&gt; &lt;Table name='TBL_CUSTOMER'&gt; &lt;Key&gt; &lt;Column name='COMPANY_ID'/&gt; &lt;/Key&gt; &lt;/Table&gt; &lt;Link source='TBL_FARM_INCOME_STATICS' target='TBL_CUSTOMER'&gt; &lt;ForeignKey&gt; &lt;Column name='COMPANY_ID'/&gt; &lt;/ForeignKey&gt; &lt;/Link&gt; &lt;/PhysicalSchema&gt; &lt;Dimension name='TBL_FARM_INCOME_STATICS' table='TBL_FARM_INCOME_STATICS' key='COMPANY_ID'&gt; &lt;Attributes&gt; &lt;Attribute name='COMPANY_ID' keyColumn='COMPANY_ID' hasHierarchy='false'/&gt; &lt;Attribute name='INCOME_DATE' keyColumn='INCOME_DATE' hasHierarchy='false'/&gt; &lt;/Attributes&gt; &lt;Hierarchies&gt; &lt;Hierarchy name='TBL_FARM_INCOME_STATICS' allMemberName='All Warehouses'&gt; &lt;Level attribute='COMPANY_ID'/&gt; &lt;Level attribute='INCOME_DATE'/&gt; &lt;/Hierarchy&gt; &lt;/Hierarchies&gt; &lt;/Dimension&gt; &lt;Dimension name='TBL_CUSTOMER' table='TBL_CUSTOMER' key='COMPANY_ID'&gt; &lt;Attributes&gt; &lt;Attribute name='COMPANY_ID' keyColumn='COMPANY_ID' hasHierarchy='false'/&gt; &lt;Attribute name='CUSTOMER_TYPE' keyColumn='CUSTOMER_TYPE' hasHierarchy='false'/&gt; &lt;Attribute name='PHONE' keyColumn='PHONE' hasHierarchy='false'/&gt; &lt;/Attributes&gt; &lt;Hierarchies&gt; &lt;Hierarchy name='TBL_CUSTOMER' allMemberName='All Warehouses'&gt; &lt;Level attribute='COMPANY_ID'/&gt; &lt;Level attribute='CUSTOMER_TYPE'/&gt; &lt;Level attribute='PHONE'/&gt; &lt;/Hierarchy&gt; &lt;/Hierarchies&gt; &lt;/Dimension&gt; &lt;Cube name='tbl_farm_income_statics'&gt; &lt;Dimensions&gt; &lt;Dimension source='TBL_FARM_INCOME_STATICS' visible='true'/&gt; &lt;Dimension source='TBL_CUSTOMER' visible='true'/&gt; &lt;/Dimensions&gt; &lt;MeasureGroups&gt; &lt;MeasureGroup table='TBL_CUSTOMER'&gt; &lt;Measures&gt; &lt;Measure aggregator='sum' column='CONSUME_AMMOUT' name='CONSUME_AMMOUT_SUM' visible='true'/&gt; &lt;/Measures&gt; &lt;DimensionLinks&gt; &lt;FactLink dimension='TBL_FARM_INCOME_STATICS'/&gt; &lt;ForeignKeyLink dimension='TBL_CUSTOMER' foreignKeyColumn='COMPANY_ID'/&gt; &lt;/DimensionLinks&gt; &lt;/MeasureGroup&gt; &lt;MeasureGroup table='TBL_FARM_INCOME_STATICS'&gt; &lt;Measures&gt; &lt;Measure aggregator='count' column='COMPANY_ID' name='_COUNT_' visible='true'/&gt; &lt;Measure aggregator='sum' column='REFUND_AMOUNT' name='REFUND_AMOUNT_SUM' visible='true'/&gt; &lt;Measure aggregator='sum' column='COMMON_REFUND_AMOUNT' name='COMMON_REFUND_AMOUNT_SUM' visible='true'/&gt; &lt;Measure aggregator='sum' column='COMMON_INCOME' name='COMMON_INCOME_SUM' visible='true'/&gt; &lt;Measure aggregator='sum' column='SALE_AMOUNT' name='SALE_AMOUNT_SUM' visible='true'/&gt; &lt;Measure aggregator='sum' column='RENEW_AMOUNT' name='RENEW_AMOUNT_SUM' visible='true'/&gt; &lt;Measure aggregator='sum' column='TOTAL_AMOUNT' name='TOTAL_AMOUNT_SUM' visible='true'/&gt; &lt;/Measures&gt; &lt;DimensionLinks&gt; &lt;FactLink dimension='TBL_FARM_INCOME_STATICS'/&gt; &lt;ForeignKeyLink dimension='TBL_CUSTOMER' foreignKeyColumn='COMPANY_ID'/&gt; &lt;/DimensionLinks&gt; &lt;/MeasureGroup&gt; &lt;/MeasureGroups&gt; &lt;/Cube&gt; &lt;/Schema&gt; ","link":"https://tianxiawuhao.github.io/0wQHSTDWt/"},{"title":"同步 异步 阻塞 非阻塞","content":"IO操作 IO分两阶段（一旦拿到数据后就变成了数据操作，不再是IO）： 1.数据准备阶段 2.内核空间复制数据到用户进程缓冲区（用户空间）阶段 在操作系统中，程序运行的空间分为内核空间和用户空间。 应用程序都是运行在用户空间的，所以它们能操作的数据也都在用户空间。 阻塞IO和非阻塞IO的区别在于第一步发起IO请求是否会被阻塞： 如果阻塞直到完成那么就是传统的阻塞IO，如果不阻塞，那么就是非阻塞IO。 一般来讲： 阻塞IO模型、非阻塞IO模型、IO复用模型(select/poll/epoll)、信号驱动IO模型都属于同步IO，因为阶段2是阻塞的(尽管时间很短)。 同步IO和异步IO的区别就在于第二个步骤是否阻塞： 如果不阻塞，而是操作系统帮你做完IO操作再将结果返回给你，那么就是异步IO 同步和异步IO 阻塞和非阻塞IO 同步和异步IO的概念： 同步是用户线程发起I/O请求后需要等待或者轮询内核I/O操作完成后才能继续执行 异步是用户线程发起I/O请求后仍需要继续执行，当内核I/O操作完成后会通知用户线程，或者调用用户线程注册的回调函数 阻塞和非阻塞IO的概念： 阻塞是指I/O操作需要彻底完成后才能返回用户空间 非阻塞是指I/O操作被调用后立即返回一个状态值，无需等I/O操作彻底完成 同步与异步（线程间调用） 同步与异步是对应于调用者与被调用者，它们是线程之间的关系，两个线程之间要么是同步的，要么是异步的 同步操作时，调用者需要等待被调用者返回结果，才会进行下一步操作 而异步则相反，调用者不需要等待被调用者返回调用，即可进行下一步操作，被调用者通常依靠事件、回调等机制来通知调用者结果 阻塞与非阻塞（线程内调用） 阻塞与非阻塞是对同一个线程来说的，在某个时刻，线程要么处于阻塞，要么处于非阻塞 阻塞和非阻塞关注的是程序在等待调用结果（消息，返回值）时的状态： 阻塞调用是指调用结果返回之前，当前线程会被挂起。调用线程只有在得到结果之后才会返回。 非阻塞调用指在不能立刻得到结果之前，该调用不会阻塞当前线程。 同步与异步调用/线程/通信 同步就是两种东西通过一种机制实现步调一致，异步是两种东西不必步调一致 一、同步调用与异步调用： 在用在调用场景中，无非是对调用结果的不同处理。 同步调用就是调用一但返回，就能知道结果，而异步是返回时不一定知道结果，还得通过其他机制来获知结果，如： a. 状态 b. 通知 c. 回调函数 二、同步线程与异步线程： 同步线程：即两个线程步调要一致，其中一个线程可能要阻塞等待另外一个线程的运行，要相互协商。快的阻塞一下等到慢的步调一致。 异步线程：步调不用一致，各自按各自的步调运行，不受另一个线程的影响。 三、同步通信与异步通信： 同步和异步是指：发送方和接收方是否协调步调一致 同步通信是指：发送方和接收方通过一定机制，实现收发步调协调。 如：发送方发出数据后，等接收方发回响应以后才发下一个数据包的通讯方式 异步通信是指：发送方的发送不管接收方的接收状态。 如：发送方发出数据后，不等接收方发回响应，接着发送下个数据包的通讯方式。 阻塞可以是实现同步的一种手段！例如两个东西需要同步，一旦出现不同步情况，我就阻塞快的一方，使双方达到同步。 同步是两个对象之间的关系，而阻塞是一个对象的状态。 四种组合方式 同步阻塞方式： 发送方发送请求之后一直等待响应。 接收方处理请求时进行的IO操作如果不能马上等到返回结果，就一直等到返回结果后，才响应发送方，期间不能进行其他工作。 同步非阻塞方式： 发送方发送请求之后，一直等待响应。 接受方处理请求时进行的IO操作如果不能马上的得到结果，就立即返回，取做其他事情。 但是由于没有得到请求处理结果，不响应发送方，发送方一直等待。 当IO操作完成以后，将完成状态和结果通知接收方，接收方再响应发送方，发送方才进入下一次请求过程。（实际不应用） 异步阻塞方式： 发送方向接收方请求后，不等待响应，可以继续其他工作。 接收方处理请求时进行IO操作如果不能马上得到结果，就一直等到返回结果后，才响应发送方，期间不能进行其他操作。 （实际不应用） 异步非阻塞方式： 发送方向接收方请求后，不等待响应，可以继续其他工作。 接收方处理请求时进行IO操作如果不能马上得到结果，也不等待，而是马上返回去做其他事情。 当IO操作完成以后，将完成状态和结果通知接收方，接收方再响应发送方。（效率最高） ","link":"https://tianxiawuhao.github.io/1SOcADwUR/"},{"title":"NIO-FileChannel","content":"NIO介绍 在讲解Channel之前，首先了解一下NIO， Java NIO全称java non-blocking IO，是从Java 1.4版本开始引入的一个新的IO API（New IO），可以替代标准的Java IO API，NIO与原来的IO有同样的作用和目的，但是使用的方式完全不同。IO与 NIO区别： IO NIO 面向流（Stream Orientend） 面向缓冲区（Buffer Orientend） 阻塞IO（Blocking IO ) 非阻塞IO（Non Blocking IO） 选择器（Selector） NIO支持面向缓冲区的、基于通道的IO操作并以更加高效的方式进行文件的读写操作，其核心API为Channel(通道)，Buffer(缓冲区), Selector(选择器)。Channel负责传输，Buffer负责存储 。 缓冲区 public class BioTest { @Test public void test1() { //1.初始化缓冲区数组 ByteBuffer bf = ByteBuffer.allocate(1024); System.out.println(&quot;==========allocate============&quot;); System.out.println(bf.position()); System.out.println(bf.limit()); System.out.println(bf.capacity()); //2.put插入数据 bf.put(&quot;asasas&quot;.getBytes()); System.out.println(&quot;==========put============&quot;); System.out.println(bf.position()); System.out.println(bf.limit()); System.out.println(bf.capacity()); //3.改为读状态 bf.flip(); System.out.println(&quot;==========flip============&quot;); System.out.println(bf.position()); System.out.println(bf.limit()); System.out.println(bf.capacity()); //4.获取缓冲区数据 final byte[] bytes = new byte[bf.limit()]; bf.get(bytes); System.out.println(&quot;==========get============&quot;); System.out.println(new String(bytes,0,bytes.length)); System.out.println(bf.position()); System.out.println(bf.limit()); System.out.println(bf.capacity()); //5.重置读状态 bf.rewind(); System.out.println(&quot;==========rewind============&quot;); System.out.println(bf.position()); System.out.println(bf.limit()); System.out.println(bf.capacity()); //6.清除数据标识 bf.clear(); System.out.println(&quot;==========clear============&quot;); System.out.println(bf.position()); System.out.println(bf.limit()); System.out.println(bf.capacity()); } } 直接缓冲区与非直接缓冲区： 非直接缓冲区：通过 allocate() 方法分配缓冲区，将缓冲区建立在 JVM 的内存中 直接缓冲区：通过 allocateDirect() 方法分配直接缓冲区，将缓冲区建立在物理内存中。可以提高效率 字节缓冲区要么是直接的，要么是非直接的。如果为直接字节缓冲区，则 Java 虚拟机会尽最大努力直接在机 此缓冲区上执行本机 I/O 操作。也就是说，在每次调用基础操作系统的一个本机 I/O 操作之前（或之后）， 虚拟机都会尽量避免将缓冲区的内容复制到中间缓冲区中（或从中间缓冲区中复制内容）。 直接字节缓冲区可以通过调用此类的 allocateDirect() 工厂方法 来创建。此方法返回的 缓冲区进行分配和取消分配所需成本通常高于非直接缓冲区 。直接缓冲区的内容可以驻留在常规的垃圾回收堆之外，因此，它们对应用程序的内存需求量造成的影响可能并不明显。所以，建议将直接缓冲区主要分配给那些易受基础系统的本机 I/O 操作影响的大型、持久的缓冲区。一般情况下，最好仅在直接缓冲区能在程序性能方面带来明显好处时分配它们。 直接字节缓冲区还可以过 通过FileChannel 的 map() 方法 将文件区域直接映射到内存中来创建 。该方法返回MappedByteBuffer 。Java 平台的实现有助于通过 JNI 从本机代码创建直接字节缓冲区。如果以上这些缓冲区中的某个缓冲区实例指的是不可访问的内存区域，则试图访问该区域不会更改该缓冲区的内容，并且将会在访问期间或稍后的某个时间导致抛出不确定的异常。 字节缓冲区是直接缓冲区还是非直接缓冲区可通过调用其 isDirect() 方法来确定。提供此方法是为了能够在性能关键型代码中执行显式缓冲区管理。 非直接缓冲区： 直接缓冲区： 通道（Channel ） 通道表示打开到 IO 设备(例如：文件、套接字)的连接。若需要使用 NIO 系统，需要获取用于连接 IO 设备的通道以及用于容纳数据的缓冲区。然后操作缓冲区，对数据进行处理。 Channel相比IO中的Stream更加高效，可以异步双向传输，但是必须和buffer一起使用。 主要实现类 FileChannel，读写文件中的数据。 SocketChannel，通过TCP读写网络中的数据。 ServerSockectChannel，监听新进来的TCP连接，像Web服务器那样。对每一个新进来的连接都会创建一个SocketChannel。 DatagramChannel，通过UDP读写网络中的数据。 Channel聚集(gather)写入 聚集写入（ Gathering Writes）是指将多个 Buffer 中的数据“聚集”到 Channel。 特别注意：按照缓冲区的顺序，写入 position 和 limit 之间的数据到 Channel 。 Channel分散(scatter)读取 分散读取（ Scattering Reads）是指从 Channel 中读取的数据“分散” 到多个 Buffer 中。 特别注意：按照缓冲区的顺序，从 Channel 中读取的数据依次将 Buffer 填满。 “零拷贝”（FileChannel的transferTo和transferFrom） Java NIO中提供的FileChannel拥有transferTo和transferFrom两个方法，可直接把FileChannel中的数据拷贝到另外一个Channel，或者直接把另外一个Channel中的数据拷贝到FileChannel。该接口常被用于高效的网络/文件的数据传输和大文件拷贝。在操作系统支持的情况下，通过该方法传输数据并不需要将源数据从内核态拷贝到用户态，再从用户态拷贝到目标通道的内核态，同时也避免了两次用户态和内核态间的上下文切换，也即使用了“零拷贝”，所以其性能一般高于Java IO中提供的方法。 代码案例 /** * @author wuhao * @desc 本地io: * FileInputStreanm/FileOutputStream * RandomAccessFile * 网络io: * Socket * ServerSocket * DatagramSocket * @date 2021-09-17 15:22:26 */ public class ChannelTest { //利用通道完成文件的复制,非直接缓冲区 @Test @SneakyThrows public void test(){ FileInputStream fis = new FileInputStream(&quot;D:\\\\1.jpg&quot;); FileOutputStream fos = new FileOutputStream(&quot;D:\\\\2.jpg&quot;); //获取通道 FileChannel inChannel = fis.getChannel(); FileChannel outChannel = fos.getChannel(); //分配指定大小缓存区 ByteBuffer buff = ByteBuffer.allocate(1024);// position 0 ,limit 1024 //将通道的数据存入缓存区 while(inChannel.read(buff)!=-1){// position 1024 ,limit 1024 ,相当于put //切换读模式 buff.flip();//position 0 ,limit 1024 //将缓存去的数据写入通道 outChannel.write(buff);//position 1024 ,limit 1024,相当于get //清空缓冲区 buff.clear();//position 0 ,limit 1024 } outChannel.close(); inChannel.close(); fis.close(); fos.close(); } //利用通道完成文件的复制,直接缓冲区,利用物理内存映射文件 //会出现文件复制完了，但程序还没结束，原因是JVM资源还在用，当垃圾回收机制回收之后程序就会结束,不稳定 @Test @SneakyThrows public void test1(){ FileChannel inChannel = FileChannel.open(Paths.get(&quot;D:\\\\1.jpg&quot;), StandardOpenOption.READ); FileChannel outChannel = FileChannel.open(Paths.get(&quot;D:\\\\4.jpg&quot;), StandardOpenOption.READ,StandardOpenOption.WRITE,StandardOpenOption.CREATE); //内存映射文件 MappedByteBuffer inMapBuff = inChannel.map(FileChannel.MapMode.READ_ONLY, 0, inChannel.size());//==allocateDirect MappedByteBuffer outMapBuff = outChannel.map(FileChannel.MapMode.READ_WRITE, 0, inChannel.size()); byte[] by = new byte[inMapBuff.limit()]; inMapBuff.get(by); outMapBuff.put(by); outChannel.close(); inChannel.close(); } /** * 从源信道读取字节到这个通道的文件中。如果源通道的剩余空间小于 count 个字节，则所传输的字节数要小于请求的字节数。这种方法可能比从源通道读取并写入此通道的简单循环更有效率。 * @param SRC 源通道 * @param position 调动开始的文件内的位置，必须是非负的 * @param count 要传输的最大字节数，必须是非负 * @return 传输文件的大小（单位字节），可能为零， * public abstract long transferFrom(ReadableByteChannel src, long position, long count) throws IOException; */ //复制图片，利用直接缓存区 @Test @SneakyThrows public void test2(){ FileChannel inChannel = FileChannel.open(Paths.get(&quot;D:\\\\1.jpg&quot;), StandardOpenOption.READ); FileChannel outChannel = FileChannel.open(Paths.get(&quot;D:\\\\2.jpg&quot;), StandardOpenOption.READ, StandardOpenOption.WRITE, StandardOpenOption.CREATE); outChannel.transferFrom(inChannel, 0, inChannel.size()); inChannel.close(); outChannel.close(); } /** * 将字节从这个通道的文件传输到给定的可写字节通道。 * @param position 调动开始的文件内的位置，必须是非负的 * @param count 要传输的最大字节数，必须是非负 * @param target 目标通道 * @return 传输文件的大小（单位字节），可能为零， * public abstract long transferTo(long position, long count, WritableByteChannel target) throws IOException; */ //复制图片，利用直接缓存区 @Test @SneakyThrows public void test3(){ FileChannel inChannel = FileChannel.open(Paths.get(&quot;D:\\\\1.jpg&quot;), StandardOpenOption.READ); FileChannel outChannel = FileChannel.open(Paths.get(&quot;D:\\\\3.jpg&quot;), StandardOpenOption.READ, StandardOpenOption.WRITE, StandardOpenOption.CREATE); inChannel.transferTo(0, inChannel.size(), outChannel); inChannel.close(); outChannel.close(); } // 分散读取聚集写入实现文件复制 @Test @SneakyThrows public void test4(){ RandomAccessFile randomAccessFile = null; RandomAccessFile randomAccessFile1 = null; FileChannel inChannel = null; FileChannel outChannel = null; try { randomAccessFile = new RandomAccessFile(new File(&quot;d:\\\\old.txt&quot;), &quot;rw&quot;); randomAccessFile1 = new RandomAccessFile(new File(&quot;d:\\\\new.txt&quot;), &quot;rw&quot;); inChannel = randomAccessFile.getChannel(); outChannel = randomAccessFile1.getChannel(); // 分散为三个bytebuffer读取,capcity要设置的足够大，不然如果文件太大，会导致复制的内容不完整 ByteBuffer byteBuffer1 = ByteBuffer.allocate(1024); ByteBuffer byteBuffer2 = ByteBuffer.allocate(1024); ByteBuffer byteBuffer3 = ByteBuffer.allocate(10240); ByteBuffer[] bbs = new ByteBuffer[]{byteBuffer1,byteBuffer2,byteBuffer3}; inChannel.read(bbs);// 分散读取 // 切换为写入模式 for (int i = 0; i &lt; bbs.length; i++) { bbs[i].flip(); } outChannel.write(bbs); } catch (Exception ex) { ex.printStackTrace(); } } } ","link":"https://tianxiawuhao.github.io/7Wmx9Q2PP/"},{"title":"服务器变慢诊断命令","content":"top命令详解 第一行，任务队列信息，同 uptime 命令的执行结果 系统时间：07:27:05 运行时间：up 1:57 min, 当前登录用户： 3 user 负载均衡(uptime) load average: 0.00, 0.00, 0.00 average后面的三个数分别是1分钟、5分钟、15分钟的负载情况。 load average数据是每隔5秒钟检查一次活跃的进程数，然后按特定算法计算出的数值。如果这个数除以逻辑CPU的数量，结果高于5的时候就表明系统在超负荷运转了 第二行，Tasks — 任务（进程） 总进程:150 total, 运行:1 running, 休眠:149 sleeping, 停止: 0 stopped, 僵尸进程: 0 zombie 第三行，cpu状态信息 0.0%us【user space】— 用户空间占用CPU的百分比。 0.3%sy【sysctl】— 内核空间占用CPU的百分比。 0.0%ni【】— 改变过优先级的进程占用CPU的百分比 99.7%id【idolt】— 空闲CPU百分比 0.0%wa【wait】— IO等待占用CPU的百分比 0.0%hi【Hardware IRQ】— 硬中断占用CPU的百分比 0.0%si【Software Interrupts】— 软中断占用CPU的百分比 第四行,内存状态 1003020k total, 234464k used, 777824k free, 24084k buffers【缓存的内存量】 第五行，swap交换分区信息 2031612k total, 536k used, 2031076k free, 505864k cached【缓冲的交换区总量】 备注： 可用内存=free + buffer + cached 对于内存监控，在top里我们要时刻监控第五行swap交换分区的used，如果这个数值在不断的变化，说明内核在不断进行内存和swap的数据交换，这是真正的内存不够用了。 第四行中使用中的内存总量（used）指的是现在系统内核控制的内存数， 第四行中空闲内存总量（free）是内核还未纳入其管控范围的数量。 纳入内核管理的内存不见得都在使用中，还包括过去使用过的现在可以被重复利用的内存，内核并不把这些可被重新使用的内存交还到free中去，因此在linux上free内存会越来越少，但不用为此担心。 第六行，空行 第七行以下：各进程（任务）的状态监控 PID — 进程id USER — 进程所有者 PR — 进程优先级 NI — nice值。负值表示高优先级，正值表示低优先级 VIRT — 进程使用的虚拟内存总量，单位kb。VIRT=SWAP+RES RES — 进程使用的、未被换出的物理内存大小，单位kb。RES=CODE+DATA SHR — 共享内存大小，单位kb S —进程状态。D=不可中断的睡眠状态 R=运行 S=睡眠 T=跟踪/停止 Z=僵尸进程 %CPU — 上次更新到现在的CPU时间占用百分比 %MEM — 进程使用的物理内存百分比 TIME+ — 进程使用的CPU时间总计，单位1/100秒 COMMAND — 进程名称（命令名/命令行） 详解 **VIRT：virtual memory usage 虚拟内存 **1、进程“需要的”虚拟内存大小，包括进程使用的库、代码、数据等 2、假如进程申请100m的内存，但实际只使用了10m，那么它会增长100m，而不是实际的使用量 RES：resident memory usage 常驻内存 1、进程当前使用的内存大小，但不包括swap out 2、包含其他进程的共享 3、如果申请100m的内存，实际使用10m，它只增长10m，与VIRT相反 4、关于库占用内存的情况，它只统计加载的库文件所占内存大小 SHR：shared memory 共享内存 1、除了自身进程的共享内存，也包括其他进程的共享内存 2、虽然进程只使用了几个共享库的函数，但它包含了整个共享库的大小 3、计算某个进程所占的物理内存大小公式：RES – SHR 4、swap out后，它将会降下来 DATA 1、数据占用的内存。如果top没有显示，按f键可以显示出来。 2、真正的该程序要求的数据空间，是真正在运行中要使用的。 top 运行中可以通过 top 的内部命令对进程的显示方式进行控制。内部命令如下： s – 改变画面更新频率 l – 关闭或开启第一部分第一行 top 信息的表示 t – 关闭或开启第一部分第二行 Tasks 和第三行 Cpus 信息的表示 m – 关闭或开启第一部分第四行 Mem 和 第五行 Swap 信息的表示 N – 以 PID 的大小的顺序排列表示进程列表 P – 以 CPU 占用率大小的顺序排列进程列表 M – 以内存占用率大小的顺序排列进程列表 h – 显示帮助 n – 设置在进程列表所显示进程的数量 q – 退出 top s – 改变画面更新周期 vmstat命令详解 vmstat命令是最常见的Linux/Unix监控工具，可以展现给定时间间隔的服务器的状态值,包括服务器的CPU使用率，内存使用，虚拟内存交换情况,IO读写情况。这个命令是我查看Linux/Unix最喜爱的命令，一个是Linux/Unix都支持，二是相比top，我可以看到整个机器的CPU,内存,IO的使用情况，而不是单单看到各个进程的CPU使用率和内存使用率(使用场景不一样)。 一般vmstat工具的使用是通过两个数字参数来完成的，第一个参数是采样的时间间隔数，单位是秒，第二个参数是采样的次数，如: root@ubuntu:~# vmstat 2 1 procs -----------memory---------- ---swap-- -----io---- -system-- ----cpu---- r b swpd free buff cache si so bi bo in cs us sy id wa 1 0 0 3498472 315836 3819540 0 0 0 1 2 0 0 0 100 0 2表示每个两秒采集一次服务器状态，1表示只采集一次。 实际上，在应用过程中，我们会在一段时间内一直监控，不想监控直接结束vmstat就行了,例如: root@ubuntu:~# vmstat 2 procs -----------memory---------- ---swap-- -----io---- -system-- ----cpu---- r b swpd free buff cache si so bi bo in cs us sy id wa 1 0 0 3499840 315836 3819660 0 0 0 1 2 0 0 0 100 0 0 0 0 3499584 315836 3819660 0 0 0 0 88 158 0 0 100 0 0 0 0 3499708 315836 3819660 0 0 0 2 86 162 0 0 100 0 0 0 0 3499708 315836 3819660 0 0 0 10 81 151 0 0 100 0 1 0 0 3499732 315836 3819660 0 0 0 2 83 154 0 0 100 0 这表示vmstat每2秒采集数据，一直采集，直到我结束程序，这里采集了5次数据我就结束了程序。 参数详解 r 表示运行队列(就是说多少个进程真的分配到CPU)，我测试的服务器目前CPU比较空闲，没什么程序在跑，当这个值超过了CPU数目，就会出现CPU瓶颈了。这个也和top的负载有关系，一般负载超过了3就比较高，超过了5就高，超过了10就不正常了，服务器的状态很危险。top的负载类似每秒的运行队列。如果运行队列过大，表示你的CPU很繁忙，一般会造成CPU使用率很高。 b 表示阻塞的进程,这个不多说，进程阻塞，大家懂的。 swpd 虚拟内存已使用的大小，如果大于0，表示你的机器物理内存不足了，如果不是程序内存泄露的原因，那么你该升级内存了或者把耗内存的任务迁移到其他机器。 free 空闲的物理内存的大小，我的机器内存总共8G，剩余3415M。 buff Linux/Unix系统是用来存储，目录里面有什么内容，权限等的缓存，我本机大概占用300多M cache cache直接用来记忆我们打开的文件,给文件做缓冲，我本机大概占用300多M(这里是Linux/Unix的聪明之处，把空闲的物理内存的一部分拿来做文件和目录的缓存，是为了提高 程序执行的性能，当程序使用内存时，buffer/cached会很快地被使用。) si 每秒从磁盘读入虚拟内存的大小，如果这个值大于0，表示物理内存不够用或者内存泄露了，要查找耗内存进程解决掉。我的机器内存充裕，一切正常。 so 每秒虚拟内存写入磁盘的大小，如果这个值大于0，同上。 bi 块设备每秒接收的块数量，这里的块设备是指系统上所有的磁盘和其他块设备，默认块大小是1024byte，我本机上没什么IO操作，所以一直是0，但是我曾在处理拷贝大量数据(2-3T)的机器上看过可以达到140000/s，磁盘写入速度差不多140M每秒 bo 块设备每秒发送的块数量，例如我们读取文件，bo就要大于0。bi和bo一般都要接近0，不然就是IO过于频繁，需要调整。 in 每秒CPU的中断次数，包括时间中断 cs 每秒上下文切换次数，例如我们调用系统函数，就要进行上下文切换，线程的切换，也要进程上下文切换，这个值要越小越好，太大了，要考虑调低线程或者进程的数目,例如在apache和nginx这种web服务器中，我们一般做性能测试时会进行几千并发甚至几万并发的测试，选择web服务器的进程可以由进程或者线程的峰值一直下调，压测，直到cs到一个比较小的值，这个进程和线程数就是比较合适的值了。系统调用也是，每次调用系统函数，我们的代码就会进入内核空间，导致上下文切换，这个是很耗资源，也要尽量避免频繁调用系统函数。上下文切换次数过多表示你的CPU大部分浪费在上下文切换，导致CPU干正经事的时间少了，CPU没有充分利用，是不可取的。 us 用户CPU时间，我曾经在一个做加密解密很频繁的服务器上，可以看到us接近100,r运行队列达到80(机器在做压力测试，性能表现不佳)。 sy 系统CPU时间，如果太高，表示系统调用时间长，例如是IO操作频繁。 id 空闲 CPU时间，一般来说，id + us + sy = 100,一般我认为id是空闲CPU使用率，us是用户CPU使用率，sy是系统CPU使用率。 wt 等待IO CPU时间。 pid命令详解 pidstat是sysstat工具的一个命令，用于监控全部或指定进程的cpu、内存、线程、设备IO等系统资源的占用情况。pidstat首次运行时显示自系统启动开始的各项统计信息，之后运行pidstat将显示自上次运行该命令以后的统计信息。用户可以通过指定统计的次数和时间来获得所需的统计信息。 pidstat 的用法： pidstat [ 选项 ] [ &lt;时间间隔&gt; ] [ &lt;次数&gt; ] 如下图： 常用的参数： -u：默认的参数，显示各个进程的cpu使用统计 -r：显示各个进程的内存使用统计 -d：显示各个进程的IO使用情况 -p：指定进程号 -w：显示每个进程的上下文切换情况 -t：显示选择任务的线程的统计信息外的额外信息 -T { TASK | CHILD | ALL } 这个选项指定了pidstat监控的。TASK表示报告独立的task，CHILD关键字表示报告进程下所有线程统计信息。ALL表示报告独立的task和task下面的所有线程。 注意：task和子线程的全局的统计信息和pidstat选项无关。这些统计信息不会对应到当前的统计间隔，这些统计信息只有在子线程kill或者完成的时候才会被收集。 -V：版本号 -h：在一行上显示了所有活动，这样其他程序可以容易解析。 -I：在SMP环境，表示任务的CPU使用率/内核数量 -l：显示命令名和所有参数 pidstat pidstat -u -p ALL pidstat 和 pidstat -u -p ALL 是等效的。 pidstat 默认显示了所有进程的cpu使用率。 参数详解 PID：进程ID %usr：进程在用户空间占用cpu的百分比 %system：进程在内核空间占用cpu的百分比 %guest：进程在虚拟机占用cpu的百分比 %CPU：进程占用cpu的百分比 CPU：处理进程的cpu编号 Command：当前进程对应的命令 free命令详解 free 命令显示系统内存的使用情况，包括物理内存、交换内存(swap)和内核缓冲区内存。 如果加上 -h 选项，输出的结果会友好很多： 有时我们需要持续的观察内存的状况，此时可以使用 -s 选项并指定间隔的秒数： $ free -h -s 3 上面的命令每隔 3 秒输出一次内存的使用情况，直到你按下 ctrl + c。 由于 free 命令本身比较简单，所以本文的重点会放在如何通过 free 命令了解系统当前的内存使用状况。 输出说明 Mem 行(第二行)是内存的使用情况。 Swap 行(第三行)是交换空间的使用情况。 total 列显示系统总的可用物理内存和交换空间大小。 used 列显示已经被使用的物理内存和交换空间。 free 列显示还有多少物理内存和交换空间可用使用。 shared 列显示被共享使用的物理内存大小。 buff/cache 列显示被 buffer 和 cache 使用的物理内存大小。 available 列显示还可以被应用程序使用的物理内存大小。 df命令详解 Linux df（英文全拼：disk free） 命令用于显示目前在 Linux 系统上的文件系统磁盘使用情况统计。 语法 df [选项]... [FILE]... 文件-a, --all 包含所有的具有 0 Blocks 的文件系统 文件--block-size={SIZE} 使用 {SIZE} 大小的 Blocks 文件-h, --human-readable 使用人类可读的格式(预设值是不加这个选项的...) 文件-H, --si 很像 -h, 但是用 1000 为单位而不是用 1024 文件-i, --inodes 列出 inode 资讯，不列出已使用 block 文件-k, --kilobytes 就像是 --block-size=1024 文件-l, --local 限制列出的文件结构 文件-m, --megabytes 就像 --block-size=1048576 文件--no-sync 取得资讯前不 sync (预设值) 文件-P, --portability 使用 POSIX 输出格式 文件--sync 在取得资讯前 sync 文件-t, --type=TYPE 限制列出文件系统的 TYPE 文件-T, --print-type 显示文件系统的形式 文件-x, --exclude-type=TYPE 限制列出文件系统不要显示 TYPE 文件-v (忽略) 文件--help 显示这个帮手并且离开 文件--version 输出版本资讯并且离开 实例 显示文件系统的磁盘使用情况统计： # df Filesystem 1K-blocks Used Available Use% Mounted on /dev/sda6 29640780 4320704 23814388 16% / udev 1536756 4 1536752 1% /dev tmpfs 617620 888 616732 1% /run none 5120 0 5120 0% /run/lock none 1544044 156 1543888 1% /run/shm 第一列指定文件系统的名称，第二列指定一个特定的文件系统1K-块1K是1024字节为单位的总内存。用和可用列正在使用中，分别指定的内存量。 使用列指定使用的内存的百分比，而最后一栏&quot;安装在&quot;指定的文件系统的挂载点。 df也可以显示磁盘使用的文件系统信息： # df test Filesystem 1K-blocks Used Available Use% Mounted on /dev/sda6 29640780 4320600 23814492 16% / 用一个-i选项的df命令的输出显示inode信息而非块使用量。 df -i Filesystem Inodes IUsed IFree IUse% Mounted on /dev/sda6 1884160 261964 1622196 14% / udev 212748 560 212188 1% /dev tmpfs 216392 477 215915 1% /run none 216392 3 216389 1% /run/lock none 216392 8 216384 1% /run/shm 显示所有的信息: # df --total Filesystem 1K-blocks Used Available Use% Mounted on /dev/sda6 29640780 4320720 23814372 16% / udev 1536756 4 1536752 1% /dev tmpfs 617620 892 616728 1% /run none 5120 0 5120 0% /run/lock none 1544044 156 1543888 1% /run/shm total 33344320 4321772 27516860 14% 我们看到输出的末尾，包含一个额外的行，显示总的每一列。 -h选项，通过它可以产生可读的格式df命令的输出： # df -h Filesystem Size Used Avail Use% Mounted on /dev/sda6 29G 4.2G 23G 16% / udev 1.5G 4.0K 1.5G 1% /dev tmpfs 604M 892K 603M 1% /run none 5.0M 0 5.0M 0% /run/lock none 1.5G 156K 1.5G 1% /run/shm 我们可以看到输出显示的数字形式的'G'（千兆字节），&quot;M&quot;（兆字节）和&quot;K&quot;（千字节）。 这使输出容易阅读和理解，从而使显示可读的。请注意，第二列的名称也发生了变化，为了使显示可读的&quot;大小&quot;。 iostat命令详解 用法：iostat [ 选项 ] [ &lt;时间间隔&gt; [ &lt;次数&gt; ]] 常用选项说明： -c：只显示系统CPU统计信息，即单独输出avg-cpu结果，不包括device结果 -d：单独输出Device结果，不包括cpu结果 -k/-m：输出结果以kB/mB为单位，而不是以扇区数为单位 -x:输出更详细的io设备统计信息 interval/count：每次输出间隔时间，count表示输出次数，不带count表示循环输出 说明：更多选项使用使用man iostat查看 实例 1、iostat，结果为从系统开机到当前执行时刻的统计信息 输出含义： avg-cpu: 总体cpu使用情况统计信息，对于多核cpu，这里为所有cpu的平均值。重点关注iowait值，表示CPU用于等待io请求的完成时间。 Device: 各磁盘设备的IO统计信息。各列含义如下： Device: 以sdX形式显示的设备名称 tps: 每秒进程下发的IO读、写请求数量 KB_read/s: 每秒从驱动器读入的数据量，单位为K。 KB_wrtn/s: 每秒从驱动器写入的数据量，单位为K。 KB_read: 读入数据总量，单位为K。 KB_wrtn: 写入数据总量，单位为K。 2、iostat -x -k -d 1 2。每隔1S输出磁盘IO的详细详细，总共采样2次。 以上各列的含义如下： rrqm/s: 每秒对该设备的读请求被合并次数，文件系统会对读取同块(block)的请求进行合并 wrqm/s: 每秒对该设备的写请求被合并次数 r/s: 每秒完成的读次数 w/s: 每秒完成的写次数 rkB/s: 每秒读数据量(kB为单位) wkB/s: 每秒写数据量(kB为单位) avgrq-sz:平均每次IO操作的数据量(扇区数为单位) avgqu-sz: 平均等待处理的IO请求队列长度 await: 平均每次IO请求等待时间(包括等待时间和处理时间，毫秒为单位) svctm: 平均每次IO请求的处理时间(毫秒为单位) %util: 采用周期内用于IO操作的时间比率，即IO队列非空的时间比率 重点关注参数 1、iowait% 表示CPU等待IO时间占整个CPU周期的百分比，如果iowait值超过50%，或者明显大于%system、%user以及%idle，表示IO可能存在问题。 2、avgqu-sz 表示磁盘IO队列长度，即IO等待个数。 3、await 表示每次IO请求等待时间，包括等待时间和处理时间 4、svctm 表示每次IO请求处理的时间 5、%util 表示磁盘忙碌情况，一般该值超过80%表示该磁盘可能处于繁忙状态。 ifstat命令详解 统计网络接口流量状态 下载 wget http://gael.roualland.free.fr/ifstat/ifstat-1.1.tar.gz 编译安装 tar -zxvf ifstat-1.1.tar.gz cd ifstat-1.1 ./configure make make install # 默认会安装到/usr/local/bin/目录中 注释：执行which ifstat输出/usr/local/bin/ifstat 选项 -l 监测环路网络接口（lo）。缺省情况下，ifstat监测活动的所有非环路网络接口。经使用发现，加上-l参数能监测所有的网络接口的信息，而不是只监测 lo的接口信息，也就是说，加上-l参数比不加-l参数会多一个lo接口的状态信息。 -a 监测能检测到的所有网络接口的状态信息。使用发现，比加上-l参数还多一个plip0的接口信息，搜索一下发现这是并口（网络设备中有一 个叫PLIP (Parallel Line Internet Protocol). 它提供了并口...） -z 隐藏流量是无的接口，例如那些接口虽然启动了但是未用的 -i 指定要监测的接口,后面跟网络接口名 -s 等于加-d snmp:[comm@][#]host[/nn]] 参数，通过SNMP查询一个远程主机 -h 显示简短的帮助信息 -n 关闭显示周期性出现的头部信息（也就是说，不加-n参数运行ifstat时最顶部会出现网络接口的名称，当一屏显示不下时，会再一次出现接口的名称，提示我们显示的流量信息具体是哪个网络接口的。加上-n参数把周期性的显示接口名称关闭，只显示一次） -t 在每一行的开头加一个时间 戳（能告诉我们具体的时间） -T 报告所有监测接口的全部带宽（最后一列有个total，显示所有的接口的in流量和所有接口的out流量，简单的把所有接口的in流量相加,out流量相 加） -w 用指定的列宽，而不是为了适应接口名称的长度而去自动放大列宽 -W 如果内容比终端窗口的宽度还要宽就自动换行 -S 在同一行保持状态更新（不滚动不换行）注：如果不喜欢屏幕滚动则此项非常方便，与bmon的显示方式类似 -b 用kbits/s显示带宽而不是kbytes/s -q 安静模式，警告信息不出现 -v 显示版本信息 -d 指定一个驱动来收集状态信息 实例 默认使用 #ifstat eth0 eth1 KB/s in KB/s out KB/s in KB/s out 0.07 0.20 0.00 0.00 0.07 0.15 0.58 0.00 默认ifstat不监控回环接口，显示的流量单位是KB。 ifstat -tT time eth0 eth1 eth2 eth3 Total HH:MM:ss KB/s in KB/s out KB/s in KB/s out KB/s in KB/s out KB/s in KB/s out KB/s in KB/s out 16:53:04 0.84 0.62 1256.27 1173.05 0.12 0.18 0.00 0.00 1257.22 1173.86 16:53:05 0.57 0.40 0.57 0.76 0.00 0.00 0.00 0.00 1.14 1.17 16:53:06 1.58 0.71 0.42 0.78 0.00 0.00 0.00 0.00 2.01 1.48 16:53:07 0.57 0.40 1.91 2.61 0.00 0.00 0.00 0.00 2.48 3.01 16:53:08 0.73 0.40 924.02 1248.91 0.00 0.00 0.00 0.00 924.76 1249.31 监控所有网络接口 ifstat -a lo eth0 eth1 KB/s in KB/s out KB/s in KB/s out KB/s in KB/s out 0.00 0.00 0.28 0.58 0.06 0.06 0.00 0.00 1.41 1.13 0.00 0.00 0.61 0.61 0.26 0.23 0.00 0.00 ","link":"https://tianxiawuhao.github.io/Um55sd5Z3/"},{"title":"CAS/ABA/AtomicReference","content":"锁是用来做并发最简单的方式，当然代价也是最高的。 独占锁是一种悲观锁，synchronized就是一种独占锁；它假设最坏的情况，并且只有在确保其它线程不会造成干扰的情况下执行，会导致其它所有需要锁的线程挂起直到持有锁的线程释放锁。 所谓乐观锁就是每次不加锁,假设没有冲突而去完成某项操作;如果发生冲突了那就去重试，直到成功为止。 CAS(Compare And Swap)是一种有名的无锁算法。CAS算法是乐观锁的一种实现。CAS有3个操作数，内存值V，旧的预期值A，要修改的新值B。当且仅当预期值A和内存值V相同时，将内存值V修改为B并返回true，否则返回false。 注:synchronized和ReentrantLock都是悲观锁。 注:什么时候使用悲观锁效率更高、什么使用使用乐观锁效率更高，要根据实际情况来判断选择。 提示:atomic中包下的类，采用的即为CAS乐观算法。 以AtomicInteger的public final int getAndSet(int newValue)方法，进行简单说明， 该方法是这样的: 其调用了Unsafe类的public final int getAndSetInt(Object var1, long var2, int var4)方法: 而该方法又do{…}while(…)循环调用了本地方法public final native boolean compareAndSwapInt(Object var1, long var2, int var4, int var5); 注:至于Windows/Linux下public final native boolean compareAndSwapInt(Object var1, long var2, int var4, int var5)本地方法是如何实现的，推荐阅读https://blog.csdn.net/v123411739/article/details/79561458。 CAS(Compare And Swap)原理简述： 某一线程执行一个CAS逻辑(如上图线程A),如果中途有其他线程修改了共享变量的值(如:上图中线程A执行到笑脸那一刻时),导致这个线程的CAS逻辑运算后得到的值与期望结果不一致，那么这个线程会再次执行CAS逻辑(这里是一个do while循环),直到成功为止。 ABA问题： 就是说一个线程把数据A变为了B，然后又重新变成了A。此时另外一个线程读取的时候，发现A没有变化，就误以为是原来的那个A。这就是有名的ABA问题。 注:根据实际情况，判断是否处理ABA问题。如果ABA问题并不会影响我们的业务结果，可以选择性处理或不处理;如果 ABA会影响我们的业务结果的，这时就必须处理ABA问题了。 追注:对于AtomicInteger等,没有什么可修改的属性;且我们只在意其结果值，所以对于这些类来说，本身就算发生了ABA现象，也不会对原线程的结果造成什么影响。 AtomicReference原子应用类，可以保证你在修改对象引用时的线程安全性，比较时可以按照偏移量进行 怎样使用AtomicReference： AtomicReference&lt;String&gt; ar = new AtomicReference&lt;String&gt;(); ar.set(&quot;hello&quot;); //CAS操作更新 ar.compareAndSet(&quot;hello&quot;, &quot;hello1&quot;); AtomicReference的成员变量： private static final long serialVersionUID = -1848883965231344442L; //unsafe类,提供cas操作的功能 private static final Unsafe unsafe = Unsafe.getUnsafe(); //value变量的偏移地址,说的就是下面那个value,这个偏移地址在static块里初始化 private static final long valueOffset; //实际传入需要原子操作的那个类实例 private volatile V value; 类装载的时候初始化偏移地址： static { try { valueOffset = unsafe.objectFieldOffset (AtomicReference.class.getDeclaredField(&quot;value&quot;)); } catch (Exception ex) { throw new Error(ex); } } compareAndSet方法： //就是调用Unsafe的cas操作,传入对象,expect值,偏移地址,需要更新的值,即可,如果更新成功,返回true,如果失败,返回false public final boolean compareAndSet(V expect, V update) { return unsafe.compareAndSwapObject(this, valueOffset, expect, update); } 对于String变量来说,必须是对象相同才视为相同,而不是字符串的内容相同就可以相同: AtomicReference&lt;String&gt; ar = new AtomicReference&lt;String&gt;(); ar.set(&quot;hello&quot;); System.out.println(ar.compareAndSet(new String(&quot;hello&quot;), &quot;hello1&quot;));//false AtomicReference可解决volatile不具有原子性i++操作 AtomicReference可以保证结果是正确的. private static volatile Integer num1 = 0; private static AtomicReference&lt;Integer&gt; ar=new AtomicReference&lt;Integer&gt;(num1); public void dfasd111() throws InterruptedException{ for (int i = 0; i &lt; 1000; i++) { new Thread(new Runnable(){ @Override public void run() { for (int i = 0; i &lt; 10000; i++) while(true){ Integer temp=ar.get(); if(ar.compareAndSet(temp, temp+1))break; } } }).start(); } Thread.sleep(10000); System.out.println(ar.get()); //10000000 } 类似i++这样的&quot;读-改-写&quot;复合操作(在一个操作序列中, 后一个操作依赖前一次操作的结果), 在多线程并发处理的时候会出现问题, 因为可能一个线程修改了变量, 而另一个线程没有察觉到这样变化, 当使用原子变量之后, 则将一系列的复合操作合并为一个原子操作,从而避免这种问题, i++=&gt;i.incrementAndGet() 这里的compareAndSet方法即cas操作本身是原子的，但是在某些场景下会出现异常场景 此处说一下ABA问题： 比如，我们只是简单得要做一个数值加法，即使在我取得期望值后，这个数字被不断的修改，只要它最终改回了我的期望值，我的加法计算就不会出错。也就是说，当你修改的对象没有过程的状态信息，所有的信息都只保存于对象的数值本身。 但是，在现实中，还可能存在另外一种场景。就是我们是否能修改对象的值，不仅取决于当前值，还和对象的过程变化有关，这时，AtomicReference就无能为力了。 AtomicStampedReference与AtomicReference的区别： AtomicStampedReference它内部不仅维护了对象值，还维护了一个时间戳（我这里把它称为时间戳，实际上它可以使任何一个整数，它使用整数来表示状态值）。当AtomicStampedReference对应的数值被修改时，除了更新数据本身外，还必须要更新时间戳。当AtomicStampedReference设置对象值时，对象值以及时间戳都必须满足期望值，写入才会成功。因此，即使对象值被反复读写，写回原值，只要时间戳发生变化，就能防止不恰当的写入。 解决ABA问题 public static void main(String[] args) { String str1 = &quot;aaa&quot;; String str2 = &quot;bbb&quot;; AtomicStampedReference&lt;String&gt; reference = new AtomicStampedReference&lt;String&gt;(str1,1); reference.compareAndSet(str1,str2,reference.getStamp(),reference.getStamp()+1); System.out.println(&quot;reference.getReference() = &quot; + reference.getReference()); boolean b = reference.attemptStamp(str2, reference.getStamp() + 1); System.out.println(&quot;b: &quot;+b); System.out.println(&quot;reference.getStamp() = &quot;+reference.getStamp()); boolean c = reference.weakCompareAndSet(str2,&quot;ccc&quot;,4, reference.getStamp()+1); System.out.println(&quot;reference.getReference() = &quot;+reference.getReference()); System.out.println(&quot;c = &quot; + c); } 输出: reference.getReference() = bbb b: true reference.getStamp() = 3 reference.getReference() = bbb c = false ","link":"https://tianxiawuhao.github.io/2nzKhQbUe/"},{"title":"代理模式","content":"静态代理、JDK动态代理以及CGLIB动态代理 代理模式是java中最常用的设计模式之一，尤其是在spring框架中广泛应用。对于java的代理模式，一般可分为：静态代理、动态代理、以及CGLIB实现动态代理。 对于上述三种代理模式，分别进行说明。 静态代理 静态代理其实就是在程序运行之前，提前写好被代理方法的代理类，编译后运行。在程序运行之前，class已经存在。 下面我们实现一个静态代理demo: 定义一个接口Target package com.test.proxy; public interface Target { public String execute(); } TargetImpl 实现接口Target package com.test.proxy; public class TargetImpl implements Target { @Override public String execute() { System.out.println(&quot;TargetImpl execute！&quot;); return &quot;execute&quot;; } } 代理类 package com.test.proxy; public class Proxy implements Target{ private Target target; public Proxy(Target target) { this.target = target; } @Override public String execute() { System.out.println(&quot;perProcess&quot;); String result = this.target.execute(); System.out.println(&quot;postProcess&quot;); return result; } } 测试类: package com.test.proxy; public class ProxyTest { public static void main(String[] args) { Target target = new TargetImpl(); Proxy p = new Proxy(target); String result = p.execute(); System.out.println(result); } } 运行结果: perProcess TargetImpl execute！ postProcess execute 静态代理需要针对被代理的方法提前写好代理类，如果被代理的方法非常多则需要编写很多代码，因此，对于上述缺点，通过动态代理的方式进行了弥补。 动态代理 jdk代理 动态代理主要是通过反射机制，在运行时动态生成所需代理的class. 接口 package com.test.dynamic; public interface Target { public String execute(); } 实现类 package com.test.dynamic; public class TargetImpl implements Target { @Override public String execute() { System.out.println(&quot;TargetImpl execute！&quot;); return &quot;execute&quot;; } } 代理类 package com.test.dynamic; import java.lang.reflect.InvocationHandler; import java.lang.reflect.Method; public class DynamicProxyHandler implements InvocationHandler{ private Target target; public DynamicProxyHandler(Target target) { this.target = target; } @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { System.out.println(&quot;========before==========&quot;); Object result = method.invoke(target,args); System.out.println(&quot;========after===========&quot;); return result; } } 测试类 package com.test.dynamic; import java.lang.reflect.Proxy; public class DynamicProxyTest { public static void main(String[] args) { Target target = new TargetImpl(); DynamicProxyHandler handler = new DynamicProxyHandler(target); Target proxySubject = (Target) Proxy.newProxyInstance(TargetImpl.class.getClassLoader(),TargetImpl.class.getInterfaces(),handler); String result = proxySubject.execute(); System.out.println(result); } } 运行结果： before== TargetImpl execute！ after=== execute 无论是动态代理还是静态带领，都需要定义接口，然后才能实现代理功能。这同样存在局限性，因此，为了解决这个问题，出现了第三种代理方式：cglib代理。 cglib代理 CGLib采用了非常底层的字节码技术，其原理是通过字节码技术为一个类创建子类，并在子类中采用方法拦截的技术拦截所有父类方法的调用，顺势织入横切逻辑。JDK动态代理与CGLib动态代理均是实现Spring AOP的基础。 目标类 package com.test.cglib; public class Target { public String execute() { String message = &quot;-----------test------------&quot;; System.out.println(message); return message; } } 通用代理类 package com.test.cglib; import net.sf.cglib.proxy.MethodInterceptor; import net.sf.cglib.proxy.MethodProxy; import java.lang.reflect.Method; public class MyMethodInterceptor implements MethodInterceptor{ @Override public Object intercept(Object obj, Method method, Object[] args, MethodProxy proxy) throws Throwable { System.out.println(&quot;&gt;&gt;&gt;&gt;MethodInterceptor start...&quot;); Object result = proxy.invokeSuper(obj,args); System.out.println(&quot;&gt;&gt;&gt;&gt;MethodInterceptor ending...&quot;); return &quot;result&quot;; } } 测试类 package com.test.cglib; import net.sf.cglib.proxy.Enhancer; public class CglibTest { public static void main(String[] args) { System.out.println(&quot;***************&quot;); Target target = new Target(); CglibTest test = new CglibTest(); Target proxyTarget = (Target) test.createProxy(Target.class); String res = proxyTarget.execute(); System.out.println(res); } public Object createProxy(Class targetClass) { Enhancer enhancer = new Enhancer(); enhancer.setSuperclass(targetClass); enhancer.setCallback(new MyMethodInterceptor()); return enhancer.create(); } } 执行结果: MethodInterceptor start... -----------test------------ MethodInterceptor ending... result 代理对象的生成过程由Enhancer类实现，大概步骤如下： 生成代理类Class的二进制字节码； 通过Class.forName加载二进制字节码，生成Class对象； 通过反射机制获取实例构造，并初始化代理类对象。 ","link":"https://tianxiawuhao.github.io/CO5c8R39E/"},{"title":"OOM","content":"Stack overflow public class StackOverFlowErrorDemo { public static void main(String[] args) { StackOverFlowError(); } private static void StackOverFlowError() { StackOverFlowError(); } } Exception in thread &quot;main&quot; java.lang.StackOverflowError at com.example.interview.oom.StackOverFlowErrorDemo.StackOverFlowError(StackOverFlowErrorDemo.java:14) 递归调用自身方法，不断向栈内压入栈帧，直到撑破栈空间，重复次数不确定 OutOfMemoryError：Java heap space /** * @author wuhao * @desc -Xmx10m -Xms10m -XX:+PrintGCDetails * 最大堆内存10M,初始堆内存10M,打印GC详细信息 * @date 2021-09-10 15:14:50 */ public class JavaHeapSpaceDemo { public static void main(String[] args) { // String str= &quot;java&quot;; // while (true){ // str+=str+new Random().nextInt(111111)+new Random().nextInt(121312); // str.intern(); // } //创建大对象 byte[] bytes=new byte[20*1024*1024]; } } Exception in thread &quot;main&quot; java.lang.OutOfMemoryError: Java heap space at com.example.interview.oom.JavaHeapSpaceDemo.main(JavaHeapSpaceDemo.java:18) 常量池，对象空间地址位于堆空间中，循环生成String或者生成超大对象会导致堆空间被占满溢出 OutOfMemoryError：GC overhead limit exceeded /** * @author wuhao * @desc -Xmx20m -Xms10m -XX:+PrintGCDetails * 最大堆内存20M,初始堆内存10M,堆外内存(直接内存)5M,打印GC信息 * GC overhead limit exceeded:超出GC开销限制 * @date 2021-09-10 15:24:48 */ public class GCOverHeadDemo { public static void main(String[] args) { int i=0; List&lt;String&gt; list=new ArrayList&lt;&gt;(); while (true){ list.add(String.valueOf(++i).intern()); } } } [Full GC (Ergonomics) [PSYoungGen: 2560K-&gt;0K(4608K)] [ParOldGen: 13581K-&gt;818K(10752K)] 16141K-&gt;818K(15360K), [Metaspace: 3626K-&gt;3626K(1056768K)], 0.0078165 secs] [Times: user=0.00 sys=0.00, real=0.01 secs] Heap PSYoungGen total 4608K, used 219K [0x00000000ff980000, 0x0000000100000000, 0x0000000100000000) eden space 2560K, 8% used [0x00000000ff980000,0x00000000ff9b6f28,0x00000000ffc00000) from space 2048K, 0% used [0x00000000ffe00000,0x00000000ffe00000,0x0000000100000000) to space 2048K, 0% used [0x00000000ffc00000,0x00000000ffc00000,0x00000000ffe00000) ParOldGen total 10752K, used 818K [0x00000000fec00000, 0x00000000ff680000, 0x00000000ff980000) object space 10752K, 7% used [0x00000000fec00000,0x00000000fecccac0,0x00000000ff680000) Metaspace used 3719K, capacity 4536K, committed 4864K, reserved 1056768K class space used 406K, capacity 428K, committed 512K, reserved 1048576K Exception in thread &quot;main&quot; java.lang.OutOfMemoryError: GC overhead limit exceeded at java.lang.Integer.toString(Integer.java:401) at java.lang.String.valueOf(String.java:3099) at com.example.interview.oom.GCOverHeadDemo.main(GCOverHeadDemo.java:17) 最大堆内存要大于初始堆内存给GC创造条件，如果两值相等就没有重新分配堆空间操作会直接爆出Java heap space异常 OutOfMemoryError：Direct buffer memory import java.nio.ByteBuffer; /** * @author wuhao * @desc -Xmx20m -Xms10m -XX:+PrintGCDetails -XX:MaxDirectMemorySize=5m * 最大堆内存20M,初始堆内存10M,堆外内存(直接内存)5M,打印GC信息 * @date 2021-09-10 15:38:57 */ public class DirectBufferMemoryDemo { public static void main(String[] args) { System.out.println(&quot;配置的DirectMemory&quot;+(sun.misc.VM.maxDirectMemory()/(double)1024/1024)+&quot;mb&quot;); ByteBuffer bf=ByteBuffer.allocateDirect(6*1024*1024); } } [GC (Allocation Failure) [PSYoungGen: 2048K-&gt;504K(2560K)] 2048K-&gt;758K(9728K), 0.0008256 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] 配置的DirectMemory5.0mb [GC (System.gc()) [PSYoungGen: 908K-&gt;504K(2560K)] 1162K-&gt;862K(9728K), 0.0009251 secs] [Times: user=0.06 sys=0.00, real=0.00 secs] [Full GC (System.gc()) [PSYoungGen: 504K-&gt;0K(2560K)] [ParOldGen: 358K-&gt;801K(7168K)] 862K-&gt;801K(9728K), [Metaspace: 3500K-&gt;3500K(1056768K)], 0.0046698 secs] [Times: user=0.00 sys=0.00, real=0.01 secs] Exception in thread &quot;main&quot; java.lang.OutOfMemoryError: Direct buffer memory at java.nio.Bits.reserveMemory(Bits.java:694) at java.nio.DirectByteBuffer.&lt;init&gt;(DirectByteBuffer.java:123) at java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:311) at com.example.interview.oom.DirectBufferMemoryDemo.main(DirectBufferMemoryDemo.java:13) Heap PSYoungGen total 2560K, used 1068K [0x00000000ff980000, 0x00000000ffc80000, 0x0000000100000000) eden space 2048K, 52% used [0x00000000ff980000,0x00000000ffa8b200,0x00000000ffb80000) from space 512K, 0% used [0x00000000ffc00000,0x00000000ffc00000,0x00000000ffc80000) to space 512K, 0% used [0x00000000ffb80000,0x00000000ffb80000,0x00000000ffc00000) ParOldGen total 7168K, used 801K [0x00000000fec00000, 0x00000000ff300000, 0x00000000ff980000) object space 7168K, 11% used [0x00000000fec00000,0x00000000fecc86d0,0x00000000ff300000) Metaspace used 4057K, capacity 4568K, committed 4864K, reserved 1056768K class space used 446K, capacity 460K, committed 512K, reserved 1048576K allocateDirect分配的字节缓冲区用中文叫做直接缓冲区（DirectByteBuffer），用allocate分配的ByteBuffer叫做堆字节缓冲区(HeapByteBuffer).. 其实根据类名就可以看出，HeapByteBuffer所创建的字节缓冲区就是在jvm堆中的，即内部所维护的java字节数组。而DirectByteBuffer是直接操作操作系统本地代码创建的内存缓冲数组（c、c++的数组）。 HeapByteBuffer底层其实是java的字节数组，而java字节数组是一个java对象，对象的内存是由jvm的堆进行管理的，那么不可避免的是GC时年轻代的eden、suvivor到老年代的各种复制以及回收。。。当字节数组比较小的时候还好说，如果是大对象，那么对于jvm的GC来说是一个很大的负担。。而使用DirectByteBuffer，则是把字节数组交给操作系统管理（堆外内存） OutOfMemoryError：Unable to create to native thread public class UnableToCreateToNativeThreadDemo { public static void main(String[] args) { for (int i = 0; ; i++) { System.out.println(&quot;================ i=&quot; + i); new Thread(() -&gt; { try { Thread.sleep(Integer.MAX_VALUE); } catch (InterruptedException e) { e.printStackTrace(); } }, &quot;&quot; + i).start(); } } } linux一般用户可以最大创建1024个线程，不要在电脑上随意尝试 meta space /** * @author wuhao * @desc -XX:MetaspaceSize=8m -XX:MaxMetaspaceSize=8m -XX:+PrintGCDetails * @date 2021-09-13 10:36:15 */ public class MetaSpaceOOMDemo { static class OOMTest{ } public static void main(String[] args) { int i=0; try { while (true){ i++; Enhancer enhancer=new Enhancer(); enhancer.setSuperclass(OOMTest.class); enhancer.setUseCache(false); enhancer.setCallback(new MethodInterceptor() { @Override public Object intercept(Object o, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable { return methodProxy.invokeSuper(o,args); } }); enhancer.create(); } }catch (Throwable e){ System.out.println(&quot;=========&quot;+i+&quot;次后发生了异常&quot;); } } } [Full GC (Last ditch collection) [PSYoungGen: 0K-&gt;0K(116224K)] [ParOldGen: 2016K-&gt;2016K(225792K)] 2016K-&gt;2016K(342016K), [Metaspace: 9911K-&gt;9911K(1058816K)], 0.0093402 secs] [Times: user=0.02 sys=0.00, real=0.01 secs] Heap PSYoungGen total 116224K, used 3425K [0x00000000d6400000, 0x00000000de400000, 0x0000000100000000) eden space 115712K, 2% used [0x00000000d6400000,0x00000000d67584a0,0x00000000dd500000) from space 512K, 0% used [0x00000000de380000,0x00000000de380000,0x00000000de400000) to space 5120K, 0% used [0x00000000dda00000,0x00000000dda00000,0x00000000ddf00000) ParOldGen total 225792K, used 2016K [0x0000000082c00000, 0x0000000090880000, 0x00000000d6400000) object space 225792K, 0% used [0x0000000082c00000,0x0000000082df8128,0x0000000090880000) Metaspace used 9942K, capacity 10090K, committed 10240K, reserved 1058816K class space used 884K, capacity 913K, committed 1024K, reserved 1048576K Exception in thread &quot;main&quot; java.lang.OutOfMemoryError: Metaspace ","link":"https://tianxiawuhao.github.io/QvbaHSogu/"},{"title":"类加载","content":"类加载过程 在java中编译并不进行链接工作，类型的加载、链接和初始化工作都是在jvm执行过程中进行的。在Java程序启动时，jvm通过加载指定的类，然后调用该类的main方法而启动。在JVM启动过程中， 外部class字节码文件会经过一系列过程转化为JVM中执行的数据，这一系列过程我们称为类加载过程。 类加载整体流程 从类被JVM加载到内存开始到卸载出内存为止，整个生命周期包括：加载、链接、初始化、使用和卸载五个过程。其中链接又包括验证、准备和解析三个过程。如下图所示： 类加载时机 java虚拟机规范通过对初始化阶段进行严格规定，来保证初始化的完成，而作为其之前的必须启动的过程，加载、验证、准备也需要在此之前开始。 Java虚拟机规定，以下五种情况必须对类进行初始化： 虚拟机在用户指定包含main方法的主类后启动时，必须先对主类进行初始化。 当使用new关键字对类进行实例化时、读取或者写入类的静态字段时、调用类的静态方法时，必须先触发对该类的实例化。 使用反射对类进行反射调用时，如果该类没有初始化先对其进行初始化。 初始化一个类，而该类的父类还未初始化，需要先对其父类进行初始化。 在JDK1.7之后的版本中使用动态语言支持，java.lang.invoke.MethodHandle实例解析的结果是REF_getStatic、REF_putStatic、REF_invokeStatic的方法句柄，而该句柄对应的类 还未初始化，必须先触发其实例化。 加载 在加载阶段，虚拟机需要完成三件事： 通过一个类的全限定名来获取此类的class字节码二进制流。 将这个字节码二进制流中的静态存储结构转化为方法区中的运行时数据结构。 在内存中生成一个代表该类的java.lang.Class对象，作为方法区中这个类的各种数据的访问入口。 对于Class对象，Java虚拟机规范并没有规定要存储在堆中，HotSpot虚拟机将其存放在方法区中。 验证 验证作为链接的第一步，大致会完成四个阶段的检验： 文件格式验证：该阶段主要在字节流转化为方法区中的运行时数据时，负责检查字节流是否符合Class文件规范，保证其可以正确的被解析并存储在方法区中。后面的检查都是基于方法区的 存储结构进行检验，不会再直接操作字节流。 元数据验证：该阶段负责分析存储于方法区的结构是否符合Java语言规范。此阶段进行数据类型的校验，保证符合不存在非法的元数据信息。 字节码验证：元数据验证保证了字节码中的数据符合语言的规范，该阶段则负责分析数据流和控制流，确定方法体的合法性，保证被校验的方法在运行时不会危害虚拟机的运行。 符号引用验证：在解析阶段会将虚拟机中的符号引用转化为直接引用，该阶段则负责对各种符号引用进行匹配性校验，保证外部依赖真实存在，并且符合外部依赖类、字段、方法的访问性。 准备 准备阶段正式为类的字段变量（被static修饰的类变量）分配内存并设置初始值。这些变量存储在方法区中。当类字段为常量类型（即被static final修饰），由于字段的值已经确定，并不会在后面修改，此时会直接赋值为指定的值。 解析 解析阶段将常量池中的符号引用替换为直接引用。在字节码文件中，类、接口、字段、方法等类型都是由一组符号来表示。其形式由java虚拟机规范中的Class文件格式定义。在虚拟机执行 指定指令之前，需要将符号引用转化为目标的指针、相对偏移量或者句柄，这样可以通过此类直接引用在内存中定位调用的具体位置。 初始化 在类的class文件中。包含两个特殊的方法：clinit和init，这两方法由编译器自动生成，分别代表类构造器和构造函数，其中构造函数编程实现，初始化阶段就是负责调用类构造器，来初始化 变量和资源。 clinit方法由编译器自动收集类的赋值动作和静态语句块（static）中的语句合并生成的，有以下特点： 编译器收集顺序又代码顺序决定，静态语句块只能访问它之前定义的变量，在它之后定义的变量只能进行赋值不能访问。 虚拟机保证在子类的clinit方法执行前，父类的clinit已经执行完毕。 clinit不是必须的，如果一个类或接口没有变量赋值和静态代码块，则编译器可以不生成clinit。 虚拟机会保证clinit方法在多线程中被正确的加锁和同步。如果多个线程同时初始化一个类，那么只有一个线程执行clinit，其他线程会被阻塞。 双亲委派模型 类加载器 定义：实现类加载阶段的“通过一个里的全限定名来获取描述此类的二进制字节流”的动作的代码模块成为“类加载器”。对于任意一个类，都需要由加载它的类加载器和这个类本身一同确立其在java虚拟机中的唯一性，每一个类加载器，都拥有一个独立的类名称空间。比较两个类是否“相等”，只有在这两个类是同一个类加载器加载的前提下才有意义。 类加载器种类 从Java虚拟机的角度只有两种类加载器： （1）启动类加载器（BootStrap ClassLoader），这个类加载器使用C++语言实现，是虚拟机自身的一部分。 （2）另一种就是所有其他类的加载器，这些类加载器都是由Java语言实现，独立于虚拟机外部，并且都继承自抽象类java.lang.ClassLoader。 从Java开发人员的角度，类加载器还可分为3种系统提供的类加载器和用户自定义的类加载器。 （1）启动类加载器（BootStrap ClassLoader）：负责加载存放java_home\\lib目录中的，或者被-Xbootclasspath参数所指定的路径中的类。 （2）扩展类加载器（Extension ClassLoader）：这个加载器sun.misc.Launcher ExtClassLoader实现，它负责加载java_home\\lib\\ext目录中的，或者被java.ext.dirs系统变量所指定的路径中的所有类库，开发者可以直接使用扩展类加载器。 如果应用程序中没有自定义的类加载器，一般情况下 这个就是程序中默认的类加载器。 （3）自定义类加载器（User ClassLoader）：用户自定义的类加载器。用户在编写自己定义的类加载器时，如果需要把请求委派给引导类加载器，那直接使用numm代替即可。要创建用户自己 的类加载器，只需要继承java.lang.ClassLoader，然后覆盖它的findClass(String name)方法即可。如果要符合双亲委派模型，则重写findClass()方法。如果要破坏的话，则重写 loadClass()方法。 双亲委派模型 上图展示的类加载器之间的这种层次关系称为类加载器的双亲委派模型。 双亲委派模型要求除了顶层的启动类加载器之外，其余的类加载器都应当有自己的父类加载器。 类加载器的双亲委派模型在jdk1.2被引入，但它不是一个强制性的约束模型，而是Java设计者推荐给开发者的一种类加载器的实现方式。 双亲委派模型的工作过程如下： 当一个类加载器收到某个类的加载请求时，该类加载器不会去加载该类，而是先查看该类加载器的缓存空间是否已经加载了该类，如果已经加载过了就直接返回，如果不存在，则委托给父亲加载器去加载（也是先查看父亲加载器的缓存空间是否已经加载过该类，如果已加载则直接返回），每一层都是如此，如果缓存空间中都没有加载过该类的记录，最终类加载的请求就会传送到顶端的启动类加载器，如果启动类加载器不能加载该类，则返回给拓展类加载器（儿子）去尝试加载，如果还是不能加载，则再到应用类加载器，再到自定义加载器加载 对象的创建、存储和访问 对象的创建 类加载检查：虚拟机遇到一条new指令，首先检查这个指令的参数是否能在常量池中（Class文件的静态常量池）定位到这个类的符号引用，并且检查这个符号引用代表的类是否 已经被加载、解析和初始化过。如果没有，那必须先执行相应的类加载过程。 分配内存：对象所需内存大小在类加载完成后便可完全确定，为对象分配空间的任务等同于把一块确定大小的内存从Java堆中划分出来。但是不同垃圾回收器的算法会导致堆内存存在两种情况：绝对规整和相互交错。（比如标记清楚算法和标记整理算法） （1）指针碰撞：假设Java堆内存是绝对规整的，所有用过的内存都存放在一起，空闲的内存存放在另一边，中间放着一个指示器作为分界点的指示器，所分配的内存就仅仅是把那个指针向空闲空间那边挪动一段与对象大小相等的距离，这种分配方式成为”指针碰撞“。 （2）空闲列表：如果是相互交错的，那么虚拟机会维护一个列表，记录哪些内存块是可用的，在分配的时候从列表中找到一块足够大的空间划给对象实例，并更新列表上的记录。这种分配方式成为”空闲列表“。 分配内存的并发问题：即使是仅仅修改一个指针所指向的位置，在并发情况下也不是线程安全的，可能出现正在给对象A分配内存，指针还没来得及修改，对象B又同时使用了原来的 指针来分配内存的情况。针对这个问题有两种解决方案： （1）失败重试：对分配内存空间的动作进行同步处理，虚拟机采用CAS和失败重试机制保证更新操作的原子性。 （2）本地线程分配缓存：哪个线程要分配内存，就在哪个线程的TLAB（Thread Local Allocation Buffer）上分配，只有TLAB用完并分配新的TLAB时，才需要同步锁定。 内存空间初始化零值：内存分配完成后，虚拟机需要将分配到的内存空间都初始化零值，这一步操作保证了对象的实例字段（成员变量）在Java代码中可以不赋值就直接使用，程序能够访问到这些字段的数据类型所对应的零值。 对象设置：接下来虚拟机会对对象进行必要的设置，例如这个对象是哪个类的实例，如何才能找到类的元数据信息、对象的哈希吗、对象的GC分代年龄等信息。这些信息存放在对象头中。至此一个新的对象产生了。 实例构造器的init方法：虽然对象产生了，但是init方法并没有执行，所欲字段还需要赋值（包括成员变量赋值，普通语句块执行，构造函数执行等。） Clinit和init Clinit 类构造器的方法，与类的初始化有关。例如静态变量（类变量）和静态对象赋值，静态语句块的执行。如果一个类中没有静态语句块，也没有静态变量或静态对象的赋值， 那么编译器可以不为这个类生成方法。 init 实例构造器（即成员变量，成员对象等），例如成员变量和成员对象的赋值，普通语句块的执行，构造函数的执行。 对象的内存布局 在HotSpot虚拟机中，对象在内存中存储的布局可以分为三个区域：对象头、实例数据和对齐填充。 对象头 对象头包括两部分信息：运行时数据和类型指针。 运行时数据 第一部分用于存储对象自身的运行时数据，如哈希吗（HashCode）、GC分代年龄、锁状态标志、线程持有的锁、偏向线程ID、偏向时间戳等。 下面是HotSpot虚拟机对象头Mark Word： 类型指针 对象头的另一部分是类型指针，即对象指向他元数据的指针，虚拟机可以通过这个指针确定这个对象是哪个类的实例。但是如果对象是一个Java数组，那么在对象头中还必须有一块用于记录数据长度的数据。 对象的实例数据 接着数据头的是对象的实例数据，这部分是真正存储的有效信息。无论是从父类中继承下来的还是在子类中定义的，都需要记录下来。 对齐填充 最后一部分对齐填充并不是必然存在的，也没有特别的含义，仅仅起着占位符的作用。由于HotSpot虚拟机的自动内存管理系统要求对象的起始地址必须是8字节的整数倍，也就是 对象的大小必须是8字节的整数倍。而对象头部分是8字节的倍数，当实例数据没有对齐的时候，需要对齐填充凑够8字节的整数倍。 对象的访问定位 建立对象是为了使用对象，我们的Java程序需要通过栈上的引用数据来操作堆上的具体对象。对象的访问方式取决于虚拟机的实现，目前主流的访问方式有使用句柄和直接指针两种。 句柄引用和直接引用不同在于：使用句柄引用的话，那么Java对堆中将会划分出一块内存来作为句柄池，引用中存储的就是对象的句柄地址，但是直接引用引用中存储的直接就是对象地址。Java使用的是直接指针访问对象的方式，因为它最大的好处就是速度更快，它节省了一次指针定位的时间开销，由于对象的访问在Java中非常频繁，因此这类开销积少成多后也是一项 非常可观的执行成本。 下面是通过直接指针访问对象 ","link":"https://tianxiawuhao.github.io/DikSLfU_z/"},{"title":"线程池ThreadPoolExecutor","content":"Executors目前提供了5种不同的线程池创建配置： newCachedThreadPool（），它是用来处理大量短时间工作任务的线程池，具有几个鲜明特点：它会试图缓存线程并重用，当无缓存线程可用时，就会创建新的工作线程；如果线程闲置时间超过60秒，则被终止并移除缓存；长时间闲置时，这种线程池，不会消耗什么资源。其内部使用SynchronousQueue作为工作队列。 newFixedThreadPool（int nThreads），重用指定数目（nThreads）的线程，其背后使用的是无界的工作队列，任何时候最多有nThreads个工作线程是活动的。这意味着，如果任务数量超过了活动线程数目，将在工作队列中等待空闲线程出现；如果工作线程退出，将会有新的工作线程被创建，以补足指定数目nThreads。 newSingleThreadExecutor()，它的特点在于工作线程数目限制为1，操作一个无界的工作队列，所以它保证了所有的任务都是被顺序执行，最多会有一个任务处于活动状态，并且不予许使用者改动线程池实例，因此可以避免改变线程数目。 newSingleThreadScheduledExecutor()和newScheduledThreadPool(int corePoolSize)，创建的是个ScheduledExecutorService，可以进行定时或周期性的工作调度，区别在于单一工作线程还是多个工作线程。 newWorkStealingPool(int parallelism)，这是一个经常被人忽略的线程池，Java 8 才加入这个创建方法，其内部会构建ForkJoinPool，利用Work-Stealing算法，并行地处理任务，不保证处理顺序。 public class MyThreadPool { public static void main(String[] args) { // ExecutorService executorService = Executors.newFixedThreadPool(5); // ExecutorService executorService= Executors.newSingleThreadExecutor(); // ExecutorService executorService= Executors.newCachedThreadPool(); // ExecutorService executorService= Executors.newScheduledThreadPool(5); ExecutorService executorService = new ThreadPoolExecutor(2, 5, 2, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;Runnable&gt;(3), Executors.defaultThreadFactory(), new ThreadPoolExecutor.AbortPolicy() ); ThreadPoolInit(executorService); } private static void ThreadPoolInit(ExecutorService executorService) { try { for (int i = 1; i &lt;= 10; i++) { executorService.execute(() -&gt; { System.out.println(Thread.currentThread().getName() + &quot;办理业务&quot;); }); } } catch (Exception e) { e.printStackTrace(); } finally { executorService.shutdown(); } } } Executor框架的基本组成 而我们创建时，一般使用它的子类：ThreadPoolExecutor. public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) 这是其中最重要的一个构造方法，这个方法决定了创建出来的线程池的各种属性，下面依靠一张图来更好的理解线程池和这几个参数： （1）corePoolSize：线程池中常驻核心线程数 （2）maximumPoolSize：线程池能够容纳同时执行的最大线程数，此值必须大于等于1 （3）keepAliveTime：多余的空闲线程存活时间。当前线程池数量超过corePoolSize时，当空闲时间到达keepAliveTime值时，多余空闲线程会被销毁直到只剩下corePoolSize个线程为止。 （4）unit：keepAliveTime的时间单位 （5）workQueue：任务队列，被提交但尚未执行的任务 （6）threadFactory：表示生成线程池中的工作线程的线程工厂，用于创建线程，一般为默认线程工厂即可 （7）handler：拒绝策略，表示当队列满了并且工作线程大于等于线程池的最大线程数（maximumPoolSize）时如何来拒绝来请求的Runnable的策略 handler的拒绝策略： 有四种： 第一种AbortPolicy:不执行新任务，直接抛出异常，提示线程池已满 第二种DisCardPolicy:不执行新任务，也不抛出异常 第三种DisCardOldSetPolicy:将消息队列中的第一个任务替换为当前新进来的任务执行 第四种CallerRunsPolicy:直接调用execute来执行当前任务 在spring项目中的使用 先创建一个线程池的配置，让Spring Boot加载，用来定义如何创建一个ThreadPoolTaskExecutor，要使用@Configuration和@EnableAsync这两个注解，表示这是个配置类，并且是线程池的配置类 @Configuration @EnableAsync public class ExecutorConfig { private static final Logger logger = LoggerFactory.getLogger(ExecutorConfig.class); @Value(&quot;${async.executor.thread.core_pool_size}&quot;) private int corePoolSize; @Value(&quot;${async.executor.thread.max_pool_size}&quot;) private int maxPoolSize; @Value(&quot;${async.executor.thread.queue_capacity}&quot;) private int queueCapacity; @Value(&quot;${async.executor.thread.name.prefix}&quot;) private String namePrefix; @Bean(name = &quot;asyncServiceExecutor&quot;) public Executor asyncServiceExecutor() { logger.info(&quot;start asyncServiceExecutor&quot;); ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor(); //配置核心线程数 executor.setCorePoolSize(corePoolSize); //配置最大线程数 executor.setMaxPoolSize(maxPoolSize); //配置队列大小 executor.setQueueCapacity(queueCapacity); //配置线程池中的线程的名称前缀 executor.setThreadNamePrefix(namePrefix); // rejection-policy：当pool已经达到max size的时候，如何处理新任务 // CALLER_RUNS：不在新线程中执行任务，而是有调用者所在的线程来执行 executor.setRejectedExecutionHandler(new ThreadPoolExecutor.CallerRunsPolicy()); //执行初始化 executor.initialize(); return executor; } } @Value是我配置在application.properties，可以参考配置，自由定义 # 异步线程配置 # 配置核心线程数 async.executor.thread.core_pool_size = 5 # 配置最大线程数 async.executor.thread.max_pool_size = 5 # 配置队列大小 async.executor.thread.queue_capacity = 99999 # 配置线程池中的线程的名称前缀 async.executor.thread.name.prefix = async-service- 创建一个Service接口，是异步线程的接口 public interface AsyncService { /** * 执行异步任务 * 可以根据需求，自己加参数拟定，我这里就做个测试演示 */ void executeAsync(); } 实现类 @Service public class AsyncServiceImpl implements AsyncService { private static final Logger logger = LoggerFactory.getLogger(AsyncServiceImpl.class); @Override @Async(&quot;asyncServiceExecutor&quot;) public void executeAsync() { logger.info(&quot;start executeAsync&quot;); System.out.println(&quot;异步线程要做的事情&quot;); System.out.println(&quot;可以在这里执行批量插入等耗时的事情&quot;); logger.info(&quot;end executeAsync&quot;); } } 将Service层的服务异步化，在executeAsync()方法上增加注解@Async(&quot;asyncServiceExecutor&quot;)，asyncServiceExecutor方法是前面ExecutorConfig.java 中的方法名，表明executeAsync方法进入的线程池是asyncServiceExecutor方法创建的 接下来就是在Controller里或者是哪里通过注解@Autowired注入这个Service @Autowired private AsyncService asyncService; @GetMapping(&quot;/async&quot;) public void async(){ asyncService.executeAsync(); } 用postmain或者其他工具来多次测试请求一下 2018-07-16 22:15:47.655 INFO 10516 --- [async-service-5] c.u.d.e.executor.impl.AsyncServiceImpl : start executeAsync 异步线程要做的事情 可以在这里执行批量插入等耗时的事情 2018-07-16 22:15:47.655 INFO 10516 --- [async-service-5] c.u.d.e.executor.impl.AsyncServiceImpl : end executeAsync 2018-07-16 22:15:47.770 INFO 10516 --- [async-service-1] c.u.d.e.executor.impl.AsyncServiceImpl : start executeAsync 异步线程要做的事情 可以在这里执行批量插入等耗时的事情 2018-07-16 22:15:47.770 INFO 10516 --- [async-service-1] c.u.d.e.executor.impl.AsyncServiceImpl : end executeAsync 2018-07-16 22:15:47.816 INFO 10516 --- [async-service-2] c.u.d.e.executor.impl.AsyncServiceImpl : start executeAsync 异步线程要做的事情 可以在这里执行批量插入等耗时的事情 2018-07-16 22:15:47.816 INFO 10516 --- [async-service-2] c.u.d.e.executor.impl.AsyncServiceImpl : end executeAsync 2018-07-16 22:15:48.833 INFO 10516 --- [async-service-3] c.u.d.e.executor.impl.AsyncServiceImpl : start executeAsync 异步线程要做的事情 可以在这里执行批量插入等耗时的事情 2018-07-16 22:15:48.834 INFO 10516 --- [async-service-3] c.u.d.e.executor.impl.AsyncServiceImpl : end executeAsync 2018-07-16 22:15:48.986 INFO 10516 --- [async-service-4] c.u.d.e.executor.impl.AsyncServiceImpl : start executeAsync 异步线程要做的事情 可以在这里执行批量插入等耗时的事情 2018-07-16 22:15:48.987 INFO 10516 --- [async-service-4] c.u.d.e.executor.impl.AsyncServiceImpl : end executeAsync 通过以上日志可以发现，[async-service-]是有多个线程的，显然已经在我们配置的线程池中执行了，并且每次请求中，controller的起始和结束日志都是连续打印的，表明每次请求都快速响应了，而耗时的操作都留给线程池中的线程去异步执行； 虽然我们已经用上了线程池，但是还不清楚线程池当时的情况，有多少线程在执行，多少在队列中等待呢？这里我创建了一个ThreadPoolTaskExecutor的子类，在每次提交线程的时候都会将当前线程池的运行状况打印出来 import org.slf4j.Logger; import org.slf4j.LoggerFactory; import org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor; import org.springframework.util.concurrent.ListenableFuture; import java.util.concurrent.Callable; import java.util.concurrent.Future; import java.util.concurrent.ThreadPoolExecutor; /** * @Author: ChenBin * @Date: 2018/7/16/0016 22:19 */ public class VisiableThreadPoolTaskExecutor extends ThreadPoolTaskExecutor { private static final Logger logger = LoggerFactory.getLogger(VisiableThreadPoolTaskExecutor.class); private void showThreadPoolInfo(String prefix) { ThreadPoolExecutor threadPoolExecutor = getThreadPoolExecutor(); if (null == threadPoolExecutor) { return; } logger.info(&quot;{}, {},taskCount [{}], completedTaskCount [{}], activeCount [{}], queueSize [{}]&quot;, this.getThreadNamePrefix(), prefix, threadPoolExecutor.getTaskCount(), threadPoolExecutor.getCompletedTaskCount(), threadPoolExecutor.getActiveCount(), threadPoolExecutor.getQueue().size()); } @Override public void execute(Runnable task) { showThreadPoolInfo(&quot;1. do execute&quot;); super.execute(task); } @Override public void execute(Runnable task, long startTimeout) { showThreadPoolInfo(&quot;2. do execute&quot;); super.execute(task, startTimeout); } @Override public Future&lt;?&gt; submit(Runnable task) { showThreadPoolInfo(&quot;1. do submit&quot;); return super.submit(task); } @Override public &lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task) { showThreadPoolInfo(&quot;2. do submit&quot;); return super.submit(task); } @Override public ListenableFuture&lt;?&gt; submitListenable(Runnable task) { showThreadPoolInfo(&quot;1. do submitListenable&quot;); return super.submitListenable(task); } @Override public &lt;T&gt; ListenableFuture&lt;T&gt; submitListenable(Callable&lt;T&gt; task) { showThreadPoolInfo(&quot;2. do submitListenable&quot;); return super.submitListenable(task); } } 如上所示，showThreadPoolInfo方法中将任务总数、已完成数、活跃线程数，队列大小都打印出来了，然后Override了父类的execute、submit等方法，在里面调用showThreadPoolInfo方法，这样每次有任务被提交到线程池的时候，都会将当前线程池的基本情况打印到日志中； 修改ExecutorConfig.java的asyncServiceExecutor方法，将ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor()改为ThreadPoolTaskExecutor executor = new VisiableThreadPoolTaskExecutor() @Bean(name = &quot;asyncServiceExecutor&quot;) public Executor asyncServiceExecutor() { logger.info(&quot;start asyncServiceExecutor&quot;); //在这里修改 ThreadPoolTaskExecutor executor = new VisiableThreadPoolTaskExecutor(); //配置核心线程数 executor.setCorePoolSize(corePoolSize); //配置最大线程数 executor.setMaxPoolSize(maxPoolSize); //配置队列大小 executor.setQueueCapacity(queueCapacity); //配置线程池中的线程的名称前缀 executor.setThreadNamePrefix(namePrefix); // rejection-policy：当pool已经达到max size的时候，如何处理新任务 // CALLER_RUNS：不在新线程中执行任务，而是有调用者所在的线程来执行 executor.setRejectedExecutionHandler(new ThreadPoolExecutor.CallerRunsPolicy()); //执行初始化 executor.initialize(); return executor; } 再次启动该工程测试 2018-07-16 22:23:30.951 INFO 14088 --- [nio-8087-exec-2] u.d.e.e.i.VisiableThreadPoolTaskExecutor : async-service-, 2. do submit,taskCount [0], completedTaskCount [0], activeCount [0], queueSize [0] 2018-07-16 22:23:30.952 INFO 14088 --- [async-service-1] c.u.d.e.executor.impl.AsyncServiceImpl : start executeAsync 异步线程要做的事情 可以在这里执行批量插入等耗时的事情 2018-07-16 22:23:30.953 INFO 14088 --- [async-service-1] c.u.d.e.executor.impl.AsyncServiceImpl : end executeAsync 2018-07-16 22:23:31.351 INFO 14088 --- [nio-8087-exec-3] u.d.e.e.i.VisiableThreadPoolTaskExecutor : async-service-, 2. do submit,taskCount [1], completedTaskCount [1], activeCount [0], queueSize [0] 2018-07-16 22:23:31.353 INFO 14088 --- [async-service-2] c.u.d.e.executor.impl.AsyncServiceImpl : start executeAsync 异步线程要做的事情 可以在这里执行批量插入等耗时的事情 2018-07-16 22:23:31.353 INFO 14088 --- [async-service-2] c.u.d.e.executor.impl.AsyncServiceImpl : end executeAsync 2018-07-16 22:23:31.927 INFO 14088 --- [nio-8087-exec-5] u.d.e.e.i.VisiableThreadPoolTaskExecutor : async-service-, 2. do submit,taskCount [2], completedTaskCount [2], activeCount [0], queueSize [0] 2018-07-16 22:23:31.929 INFO 14088 --- [async-service-3] c.u.d.e.executor.impl.AsyncServiceImpl : start executeAsync 异步线程要做的事情 可以在这里执行批量插入等耗时的事情 2018-07-16 22:23:31.930 INFO 14088 --- [async-service-3] c.u.d.e.executor.impl.AsyncServiceImpl : end executeAsync 2018-07-16 22:23:32.496 INFO 14088 --- [nio-8087-exec-7] u.d.e.e.i.VisiableThreadPoolTaskExecutor : async-service-, 2. do submit,taskCount [3], completedTaskCount [3], activeCount [0], queueSize [0] 2018-07-16 22:23:32.498 INFO 14088 --- [async-service-4] c.u.d.e.executor.impl.AsyncServiceImpl : start executeAsync 异步线程要做的事情 可以在这里执行批量插入等耗时的事情 2018-07-16 22:23:32.499 INFO 14088 --- [async-service-4] c.u.d.e.executor.impl.AsyncServiceImpl : end executeAsync 注意这一行日志： 2018-07-16 22:23:32.496 INFO 14088 --- [nio-8087-exec-7] u.d.e.e.i.VisiableThreadPoolTaskExecutor : async-service-, 2. do submit,taskCount [3], completedTaskCount [3], activeCount [0], queueSize [0] 这说明提交任务到线程池的时候，调用的是submit(Callable task)这个方法，当前已经提交了3个任务，完成了3个，当前有0个线程在处理任务，还剩0个任务在队列中等待，线程池的基本情况一路了然 ","link":"https://tianxiawuhao.github.io/y8VkHlUlp/"},{"title":"手写一个生产者消费者模式","content":"import lombok.SneakyThrows; import org.springframework.util.StringUtils; import java.util.concurrent.ArrayBlockingQueue; import java.util.concurrent.BlockingQueue; import java.util.concurrent.TimeUnit; import java.util.concurrent.atomic.AtomicInteger; class MyResource { private volatile Boolean FLAG = true; private final AtomicInteger atomicInteger = new AtomicInteger(); BlockingQueue&lt;String&gt; blockingQueue = null; public MyResource(BlockingQueue&lt;String&gt; blockingQueue) { this.blockingQueue = blockingQueue; System.out.println(blockingQueue.getClass().getName()); } @SneakyThrows public void myProd() { String data = null; while (FLAG) { data = String.valueOf(atomicInteger.incrementAndGet()); if (blockingQueue.offer(data, 2L, TimeUnit.SECONDS)) { System.out.println(Thread.currentThread().getName() + &quot;插入数据&quot; + data + &quot;成功&quot;); } else { System.out.println(Thread.currentThread().getName() + &quot;插入数据&quot; + data + &quot;失败&quot;); } TimeUnit.SECONDS.sleep(1L); } System.out.println(Thread.currentThread().getName() + &quot;生产停止&quot;); } @SneakyThrows public void myConsumer() { String result = null; while (FLAG) { result = blockingQueue.poll(2L, TimeUnit.SECONDS); if (StringUtils.isEmpty(result)) { FLAG = false; System.out.println(Thread.currentThread().getName() + &quot;超过2秒没有取到值，结束&quot;); return; } System.out.println(Thread.currentThread().getName() + &quot;消费数据&quot; + result + &quot;成功&quot;); } } public void stop(){ FLAG=false; } } public class ProdConsumer_BlockQueue { @SneakyThrows public static void main(String[] args) { MyResource myResource = new MyResource(new ArrayBlockingQueue&lt;&gt;(10)); new Thread(() -&gt; { System.out.println(Thread.currentThread().getName() + &quot;生产线程启动&quot;); myResource.myProd(); }, &quot;Prod&quot;).start(); new Thread(() -&gt; { System.out.println(Thread.currentThread().getName() + &quot;消费线程启动&quot;); myResource.myConsumer(); }, &quot;Consumer&quot;).start(); TimeUnit.SECONDS.sleep(5L); System.out.println(&quot;5秒时间到，结束&quot;); myResource.stop(); } } ","link":"https://tianxiawuhao.github.io/5QxJ3UtmK/"},{"title":"阻塞队列BlockingQueue","content":"为什么要使用阻塞队列 之前，介绍了一下 ThreadPoolExecutor 的各参数的含义（线程池ThreadPoolExecutor），其中有一个 BlockingQueue，它是一个阻塞队列。那么，小伙伴们有没有想过，为什么此处的线程池要用阻塞队列呢？ 我们知道队列是先进先出的。当放入一个元素的时候，会放在队列的末尾，取出元素的时候，会从队头取。那么，当队列为空或者队列满的时候怎么办呢。 这时，阻塞队列，会自动帮我们处理这种情况。 当阻塞队列为空的时候，从队列中取元素的操作就会被阻塞。当阻塞队列满的时候，往队列中放入元素的操作就会被阻塞。 而后，一旦空队列有数据了，或者满队列有空余位置时，被阻塞的线程就会被自动唤醒。 这就是阻塞队列的好处，你不需要关心线程何时被阻塞，也不需要关心线程何时被唤醒，一切都由阻塞队列自动帮我们完成。我们只需要关注具体的业务逻辑就可以了。 而这种阻塞队列经常用在生产者消费者模式中。（可参看：手写一个生产者消费者模式） 常用的阻塞队列 那么，一般我们用到的阻塞队列有哪些呢。下面，通过idea的类图，列出来常用的阻塞队列，然后一个一个讲解（不懂怎么用的，可以参考这篇文章：怎么用IDEA快速查看类图关系）。 阻塞队列中，所有常用的方法都在 BlockingQueue 接口中定义。如 插入元素的方法： put，offer，add。移除元素的方法： remove，poll，take。 它们有四种不同的处理方式，第一种是在失败时抛出异常，第二种是在失败时返回特殊值，第三种是一直阻塞当前线程，最后一种是在指定时间内阻塞，否则返回特殊值。（以上特殊值，是指在插入元素时，失败返回false，在取出元素时，失败返回null） 抛异常 特殊值 阻塞 超时 插入 add(e) offer(e) put(e) offer(e,time,unit) 移除 remove() poll() take() poll(time,unit) 1） ArrayBlockingQueue 这是一个由数组结构组成的有界阻塞队列。首先看下它的构造方法，有三个。 第一个可以指定队列的大小，第二个还可以指定队列是否公平，不指定的话，默认是非公平。它是使用 ReentrantLock 的公平锁和非公平锁实现的（后续讲解AQS时，会详细说明）。 简单理解就是，ReentrantLock 内部会维护一个有先后顺序的等待队列，假如有五个任务一起过来，都被阻塞了。如果是公平的，则等待队列中等待最久的任务就会先进入阻塞队列。如果是非公平的，那么这五个线程就需要抢锁，谁先抢到，谁就先进入阻塞队列。 第三个构造方法，是把一个集合的元素初始化到阻塞队列中。 另外，ArrayBlockingQueue 没有实现读写分离，也就是说，读和写是不能同时进行的。因为，它读写时用的是同一把锁，如下图所示： 2) LinkedBlockingQueue 这是一个由链表结构组成的有界阻塞队列。它的构造方法有三个。 可以看到和 ArrayBlockingQueue 的构造方法大同小异，不过是，LinkedBlockingQueue 可以不指定队列的大小，默认值是 Integer.MAX_VALUE 。 但是，最好不要这样做，建议指定一个固定大小。因为，如果生产者的速度比消费者的速度大的多的情况下，这会导致阻塞队列一直膨胀，直到系统内存被耗尽（此时，还没达到队列容量的最大值）。 此外，LinkedBlockingQueue 实现了读写分离，可以实现数据的读和写互不影响，这在高并发的场景下，对于效率的提高无疑是非常巨大的。 3） SynchronousQueue 这是一个没有缓冲的无界队列。什么意思，看一下它的 size 方法： 总是返回 0 ，因为它是一个没有容量的队列。 当执行插入元素的操作时，必须等待一个取出操作。也就是说，put元素的时候，必须等待 take 操作。 那么，有的同学就好奇了，这没有容量，还叫什么队列啊，这有什么意义呢。 我的理解是，这适用于并发任务不大，而且生产者和消费者的速度相差不多的场景下，直接把生产者和消费者对接，不用经过队列的入队出队这一系列操作。所以，效率上会高一些。 可以去查看一下 Excutors.newCachedThreadPool 方法用的就是这种队列。 这个队列有两个构造方法，用于传入是公平还是非公平，默认是非公平。 4）PriorityBlockingQueue 这是一个支持优先级排序的无界队列。有四个构造方法： 可以指定初始容量大小（注意初始容量并不代表最大容量），或者不指定，默认大小为 11。也可以传入一个比较器，把元素按一定的规则排序，不指定比较器的话，默认是自然顺序。 PriorityBlockingQueue 是基于二叉树最小堆实现的，每当取元素的时候，就会把优先级最高的元素取出来。我们测试一下： public class Person { private int id; private String name; public int getId() { return id; } public void setId(int id) { this.id = id; } public String getName() { return name; } public void setName(String name) { this.name = name; } @Override public String toString() { return &quot;Person{&quot; + &quot;id=&quot; + id + &quot;, name='&quot; + name + '\\'' + '}'; } public Person(int id, String name) { this.id = id; this.name = name; } public Person() { } } public class QueueTest { public static void main(String[] args) throws InterruptedException { PriorityBlockingQueue&lt;Person&gt; priorityBlockingQueue = new PriorityBlockingQueue&lt;&gt;(1, new Comparator&lt;Person&gt;() { @Override public int compare(Person o1, Person o2) { return o1.getId() - o2.getId(); } }); Person p2 = new Person(7, &quot;李四&quot;); Person p1 = new Person(9, &quot;张三&quot;); Person p3 = new Person(6, &quot;王五&quot;); Person p4 = new Person(2, &quot;赵六&quot;); priorityBlockingQueue.add(p1); priorityBlockingQueue.add(p2); priorityBlockingQueue.add(p3); priorityBlockingQueue.add(p4); //由于二叉树最小堆实现，用这种方式直接打印元素，不能保证有序 System.out.println(priorityBlockingQueue); System.out.println(priorityBlockingQueue.take()); System.out.println(priorityBlockingQueue); System.out.println(priorityBlockingQueue.take()); System.out.println(priorityBlockingQueue); } } 打印结果： [Person{id=2, name='赵六'}, Person{id=6, name='王五'}, Person{id=7, name='李四'}, Person{id=9, name='张三'}] Person{id=2, name='赵六'} [Person{id=6, name='王五'}, Person{id=9, name='张三'}, Person{id=7, name='李四'}] Person{id=6, name='王五'} [Person{id=7, name='李四'}, Person{id=9, name='张三'}] 可以看到，第一次取出的是 id 最小值 2， 第二次取出的是 6 。 5）DelayQueue 这是一个带有延迟时间的无界阻塞队列。队列中的元素，只有等延时时间到了，才能取出来。此队列一般用于过期数据的删除，或任务调度。以下，模拟一下定长时间的数据删除。 首先定义数据元素，需要实现 Delayed 接口，实现 getDelay 方法用于计算剩余时间，和 CompareTo方法用于优先级排序。 public class DelayData implements Delayed { private int id; private String name; //数据到期时间 private long endTime; private TimeUnit timeUnit = TimeUnit.MILLISECONDS; public int getId() { return id; } public void setId(int id) { this.id = id; } public String getName() { return name; } public void setName(String name) { this.name = name; } public long getEndTime() { return endTime; } public void setEndTime(long endTime) { this.endTime = endTime; } public DelayData(int id, String name, long endTime) { this.id = id; this.name = name; //需要把传入的时间endTime 加上当前系统时间，作为数据的到期时间 this.endTime = endTime + System.currentTimeMillis(); } public DelayData() { } @Override public long getDelay(TimeUnit unit) { return this.endTime - System.currentTimeMillis(); } @Override public int compareTo(Delayed o) { return o.getDelay(this.timeUnit) - this.getDelay(this.timeUnit) &lt; 0 ? 1: -1; } } 模拟三条数据，分别设置不同的过期时间： public class ProcessData { public static void main(String[] args) throws InterruptedException { DelayQueue&lt;DelayData&gt; delayQueue = new DelayQueue&lt;&gt;(); DelayData a = new DelayData(5, &quot;A&quot;, 5000); DelayData b = new DelayData(8, &quot;B&quot;, 8000); DelayData c = new DelayData(2, &quot;C&quot;, 2000); delayQueue.add(a); delayQueue.add(b); delayQueue.add(c); System.out.println(&quot;开始计时时间:&quot; + System.currentTimeMillis()); for (int i = 0; i &lt; 3; i++) { DelayData data = delayQueue.take(); System.out.println(&quot;id:&quot;+data.getId()+&quot;，数据:&quot;+data.getName()+&quot;被移除，当前时间:&quot;+System.currentTimeMillis()); } } } 最后结果： 开始计时时间:1583333583216 id:2，数据:C被移除，当前时间:1583333585216 id:5，数据:A被移除，当前时间:1583333588216 id:8，数据:B被移除，当前时间:1583333591216 可以看到，数据是按过期时间长短，按顺序移除的。C的时间最短 2 秒，然后过了 3 秒 A 也过期，再过 3 秒，B 过期。 ","link":"https://tianxiawuhao.github.io/volSqv4Kf/"},{"title":"Redis的缓存淘汰策略LRU","content":"redis缓存淘汰策略与Redis键的过期删除策略并不完全相同，前者是在Redis内存使用超过一定值的时候（一般这个值可以配置）使用的淘汰策略；而后者是通过定期删除+惰性删除两者结合的方式进行内存淘汰的。 Redis内存不足的缓存淘汰策略 noeviction：当内存使用超过配置的时候会返回错误，不会驱逐任何键 allkeys-lru：加入键的时候，如果过限，首先通过LRU算法驱逐最久没有使用的键 volatile-lru：加入键的时候如果过限，首先从设置了过期时间的键集合中驱逐最久没有使用的键 allkeys-random：加入键的时候如果过限，从所有key随机删除 volatile-random：加入键的时候如果过限，从过期键的集合中随机驱逐 volatile-ttl：从配置了过期时间的键中驱逐马上就要过期的键 volatile-lfu：从所有配置了过期时间的键中驱逐使用频率最少的键 allkeys-lfu：从所有键中驱逐使用频率最少的键 lru算法实现 取巧算法 import java.util.LinkedHashMap; import java.util.Map; class LRUCache&lt;K,V&gt; extends LinkedHashMap&lt;K,V&gt; { private int capacity; /** * the ordering mode * - &lt;tt&gt;true&lt;/tt&gt; for access-order, * - &lt;tt&gt;false&lt;/tt&gt; for insertion-order * @param capacity */ public LRUCache(int capacity) { super(capacity, 0.75F, true); this.capacity = capacity; } //数据超过容量大小删除 @Override protected boolean removeEldestEntry(Map.Entry&lt;K,V&gt; eldest) { return super.size()&gt;capacity; } } 数据结构实现 import java.util.HashMap; import java.util.Map; class LRUCacheDemo { //构建承载体node class Node&lt;K, V&gt; { K key; V value; Node&lt;K, V&gt; prev; Node&lt;K, V&gt; next; public Node() { this.prev = this.next = null; } public Node(K key, V value) { this.key = key; this.value = value; this.prev = this.next = null; } } //构建双向队列 class DoubleLinkedList&lt;K, V&gt; { Node&lt;K, V&gt; head; Node&lt;K, V&gt; tail; public DoubleLinkedList() { head = new Node&lt;&gt;(); tail = new Node&lt;&gt;(); head.next = tail; tail.prev = head; } //添加头节点 public void addHead(Node&lt;K, V&gt; node) { node.next = head.next; node.prev = head; head.next.prev = node; head.next = node; } //删除节点 public void removeNode(Node&lt;K, V&gt; node) { node.next.prev = node.prev; node.prev.next = node.next; node.prev = null; node.next = null; } //获取最后一个节点 public Node getLast() { return tail.prev; } } private int cacheSize; Map&lt;Object, Node&lt;Object, Object&gt;&gt; map; DoubleLinkedList&lt;Object, Object&gt; doubleLinkedList; public LRUCacheDemo(int cacheSize) { this.cacheSize = cacheSize; this.map = new HashMap&lt;&gt;(); this.doubleLinkedList = new DoubleLinkedList&lt;&gt;(); } public Object get(Object key) { if (!map.containsKey(key)) { return -1; } final Node&lt;Object, Object&gt; node = map.get(key); this.doubleLinkedList.removeNode(node); this.doubleLinkedList.addHead(node); return node.value; } public void put(Object key, Object value) { if (map.containsKey(key)) { final Node&lt;Object, Object&gt; node = map.get(key); node.value = value; map.put(key, node); this.doubleLinkedList.removeNode(node); this.doubleLinkedList.addHead(node); }else{ if(map.size()==this.cacheSize){ final Node last = this.doubleLinkedList.getLast(); map.remove(last.key); doubleLinkedList.removeNode(last); } //新增 Node&lt;Object,Object&gt; newNode=new Node&lt;&gt;(key,value); map.put(key, newNode); this.doubleLinkedList.addHead(newNode); } } } ","link":"https://tianxiawuhao.github.io/bGEB86JC1/"},{"title":"list、set、map等集合类线程不安全的问题及解决方法","content":"List ArrayList不是线程安全类，在多线程同时写的情况下，会抛出java.util.ConcurrentModificationException异常。 private static void listNotSafe() { List&lt;String&gt; list=new ArrayList&lt;&gt;(); for (int i = 1; i &lt;= 30; i++) { new Thread(() -&gt; { list.add(UUID.randomUUID().toString().substring(0, 8)); System.out.println(Thread.currentThread().getName() + &quot;\\t&quot; + list); }, String.valueOf(i)).start(); } } 解决方法： 使用Vector（ArrayList所有方法加synchronized，太重）。 使用Collections.synchronizedList()转换成线程安全类。 使用java.concurrent.CopyOnWriteArrayList（推荐）。 CopyOnWriteArrayList这是JUC的类，通过写时复制来实现读写分离。比如其add()方法，就是先复制一个新数组，长度为原数组长度+1，然后将新数组最后一个元素设为添加的元素。 public boolean add(E e) { final ReentrantLock lock = this.lock; lock.lock(); try { //得到旧数组 Object[] elements = getArray(); int len = elements.length; //复制新数组 Object[] newElements = Arrays.copyOf(elements, len + 1); //设置新元素 newElements[len] = e; //设置新数组 setArray(newElements); return true; } finally { lock.unlock(); } } Set 跟List类似，HashSet和TreeSet都不是线程安全的，与之对应的有CopyOnWriteSet这个线程安全类。这个类底层维护了一个CopyOnWriteArrayList数组。 private final CopyOnWriteArrayList&lt;E&gt; al; public CopyOnWriteArraySet() { al = new CopyOnWriteArrayList&lt;E&gt;(); } 使用Collections.synchronizedList()转换成线程安全类。 HashSet和HashMap HashSet底层是用HashMap实现的。既然是用HashMap实现的，那HashMap.put()需要传两个参数，而HashSet.add()只传一个参数，这是为什么？实际上HashSet.add()就是调用的HashMap.put()，只不过Value被写死了，是一个private static final Object对象。 Map HashMap不是线程安全的，Hashtable是线程安全的，但是跟Vector类似，太重量级。所以也有类似CopyOnWriteMap，只不过叫ConcurrentHashMap。 关于集合安全类 import java.util.*; import java.util.concurrent.ConcurrentHashMap; import java.util.concurrent.CopyOnWriteArrayList; import java.util.concurrent.CopyOnWriteArraySet; public class ContainerNotSafeDemo { public static void main(String[] args) { listNotSafe(); setNoSafe(); mapNotSafe(); } private static void mapNotSafe() { //Map&lt;String,String&gt; map=new HashMap&lt;&gt;(); Map&lt;String, String&gt; map = new ConcurrentHashMap&lt;&gt;(); for (int i = 1; i &lt;= 30; i++) { new Thread(() -&gt; { map.put(Thread.currentThread().getName(), UUID.randomUUID().toString().substring(0, 8)); System.out.println(Thread.currentThread().getName() + &quot;\\t&quot; + map); }, String.valueOf(i)).start(); } } private static void setNoSafe() { //Set&lt;String&gt; set=new HashSet&lt;&gt;(); Set&lt;String&gt; set = new CopyOnWriteArraySet&lt;&gt;(); for (int i = 1; i &lt;= 30; i++) { new Thread(() -&gt; { set.add(UUID.randomUUID().toString().substring(0, 8)); System.out.println(Thread.currentThread().getName() + &quot;\\t&quot; + set); }, String.valueOf(i)).start(); } } private static void listNotSafe() { //List&lt;String&gt; list=new ArrayList&lt;&gt;(); List&lt;String&gt; list = new CopyOnWriteArrayList&lt;&gt;(); for (int i = 1; i &lt;= 30; i++) { new Thread(() -&gt; { list.add(UUID.randomUUID().toString().substring(0, 8)); System.out.println(Thread.currentThread().getName() + &quot;\\t&quot; + list); }, String.valueOf(i)).start(); } } } ","link":"https://tianxiawuhao.github.io/GALzQf_b-/"},{"title":"gradle依赖,插件","content":"依赖 configurations 设置configurations 配置依赖信息 build.gradle configurations { // 针对需要组件API的消费者的配置 exposedApi { // canBeResolved 为true 则为可解析配置，为消费者 canBeResolved = false // canBeConsumed 为true 则为消费析配置，为生产者 canBeConsumed = true } // 为需要实现该组件的消费者提供的配置。 exposedRuntime { canBeResolved = false canBeConsumed = true } } 依赖方式 模块依赖 build.gradle dependencies { runtimeOnly group: 'org.springframework', name: 'spring-core', version: '2.5' runtimeOnly 'org.springframework:spring-core:2.5', 'org.springframework:spring-aop:2.5' runtimeOnly( [group: 'org.springframework', name: 'spring-core', version: '2.5'], [group: 'org.springframework', name: 'spring-aop', version: '2.5'] ) runtimeOnly('org.hibernate:hibernate:3.0.5') { transitive = true } runtimeOnly group: 'org.hibernate', name: 'hibernate', version: '3.0.5', transitive: true runtimeOnly(group: 'org.hibernate', name: 'hibernate', version: '3.0.5') { transitive = true } } 文件依赖 build.gradle dependencies { runtimeOnly files('libs/a.jar', 'libs/b.jar') runtimeOnly fileTree('libs') { include '*.jar' } } 项目依赖 build.gradle dependencies { implementation project(':shared') } 依赖方式 compileOnly —用于编译生产代码所必需的依赖关系，但不应该属于运行时类路径的一部分 implementation（取代compile）-用于编译和运行时 runtimeOnly（取代runtime）-仅在运行时使用，不用于编译 testCompileOnly—与compileOnly测试相同 testImplementation —测试相当于 implementation testRuntimeOnly —测试相当于 runtimeOnly repositories 流行的公共存储库包括Maven Central， Bintray JCenter和Google Android存储库。 repositories { mavenCentral() // Maven Central存储库 jcenter() // JCenter Maven存储库 google() // Google Maven存储库 mavenLocal() // 将本地Maven缓存添加为存储库（不推荐） //flat存储库解析器 flatDir { dirs 'lib' } flatDir { dirs 'lib1', 'lib2' } //添加定制的Maven仓库 maven { url &quot;http://repo.mycompany.com/maven2&quot; // 为JAR文件添加附加的Maven存储库 artifactUrls &quot;http://repo.mycompany.com/jars&quot; } //Ivy ivy { url &quot;http://repo.mycompany.com/repo&quot; layout &quot;maven&quot; // 有效的命名布局值是'gradle'（默认值）'maven'和'ivy'。 } } 声明存储库过滤器 声明存储库内容 build.gradle repositories { maven { url &quot;https://repo.mycompany.com/maven2&quot; content { // this repository *only* contains artifacts with group &quot;my.company&quot; includeGroup &quot;my.company&quot; } } jcenter { content { // this repository contains everything BUT artifacts with group starting with &quot;my.company&quot; excludeGroupByRegex &quot;my\\\\.company.*&quot; } } } 默认情况下，存储库包含所有内容，不包含任何内容： 如果声明include，那么它排除了一切 include 以外的内容。 如果声明exclude，则它将包括除exclude之外的所有内容。 如果声明include和exclude，则它仅包括显式包括但不排除的内容。 分割快照和发行版 build.gradle repositories { maven { url &quot;https://repo.mycompany.com/releases&quot; mavenContent { releasesOnly() } } maven { url &quot;https://repo.mycompany.com/snapshots&quot; mavenContent { snapshotsOnly() } } } 支持的元数据源 受支持的元数据源 元数据源 描述 排序 Maven Ivy/flat dir gradleMetadata() 寻找Gradle.module文件 1 是 是 mavenPom() 查找Maven.pom文件 2 是 是 ivyDescriptor() 查找ivy.xml文件 2 没有 是 artifact() 直接寻找artifact 3 是 是 从Gradle 5.3开始，解析元数据文件（无论是Ivy还是Maven）时，Gradle将寻找一个标记，指示存在匹配的Gradle Module元数据文件。如果找到它，它将代替Ivy或Maven文件使用。 从Gradle5.6开始，您可以通过添加ignoreGradleMetadataRedirection()到metadataSources声明来禁用此行为。 例.不使用gradle元数据重定向的Maven存储库 build.gradle repositories { maven { url &quot;http://repo.mycompany.com/repo&quot; metadataSources { mavenPom() artifact() ignoreGradleMetadataRedirection() } } } GAV坐标 GAV坐标一般指group，artifact，version 变体 构建变体是针对不同环境的配置，例如android开发中一般有debug和release两种变体 声明功能变体 可以通过应用java或java-library插件来声明功能变体。以下代码说明了如何声明名为mongodbSupport的功能： 示例1.声明一个功能变量 Groovy``Kotlin build.gradle group = 'org.gradle.demo' version = '1.0' java { registerFeature('mongodbSupport') { usingSourceSet(sourceSets.main) } } 元数据 从存储库中提取的每个模块都有与之关联的元数据，例如其组，名称，版本以及它提供的带有工件和依赖项的不同变体 可配置组件元数据规则的示例 build.gradle class TargetJvmVersionRule implements ComponentMetadataRule { final Integer jvmVersion @Inject TargetJvmVersionRule(Integer jvmVersion) { this.jvmVersion = jvmVersion } @Inject ObjectFactory getObjects() { } void execute(ComponentMetadataContext context) { context.details.withVariant(&quot;compile&quot;) { attributes { attribute(TargetJvmVersion.TARGET_JVM_VERSION_ATTRIBUTE, jvmVersion) attribute(Usage.USAGE_ATTRIBUTE, objects.named(Usage, Usage.JAVA_API)) } } } } dependencies { components { withModule(&quot;commons-io:commons-io&quot;, TargetJvmVersionRule) { params(7) } withModule(&quot;commons-collections:commons-collections&quot;, TargetJvmVersionRule) { params(8) } } implementation(&quot;commons-io:commons-io:2.6&quot;) implementation(&quot;commons-collections:commons-collections:3.2.2&quot;) } 可以通过以下方法进行修改变体： allVariants：修改组件的所有变体 withVariant(name)：修改由名称标识的单个变体 addVariant(name)或addVariant(name, base)：从头开始 或通过 复制 现有变体的详细信息（基础）向组件添加新变体 可以调整每个变体的以下详细信息： 标识变体的属性-attributes {}块 该变体提供的功能-withCapabilities { }块 变体的依赖项，包括丰富的版本-withDependencies {}块 变体的依赖关系约束，包括丰富版本-withDependencyConstraints {}块 构成变体实际内容的已发布文件的位置-withFiles { }块 平台 使用平台 获取平台中声明的版本 build.gradle dependencies { // get recommended versions from the platform project api platform(project(':platform')) // no version required api 'commons-httpclient:commons-httpclient' } platform表示法是一种简写表示法，实际上在后台执行了一些操作： 它将org.gradle.category属性设置为platform，这意味着Gradle将选择依赖项的 平台 组件。 它默认设置endorseStrictVersions行为， 这意味着如果平台声明了严格的依赖关系，则将强制执行它们。 这意味着默认情况下，对平台的依赖项会触发该平台中定义的所有严格版本的继承， 这对于平台作者确保所有使用者在依赖项的版本方面都遵循自己的决定很有用。 可以通过显式调用doNotEndorseStrictVersions方法来将其关闭。 例.依靠一个BOM导入其依赖约束 build.gradle dependencies { // import a BOM implementation platform('org.springframework.boot:spring-boot-dependencies:1.5.8.RELEASE') // define dependencies without versions implementation 'com.google.code.gson:gson' implementation 'dom4j:dom4j' } 导入BOM，确保其定义的版本覆盖找到的任何其他版本 build.gradle dependencies { // import a BOM. The versions used in this file will override any other version found in the graph implementation enforcedPlatform('org.springframework.boot:spring-boot-dependencies:1.5.8.RELEASE') // define dependencies without versions implementation 'com.google.code.gson:gson' implementation 'dom4j:dom4j' // this version will be overridden by the one found in the BOM implementation 'org.codehaus.groovy:groovy:1.8.6' } Capability 声明组件的capability build.gradle configurations { apiElements { outgoing { capability(&quot;com.acme:my-library:1.0&quot;) capability(&quot;com.other:module:1.1&quot;) } } runtimeElements { outgoing { capability(&quot;com.acme:my-library:1.0&quot;) capability(&quot;com.other:module:1.1&quot;) } } } 解决冲突 按Capability（能力）解决冲突，（若存在相同能力的依赖性会失败） build.gradle @CompileStatic class AsmCapability implements ComponentMetadataRule { void execute(ComponentMetadataContext context) { context.details.with { if (id.group == &quot;asm&quot; &amp;&amp; id.name == &quot;asm&quot;) { allVariants { it.withCapabilities { // Declare that ASM provides the org.ow2.asm:asm capability, but with an older version it.addCapability(&quot;org.ow2.asm&quot;, &quot;asm&quot;, id.version) } } } } } } 一个带有日志框架隐式冲突的构建文件 build.gradle dependencies { // Activate the &quot;LoggingCapability&quot; rule components.all(LoggingCapability) } @CompileStatic class LoggingCapability implements ComponentMetadataRule { final static Set&lt;String&gt; LOGGING_MODULES = [&quot;log4j&quot;, &quot;log4j-over-slf4j&quot;] as Set&lt;String&gt; void execute(ComponentMetadataContext context) { context.details.with { if (LOGGING_MODULES.contains(id.name)) { allVariants { it.withCapabilities { // Declare that both log4j and log4j-over-slf4j provide the same capability it.addCapability(&quot;log4j&quot;, &quot;log4j&quot;, id.version) } } } } } } 版本 版本规则 Gradle支持不同的版本字符串声明方式： 一个确切的版本：比如1.3，1.3.0-beta3，1.0-20150201.131010-1 一个Maven风格的版本范围：例如 [1.0,) [1.1, 2.0) (1.2, 1.5] [和]的符号表示包含性约束; (和)表示排他性约束。 当上界或下界缺失时，该范围没有上界或下界。 符号]可以被用来代替(用于排他性下界，[代替)用于排他性上界。例如]1.0, 2.0[ 前缀版本范围：例如 1.+ 1.3.+ 仅包含与+之前部分完全匹配的版本。 +本身的范围将包括任何版本。 一个latest-status版本：例如latest.integration，latest.release Maven的SNAPSHOT版本标识符：例如1.0-SNAPSHOT，1.4.9-beta1-SNAPSHOT 版本排序 每个版本均分为其组成的“部分”： 字符[. - _ +]用于分隔版本的不同“部分”。 同时包含数字和字母的任何部分都将分为以下各个部分： 1a1 == 1.a.1 仅比较版本的各个部分。实际的分隔符并不重要：1.a.1 == 1-a+1 == 1.a-1 == 1a1 使用以下规则比较2个版本的等效部分： 如果两个部分都是数字，则最高数字值 较高 ：1.1&lt;1.2 如果一个部分是数值，则认为它 高于 非数字部分：1.a&lt;1.1 如果两个部分都不是数字，则按字母顺序比较，区分大小写：1.A&lt; 1.B&lt; 1.a&lt;1.b 有额外数字部分的版本被认为比没有数字部分的版本高：1.1&lt;1.1.0 带有额外的非数字部分的版本被认为比没有数字部分的版本低：1.1.a&lt;1.1 某些字符串值出于排序目的具有特殊含义： 字符串dev被认为比任何其他字符串部分低：1.0-dev&lt; 1.0-alpha&lt; 1.0-rc。 字符串rc、release和final被认为比任何其他字符串部分都高（按顺序排列：1.0-zeta&lt; 1.0-rc&lt; 1.0-release&lt; 1.0-final&lt; 1.0。 字符串SNAPSHOT没有特殊意义，和其他字符串部分一样按字母顺序排序：1.0-alpha&lt; 1.0-SNAPSHOT&lt; 1.0-zeta&lt; 1.0-rc&lt; 1.0。 数值快照版本没有特殊意义，和其他数值部分一样进行排序：1.0&lt; 1.0-20150201.121010-123&lt; 1.1。 简单来说:数字&gt;final&gt;release&gt;rc&gt;字母&gt;dev 声明没有版本的依赖 对于较大的项目，建议的做法是声明没有版本的依赖项， 并将依赖项约束 用于版本声明。 优势在于，依赖关系约束使您可以在一处管理所有依赖关系的版本，包括可传递的依赖关系。 build.gradle dependencies { implementation 'org.springframework:spring-web' } dependencies { constraints { implementation 'org.springframework:spring-web:5.0.2.RELEASE' } } 依赖方式 strictly 与该版本符号不匹配的任何版本将被排除。这是最强的版本声明。 在声明的依赖项上，strictly可以降级版本。 在传递依赖项上，如果无法选择此子句可接受的版本，将导致依赖项解析失败。 有关详细信息，请参见覆盖依赖项版本。 该术语支持动态版本。 定义后，将覆盖先前的require声明并清除之前的 reject。 require 表示所选版本不能低于require可接受的版本，但可以通过冲突解决方案提高，即使更高版本具有排他性更高的界限。 这就是依赖项上的直接版本所转换的内容。该术语支持动态版本。 定义后，将覆盖先前的strictly声明并清除之前的 reject。 prefer 这是一个非常软的版本声明。仅当对该模块的版本没有更强的非动态观点时，才适用。 该术语不支持动态版本。 定义可以补充strictly或require。 在级别层次结构之外还有一个附加术语： reject 声明模块不接受特定版本。如果唯一的可选版本也被拒绝，这将导致依赖项解析失败。该术语支持动态版本。 动态版本 build.gradle plugins { id 'java-library' } repositories { mavenCentral() } dependencies { implementation 'org.springframework:spring-web:5.+' } 版本快照 声明一个版本变化的依赖 build.gradle plugins { id 'java-library' } repositories { mavenCentral() maven { url 'https://repo.spring.io/snapshot/' } } dependencies { implementation 'org.springframework:spring-web:5.0.3.BUILD-SNAPSHOT' } 以编程方式控制依赖项缓存 您可以使用ResolutionStrategy 对配置进行编程来微调缓存的某些方面。 如果您想永久更改设置，则编程方式非常有用。 默认情况下，Gradle将动态版本缓存24小时。 要更改Gradle将解析后的版本缓存为动态版本的时间，请使用： 例.动态版本缓存控制 build.gradle configurations.all { resolutionStrategy.cacheDynamicVersionsFor 10, 'minutes' } 默认情况下，Gradle会将更改的模块缓存24小时。 要更改Gradle将为更改的模块缓存元数据和工件的时间，请使用： 例.改变模块缓存控制 build.gradle configurations.all { resolutionStrategy.cacheChangingModulesFor 4, 'hours' } 锁定配置 锁定特定配置 build.gradle configurations { compileClasspath { resolutionStrategy.activateDependencyLocking() } } 锁定所有配置 build.gradle dependencyLocking { lockAllConfigurations() } 解锁特定配置 build.gradle configurations { compileClasspath { resolutionStrategy.deactivateDependencyLocking() } } 锁定buildscript类路径配置 如果将插件应用于构建，则可能还需要利用依赖锁定。为了锁定用于脚本插件的classpath配置，请执行以下操作： build.gradle buildscript { configurations.classpath { resolutionStrategy.activateDependencyLocking() } } 使用锁定模式微调依赖项锁定行为 虽然默认锁定模式的行为如上所述，但是还有其他两种模式可用： Strict模式 ：在该模式下，除了上述验证外，如果被标记为锁定的配置没有与之相关联的锁定状态，则依赖性锁定将失败。 Lenient模式：在这种模式下，依存关系锁定仍将固定动态版本，但除此之外，依赖解析的变化不再是错误。 锁定模式可以从dependencyLocking块中进行控制，如下所示： build.gradle dependencyLocking { lockMode = LockMode.STRICT } 版本冲突 用force强制执行一个依赖版本 build.gradle dependencies { implementation 'org.apache.httpcomponents:httpclient:4.5.4' implementation('commons-codec:commons-codec:1.9') { force = true } } 排除特定依赖声明的传递依赖 build.gradle dependencies { implementation('commons-beanutils:commons-beanutils:1.9.4') { exclude group: 'commons-collections', module: 'commons-collections' } } 版本冲突时失败 build.gradle configurations.all { resolutionStrategy { failOnVersionConflict() } } 使用动态版本时失败 build.gradle configurations.all { resolutionStrategy { failOnDynamicVersions() } } 改变版本时失败 build.gradle configurations.all { resolutionStrategy { failOnChangingVersions() } } 解析无法再现时失败 build.gradle configurations.all { resolutionStrategy { failOnNonReproducibleResolution() } } 插件 插件作用：将插件应用于项目可以使插件扩展项目的功能。它可以执行以下操作： 扩展Gradle模型（例如，添加可以配置的新DSL元素） 根据约定配置项目（例如，添加新任务或配置合理的默认值） 应用特定的配置（例如，添加组织存储库或强制执行标准） 简单来说，插件可以拓展项目功能，如任务，依赖，拓展属性，约束 插件类型 二进制插件 ：通过实现插件接口以编程方式编写二进制插件，或使用Gradle的一种DSL语言以声明方式编写二进制插件 脚本插件 ：脚本插件是其他构建脚本，可以进一步配置构建，并通常采用声明式方法来操纵构建 插件通常起初是脚本插件（因为它们易于编写），然后，随着代码变得更有价值，它被迁移到可以轻松测试并在多个项目或组织之间共享的二进制插件。 应用插件 二进制插件 实现了org.gradle.api.Plugin接口 apply plugin: 'com.android.application' apply plugin apply plugin: 'java' //id == apply plugin: org.gradle.api.plugins.JavaPlugin //类型 == apply plugin: JavaPlugin //org.gradle.api.plugins默认导入 plugins DSL plugins { id 'java' //应用核心插件 id 'com.jfrog.bintray' version '0.4.1' //应用社区插件 id 'com.example.hello' version '1.0.0' apply false //使用`apply false`语法告诉Gradle不要将插件应用于当前项目 } 脚本插件 脚本插件会自动解析，可以从本地文件系统或远程位置的脚本中应用。可以将多个脚本插件（任意一种形式）应用于给定目标。 apply from:'version.gradle' apply可传入内容 void apply(Map&lt;String,? options); void apply(Closure closure); void apply(Action&lt;? super ObjectConfigurationAction&gt; action); 定义插件 定义一个带有ID的buildSrc插件 buildSrc / build.gradle plugins { id 'java-gradle-plugin' } gradlePlugin { plugins { myPlugins { id = 'my-plugin' implementationClass = 'my.MyPlugin' } } } 第三方插件 通过将插件添加到构建脚本classpath中，然后应用该插件，可以将已发布为外部jar文件的二进制插件添加到项目中。可以使用buildscript {}块将外部jar添加到构建脚本classpath中。 buildscript { repositories { google() jcenter() } dependencies { classpath &quot;com.android.tools.build:gradle:4.0.1&quot; } } 插件管理 pluginManagement {}块只能出现在settings.gradle文件中，必须是文件中的第一个块，也可以以settings形式出现在初始化脚本中。 settings.gradle pluginManagement { plugins { } resolutionStrategy { } repositories { } } rootProject.name = 'plugin-management' init.gradle settingsEvaluated { settings -&gt; settings.pluginManagement { plugins { } resolutionStrategy { } repositories { } } } 例：通过pluginManagement管理插件版本。 settings.gradle pluginManagement { plugins { id 'com.example.hello' version &quot;${helloPluginVersion}&quot; } } gradle.properties helloPluginVersion=1.0.0 自定义插件存储库 要指定自定义插件存储库，请使用repositories {}块其中的pluginManagement {}： settings.gradle pluginManagement { repositories { maven { url '../maven-repo' } gradlePluginPortal() ivy { url '../ivy-repo' } } } java库 导入java apply plugin:'java' 自定义路径 sourceSets { main { java { srcDirs = ['src'] } } test { java { srcDirs = ['test'] } } } sourceSets { main { java { srcDir 'thirdParty/src/main/java' } } } 导入依赖 repositories { jcenter() } dependencies { implementation group:'com.android.support',name:'appcompat-v7',version:'28.0.0' implementation 'com.android.support:appcompat-v7:28.0.0' implementation protect(':p') implementation file('libs/ss.jar','libs/ss2.jar') implementation fileTree(dir: &quot;libs&quot;, include: [&quot;*.jar&quot;]) } 多项目 设置 settings.gradle include ':app' rootProject.name = &quot;GradleTest&quot; 安卓实用 设置签名 android { signingConfig = { release { storeFile file(&quot;MYKEY.keystore&quot;) storePassword &quot;storePassword&quot; keyAlias &quot;keyAlias&quot; keyPassword &quot;keyPassword&quot; } } } 自定义输出apk文件名称 applicationVariants.all { variant -&gt; variant.outputs.all { output -&gt; def fileName = &quot;自定义名称_${variant.versionName}_release.apk&quot; def outFile = output.outputFile if (outFile != null &amp;&amp; outFile.name.endsWith('.apk')) { outputFileName = fileName } } } 动态AndroidManifest &lt;meta-data android:name=&quot;paramName&quot; android:value=&quot;${PARAM_NAME}&quot;&gt; android { productFlavors{ google{ manifestPlaceholders.put(&quot;PARAM_NAME&quot;,'google') } } } 多渠道 android { productFlavors{ google{ }, baidu{ } } productFlavors.all{ flavor-&gt; manifestPlaceholders.put(&quot;PARAM_NAME&quot;,name) } } adb设置 adb工具 android { adbOptions{ timeOutInMs = 5000 //5s超时 installOptions '-r','-s' //安装指令 } } dexOptions dex工具 android { dexOptions{ incremental true //增量 javaMaxHeapSize '4G'//dx最大队内存 jumboMode true //强制开启jumbo跳过65535限制 preDexLibraries true //提高增量构建速度 threadCount 1 //dx线程数量 } } Ant 例.将嵌套元素传递给Ant任 build.gradle task zip { doLast { ant.zip(destfile: 'archive.zip') { fileset(dir: 'src') { include(name: '**.xml') exclude(name: '**.java') } } } } 例.使用Ant类型 build.gradle task list { doLast { def path = ant.path { fileset(dir: 'libs', includes: '*.jar') } path.list().each { println it } } } 例.使用定制的Ant任务 build.gradle task check { doLast { ant.taskdef(resource: 'checkstyletask.properties') { classpath { fileset(dir: 'libs', includes: '*.jar') } } ant.checkstyle(config: 'checkstyle.xml') { fileset(dir: 'src') } } } Lint Lint：android tool目录下的工具，一个代码扫描工具，能够帮助我们识别资源、代码结构存在的问题 lintOptions android { lintOptions{ abortOnError true //发生错误时推出Gradle absolutePaths true //配置错误输出是否显示绝对路径 check 'NewApi','InlinedApi' // 检查lint check的issue id enable 'NewApi','InlinedApi' //启动 lint check的issue id disable 'NewApi','InlinedApi' //关闭 lint check的issue id checkAllWarnings true //检查所有警告issue ignoreWarning true //忽略警告检查，默认false checkReleaseBuilds true //检查致命错误，默认true explainIssues true //错误报告是否包含解释说明，默认true htmlOutput new File(&quot;/xx.html&quot;) //html报告输出路径 htmlReport true // 是否生成html报告，默认true lintConfig new File(&quot;/xx.xml&quot;) //lint配置 noLines true // 输出不带行号 默认true quite true // 安静模式 showAll true //是否显示所有输出，不截断 } } ","link":"https://tianxiawuhao.github.io/Fe3q50EYd/"},{"title":"gradle-Logging","content":"Logging 日志级别 日志级别 说明 ERROR 错误讯息 QUIET 重要信息消息 WARNING 警告讯息 LIFECYCLE 进度信息消息 INFO 信息讯息 DEBUG 调试信息 选择日志级别 可以通过命令行选项或者gradle.properties文件配置 选项 输出日志级别 没有记录选项 LIFECYCLE及更高 -q or--quiet QUIET及更高 -w or --warn WARNING及更高 -i or --info INFO及更高 -d or --debug DEBUG及更高版本（即所有日志消息） Stacktrace命令行选项 -s or --stacktrace 打印简洁的堆栈跟踪信息，推荐使用 -S or --full-stacktrace 打印完整的堆栈跟踪信息。 编写日志 使用stdout编写日志消息 println 'A message which is logged at QUIET level' 编写自己的日志消息 logger.quiet('An info log message which is always logged.') logger.error('An error log message.') logger.warn('A warning log message.') logger.lifecycle('A lifecycle info log message.') logger.info('An info log message.') logger.debug('A debug log message.') logger.trace('A trace log message.') // Gradle never logs TRACE level logs // 用占位符写一条日志消息 logger.info('A {} log message', 'info') logger构建脚本提供了一个属性，该脚本是Logger的实例。该接口扩展了SLF4JLogger接口，并向其中添加了一些Gradle特定的方法 使用SLF4J写入日志消息 import org.slf4j.LoggerFactory def slf4jLogger = LoggerFactory.getLogger('some-logger') slf4jLogger.info('An info log message logged using SLF4j') 构建生命周期 Gradle的核心是一种基于依赖性的编程语言，这意味着你可以定义任务和任务之间的依赖关系。 Gradle保证这些任务按照其依赖关系的顺序执行，并且每个任务只执行一次。 这些任务形成一个定向无环图。 构建阶段 Gradle构建具有三个不同的阶段。 初始化 Gradle支持单项目和多项目构建。在初始化阶段，Gradle决定要参与构建的项目，并为每个项目创建一个Project实例。 配置 在此阶段，将配置项目对象。执行作为构建一部分的 所有 项目的构建脚本。 执行 Gradle确定要在配置阶段创建和配置的任务子集。子集由传递给gradle命令的任务名称参数和当前目录确定。然后Gradle执行每个选定的任务。 设置文件 默认名称是settings.gradle 项目构建在多项目层次结构的根项目中必须具有一个settings.gradle文件。 对于单项目构建，设置文件是可选的 初始化 查找settings.gradle文件判断是否多项目 没有settings.gradle或settings.gradle没有多项目配置则为单项目 例：将test任务添加到每个具有特定属性集的项目 build.gradle allprojects { afterEvaluate { project -&gt; if (project.hasTests) { println &quot;Adding test task to $project&quot; project.task('test') { doLast { println &quot;Running tests for $project&quot; } } } } } 输出 gradle -q test &gt; gradle -q test Adding test task to project ':project-a' Running tests for project ':project-a' 初始化脚本 初始化脚本与Gradle中的其他脚本相似。但是，这些脚本在构建开始之前运行。初始化脚本不能访问buildSrc项目中的类。 使用初始化脚本 有几种使用初始化脚本的方法： 在命令行中指定一个文件。命令行选项是-I或-init-script，后面是脚本的路径。 命令行选项可以出现一次以上，每次都会添加另一个 init 脚本。 如果命令行上指定的文件不存在，编译将失败。 在 USER_HOME /.gradle/目录中放置一个名为init.gradle（或init.gradle.ktsKotlin）的文件。 在 USER_HOME /.gradle/init.d/目录中放置一个以.gradle（或.init.gradle.ktsKotlin）结尾的文件。 在Gradle发行版的 GRADLE_HOME /init.d/目录中放置一个以.gradle（或.init.gradle.ktsKotlin）结尾的文件。这使您可以打包包含一些自定义构建逻辑和插件的自定义Gradle发行版。您可以将其与Gradle Wrapper结合使用，以使自定义逻辑可用于企业中的所有内部版本。 如果发现一个以上的初始化脚本，它们将按照上面指定的顺序依次执行。 示例 build.gradle repositories { mavenCentral() } task showRepos { doLast { println &quot;All repos:&quot; println repositories.collect { it.name } } } init.gradle allprojects { repositories { mavenLocal() } } 运行任务： gradle --init-script init.gradle -q showRepos 初始化脚本里面依赖添加依赖 init.gradle initscript { repositories { mavenCentral() } dependencies { classpath 'org.apache.commons:commons-math:2.0' } } 多项目 可在settings.gradle文件中设置多个项目关系，如下项目结构 . ├── app │ ... │ └── build.gradle └── settings.gradle settings.gradle rootProject.name = 'basic-multiproject' //根项目名 include 'app' //子项目 子项目间依赖 dependencies { implementation(project(&quot;:shared&quot;)) } ","link":"https://tianxiawuhao.github.io/wYNztHepH/"},{"title":"Groovy基础","content":"Groovy基础 基本规则 没有分号 方法括号可以省略 方法可以不写return，返回最后一句代码 代码块可以作为参数传递 定义 def param = 'hello world' def param1 = &quot;hello world&quot; println &quot;${param1} ,li&quot; ${}里面可以放变量，也可以是表达式，只有双引号里面可以使用 声明变量 可以在构建脚本中声明两种变量：局部变量和额外属性。 局部变量 局部变量用def关键字声明。它们仅在声明它们的范围内可见。局部变量是基础Groovy语言的功能。 def dest = &quot;dest&quot; task copy(type: Copy) { from &quot;source&quot; into dest } ext属性 Gradle的域模型中的所有增强对象都可以容纳额外的用户定义属性。 可以通过拥有对象的ext属性添加，读取和设置其他属性。可以使用一个ext块一次添加多个属性。 plugins { id 'java' } ext { springVersion = &quot;3.1.0.RELEASE&quot; emailNotification = &quot;build@master.org&quot; } sourceSets.all { ext.purpose = null } sourceSets { main { purpose = &quot;production&quot; } test { purpose = &quot;test&quot; } plugin { purpose = &quot;production&quot; } } task printProperties { doLast { println springVersion println emailNotification sourceSets.matching { it.purpose == &quot;production&quot; }.each { println it.name } } } 输出 gradle -q printProperties &gt; gradle -q printProperties 3.1.0.RELEASE build@master.org main plugin 变量范围：本地和脚本范围 用类型修饰符声明的变量在闭包中可见，但在方法中不可见。 String localScope1 = 'localScope1' def localScope2 = 'localScope2' scriptScope = 'scriptScope' println localScope1 println localScope2 println scriptScope closure = { println localScope1 println localScope2 println scriptScope } def method() { try { localScope1 } catch (MissingPropertyException e) { println 'localScope1NotAvailable' } try { localScope2 } catch(MissingPropertyException e) { println 'localScope2NotAvailable' } println scriptScope } closure.call() method() 输出 groovy scope.groovy &gt; groovy作用域 localScope1 localScope2 scriptScope localScope1 localScope2 scriptScope localScope1NotAvailable localScope2NotAvailable scriptScope 对象 使用对象 您可以按照以下易读的方式配置任意对象。 build.gradle import java.text.FieldPosition task configure { doLast { def pos = configure(new FieldPosition(10)) { beginIndex = 1 endIndex = 5 } println pos.beginIndex println pos.endIndex } } &gt; gradle -q configure 1 5 使用外部脚本配置任意对象 您也可以使用外部脚本配置任意对象。 build.gradle task configure { doLast { def pos = new java.text.FieldPosition(10) // Apply the script apply from: 'other.gradle', to: pos println pos.beginIndex println pos.endIndex } } other.gradle // Set properties. beginIndex = 1 endIndex = 5 输出 gradle -q configure &gt; gradle -q configure 1 5 属性访问器 Groovy自动将属性引用转换为对适当的getter或setter方法的调用。 build.gradle // Using a getter method println project.buildDir println getProject().getBuildDir() // Using a setter method project.buildDir = 'target' getProject().setBuildDir('target') 闭包 闭包（闭合代码块，可以引用传入的变量） task testClosure { doLast { func { println it } funa { a, b -&gt; println a + b } } } def funa(closure) { closure(10, 3) } def func(closure) { closure(10) } 闭包委托 每个闭包都有一个delegate对象，Groovy使用该对象来查找不是闭包的局部变量或参数的变量和方法引用。 例.闭包委托 class Info { int id; String code; def log() { println(&quot;code:${code};id:${id}&quot;) } } def info(Closure&lt;Info&gt; closure) { Info p = new Info() closure.delegate = p // 委托模式优先 closure.setResolveStrategy(Closure.DELEGATE_FIRST) closure(p) } task configClosure { doLast { info { code = &quot;cix&quot; id = 1 log() } } } 输出 &gt; Task :configClosure code:cix;id:1 BUILD SUCCESSFUL in 276ms 例：使用必包委托设置依赖 dependencies { assert delegate == project.dependencies testImplementation('junit:junit:4.13') delegate.testImplementation('junit:junit:4.13') } 方法 方法调用上的可选括号 括号对于方法调用是可选的。 build.gradle test.systemProperty 'some.prop', 'value' test.systemProperty('some.prop', 'value') 闭包作为方法中的最后一个参数 当方法的最后一个参数是闭包时，可以将闭包放在方法调用之后： build.gradle repositories { println &quot;in a closure&quot; } repositories() { println &quot;in a closure&quot; } repositories({ println &quot;in a closure&quot; }) 集合 List def list = [1,2,3,4] println list[0] // 1 println list[-1] // 4 最后一个 println list[-2] // 3 倒数第二个 println list[0..2] // 第1-3个 list.each { //迭代 println it } Map def map= ['name':'li', 'age':18] println map[name] // li println map.age // 18 list.each { //迭代 println &quot;${it.key}:${it.value}&quot; } JavaBean class A{ private int a; //可通过A().a 获取修改 public int getB(){//可通过A().b获取，但不能修改 1 } } build.gradle // List literal test.includes = ['org/gradle/api/**', 'org/gradle/internal/**'] List&lt;String&gt; list = new ArrayList&lt;String&gt;() list.add('org/gradle/api/**') list.add('org/gradle/internal/**') test.includes = list // Map literal. Map&lt;String, String&gt; map = [key1:'value1', key2: 'value2'] // Groovy will coerce named arguments // into a single map argument apply plugin: 'java' 默认导入 为了使构建脚本更简洁，Gradle自动向Gradle脚本添加了一些类。 import org.gradle.* import org.gradle.api.* import org.gradle.api.artifacts.* import org.gradle.api.artifacts.component.* import org.gradle.api.artifacts.dsl.* import org.gradle.api.artifacts.ivy.* import org.gradle.api.artifacts.maven.* import org.gradle.api.artifacts.query.* import org.gradle.api.artifacts.repositories.* import org.gradle.api.artifacts.result.* import org.gradle.api.artifacts.transform.* import org.gradle.api.artifacts.type.* import org.gradle.api.artifacts.verification.* import org.gradle.api.attributes.* import org.gradle.api.attributes.java.* import org.gradle.api.capabilities.* import org.gradle.api.component.* import org.gradle.api.credentials.* import org.gradle.api.distribution.* import org.gradle.api.distribution.plugins.* import org.gradle.api.execution.* import org.gradle.api.file.* import org.gradle.api.initialization.* import org.gradle.api.initialization.definition.* import org.gradle.api.initialization.dsl.* import org.gradle.api.invocation.* import org.gradle.api.java.archives.* import org.gradle.api.jvm.* import org.gradle.api.logging.* import org.gradle.api.logging.configuration.* import org.gradle.api.model.* import org.gradle.api.plugins.* import org.gradle.api.plugins.antlr.* import org.gradle.api.plugins.quality.* import org.gradle.api.plugins.scala.* import org.gradle.api.provider.* import org.gradle.api.publish.* import org.gradle.api.publish.ivy.* import org.gradle.api.publish.ivy.plugins.* import org.gradle.api.publish.ivy.tasks.* import org.gradle.api.publish.maven.* import org.gradle.api.publish.maven.plugins.* import org.gradle.api.publish.maven.tasks.* import org.gradle.api.publish.plugins.* import org.gradle.api.publish.tasks.* import org.gradle.api.reflect.* import org.gradle.api.reporting.* import org.gradle.api.reporting.components.* import org.gradle.api.reporting.dependencies.* import org.gradle.api.reporting.dependents.* import org.gradle.api.reporting.model.* import org.gradle.api.reporting.plugins.* import org.gradle.api.resources.* import org.gradle.api.services.* import org.gradle.api.specs.* import org.gradle.api.tasks.* import org.gradle.api.tasks.ant.* import org.gradle.api.tasks.application.* import org.gradle.api.tasks.bundling.* import org.gradle.api.tasks.compile.* import org.gradle.api.tasks.diagnostics.* import org.gradle.api.tasks.incremental.* import org.gradle.api.tasks.javadoc.* import org.gradle.api.tasks.options.* import org.gradle.api.tasks.scala.* import org.gradle.api.tasks.testing.* import org.gradle.api.tasks.testing.junit.* import org.gradle.api.tasks.testing.junitplatform.* import org.gradle.api.tasks.testing.testng.* import org.gradle.api.tasks.util.* import org.gradle.api.tasks.wrapper.* import org.gradle.authentication.* import org.gradle.authentication.aws.* import org.gradle.authentication.http.* import org.gradle.build.event.* import org.gradle.buildinit.plugins.* import org.gradle.buildinit.tasks.* import org.gradle.caching.* import org.gradle.caching.configuration.* import org.gradle.caching.http.* import org.gradle.caching.local.* import org.gradle.concurrent.* import org.gradle.external.javadoc.* import org.gradle.ide.visualstudio.* import org.gradle.ide.visualstudio.plugins.* import org.gradle.ide.visualstudio.tasks.* import org.gradle.ide.xcode.* import org.gradle.ide.xcode.plugins.* import org.gradle.ide.xcode.tasks.* import org.gradle.ivy.* import org.gradle.jvm.* import org.gradle.jvm.application.scripts.* import org.gradle.jvm.application.tasks.* import org.gradle.jvm.platform.* import org.gradle.jvm.plugins.* import org.gradle.jvm.tasks.* import org.gradle.jvm.tasks.api.* import org.gradle.jvm.test.* import org.gradle.jvm.toolchain.* import org.gradle.language.* import org.gradle.language.assembler.* import org.gradle.language.assembler.plugins.* import org.gradle.language.assembler.tasks.* import org.gradle.language.base.* import org.gradle.language.base.artifact.* import org.gradle.language.base.compile.* import org.gradle.language.base.plugins.* import org.gradle.language.base.sources.* import org.gradle.language.c.* import org.gradle.language.c.plugins.* import org.gradle.language.c.tasks.* import org.gradle.language.coffeescript.* import org.gradle.language.cpp.* import org.gradle.language.cpp.plugins.* import org.gradle.language.cpp.tasks.* import org.gradle.language.java.* import org.gradle.language.java.artifact.* import org.gradle.language.java.plugins.* import org.gradle.language.java.tasks.* import org.gradle.language.javascript.* import org.gradle.language.jvm.* import org.gradle.language.jvm.plugins.* import org.gradle.language.jvm.tasks.* import org.gradle.language.nativeplatform.* import org.gradle.language.nativeplatform.tasks.* import org.gradle.language.objectivec.* import org.gradle.language.objectivec.plugins.* import org.gradle.language.objectivec.tasks.* import org.gradle.language.objectivecpp.* import org.gradle.language.objectivecpp.plugins.* import org.gradle.language.objectivecpp.tasks.* import org.gradle.language.plugins.* import org.gradle.language.rc.* import org.gradle.language.rc.plugins.* import org.gradle.language.rc.tasks.* import org.gradle.language.routes.* import org.gradle.language.scala.* import org.gradle.language.scala.plugins.* import org.gradle.language.scala.tasks.* import org.gradle.language.scala.toolchain.* import org.gradle.language.swift.* import org.gradle.language.swift.plugins.* import org.gradle.language.swift.tasks.* import org.gradle.language.twirl.* import org.gradle.maven.* import org.gradle.model.* import org.gradle.nativeplatform.* import org.gradle.nativeplatform.platform.* import org.gradle.nativeplatform.plugins.* import org.gradle.nativeplatform.tasks.* import org.gradle.nativeplatform.test.* import org.gradle.nativeplatform.test.cpp.* import org.gradle.nativeplatform.test.cpp.plugins.* import org.gradle.nativeplatform.test.cunit.* import org.gradle.nativeplatform.test.cunit.plugins.* import org.gradle.nativeplatform.test.cunit.tasks.* import org.gradle.nativeplatform.test.googletest.* import org.gradle.nativeplatform.test.googletest.plugins.* import org.gradle.nativeplatform.test.plugins.* import org.gradle.nativeplatform.test.tasks.* import org.gradle.nativeplatform.test.xctest.* import org.gradle.nativeplatform.test.xctest.plugins.* import org.gradle.nativeplatform.test.xctest.tasks.* import org.gradle.nativeplatform.toolchain.* import org.gradle.nativeplatform.toolchain.plugins.* import org.gradle.normalization.* import org.gradle.platform.base.* import org.gradle.platform.base.binary.* import org.gradle.platform.base.component.* import org.gradle.platform.base.plugins.* import org.gradle.play.* import org.gradle.play.distribution.* import org.gradle.play.platform.* import org.gradle.play.plugins.* import org.gradle.play.plugins.ide.* import org.gradle.play.tasks.* import org.gradle.play.toolchain.* import org.gradle.plugin.devel.* import org.gradle.plugin.devel.plugins.* import org.gradle.plugin.devel.tasks.* import org.gradle.plugin.management.* import org.gradle.plugin.use.* import org.gradle.plugins.ear.* import org.gradle.plugins.ear.descriptor.* import org.gradle.plugins.ide.* import org.gradle.plugins.ide.api.* import org.gradle.plugins.ide.eclipse.* import org.gradle.plugins.ide.idea.* import org.gradle.plugins.javascript.base.* import org.gradle.plugins.javascript.coffeescript.* import org.gradle.plugins.javascript.envjs.* import org.gradle.plugins.javascript.envjs.browser.* import org.gradle.plugins.javascript.envjs.http.* import org.gradle.plugins.javascript.envjs.http.simple.* import org.gradle.plugins.javascript.jshint.* import org.gradle.plugins.javascript.rhino.* import org.gradle.plugins.signing.* import org.gradle.plugins.signing.signatory.* import org.gradle.plugins.signing.signatory.pgp.* import org.gradle.plugins.signing.type.* import org.gradle.plugins.signing.type.pgp.* import org.gradle.process.* import org.gradle.swiftpm.* import org.gradle.swiftpm.plugins.* import org.gradle.swiftpm.tasks.* import org.gradle.testing.base.* import org.gradle.testing.base.plugins.* import org.gradle.testing.jacoco.plugins.* import org.gradle.testing.jacoco.tasks.* import org.gradle.testing.jacoco.tasks.rules.* import org.gradle.testkit.runner.* import org.gradle.vcs.* import org.gradle.vcs.git.* import org.gradle.work.* import org.gradle.workers.* 导入依赖 buildscript { repositories { mavenCentral() } dependencies { classpath group: 'commons-codec', name: 'commons-codec', version: '1.2' } } ","link":"https://tianxiawuhao.github.io/4WWjxpN2N/"},{"title":"gradle项目与任务","content":"项目与任务 Gradle中的所有内容都位于两个基本概念之上： projects ：每个Gradle构建都由一个或多个 projects组成 ，一个projects代表什么取决于您使用Gradle做的事情。例如，一个projects可能代表一个JAR库或一个Web应用程序。 tasks ：每个projects由一个或多个 tasks组成 。tasks代表构建执行的一些原子工作。这可能是编译某些类，创建JAR，生成Javadoc或将一些存档发布到存储库。 项目 表.项目属性 名称 类型 默认值 project Project 该Project实例 name String 项目目录的名称。 path String 项目的绝对路径。 description String 项目说明。 projectDir File 包含构建脚本的目录。 buildDir File projectDir /build group Object unspecified version Object unspecified ant ant build 一个AntBuilder实例 任务 定义任务 使用字符串作为任务名称定义任务 使用tasks容器定义任务 使用DSL特定语法定义任务 例： build.gradle // 使用字符串作为任务名称定义任务 task('hello') { doLast { println &quot;hello&quot; } } // 使用tasks容器定义任务 tasks.create('hello') { doLast { println &quot;hello&quot; } } // 使用DSL特定语法定义任务 task(hello) { doLast { println &quot;hello&quot; } } task('copy', type: Copy) { from(file('srcDir')) into(buildDir) } 定位任务 使用DSL特定语法访问任务 通过任务集合访问任务 通过路径访问 按任务类型访问任务 task hello task copy(type: Copy) // 使用DSL特定语法访问任务 println hello.name println project.hello.name println copy.destinationDir println project.copy.destinationDir // 通过任务集合访问任务 println tasks.hello.name println tasks.named('hello').get().name println tasks.copy.destinationDir println tasks.named('copy').get().destinationDir //按任务类型访问任务 tasks.withType(Tar).configureEach { enabled = false } task test { dependsOn tasks.withType(Copy) } 通过路径访问 project-a / build.gradle task hello build.gradle task hello println tasks.getByPath('hello').path println tasks.getByPath(':hello').path println tasks.getByPath('project-a:hello').path println tasks.getByPath(':project-a:hello').path 配置任务 使用API配置任务 例： build.gradle Copy myCopy = tasks.getByName(&quot;myCopy&quot;) myCopy.from 'resources' myCopy.into 'target' myCopy.include('**/*.txt', '**/*.xml', '**/*.properties') 使用DSL特定语法配置任务 例： build.gradle // Configure task using Groovy dynamic task configuration block myCopy { from 'resources' into 'target' } myCopy.include('**/*.txt', '**/*.xml', '**/*.properties') 用配置块定义一个任务 例： build.gradle task copy(type: Copy) { from 'resources' into 'target' include('**/*.txt', '**/*.xml', '**/*.properties') } 将参数传递给任务构造函数 与Task在创建后配置可变属性相反，您可以将参数值传递给Task类的构造函数。为了将值传递给Task构造函数，您必须使用@javax.inject.Inject注释相关的构造函数。 首先创建带有@Inject构造函数的任务类 class CustomTask extends DefaultTask { final String message final int number @Inject CustomTask(String message, int number) { this.message = message this.number = number } } 然后创建一个任务，并在参数列表的末尾传递构造函数参数。 tasks.create('myTask', CustomTask, 'hello', 42) 你也可以使用Map创建带有构造函数参数的任务 task myTask(type: CustomTask, constructorArgs: ['hello', 42]) 向任务添加依赖项 从另一个项目添加对任务的依赖 project('project-a') { task taskX { dependsOn ':project-b:taskY' doLast { println 'taskX' } } } project('project-b') { task taskY { doLast { println 'taskY' } } } 使用任务对象添加依赖 task taskX { doLast { println 'taskX' } } task taskY { doLast { println 'taskY' } } taskX.dependsOn taskY 使用惰性块添加依赖项 task taskX { doLast { println 'taskX' } } // Using a Groovy Closure taskX.dependsOn { tasks.findAll { task -&gt; task.name.startsWith('lib') } } task lib1 { doLast { println 'lib1' } } task lib2 { doLast { println 'lib2' } } task notALib { doLast { println 'notALib' } } 任务排序 控制任务排序的两种方式： must run after ：必须在之后运行 should run after：应该在之后运行 例 task taskX { doLast { println 'taskX' } } task taskY { doLast { println 'taskY' } } taskY.mustRunAfter taskX should run after被忽略的情况 引入排序周期。 使用并行执行时，除了 &quot;should run after &quot;任务外，一个任务的所有依赖关系都已被满足， 引入排序周期例子 task taskX { doLast { println 'taskX' } } task taskY { doLast { println 'taskY' } } task taskZ { doLast { println 'taskZ' } } taskX.dependsOn taskY taskY.dependsOn taskZ taskZ.shouldRunAfter taskX 为任务添加描述 您可以在任务中添加描述。执行gradle tasks时将显示此描述。 build.gradle task copy(type: Copy) { description 'Copies the resource directory to the target directory.' from 'resources' into 'target' include('**/*.txt', '**/*.xml', '**/*.properties') } 跳过任务 onlyIf跳过 hello.onlyIf { !project.hasProperty('skipHello') } //StopExecutionException跳过 compile.doFirst { if (true) { throw new StopExecutionException() } } 禁用任务 task disableMe { doLast { println 'This should not be printed if the task is disabled.' } } disableMe.enabled = false 任务超时 task hangingTask() { doLast { Thread.sleep(100000) } timeout = Duration.ofMillis(500) } 任务规则 有时您想执行一个任务，该任务的行为取决于较大或无限数量的参数值范围。提供此类任务的一种非常好的表达方式是任务规则： 任务规则 tasks.addRule(&quot;Pattern: ping&lt;ID&gt;&quot;) { String taskName -&gt; if (taskName.startsWith(&quot;ping&quot;)) { task(taskName) { doLast { println &quot;Pinging: &quot; + (taskName - 'ping') } } } } task groupPing { dependsOn pingServer1, pingServer2 } &gt; gradle -q groupPing Ping：Server1 Ping：Server2 终结器任务 计划运行终结任务时，终结任务会自动添加到任务图中。即使完成任务失败，也将执行终结器任务。 task taskX { doLast { println 'taskX' } } task taskY { doLast { println 'taskY' } } taskX.finalizedBy taskY &gt; gradle -q taskX TaskX TaskY 动态任务 Groovy或Kotlin的功能可用于定义任务以外的其他功能。例如，您还可以使用它来动态创建任务。 build.gradle 4.times { counter -&gt; task &quot;task$counter&quot; { doLast { println &quot;I'm task number $counter&quot; } } } gradle -q task1 输出 &gt; gradle -q task1 I'm task number 1 Groovy_DSL快捷方式符号 访问任务有一种方便的表示法。每个任务都可以作为构建脚本的属性来使用： 例.作为构建脚本的属性访问任务 build.gradle task hello { doLast { println 'Hello world!' } } hello.doLast { println &quot;Greetings from the $hello.name task.&quot; } 输出 gradle -q hello &gt; gradle -q hello Hello world! Greetings from the hello task. 这将启用非常可读的代码，尤其是在使用插件提供的任务（例如compile任务）时。 额外任务属性 您可以将自己的属性添加到任务。要添加名为的属性myProperty，请设置ext.myProperty为初始值。从那时起，可以像预定义的任务属性一样读取和设置属性。 build.gradle task myTask { ext.myProperty = &quot;myValue&quot; } task printTaskProperties { doLast { println myTask.myProperty } } 输出 gradle -q printTaskProperties &gt; gradle -q printTaskProperties myValue 额外的属性不仅限于任务。您可以在Extra属性中阅读有关它们的更多信息。 默认任务 Gradle允许您定义一个或多个默认任务。 build.gradle defaultTasks 'clean', 'run' task clean { doLast { println 'Default Cleaning!' } } task run { doLast { println 'Default Running!' } } task other { doLast { println &quot;I'm not a default task!&quot; } } 输出 gradle -q &gt; gradle -q Default Cleaning! Default Running! 这等效于运行gradle clean run。在多项目构建中，每个子项目可以有其自己的特定默认任务。如果子项目未指定默认任务，则使用父项目的默认任务（如果已定义）。 ","link":"https://tianxiawuhao.github.io/lhyr_DiZc/"},{"title":"Gradle基础","content":"Gradle概述 Gradle是专注于灵活性和性能的开源构建自动化工具，一般使用Groovy或KotlinDSL编写构建脚本。 本文只使用Groovy Gradle的特点： 高性能 Gradle通过仅运行需要运行的任务来避免不必要的工作。 可以使用构建缓存来重用以前运行的任务输出，甚至可以使用其他计算机（具有共享的构建缓存）重用任务输出。 JVM基础 Gradle在JVM上运行。熟悉Java的用户来可以在构建逻辑中使用标准Java API，例如自定义任务类型和插件。 这使得Gradle跨平台更加简单。（Gradle不仅限于构建JVM项目，它甚至附带对构建本机项目的支持。） 约束 和Maven一样，Gradle通过实现约束使常见类型的项目（例如Java项目）易于构建。 应用适当的插件，您可以轻松地为许多项目使用精简的构建脚本。 但是这些约定并没有限制您：Gradle允许您覆盖它们，添加自己的任务以及对基于约定的构建进行许多其他自定义操作。 可扩展性 您可以轻松扩展Gradle以提供您自己的任务类型甚至构建模型。 IDE支持 支持IDE：Android Studio，IntelliJ IDEA，Eclipse和NetBeans。 Gradle还支持生成将项目加载到Visual Studio所需的解决方案文件。 可洞察性 构建扫描提供了有关构建运行的广泛信息，可用于识别构建问题。他们特别擅长帮助您确定构建性能的问题。 您还可以与其他人共享构建扫描，如果您需要咨询以解决构建问题，这将特别有用。 您需要了解有关Gradle的五件事 本节在官方文档里面反复提及，具体可见Gradle文档 1. Gradle是通用的构建工具 Gradle允许您构建任何软件，因为它不关心你具体的工作。 2. 核心模型基于任务 Gradle将其构建模型建模为任务（工作单元）的有向无环图（DAG）。这意味着构建实质上配置了一组任务，并根据它们的依赖关系将它们连接在一起以创建该DAG。创建任务图后，Gradle将确定需要按顺序运行的任务，然后继续执行它们。 任务本身包括以下部分，它们通过依赖链接在一起： 动作-做某事的工作，例如复制文件或编译源代码 输入-操作使用或对其进行操作的值，文件和目录 输出-操作修改或生成的文件和目录 3. Gradle有几个固定的构建阶段 重要的是要了解Gradle分三个阶段评估和执行构建脚本： 初始化 设置构建环境，并确定哪些项目将参与其中。 配置 构造和配置构建的任务图，然后根据用户要运行的任务确定需要运行的任务和运行顺序。 执行 运行在配置阶段结束时选择的任务。 这些阶段构成了Gradle的构建生命周期。 4. Gradle的扩展方式不止一种 Gradle捆绑的构建逻辑不可能满足所有构建情况，大多数构建都有一些特殊要求，你需要添加自定义构建逻辑。Gradle提供了多种机制来扩展它，例如： 自定义任务类型。 自定义任务动作。 项目和任务的额外属性。 自定义约束。 自定义module。 5. 构建脚本针对API运行 可以将Gradle的构建脚本视为可执行代码，但设计良好的构建脚本描述了构建软件需要哪些步骤，而不关心这些步骤应该如何完成工作。 由于Gradle在JVM上运行，因此构建脚本也可以使用标准Java API。Groovy构建脚本可以另外使用Groovy API，而Kotlin构建脚本可以使用Kotlin。 功能的生命周期 功能可以处于以下四种状态之一： Internal：内部功能，不提供接口 Incubating： 孵化功能。在成为公共功能之前会继续更改 Public：公共功能，可放心使用 Deprecated：废弃功能，将在未来删除 Gradle安装 安装JDK 安装JDK过程已有太多资料，本文不做详细介绍。可使用命令检测自己电脑是否成功安装 安装Gradle 用软件包安装Gradle SDKMAN sdk install gradle Homebrew brew install gradle 手动安装（推荐方式） 下载 services.gradle.org/distributions （全部版本目录地址，可以查看最新版本） services.gradle.org/distributions/gradle-7.5-all.zip （截止至2022.07.18最新） 建议下载：services.gradle.org/distributions/gradle-7.0.2-all.zip (支持jdk8) 文件介绍 gradle-7.0.2-docs.zip //文档 gradle-7.0.2-src.zip //源码 gradle-7.0.2-bin.zip //软件包 gradle-7.0.2-all.zip //全部文件 bin ：运行文件 lib：依赖库 docs：文档 src：源文件 init.d :初始化脚本目录，可自己添加 配置环境变量 export GRADLE_HOME=/Users/temp/gradle-7.0.2 export PATH=$PATH:$GRADLE_HOME/bin 运行 输入gradle -v 检测是否配置成功 HelloWord 编写一个build.gradle文件，输入以下内容 task hello{ doLast { println 'Hello World' } } 命令行输入gradle -q hello即可运行 Gradle Wrapper 定义 Gradle Wrapper是一个脚本，可调用Gradle的声明版本，并在必要时预先下载。因此，开发人员可以快速启动并运行Gradle项目，而无需遵循手动安装过程 添加wrapper 在build.gradle同级目录下使用命令gradle wrapper可以生成gradle wrapper目录 gradle wrapper gradle-wrapper.jar WrapJAR文件，其中包含用于下载Gradle发行版的代码。 gradle-wrapper.properties 一个属性文件，负责配置Wrapper运行时行为，例如与该版本兼容的Gradle版本。请注意，更多常规设置（例如，将 Wrap配置为使用代理）需要进入其他文件。 gradlew， gradlew.bat 一个shell脚本和一个Windows批处理脚本，用于使用 Wrap程序执行构建。 可以通过命令控制生成选项 #用于下载和执行 Wrap程序的Gradle版本。 --gradle-version #Wrap使用的Gradle分布类型。可用的选项是bin和all。默认值为bin。 --distribution-type #指向Gradle分发ZIP文件的完整URL。使用此选项，--gradle-version并且--distribution- type过时的网址已经包含此信息。如果您想在公司网络内部托管Gradle发行版，则此选项非常有价值。 --gradle-distribution-url #SHA256哈希和用于验证下载的Gradle分布。 --gradle-distribution-sha256-sum 例： gradle wrapper --gradle-version 7.0.2 --distribution-type all Wrapper属性文件 一般生成Wrapper会得到如下属性文件 gradle-wrapper.properties distributionBase=GRADLE_USER_HOME distributionPath=wrapper/dists distributionUrl=https\\://services.gradle.org/distributions/gradle-7.4.1-bin.zip zipStoreBase=GRADLE_USER_HOME zipStorePath=wrapper/dists GRADLE_USER_HOME是你的环境变量，如果没配置，则默认是用户目录下的.gradle文件夹 distributionBase 下载的 Gradle压缩包解压后存储的主目录 distributionPath 相对于 distributionBase的解压后的 Gradle压缩包的路径 zipStoreBase 同 distributionBase，只不过是存放 zip压缩包的 zipStorePath 同 distributionPath，只不过是存放 zip压缩包的 distributionUrl Gradle发行版压缩包的下载地址 使用wrapper构建 在 gradlew目录下执行命令： windows： gradlew.bat build shell： ./gradlew build 升级 更改gradle-wrapper.properties文件中的distributionUrl属性 使用gradlew wrap --gradle-version 命令 ./gradlew wrap --gradle-version 7.4.2 自定义Gradle_Wrap 可以通过自定义wrapper少去一些重复操作或定制功能，如 build.gradle tasks.named('wrapper') { distributionType = Wrapper.DistributionType.ALL } task wrapper(type: Wrapper) { gradleVersion = '7.4.2' } Gradle 环境 环境变量 GRADLE_OPTS 指定启动Gradle客户端VM时要使用的JVM参数。客户端VM仅处理命令行输入/输出，因此很少需要更改其VM选项。实际的构建由Gradle守护程序运行，不受此环境变量的影响。 GRADLE_USER_HOME 指定Gradle用户的主目录（如果未设置，则默认为$USER_HOME/.gradle）。 JAVA_HOME 指定要用于客户端VM的JDK安装目录。除非Gradle属性文件使用org.gradle.java.home指定了另一个虚拟机，否则此虚拟机也用于守护程序。 注意：命令行选项和系统属性优先于环境变量。 Gradle属性 你可以通过以下方式自己配置你的项目属性，如果存在多个，则从上到下优先读取 ： 系统属性，例如在命令行上设置 -Dgradle.user.home GRADLE_USER_HOME目录中的gradle.properties 项目根目录中的gradle.properties Gradle安装目录中的gradle.properties gradle.properties # 当设置为true时，Gradle将在可能的情况下重用任何先前构建的任务输出，从而使构建速度更快 org.gradle.caching=true # 设置为true时，单个输入属性哈希值和每个任务的构建缓存键都记录在控制台上。 org.gradle.caching.debug=true # 启用按需孵化配置，Gradle将尝试仅配置必要的项目。 org.gradle.configureondemand=true # 自定义控制台输出的颜色或详细程度。默认值取决于Gradle的调用方式。可选(auto,plain,rich,verbose) org.gradle.console=auto # 当设置true的Gradle守护进程来运行构建。默认值为true。 org.gradle.daemon=true # 在指定的空闲毫秒数后，Gradle守护程序将自行终止。默认值为10800000（3小时）。 org.gradle.daemon.idletimeout=10800000 # 设置true为时，Gradle将在启用远程调试的情况下运行构建，侦听端口5005。 # 请注意，这等同于添加-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=5005到JVM命令行，并且将挂起虚拟机，直到连接了调试器。 # 默认值为false。 org.gradle.debug=true # 指定用于Gradle构建过程的Java主页。可以将值设置为jdk或jre位置，但是，根据您的构建方式，使用JDK更安全。 # 如果未指定设置，则从您的环境（JAVA_HOME或的路径java）派生合理的默认值。这不会影响用于启动Gradle客户端VM的Java版本（请参阅环境变量）。 org.gradle.java.home=/usr/bin/java # 指定用于Gradle守护程序的JVM参数。该设置对于配置JVM内存设置以提高构建性能特别有用。这不会影响Gradle客户端VM的JVM设置。 org.gradle.jvmargs=-Xmx2048m # 当设置为quiet,warn,lifecycle,info,debug时，Gradle将使用此日志级别。这些值不区分大小写。该lifecycle级别是默认级别。 # 可选(quiet,warn,lifecycle,info,debug) org.gradle.logging.level=debug # 配置后，Gradle将分叉到org.gradle.workers.maxJVM以并行执行项目 org.gradle.parallel=true # 指定Gradle守护程序及其启动的所有进程的调度优先级。默认值为normal。(low,normal) org.gradle.priority=normal # 在监视文件系统时配置详细日志记录。 默认为关闭 。 org.gradle.vfs.verbose=true # 切换观看文件系统。允许Gradle在下一个版本中重用有关文件系统的信息。 默认为关闭 。 org.gradle.vfs.watch=true # 当设置为all，summary或者none，Gradle会使用不同的预警类型的显示器。(all,fail,summary,none) org.gradle.warning.mode=all # 配置后，Gradle将最多使用给定数量的工人。默认值为CPU处理器数。 org.gradle.workers.max=5 系统属性 # 指定用户名以使用HTTP基本认证从服务器下载Gradle发行版 systemProp.gradle.wrapperUser = myuser # 指定使用Gradle Wrapper下载Gradle发行版的密码 systemProp.gradle.wrapperPassword = mypassword # 指定Gradle用户的主目录 systemProp.gradle.user.home=(path to directory) 项目属性 org.gradle.project.foo = bar 守护程序 Gradle在Java虚拟机（JVM）上运行，并使用一些支持库，这些库需要很短的初始化时间。但有时启动会比较慢。 解决此问题的方法是Gradle Daemon ：这是一个长期存在的后台进程，与以前相比，它可以更快地执行构建。 可通过命令获取运行守护程序状态 IDLE为空闲，BUSY为繁忙，STOPPED则已关闭 守护程序默认打开，可通过以下属性关闭 .gradle/gradle.properties org.gradle.daemon=false 也可用命令gradle --stop手动关闭守护程序 Gradle命令行 命令行格式 gradle [taskName ...] [--option-name ...] 如果指定了多个任务，则应以空格分隔。 选项和参数之间建议使用=来指定。 --console=plain 启用行为的选项具有长形式的选项，并带有由指定的反函数--no-。以下是相反的情况。 --build-cache --no-build-cache 许多命令具有缩写。例如以下命令是等效的： --help -h 使用Wrapper时候应该用./gradlew或gradlew.bat取代gradle #获取帮助 gradle -? gradle -h gradle -help # 显示所选项目的子项目列表，以层次结构显示。 gradle projects #查看可执行task gradle task #查看可执行task帮助 gradle help -task # 在Gradle构建中，通常的`build`任务是指定组装所有输出并运行所有检查。 gradle build # 执行所有验证任务（包括test和linting）。 gradle check # 清理项目 gradle clean #强制刷新依赖 gradle --refresh-dependencies assemble #缩写调用 gradle startCmd == gradle sc # 执行任务 gradle myTask # 执行多个任务 gradle myTask test # 执行 dist任务但排除test任务 gradle dist --exclude-task test # 强制执行任务 gradle test --rerun-tasks # 持续构建 # gradle test --continue # 生成扫描会提供完整的可视化报告，说明哪些依赖项存在于哪些配置，可传递依赖项和依赖项版本选择中。 $ gradle myTask --scan # 所选项目的依赖项列表 $ gradle dependencies ","link":"https://tianxiawuhao.github.io/Ke4qDxE4l/"},{"title":"gradle详情","content":"一 依赖管理 implementation:会将指定的依赖添加到编译路径，并且会将该依赖打包到输出，但是这个依赖在编译时不能暴露给其他模块，例如依赖此模块的其他模块。这种方式指定的依赖在编译时只能在当前模块中访问。 api:使用api配置的依赖会将对应的依赖添加到编译路径，并将依赖打包输出，但是这个依赖是可以传递的，比如模块A依赖模块B，B依赖库C，模块B在编译时能够访问到库C，但是与implemetation不同的是，在模块A中库C也是可以访问的。 compileOnly:compileOnly修饰的依赖会添加到编译路径中，但是不会打包，因此只能在编译时访问，且compileOnly修饰的依赖不会传递。 runtimeOnly:这个与compileOnly相反，它修饰的依赖不会添加到编译路径中，但是被打包，运行时使用。没有使用过。 annotationProcessor:用于注解处理器的依赖配置 dependencies { // 本地项目 api project(':com.test.core:core-common') //外部依赖 implementation 'org.springframework.boot:spring-boot-starter' compileOnly 'org.projectlombok:lombok' annotationProcessor 'org.projectlombok:lombok' testImplementation 'org.springframework.boot:spring-boot-starter-test' } 二 仓库 远程仓库 使用Maven中央仓库，maven仓库的URL为：http://repo1.maven.org/maven2/ repositories { mavenCentral() } 使用Maven远程仓库 repositories { //阿里云远程仓库 maven { url 'https://maven.aliyun.com/repository/central'} maven { url 'https://maven.aliyun.com/repository/public' } mavenCentral() } 本地仓库 配置Gradle使用maven本地仓库：CRADLE_USER_HOME：D:.m2\\repository 修改配置build.gradle： /** * 指定所使用的仓库，mavenCentral()表示使用中央仓库， * 此刻项目中所需要的jar包都会默认从中央仓库下载到本地指定目录 * 配置mavenLocal()表示引入jar包的时候，先从本地仓库中找，没有再去中央仓库下载 */ repositories { mavenLocal() mavenCentral() } 修改或者添加额外的私有仓库地址 直接修改 settings.gradle 来添加其它仓库： // settings.gradle //pluginManagement {}块只能出现在settings.gradle文件中，必须是文件中的第一个块，也可以以settings形式出现在初始化脚本中。 pluginManagement { repositories { maven { url 'https://maven.aliyun.com/repository/central'} maven { url 'https://maven.aliyun.com/repository/public' } mavenCentral() gradlePluginPortal() maven { url 'https://repo.spring.io/release' } if (version.endsWith('-SNAPSHOT')) { maven { url &quot;https://repo.spring.io/snapshot&quot; } } } } 三 多项目构建 多项目构成：allProjects = root项目+各子项目 settings文件声明了所需的配置来实例化项目的层次结构。在默认情况下，这个文件被命名为settings.gradle，并且和根项目的build.gradle文件放在一起，该文件在初始化阶段被执行。根项目就像一个容器，子项目会迭代访问它的配置并注入到自己的配置中。 多项目构建 多项目构建总是需要指定一个树根，树中的每一个节点代表一个项目，每一个Project对象都指定有一个表示在树中位置的路径；在设置文件中我们还可以使用一套方法来自定义构建项目树。 settings.gradle作用就是用于多项目构建，一般像这样： //父模块名称 rootProject.name = 'nacos' //引入子模块 include 'sentinel' include 'openfeign' include 'loadbalancer' include 'gateway' include 'rocketmq' include 'sleuth' include 'seata' include 'provide' findProject(':iot-service:user-center')?.name = 'user-center' buildScript buildScript块的repositories主要是为了Gradle脚本自身的执行，获取脚本依赖插件。 buildscript中的声明是gradle脚本自身需要使用的资源。可以声明的资源包括依赖项、第三方插件、maven仓库地址等。 gradle在执行脚本时，会优先执行buildscript代码块中的内容，然后才会执行剩余的build脚本。 buildscript { ext { //spring-cloud-dependencies 2020.0.0 默认不在加载bootstrap 配置文件， //如果项目中要用bootstrap 配置文件 需要手动添加spring-cloud-starter-bootstrap 依赖，不然启动项目会报错的。 set('springCloudVersion', &quot;2021.0.3&quot;) //定义一个变量，统一规定springboot的版本 set('springBootVersion', &quot;2.6.8&quot;) set('alibabaVersion', &quot;2.2.7.RELEASE&quot;) set('lombok', &quot;1.18.16&quot;) set('knife4j', &quot;2.0.9&quot;) set('hutool', &quot;4.6.3&quot;) set('rocketmq', &quot;2.2.2&quot;) } repositories { mavenLocal() maven { url 'https://maven.aliyun.com/repository/central' } maven { url 'https://maven.aliyun.com/repository/public' } mavenCentral() } dependencies { //用来打包 classpath(&quot;org.springframework.boot:spring-boot-gradle-plugin:${springBootVersion}&quot;) } } ext：ext是自定义属性，现在很多人都喜欢把所有关于版本的信息都利用ext放在另一个自己新建的gradle文件中集中管理。 allprojects allprojects块的repositories用于多项目构建，为所有项目提供共同所需依赖包。而子项目可以配置自己的repositories以获取自己独需的依赖包。 buildscript和allprojects的作用和区别： buildscript中的声明是gradle脚本自身需要使用的资源，就是说它自己需要的资源，跟你其它模块其实并没有什么关系。而allprojects声明的却是所有module所需要使用的资源，就是说你的每个module都需要用同一个第三库的时候，可以在allprojects里面声明。 allprojects { apply plugin: 'java-library' apply plugin: 'idea' //下面两句必须在所有子项目中添加，否则导致import了看起来没问题，但是编译时找不到其他模块的类。 group = 'com.example' version = '0.0.1-SNAPSHOT' // 指定JDK版本 sourceCompatibility = 1.8 targetCompatibility = 1.8 repositories { mavenLocal() maven { url 'https://maven.aliyun.com/repository/central' } maven { url 'https://maven.aliyun.com/repository/public' } mavenCentral() } //指定编码格式 tasks.withType(JavaCompile) { options.encoding = &quot;UTF-8&quot; } } subprojects子项目通用配置 subprojects是对所有Child Project的配置: subprojects { apply plugin: 'java-library' apply plugin: 'idea' apply plugin: 'org.springframework.boot' //dependency-management 插件 apply plugin: 'io.spring.dependency-management' repositories { mavenLocal() maven { url 'https://maven.aliyun.com/repository/central' } maven { url 'https://maven.aliyun.com/repository/public' } mavenCentral() } dependencies { implementation(enforcedPlatform(&quot;org.springframework.cloud:spring-cloud-dependencies:${springCloudVersion}&quot;)) implementation(enforcedPlatform(&quot;com.alibaba.cloud:spring-cloud-alibaba-dependencies:${alibabaVersion}&quot;)) implementation(enforcedPlatform(&quot;org.springframework.boot:spring-boot-dependencies:${springBootVersion}&quot;)) implementation &quot;com.github.xiaoymin:knife4j-spring-boot-starter:${knife4j}&quot; implementation &quot;cn.hutool:hutool-all:${hutool}&quot; implementation 'org.springframework.boot:spring-boot-starter' implementation 'com.alibaba.cloud:spring-cloud-starter-alibaba-nacos-config' implementation('com.alibaba.cloud:spring-cloud-starter-alibaba-nacos-discovery'){ exclude group: 'org.springframework.cloud', module: 'spring-cloud-starter-netflix-ribbon' } implementation 'org.springframework.cloud:spring-cloud-starter-bootstrap' compileOnly(&quot;org.projectlombok:lombok:${lombok}&quot;) annotationProcessor 'org.springframework.boot:spring-boot-configuration-processor' annotationProcessor(&quot;org.projectlombok:lombok:${lombok}&quot;) testImplementation(&quot;org.springframework.boot:spring-boot-starter-test:${springBootVersion}&quot;) } jar { manifest.attributes provider: 'gradle' } } 四 gradle.properties gradle中的常用属性可以写在gradle.properties中。 一般我们都把全局属性都编写在一个工具类中，如果是有环境的切换的话，那么我们还会定义一个标志来进行相应的变换。对于项目而言，有时候需要配置某些敏感信息。比如密码，帐号等。而这些信息需要被很多类共同使用，所以必须有一个全局的配置。当需要把项目push到git上时，我们不希望别人看到我们项目的key，token等。我们可以将这些信息设置在gradle.properties中。 只有在Android中才可使用。 AppKey = 1234567890 在build.gradle(module app)中进行变量的重定义，即将配置内容转化成java能够使用的形式: android { buildTypes { release { minifyEnabled true proguardFiles getDefaultProguardFile('proguard-android.txt'), 'proguard-rules.pro' //buildConfigField用于给BuildConfig文件添加一个字段 buildConfigField(&quot;String&quot;,&quot;KEY&quot;,&quot;\\&quot;${AppKey}\\&quot;&quot;) } debug{ buildConfigField(&quot;String&quot;,&quot;KEY&quot;,&quot;\\&quot;${AppKey}\\&quot;&quot;) } } } 五 Gradle 插件(Plugins) Gradle 也可以用下面的方式声明使用的插件： // plugins DSL plugins { id 'org.springframework.boot' version '2.2.1.RELEASE' id 'io.spring.dependency-management' version '1.0.8.RELEASE' id 'java' } // apply plugin apply plugin: 'java-library' apply plugin: 'idea' apply plugin: 'org.springframework.boot' //dependency-management 插件 apply plugin: 'io.spring.dependency-management' 其实是从 Gradle 官方的插件仓库https://plugins.gradle.org/m2/下载的。 六 常见的task命令 build:当运行gradle build命令时Gradle将会编译和测试你的代码，并且创建一个包含类和资源的JAR文件。 clean:当运行gradle clean命令时Gradle将会删除build生成的目录和所有生成的文件。 assemble:当运行gradle assemble命令时Gradle将会编译并打包代码，但是并不运行单元测试。 check:当运行gradle check命令时Gradle将会编译并测试你的代码，其他的插件会加入更多的检查步骤。 七 gradle-wrapper Wrapper是对Gradle的一层包装，便于在团队开发过程中统一Gradle构建的版本号，这样大家都可以使用统一的Gradle版本进行构建。 distributionBase=GRADLE_USER_HOME distributionPath=wrapper/dists distributionUrl=https\\://services.gradle.org/distributions/gradle-7.4.2-bin.zip #distributionUrl=file:///E:/gradle/gradle-7.4.2-all.zip zipStoreBase=GRADLE_USER_HOME zipStorePath=wrapper/dists distributionUrl是要下载的gradle的地址，使用哪个版本的gradle，就在这里修改。 gradle的3种版本： gradle-xx-all.zip是完整版，包含了各种二进制文件，源代码文件，和离线的文档。 gradle-xx-bin.zip是二进制版，只包含了二进制文件（可执行文件），没有文档和源代码。 gradle-xx-src.zip是源码版，只包含了Gradle源代码，不能用来编译你的工程。 zipStoreBase和zipStorePath组合在一起，是下载的gradle-3.1-bin.zip所存放的位置。 zipStorePath是zipStoreBase指定的目录下的子目录。 distributionBase和distributionPath组合在一起，是解压gradle-5.6.4-bin.zip之后的文件的存放位置。 distributionPath是distributionBase指定的目录下的子目录。 下载位置可以和解压位置不一样。 zipStoreBase和distributionBase有两种取值：GRADLE_USER_HOME和PROJECT。 其中，GRADLE_USER_HOME表示用户目录。 在windows下是%USERPROFILE%/.gradle，例如C:\\Users&lt;user_name&gt;.gradle\\。 在linux下是$HOME/.gradle，例如~/.gradle。 PROJECT表示工程的当前目录，即gradlew所在的目录。 八 依赖分组 Gradle 依赖是分组的 ,分组是在 org.gradle.api.Project 中的 configurations 中配置的 ,如 &quot; implementation &quot; , &quot; compile &quot; 等都是分组 , 这些分组都是在 org.gradle.api.Project#configurations 中进行配置 , 也就是 build.gradle#configurations 中配置 ; build.gradle#configurations 自定义依赖分组 在 build.gradle 中配置 configurations : configurations { hello { } } 则可以在 dependencies 中使上述在 configurations 配置的依赖分组 hello , dependencies { hello 'com.android.support:appcompat-v7:28.0.0' } 九 依赖版本冲突 Gradle对解决传递依赖提供了两种策略，使用最新版本或者直接导致构建失败。默认的策略是使用最新版本。虽然这样的策略能够解决一些问题，但是还是不够。常见的一种情况是，NoSuchMethond或者ClassNotFound。这时候，你可能需要一些特殊手段，比如排除不想要的传递依赖。 排除传递依赖的方式有两种： 1.使用transitive = false排除 2.在具体的某个dependency中使用exclude排除 3.使用force强制依赖某个版本 (1) 方案一：针对 A 或 D 配置 transitive。 这里针对A配置，不解析A模块的传递依赖，因此当前Module的依赖关系树中不再包含 B1 和 C，这里需要手动添加依赖 C dependencies { implementation A { transitive = false } implementation C implementation D { //transitive = false } } (2) 方案二：针对 A 或 D 配置 exclude规则，此处针对A配置，依赖关系树中不再包含B1 dependencies { implementation A { exclude B1 } implementation D { //exclude B2 } } (3) 方案三：使用force强制依赖某个版本，如强制依赖 B1 或者 B2 以下是在顶层build.gradle中配置，强制所有module依赖B1 configurations.all { resolutionStrategy { force B1 // force B2 } } 十 springcloud-gradle管理 parent:settings.gradle rootProject.name = 'nacos' include 'sentinel' include 'openfeign' include 'loadbalancer' include 'gateway' include 'rocketmq' include 'sleuth' include 'seata' include 'provide' parent:build.gradle buildscript { ext { //spring-cloud-dependencies 2020.0.0 默认不在加载bootstrap 配置文件， //如果项目中要用bootstrap 配置文件 需要手动添加spring-cloud-starter-bootstrap 依赖，不然启动项目会报错的。 set('springCloudVersion', &quot;2021.0.3&quot;) //定义一个变量，统一规定springboot的版本 set('springBootVersion', &quot;2.6.8&quot;) set('alibabaVersion', &quot;2.2.7.RELEASE&quot;) set('lombok', &quot;1.18.16&quot;) set('knife4j', &quot;2.0.9&quot;) set('hutool', &quot;4.6.3&quot;) set('rocketmq', &quot;2.2.2&quot;) } repositories { mavenLocal() maven { url 'https://maven.aliyun.com/repository/central' } maven { url 'https://maven.aliyun.com/repository/public' } mavenCentral() } dependencies { //用来打包 classpath(&quot;org.springframework.boot:spring-boot-gradle-plugin:${springBootVersion}&quot;) } } allprojects { apply plugin: 'java-library' apply plugin: 'idea' //下面两句必须在所有子项目中添加，否则导致import了看起来没问题，但是编译时找不到其他模块的类。 group = 'com.example' version = '0.0.1-SNAPSHOT' // 指定JDK版本 sourceCompatibility = 1.8 targetCompatibility = 1.8 repositories { mavenLocal() maven { url 'https://maven.aliyun.com/repository/central' } maven { url 'https://maven.aliyun.com/repository/public' } mavenCentral() } //指定编码格式 tasks.withType(JavaCompile) { options.encoding = &quot;UTF-8&quot; } } subprojects { apply plugin: 'java-library' apply plugin: 'idea' apply plugin: 'org.springframework.boot' //dependency-management 插件 apply plugin: 'io.spring.dependency-management' dependencies { implementation(enforcedPlatform(&quot;org.springframework.cloud:spring-cloud-dependencies:${springCloudVersion}&quot;)) implementation(enforcedPlatform(&quot;com.alibaba.cloud:spring-cloud-alibaba-dependencies:${alibabaVersion}&quot;)) implementation(enforcedPlatform(&quot;org.springframework.boot:spring-boot-dependencies:${springBootVersion}&quot;)) implementation &quot;com.github.xiaoymin:knife4j-spring-boot-starter:${knife4j}&quot; implementation &quot;cn.hutool:hutool-all:${hutool}&quot; implementation 'org.springframework.boot:spring-boot-starter' implementation 'com.alibaba.cloud:spring-cloud-starter-alibaba-nacos-config' implementation('com.alibaba.cloud:spring-cloud-starter-alibaba-nacos-discovery'){ exclude group: 'org.springframework.cloud', module: 'spring-cloud-starter-netflix-ribbon' } implementation 'org.springframework.cloud:spring-cloud-starter-bootstrap' compileOnly(&quot;org.projectlombok:lombok:${lombok}&quot;) annotationProcessor 'org.springframework.boot:spring-boot-configuration-processor' annotationProcessor(&quot;org.projectlombok:lombok:${lombok}&quot;) testImplementation(&quot;org.springframework.boot:spring-boot-starter-test:${springBootVersion}&quot;) } jar { manifest.attributes provider: 'gradle' } } gateway:build.gradle dependencies { implementation &quot;org.springframework.cloud:spring-cloud-starter-gateway&quot; implementation &quot;org.springframework.cloud:spring-cloud-starter-loadbalancer&quot; } openfeign:build.gradle dependencies { implementation 'org.springframework.boot:spring-boot-starter-web' implementation &quot;org.springframework.cloud:spring-cloud-starter-openfeign&quot; implementation &quot;org.springframework.cloud:spring-cloud-starter-loadbalancer&quot; } rocketmq:build.gradle dependencies { implementation 'org.springframework.boot:spring-boot-starter-web' implementation &quot;org.apache.rocketmq:rocketmq-spring-boot-starter:${rocketmq}&quot; } sentinel:build.gradle dependencies { implementation 'org.springframework.boot:spring-boot-starter-web' implementation 'com.alibaba.cloud:spring-cloud-starter-alibaba-sentinel' implementation 'org.springframework.boot:spring-boot-starter-actuator' } 项目构建--Gradle--Docker打包 在build.gradle中引入插件。 id &quot;com.google.cloud.tools.jib&quot; version &quot;3.2.1&quot; 配置 配置打包时的基础镜像、容器配置、私服地址等，和Maven 插件中的一样，只是采用闭包的书写方式。 Jib 官网文档 jib { // 基础镜像，来自dockerhub,如果是私服，需要加上鉴权信息，和to下的auth节点相同 // https://hub.docker.com/ from { image = 'xx' } // 构建后的镜像名称以及私服地址、鉴权信息 to { image = 'xx' auth { username = '登录账号' password = '登录密码' } } // 容器相关设置 container { // 创建时间 creationTime = new Date() // JVM 启动参数 jvmFlags = ['-Djava.security.egd=file:/dev/./urandom', '-Dspring.profiles.active=prod', '-Dfile.encoding=utf-8', '-Duser.timezone=GMT+08'] // 启动类 // mainClass = 'com.xxx.RunApplication' // 容器在运行时公开的端口 ports = ['8080'] // 放置应用程序内容的容器上的根目录 appRoot = '/deploy/service' } } 使用IdeaDocker插件布署 Idea安装插件 安装docker插件 配置docker： 配置Dockerfile文件 1）新建Dockerfile文件会自动弹出。 在工程根目录下新建Dockerfile文件，内容如下： FROM openjdk:8 COPY build/libs/iot-eruake-1.0.0.jar app.jar RUN bash -c &quot;touch /app.jar&quot; EXPOSE 8080 ENTRYPOINT [&quot;java&quot;,&quot;-jar&quot;,&quot;app.jar&quot;] 创建docker镜像 发布完成 ","link":"https://tianxiawuhao.github.io/29jyrX-eN/"},{"title":"IdWorker雪花算法","content":"package com.ihrm.common.utils; import java.lang.management.ManagementFactory; import java.net.InetAddress; import java.net.NetworkInterface; //雪花算法代码实现 public class IdWorker { // 时间起始标记点，作为基准，一般取系统的最近时间（一旦确定不能变动） private final static long twepoch = 1288834974657L; // 机器标识位数 private final static long workerIdBits = 5L; // 数据中心标识位数 private final static long datacenterIdBits = 5L; // 机器ID最大值 private final static long maxWorkerId = -1L ^ (-1L &lt;&lt; workerIdBits); // 数据中心ID最大值 private final static long maxDatacenterId = -1L ^ (-1L &lt;&lt; datacenterIdBits); // 毫秒内自增位 private final static long sequenceBits = 12L; // 机器ID偏左移12位 private final static long workerIdShift = sequenceBits; // 数据中心ID左移17位 private final static long datacenterIdShift = sequenceBits + workerIdBits; // 时间毫秒左移22位 private final static long timestampLeftShift = sequenceBits + workerIdBits + datacenterIdBits; //毫秒内自增最大值 private final static long sequenceMask = -1L ^ (-1L &lt;&lt; sequenceBits); /* 上次生产id时间戳 */ private static long lastTimestamp = -1L; // 0，并发控制 private long sequence = 0L; private final long workerId; // 数据标识id部分 private final long datacenterId; public IdWorker() { this.datacenterId = getDatacenterId(maxDatacenterId); this.workerId = getMaxWorkerId(datacenterId, maxWorkerId); } /** * @param workerId 工作机器ID * @param datacenterId 序列号 */ public IdWorker(long workerId, long datacenterId) { if (workerId &gt; maxWorkerId || workerId &lt; 0) { throw new IllegalArgumentException(String.format(&quot;worker Id can't be greater than % d or less than 0&quot;, maxWorkerId)); } if (datacenterId &gt; maxDatacenterId || datacenterId &lt; 0) { throw new IllegalArgumentException(String.format(&quot;datacenter Id can't be greater than % d or less than 0&quot;, maxDatacenterId)); } this.workerId = workerId; this.datacenterId = datacenterId; } /** * 获取下一个ID * * @return */ public synchronized long nextId() { long timestamp = timeGen(); if (timestamp &lt; lastTimestamp) { throw new RuntimeException(String.format(&quot;Clock moved backwards. Refusing to generate id for %d milliseconds &quot;, lastTimestamp - timestamp)); } if (lastTimestamp == timestamp) { // 当前毫秒内，则+1 sequence = (sequence + 1) &amp; sequenceMask; if (sequence == 0) { // 当前毫秒内计数满了，则等待下一秒 timestamp = tilNextMillis(lastTimestamp); } } else { sequence = 0L; } lastTimestamp = timestamp; // ID偏移组合生成最终的ID，并返回ID long nextId = ((timestamp - twepoch) &lt;&lt; timestampLeftShift) | (datacenterId &lt;&lt; datacenterIdShift) | (workerId &lt;&lt; workerIdShift) | sequence; return nextId; } private long tilNextMillis(final long lastTimestamp) { long timestamp = this.timeGen(); while (timestamp &lt;= lastTimestamp) { timestamp = this.timeGen(); } return timestamp; } private long timeGen() { return System.currentTimeMillis(); } /** * &lt;p&gt; * 获取 maxWorkerId * &lt;/p&gt; */ protected static long getMaxWorkerId(long datacenterId, long maxWorkerId) { StringBuffer mpid = new StringBuffer(); mpid.append(datacenterId); String name = ManagementFactory.getRuntimeMXBean().getName(); if (!name.isEmpty()) { /* * GET jvmPid */ mpid.append(name.split(&quot;@&quot;)[0]); } /* * MAC + PID 的 hashcode 获取16个低位 */ return (mpid.toString().hashCode() &amp; 0xffff) % (maxWorkerId + 1); } /** * &lt;p&gt; * 数据标识id部分 * &lt;/p&gt; */ protected static long getDatacenterId(long maxDatacenterId) { long id = 0L; try { InetAddress ip = InetAddress.getLocalHost(); NetworkInterface network = NetworkInterface.getByInetAddress(ip); if (network == null) { id = 1L; } else { byte[] mac = network.getHardwareAddress(); id = ((0x000000FF &amp; (long) mac[mac.length - 1]) | (0x0000FF00 &amp; (((long) mac[mac.length - 2]) &lt;&lt; 8))) &gt;&gt; 6; id = id % (maxDatacenterId + 1); } } catch (Exception e) { System.out.println(&quot; getDatacenterId: &quot; + e.getMessage()); } return id; } } ","link":"https://tianxiawuhao.github.io/4WluurBt3/"},{"title":"RestTemplate","content":"spring环境下 首先导入springboot 的 web 包 &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; 在启动类同包下创建RestTemplate.java类 import org.apache.http.client.HttpClient; import org.apache.http.client.config.RequestConfig; import org.apache.http.impl.client.DefaultHttpRequestRetryHandler; import org.apache.http.impl.client.HttpClientBuilder; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.http.client.ClientHttpRequestFactory; import org.springframework.http.client.HttpComponentsClientHttpRequestFactory; import org.springframework.web.client.RestTemplate; @Configuration public class RestTempleConfig { @Bean public RestTemplate restTemplate() { //生成一个设置了连接超时时间、请求超时时间 RequestConfig config = RequestConfig.custom() .setConnectionRequestTimeout(10000) .setConnectTimeout(10000) .setSocketTimeout(30000).build(); // 设置异常重试 HttpClientBuilder builder = HttpClientBuilder.create() .setDefaultRequestConfig(config) .setRetryHandler(new DefaultHttpRequestRetryHandler(3, false)); HttpClient httpClient = builder.build(); ClientHttpRequestFactory requestFactory = new HttpComponentsClientHttpRequestFactory(httpClient); RestTemplate restTemplate = new RestTemplate(requestFactory); // 日志拦截 //restTemplate.setInterceptors(Collections.singletonList(new RestTemplateConsumerLogger())); return restTemplate; } } 非spring环境下 导入相关依赖包(注意版本相适应) &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-web&lt;/artifactId&gt; &lt;version&gt;5.2.16.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt; &lt;artifactId&gt;httpclient&lt;/artifactId&gt; &lt;version&gt;4.5.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-core&lt;/artifactId&gt; &lt;version&gt;2.12.1&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;version&gt;2.12.1&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-annotations&lt;/artifactId&gt; &lt;version&gt;2.12.1&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; RestTemplate RestTemplate定义了36个与REST资源交互的方法，大多数都对应于HTTP的方法。 其中只有11个独立的方法，其中有十个有三种重载形式，而第十一个则重载了六次，这样一共形成了36个方法。 getForEntity() 发送一个HTTP GET请求，返回的ResponseEntity包含了响应体所映射成的对象 getForObject() 发送一个HTTP GET请求，返回的请求体将映射为一个对象 postForEntity() POST 数据到一个URL，返回包含一个对象的ResponseEntity，这个对象是从响应体中映射得到的 postForObject() POST 数据到一个URL，返回根据响应体匹配形成的对象 exchange() 在URL上执行特定的HTTP方法，返回包含对象的ResponseEntity，这个对象是从响应体中映射得到的 execute() 在URL上执行特定的HTTP方法，返回一个从响应体映射得到的对象 delete() 在特定的URL上对资源执行HTTP DELETE操作 headForHeaders() 发送HTTP HEAD请求，返回包含特定资源URL的HTTP头 optionsForAllow() 发送HTTP OPTIONS请求，返回对特定URL的Allow头信息 postForLocation() POST 数据到一个URL，返回新创建资源的URL put() PUT 资源到特定的URL getForEntity get请求就和正常在浏览器url上发送请求一样 RestTemplate restTemplate = new RestTemplate(); ResponseEntity&lt;String&gt; responseEntity=restTemplate.getForEntity(url+&quot;?name={1}&quot;, String.class, &quot;username&quot;); String body = responseEntity.getBody(); RestTemplate restTemplate = new RestTemplate(); ResponseEntity&lt;TokenBeen&gt; responseEntity =restTemplate.getForEntity(url+&quot;?name={1}&quot;, TokenBeen.class, &quot;username&quot;); if(responseEntity!=null){ TokenBeen body = responseEntity.getBody(); } #注意map的key要和参数中占位符相同 RestTemplate restTemplate = new RestTemplate(); Map&lt;String, String&gt; params = new HashMap&lt;&gt;(); params.put(&quot;name&quot;, &quot;username&quot;); ResponseEntity&lt;String&gt; responseEntity = restTemplate.getForEntity(url+&quot;?name={name}&quot;, String.class, params); String body = responseEntity.getBody(); RestTemplate restTemplate = new RestTemplate(); UriComponents uriConponents = UriComponentsBuilder.fromUriString(url+&quot;?name={name}&quot;).build().expand(&quot;username&quot;).encode(); URI uri = uriConponents.toUri(); ResponseEntity&lt;String&gt; responseEntity = restTemplate.getForEntity(uri, String.class); String body = responseEntity.getBody(); @GetMapping(&quot;getForEntity/{id}&quot;) public User getById(@PathVariable(name = &quot;id&quot;) String id) { ResponseEntity&lt;User&gt; response = restTemplate.getForEntity(&quot;http://localhost/get/{id}&quot;, User.class, id); User user = response.getBody(); return user; } getForObject getForObject 和 getForEntity 用法几乎相同,getForObject函数可以看作是对getForEntity进一步封装,指示返回值返回的是响应体,省去了我们 再去 getBody() RestTemplate restTemplate = new RestTemplate(); //注意参数中是uri UriComponents uriConponents = UriComponentsBuilder.fromUriString(url+&quot;?name={name}&quot;).build().expand(&quot;username&quot;).encode(); URI uri = uriConponents.toUri(); String body = restTemplate.getForObject(uri, String.class); RestTemplate restTemplate = new RestTemplate(); //注意参数中是uri UriComponents uriConponents = UriComponentsBuilder.fromUriString(url+&quot;?name={name}&quot;).build().expand(&quot;username&quot;).encode(); URI uri = uriConponents.toUri(); TokenBeen body = restTemplate.getForObject(uri, TokenBeen.class); @GetMapping(&quot;getForObject/{id}&quot;) public User getById(@PathVariable(name = &quot;id&quot;) String id) { User user = restTemplate.getForObject(&quot;http://localhost/get/{id}&quot;, User.class, id); return user; } postForEntity public &lt;T&gt; T postForObject(String url, @Nullable Object request, Class&lt;T&gt; responseType, Object... uriVariables)throws RestClientException {} public &lt;T&gt; T postForObject(String url, @Nullable Object request, Class&lt;T&gt; responseType, Map&lt;String, ?&gt; uriVariables) throws RestClientException {} public &lt;T&gt; T postForObject(URI url, @Nullable Object request, Class&lt;T&gt; responseType) throws RestClientException {} RestTemplate restTemplate = new RestTemplate(); HttpHeaders headers = new HttpHeaders(); //header参数 MediaType type = MediaType.parseMediaType(&quot;application/json; charset=UTF-8&quot;); headers.setContentType(type); headers.add(&quot;Accept&quot;, MediaType.APPLICATION_JSON.toString()); //body参数 JSONObject param = new JSONObject(); param.put(&quot;username&quot;, &quot;123&quot;); HttpEntity&lt;JSONObject&gt; formEntity = new HttpEntity&lt;&gt;(param, headers); //发送请求 String result = restTemplate.postForObject(url, formEntity, String.class); @RequestMapping(&quot;saveUser&quot;) public String save(User user) { ResponseEntity&lt;String&gt; response = restTemplate.postForEntity(&quot;http://localhost/save&quot;, user, String.class); String body = response.getBody(); return body; } postForObject 用法与 getForObject 一样 @RequestMapping(&quot;saveUser&quot;) public String save(User user) { String body = restTemplate.postForObject(&quot;http://localhost/save&quot;, user, String.class); return body; } exchange @PostMapping(&quot;demo&quot;) public void demo(Integer id, String name){ HttpHeaders headers = new HttpHeaders();//header参数 headers.add(&quot;authorization&quot;,Auth); headers.setContentType(MediaType.APPLICATION_JSON); JSONObject content = new JSONObject();//放入body中的json参数 content.put(&quot;userId&quot;, id); content.put(&quot;name&quot;, name); //post发送 HttpEntity&lt;JSONObject&gt; request = new HttpEntity&lt;&gt;(content,headers); //组装 ResponseEntity&lt;String&gt; response = template.exchange(&quot;http://localhost:8080/demo&quot;,HttpMethod.POST,request,String.class); //返回指定对象 ParameterizedTypeReference&lt;User&gt; responseBodyType = new ParameterizedTypeReference&lt;RestBean&lt;String&gt;&gt;() {}; User user = template.exchange(&quot;http://localhost:8080/demo&quot;,HttpMethod.POST,request,responseBodyType); //get发送 HttpEntity&lt;String&gt; request = new HttpEntity&lt;&gt;(&quot;parameters&quot;,headers); //组装 ResponseEntity&lt;String&gt; response = template.exchange(&quot;http://localhost:8080/demo&quot;,HttpMethod.GET,request,String.class); //返回指定对象 ResponseEntity&lt;User&gt; response = template.exchange(&quot;http://localhost:8080/demo&quot;,HttpMethod.GET,request,User.class); } ","link":"https://tianxiawuhao.github.io/fFTlwaLS_/"},{"title":"SpringBoot  JWT实现","content":"SpringBoot JWT实现 （只是实现了jwt,没有生成证书,安全性得不到保障,证书安全验证查看Spring security JWT） &lt;dependency&gt; &lt;groupId&gt;io.jsonwebtoken&lt;/groupId&gt; &lt;artifactId&gt;jjwt&lt;/artifactId&gt; &lt;version&gt;0.6.0&lt;/version&gt; &lt;/dependency&gt; import io.jsonwebtoken.JwtBuilder; import io.jsonwebtoken.Jwts; import io.jsonwebtoken.SignatureAlgorithm; import java.util.Date; public class CreateJwtTest3 { public static void main(String[] args) { //为了方便测试，我们将过期时间设置为1分钟 long now = System.currentTimeMillis();//当前时间 long exp = now + 1000 * 60;//过期时间为1分钟 JwtBuilder builder = Jwts.builder().setId(&quot;888&quot;) .setSubject(&quot;小白&quot;) .setIssuedAt(new Date()) .signWith(SignatureAlgorithm.HS256, &quot;itcast&quot;) .setExpiration(new Date(exp)) .claim(&quot;roles&quot;, &quot;admin&quot;) //自定义claims存储数据 .claim(&quot;logo&quot;, &quot;logo.png&quot;); System.out.println(builder.compact()); } } import io.jsonwebtoken.Claims; import io.jsonwebtoken.Jwts; import org.apache.commons.lang3.time.DateFormatUtils; import java.util.Date; public class ParseJwtTest { public static void main(String[] args) { String token = &quot;eyJhbGciOiJIUzI1NiJ9.eyJqdGkiOiI4ODgiLCJzdWIiOiLlsI_nmb0iLCJpYXQiOjE1NjExMDE3MzIsImV4cCI6MTU2MTEwMTc5MSwicm9sZXMiOiJhZG1pbiIsImxvZ28iOiJsb2dvLnBuZyJ9.5iVVdTw747L3ScHeCqle-bwj3cezK8NnE7VilQWOr8Y&quot;; Claims claims = Jwts.parser().setSigningKey(&quot;itcast&quot;).parseClaimsJws(token).getBody(); System.out.println(&quot;id:&quot; + claims.getId()); System.out.println(&quot;subject:&quot; + claims.getSubject()); System.out.println(&quot;roles:&quot; + claims.get(&quot;roles&quot;)); System.out.println(&quot;logo:&quot; + claims.get(&quot;logo&quot;)); System.out.println(&quot;签发时间:&quot;+ DateFormatUtils.format(claims.getIssuedAt(),&quot;yyyy-MM-dd hh:mm:ss&quot;)); System.out.println(&quot;过期时间:&quot;+DateFormatUtils.format(claims.getExpiration(),&quot;yyyy-MM-dd hh:mm:ss&quot;)); System.out.println(&quot;当前时间:&quot;+DateFormatUtils.format(new Date(),&quot;yyyy-MM-dd hh:mm:ss&quot;)); } } ","link":"https://tianxiawuhao.github.io/ZsckGVdlz/"},{"title":"JWT介绍","content":"JWT介绍 在介绍JWT之前先看一下传统校验令牌的方法，如下图： 资源服务器授权流程如上图，客户端先去授权服务器申请令牌，申请令牌后，携带令牌访问资源服务器，资源服务器访问授权服务校验令牌的合法性，授权服务会返回校验结果，如果校验成功会返回用户信息给资源服务器，资源服务器如果接收到的校验结果通过了，则返回资源给客户端 问题： 传统授权方法的问题是用户每次请求资源服务，资源服务都需要携带令牌访问认证服务去校验令牌的合法性，并根据令牌获取用户的相关信息，性能低下。 解决： 使用JWT的思路是，用户认证通过会得到一个JWT令牌，JWT令牌中已经包括了用户相关的信息，客户端只需要携带JWT访问资源服务，资源服务根据事先约定的算法自行完成令牌校验，无需每次都请求认证服务完成授权。 JWT令牌授权过程如下图： 什么是JWT？ JSON Web Token（JWT）是一个开放的行业标准（RFC 7519），它定义了一种简介的、自包含的协议格式，用于在通信双方传递json对象，传递的信息经过数字签名可以被验证和信任。JWT可以使用HMAC算法或使用RSA的公钥/私钥对来签名，防止被篡改。 官网：https://jwt.io/ 标准：https://tools.ietf.org/html/rfc7519 优点： jwt基于json，非常方便解析。 可以在令牌中自定义丰富的内容，易扩展。 通过非对称加密算法及数字签名技术，JWT防止篡改，安全性高。 资源服务使用JWT可不依赖认证服务即可完成授权。 缺点： JWT令牌较长，占存储空间比较大。 令牌结构 通过学习JWT令牌结构为自定义jwt令牌打好基础。 JWT令牌由三部分组成，每部分中间使用点（.）分隔，比如：xxxxx.yyyyy.zzzzz Header 头部包括令牌的类型（即JWT）及使用的哈希算法（如HMAC SHA256或RSA） 一个例子如下： 下边是Header部分的内容 { &quot;alg&quot;: &quot;HS256&quot;, &quot;typ&quot;: &quot;JWT&quot; } 将上边的内容使用Base64Url编码，得到一个字符串就是JWT令牌的第一部分。 Payload 第二部分是负载，内容也是一个json对象，它是存放有效信息的地方，它可以存放jwt提供的现成字段，比如：iss（签发者）,exp（过期时间戳）, sub（面向的用户）等，也可自定义字段。 此部分不建议存放敏感信息，因为此部分可以解码还原原始内容。 最后将第二部分负载使用Base64Url编码，得到一个字符串就是JWT令牌的第二部分。 { &quot;sub&quot;: &quot;1234567890&quot;, &quot;name&quot;: &quot;456&quot;, &quot;admin&quot;: true } Signature 第三部分是签名，此部分用于防止jwt内容被篡改。这个部分使用base64url将前两部分进行编码，编码后使用点（.）连接组成字符串，最后使用header中声明签名算法进行签名。 HMACSHA256(base64UrlEncode(header) + &quot;.&quot; +base64UrlEncode(payload),secret) base64UrlEncode(header)：jwt令牌的第一部分。 base64UrlEncode(payload)：jwt令牌的第二部分。 secret：签名所使用的密钥。 JWT入门 Spring Security 提供对JWT的支持，本节我们使用Spring Security 提供的JwtHelper来创建JWT令牌，校验JWT令牌等操作。 生成私钥和公钥 JWT令牌生成采用非对称加密算法 生成密钥证书 下边命令生成密钥证书，采用RSA 算法每个证书包含公钥和私钥 keytool -genkeypair -alias xckey -keyalg RSA -keypass xuecheng -keystore xc.keystore -storepass xuechengkeystore Keytool 是一个java提供的证书管理工具 -alias：密钥的别名 -keyalg：使用的hash算法 -keypass：密钥的访问密码 -keystore：密钥库文件名，xc.keystore保存了生成的证书 -storepass：密钥库的访问密码 查询证书信息 keytool -list -keystore xc.keystore 删除别名 keytool -delete -alias xckey -keystore xc.keystore 导出公钥 openssl是一个加解密工具包，这里使用openssl来导出公钥信息。安装 openssl：http://slproweb.com/products/Win32OpenSSL.html 配置openssl的path环境变量，本文配置在D:\\OpenSSL-Win64\\bin cmd进入xc.keystore文件所在目录执行如下命令： keytool ‐list ‐rfc ‐‐keystore xc.keystore | openssl x509 ‐inform pem ‐pubkey 输入密钥库密码： 下边这一段就是公钥内容： -----BEGIN PUBLIC KEY----- MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAijyxMdq4S6L1Af1rtB8SjCZHNgsQG8JTfGy55eYvzG0B/E4AudR2prSRBvF7NYPL47scRCNPgLnvbQczBHbBug6uOr78qnWsYxHlW6Aa5dI5NsmOD4DLtSw8eX0hFyK5Fj6ScYOSFBz9cd1nNTvx2+oIv0lJDcpQdQhsfgsEr1ntvWterZt/8r7xNN83gHYuZ6TM5MYvjQNBc5qC7Krs9wM7UoQuL+s0X6RlOib7/mcLn/lFLsLDdYQAZkSDx/6+t+1oHdMarChIPYT1sx9Dwj2j2mvFNDTKKKKAq0cv14Vrhz67Vjmz2yMJePDqUi0JYS2r0iIo7n8vN7s83v5uOQIDAQAB -----END PUBLIC KEY----- 将上边的公钥拷贝到文本文件中，合并为一行。 生成jwt令牌 在认证工程创建测试类，测试jwt令牌的生成与验证。 @Test public void testCreateJwt(){ //证书文件 String key_location = &quot;xc.keystore&quot;; //密钥库密码 String keystore_password = &quot;xuechengkeystore&quot;; //访问证书路径 ClassPathResource resource = new ClassPathResource(key_location); //密钥工厂 KeyStoreKeyFactory keyStoreKeyFactory = new KeyStoreKeyFactory(resource, keystore_password.toCharArray()); //密钥的密码，此密码和别名要匹配 String keypassword = &quot;xuecheng&quot;; //密钥别名 String alias = &quot;xckey&quot;; //密钥对（密钥和公钥） KeyPair keyPair = keyStoreKeyFactory.getKeyPair(alias,keypassword.toCharArray()); //私钥 RSAPrivateKey aPrivate = (RSAPrivateKey) keyPair.getPrivate(); //定义payload信息 Map&lt;String, Object&gt; tokenMap = new HashMap&lt;&gt;(); tokenMap.put(&quot;id&quot;, &quot;123&quot;); tokenMap.put(&quot;name&quot;, &quot;mrt&quot;); tokenMap.put(&quot;roles&quot;, &quot;r01,r02&quot;); tokenMap.put(&quot;ext&quot;, &quot;1&quot;); //生成jwt令牌 Jwt jwt = JwtHelper.encode(JSON.toJSONString(tokenMap), new RsaSigner(aPrivate)); //取出jwt令牌 String token = jwt.getEncoded(); System.out.println(&quot;token=&quot;+token); } //资源服务使用公钥验证jwt的合法性，并对jwt解码 验证jwt令牌 @Test public void testVerify(){ //jwt令牌 String token =&quot;eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHQiOiIxIiwicm9sZXMiOiJyMDEscjAyIiwibmFtZSI6Im1ydCIsImlkIjoiMTIzIn0.KK7_67N5d1Dthd1PgDHMsbi0UlmjGRcm_XJUUwseJ2eZyJJWoPP2IcEZgAU3tUaaKEHUf9wSRwaDgwhrwfyIcSHbs8oy3zOQEL8j5AOjzBBs7vnRmB7DbSaQD7eJiQVJOXO1QpdmEFgjhc_IBCVTJCVWgZw60IEW1_Lg5tqaLvCiIl26K48pJB5fle2zgYMzqR1L2LyTFkq39rG57VOqqSCi3dapsZQd4ctq95SJCXgGdrUDWtD52rp5o6_0uq‐mrbRdRxkrQfsa1j8C5IW2‐T4eUmiN3f9wF9JxUK1__XC1OQkOn‐ZTBCdqwWIygDFbU7sf6KzfHJTm5vfjp6NIA&quot;; //公钥 String publickey = &quot;‐‐‐‐‐BEGIN PUBLIC KEY‐‐‐‐‐ MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAijyxMdq4S6L1Af1rtB8SjCZHNgsQG8JTfGy55eYvzG0B/E4AudR2prSRBvF7NYPL47scRCNPgLnvbQczBHbBug6uOr78qnWsYxHlW6Aa5dI5NsmOD4DLtSw8eX0hFyK5Fj6ScYOSFBz9cd1nNTvx2+oIv0lJDcpQdQhsfgsEr1ntvWterZt/8r7xNN83gHYuZ6TM5MYvjQNBc5qC7Krs9wM7UoQuL+s0X6RlOib7/mcLn/lFLsLDdYQAZkSDx/6+t+1oHdMarChIPYT1sx9Dwj2j2mvFNDTKKKKAq0cv14Vrhz67Vjmz2yMJePDqUi0JYS2r0iIo7n8vN7s83v5u OQIDAQAB ‐‐‐‐‐END PUBLIC KEY‐‐‐‐‐&quot;; //校验jwt Jwt jwt = JwtHelper.decodeAndVerify(token, new RsaVerifier(publickey)); //获取jwt原始内容 String claims = jwt.getClaims(); //jwt令牌 String encoded = jwt.getEncoded(); System.out.println(encoded); } ","link":"https://tianxiawuhao.github.io/wWtT_Oc9B/"},{"title":"zookeeper概述","content":"zookeeper是什么 Zookeeper 分布式服务框架是Apache Hadoop 的一个子项目，它主要是用来解决分布式应用中经常遇到的一些数据管理问题，如：统一命名服务、状态同步服务、集群管理、分布式应用配置项的管理等。 简单的说，zookeeper=文件系统+通知机制。 zookeeper提供了什么 1、 文件系统 Zookeeper维护一个类似文件系统的数据结构： ​ ​ 每个子目录项如 NameService 都被称作为 znode，和文件系统一样，我们能够自由的增加、删除znode，在一个znode下增加、删除子znode，唯一的不同在于znode是可以存储数据的。 有四种类型的znode： PERSISTENT-持久化目录节点 客户端与zookeeper断开连接后，该节点依旧存在 PERSISTENT_SEQUENTIAL-持久化顺序编号目录节点 客户端与zookeeper断开连接后，该节点依旧存在，只是Zookeeper给该节点名称进行顺序编号 EPHEMERAL-临时目录节点 客户端与zookeeper断开连接后，该节点被删除 EPHEMERAL_SEQUENTIAL-临时顺序编号目录节点 客户端与zookeeper断开连接后，该节点被删除，只是Zookeeper给该节点名称进行顺序编号 2、 通知机制 ​ 客户端注册监听它关心的目录节点，当目录节点发生变化（数据改变、被删除、子目录节点增加删除）时，zookeeper会通知客户端。 我们能用zookeeper做什么 1、 命名服务 ​ 这个似乎最简单，在zookeeper的文件系统里创建一个目录，即有唯一的path。在我们使用tborg无法确定上游程序的部署机器时即可与下游程序约定好path，通过path即能互相探索发现，不见不散了。 2、 配置管理 ​ 程序总是需要配置的，如果程序分散部署在多台机器上，要逐个改变配置就变得困难。好吧，现在把这些配置全部放到zookeeper上去，保存在 Zookeeper 的某个目录节点中，然后所有相关应用程序对这个目录节点进行监听，一旦配置信息发生变化，每个应用程序就会收到 Zookeeper 的通知，然后从 Zookeeper 获取新的配置信息应用到系统中就好。 ​ 3、 集群管理 所谓集群管理无在乎两点：是否有机器退出和加入、选举master。 ​ 对于第一点，所有机器约定在父目录GroupMembers下创建临时目录节点，然后监听父目录节点的子节点变化消息。一旦有机器挂掉，该机器与 zookeeper的连接断开，其所创建的临时目录节点被删除，所有其他机器都收到通知：某个兄弟目录被删除，于是，所有人都知道：它下船了。新机器加入 也是类似，所有机器收到通知：新兄弟目录加入。 ​ 对于第二点，我们稍微改变一下，所有机器创建临时顺序编号目录节点，每次选取编号最小的机器作为master就好。 ​ 4、 分布式锁 ​ 有了zookeeper的一致性文件系统，锁的问题变得容易。锁服务可以分为两类，一个是保持独占，另一个是控制时序。 ​ 对于第一类，我们将zookeeper上的一个znode看作是一把锁，通过createznode的方式来实现。所有客户端都去创建 /distribute_lock 节点，最终成功创建的那个客户端也即拥有了这把锁。厕所有言：来也冲冲，去也冲冲，用完删除掉自己创建的distribute_lock 节点就释放出锁。 ​ 对于第二类， /distribute_lock 已经预先存在，所有客户端在它下面创建临时顺序编号目录节点，和选master一样，编号最小的获得锁，用完删除，依次方便。 ​ 5、队列管理 两种类型的队列： 同步队列，当一个队列的成员都聚齐时，这个队列才可用，否则一直等待所有成员到达。 队列按照 FIFO 方式进行入队和出队操作。 第一类，在约定目录下创建临时目录节点，监听节点数目是否是我们要求的数目。 第二类，和分布式锁服务中的控制时序场景基本原理一致，入列有编号，出列按编号。 ​ 终于了解完我们能用zookeeper做什么了，可是作为一个程序员，我们总是想狂热了解zookeeper是如何做到这一点的，单点维护一个文件系统没有什么难度，可是如果是一个集群维护一个文件系统保持数据的一致性就非常困难了。 6、分布式与数据复制 Zookeeper作为一个集群提供一致的数据服务，自然，它要在所有机器间做数据复制。数据复制的好处： 容错 一个节点出错，不致于让整个系统停止工作，别的节点可以接管它的工作； 提高系统的扩展能力 把负载分布到多个节点上，或者增加节点来提高系统的负载能力； 提高性能 让客户端本地访问就近的节点，提高用户访问速度。 从客户端读写访问的透明度来看，数据复制集群系统分下面两种： 写主(WriteMaster) 对数据的修改提交给指定的节点。读无此限制，可以读取任何一个节点。这种情况下客户端需要对读与写进行区别，俗称读写分离； 写任意(Write Any) 对数据的修改可提交给任意的节点，跟读一样。这种情况下，客户端对集群节点的角色与变化透明。 ​ 对zookeeper来说，它采用的方式是写任意。通过增加机器，它的读吞吐能力和响应能力扩展性非常好，而写，随着机器的增多吞吐能力肯定下降（这 也是它建立observer的原因），而响应能力则取决于具体实现方式，是延迟复制保持最终一致性，还是立即复制快速响应。 我们关注的重点还是在如何保证数据在集群所有机器的一致性，这就涉及到paxos算法。 7、数据一致性与paxos算法 ​ 据说Paxos算法的难理解与算法的知名度一样令人敬仰，所以我们先看如何保持数据的一致性，这里有个原则就是： 在一个分布式数据库系统中，如果各节点的初始状态一致，每个节点都执行相同的操作序列，那么他们最后能得到一个一致的状态。 ​ Paxos算法解决的什么问题呢，解决的就是保证每个节点执行相同的操作序列。好吧，这还不简单，master维护一个全局写队列，所有写操作都必须 放入这个队列编号，那么无论我们写多少个节点，只要写操作是按编号来的，就能保证一致性。没错，就是这样，可是如果master挂了呢。 ​ Paxos算法通过投票来对写操作进行全局编号，同一时刻，只有一个写操作被批准，同时并发的写操作要去争取选票，只有获得过半数选票的写操作才会被 批准（所以永远只会有一个写操作得到批准），其他的写操作竞争失败只好再发起一轮投票，就这样，在日复一日年复一年的投票中，所有写操作都被严格编号排 序。编号严格递增，当一个节点接受了一个编号为100的写操作，之后又接受到编号为99的写操作（因为网络延迟等很多不可预见原因），它马上能意识到自己 数据不一致了，自动停止对外服务并重启同步过程。任何一个节点挂掉都不会影响整个集群的数据一致性（总2n+1台，除非挂掉大于n台）。 总结 ​ Zookeeper 作为 Hadoop 项目中的一个子项目，是 Hadoop 集群管理的一个必不可少的模块，它主要用来控制集群中的数据，如它管理 Hadoop 集群中的 NameNode，还有 Hbase 中 Master Election、Server 之间状态同步等。 Zookeeper工作原理 ​ ZooKeeper是一个分布式的，开放源码的分布式应用程序协调服务，它包含一个简单的原语集，分布式应用程序可以基于它实现同步服务，配置维护和 命名服务等。Zookeeper是hadoop的一个子项目，其发展历程无需赘述。在分布式应用中，由于工程师不能很好地使用锁机制，以及基于消息的协调 机制不适合在某些应用中使用，因此需要有一种可靠的、可扩展的、分布式的、可配置的协调机制来统一系统的状态。Zookeeper的目的就在于此。 Zookeeper的基本概念 1.1 角色 Zookeeper中的角色主要有以下三类，如下表所示： ​ 系统模型如图所示： ​ 1.2 设计目的 最终一致性：client不论连接到哪个Server，展示给它都是同一个视图，这是zookeeper最重要的性能。 可靠性：具有简单、健壮、良好的性能，如果消息m被一台服务器接受，那么它将被所有的服务器接受。 实时性：Zookeeper保证客户端将在一个时间间隔范围内获得服务器的更新信息，或者服务器失效的信息。但由于网络延时等原因，Zookeeper不能保证两个客户端能同时得到刚更新的数据，如果需要最新数据，应该在读数据之前调用sync()接口。 等待无关（wait-free）：慢的或者失效的client不得干预快速的client的请求，使得每个client都能有效的等待。 原子性：更新只能成功或者失败，没有中间状态。 顺序性：包括全局有序和偏序两种：全局有序是指如果在一台服务器上消息a在消息b前发布，则在所有Server上消息a都将在消息b前被发布；偏序是指如果一个消息b在消息a后被同一个发送者发布，a必将排在b前面。 ZooKeeper的流程设计 ​ Zookeeper的核心是原子广播，这个机制保证了各个Server之间的同步。实现这个机制的协议叫做Zab协议。Zab协议有两种模式，它们分 别是恢复模式（选主）和广播模式（同步）。当服务启动或者在领导者崩溃后，Zab就进入了恢复模式，当领导者被选举出来，且大多数Server完成了和 leader的状态同步以后，恢复模式就结束了。状态同步保证了leader和Server具有相同的系统状态。 ​ 为了保证事务的顺序一致性，zookeeper采用了递增的事务id号（zxid）来标识事务。所有的提议（proposal）都在被提出的时候加上 了zxid。实现中zxid是一个64位的数字，它高32位是epoch用来标识leader关系是否改变，每次一个leader被选出来，它都会有一个 新的epoch，标识当前属于那个leader的统治时期。低32位用于递增计数。 每个Server在工作过程中有三种状态： LOOKING：当前Server不知道leader是谁，正在搜寻 LEADING：当前Server即为选举出来的leader FOLLOWING：leader已经选举出来，当前Server与之同步 2.1 选主流程 当leader崩溃或者leader失去大多数的follower，这时候zk进入恢复模式，恢复模式需要重新选举出一个新的leader，让所有的 Server都恢复到一个正确的状态。Zk的选举算法有两种：一种是基于basic paxos实现的，另外一种是基于fast paxos算法实现的。系统默认的选举算法为fast paxos。先介绍basic paxos流程： ​ 1 .选举线程由当前Server发起选举的线程担任，其主要功能是对投票结果进行统计，并选出推荐的Server； ​ 2 .选举线程首先向所有Server发起一次询问(包括自己)； ​ 3 .选举线程收到回复后，验证是否是自己发起的询问(验证zxid是否一致)，然后获取对方的id(myid)，并存储到当前询问对象列表中，最后获取对方提议的leader相关信息( id,zxid)，并将这些信息存储到当次选举的投票记录表中； ​ 4. 收到所有Server回复以后，就计算出zxid最大的那个Server，并将这个Server相关信息设置成下一次要投票的Server； ​ 5. 线程将当前zxid最大的Server设置为当前Server要推荐的Leader，如果此时获胜的Server获得n/2 + 1的Server票数， 设置当前推荐的leader为获胜的Server，将根据获胜的Server相关信息设置自己的状态，否则，继续这个过程，直到leader被选举出来。 通过流程分析我们可以得出：要使Leader获得多数Server的支持，则Server总数必须是奇数2n+1，且存活的Server的数目不得少于n+1. 每个Server启动后都会重复以上流程。在恢复模式下，如果是刚从崩溃状态恢复的或者刚启动的server还会从磁盘快照中恢复数据和会话信息，zk会记录事务日志并定期进行快照，方便在恢复时进行状态恢复。选主的具体流程图如下所示： ​ fast paxos流程是在选举过程中，某Server首先向所有Server提议自己要成为leader，当其它Server收到提议以后，解决epoch和 zxid的冲突，并接受对方的提议，然后向对方发送接受提议完成的消息，重复这个流程，最后一定能选举出Leader。其流程图如下所示： 2.2 同步流程 选完leader以后，zk就进入状态同步过程。 ​ 1. leader等待server连接； ​ 2 .Follower连接leader，将最大的zxid发送给leader； ​ 3 .Leader根据follower的zxid确定同步点； ​ 4 .完成同步后通知follower 已经成为uptodate状态； ​ 5 .Follower收到uptodate消息后，又可以重新接受client的请求进行服务了。 流程图如下所示： ​ 2.3 工作流程 2.3.1 Leader工作流程 Leader主要有三个功能： ​ 1 .恢复数据； ​ 2 .维持与Learner的心跳，接收Learner请求并判断Learner的请求消息类型； ​ 3 .Learner的消息类型主要有PING消息、REQUEST消息、ACK消息、REVALIDATE消息，根据不同的消息类型，进行不同的处理。 ​ PING消息是指Learner的心跳信息；REQUEST消息是Follower发送的提议信息，包括写请求及同步请求；ACK消息是 Follower的对提议的回复，超过半数的Follower通过，则commit该提议；REVALIDATE消息是用来延长SESSION有效时间。 Leader的工作流程简图如下所示，在实际实现中，流程要比下图复杂得多，启动了三个线程来实现功能。 ​ 2.3.2 Follower工作流程 Follower主要有四个功能： ​ 1. 向Leader发送请求（PING消息、REQUEST消息、ACK消息、REVALIDATE消息）； ​ 2 .接收Leader消息并进行处理； ​ 3 .接收Client的请求，如果为写请求，发送给Leader进行投票； ​ 4 .返回Client结果。 Follower的消息循环处理如下几种来自Leader的消息： ​ 1 .PING消息： 心跳消息； ​ 2 .PROPOSAL消息：Leader发起的提案，要求Follower投票； ​ 3 .COMMIT消息：服务器端最新一次提案的信息； ​ 4 .UPTODATE消息：表明同步完成； ​ 5 .REVALIDATE消息：根据Leader的REVALIDATE结果，关闭待revalidate的session还是允许其接受消息； ​ 6 .SYNC消息：返回SYNC结果到客户端，这个消息最初由客户端发起，用来强制得到最新的更新。 Follower的工作流程简图如下所示，在实际实现中，Follower是通过5个线程来实现功能的。 ​ 对于observer的流程不再叙述，observer流程和Follower的唯一不同的地方就是observer不会参加leader发起的投票。 ","link":"https://tianxiawuhao.github.io/DOckY7njv/"},{"title":"Gossip协议","content":"Gossip Gossip协议是一个通信协议，一种传播消息的方式，灵感来自于：瘟疫、社交网络等。使用Gossip协议的有：Redis Cluster、Consul、Apache Cassandra等。 六度分隔理论 说到社交网络，就不得不提著名的六度分隔理论。1967年，哈佛大学的心理学教授Stanley Milgram想要描绘一个连结人与社区的人际连系网。做过一次连锁信实验，结果发现了“六度分隔”现象。简单地说：“你和任何一个陌生人之间所间隔的人不会超过六个，也就是说，最多通过六个人你就能够认识任何一个陌生人。 数学解释该理论：若每个人平均认识260人，其六度就是260↑6 =1,188,137,600,000。消除一些节点重复，那也几乎覆盖了整个地球人口若干多多倍，这也是Gossip协议的雏形。 原理 Gossip协议基本思想就是：一个节点想要分享一些信息给网络中的其他的一些节点。于是，它周期性的随机选择一些节点，并把信息传递给这些节点。这些收到信息的节点接下来会做同样的事情，即把这些信息传递给其他一些随机选择的节点。一般而言，信息会周期性的传递给N个目标节点，而不只是一个。这个N被称为fanout（这个单词的本意是扇出）。 用途 Gossip协议的主要用途就是信息传播和扩散：即把一些发生的事件传播到全世界。它们也被用于数据库复制，信息扩散，集群成员身份确认，故障探测等。 基于Gossip协议的一些有名的系统：Apache Cassandra，Redis（Cluster模式），Consul等。 图解 接下来通过多张图片剖析Gossip协议是如何运行的。如下图所示，Gossip协议是周期循环执行的。图中的公式表示Gossip协议把信息传播到每一个节点需要多少次循环动作，需要说明的是，公式中的20表示整个集群有20个节点，4表示某个节点会向4个目标节点传播消息： Gossip Protocol 如下图所示，红色的节点表示其已经“受到感染”，即接下来要传播信息的源头，连线表示这个初始化感染的节点能正常连接的节点（其不能连接的节点只能靠接下来感染的节点向其传播消息）。并且N等于4，我们假设4根较粗的线路，就是它第一次传播消息的线路： first infected node 第一次消息完成传播后，新增了4个节点会被“感染”，即这4个节点也收到了消息。这时候，总计有5个节点变成红色： infected nodes 那么在下一次传播周期时，总计有5个节点，且这5个节点每个节点都会向4个节点传播消息。最后，经过3次循环，20个节点全部被感染（都变成红色节点），即说明需要传播的消息已经传播给了所有节点： infected all nodes 需要说明的是，20个节点且设置fanout=4，公式结果是2.16，这只是个近似值。真实传递时，可能需要3次甚至4次循环才能让所有节点收到消息。这是因为每个节点在传播消息的时候，是随机选择N个节点的，这样的话，就有可能某个节点会被选中2次甚至更多次。 发送消息 由前面对Gossip协议图解分析可知，节点传播消息是周期性的，并且每个节点有它自己的周期。另外，节点发送消息时的目标节点数由参数fanout决定。至于往哪些目标节点发送，则是随机的。 一旦消息被发送到目标节点，那么目标节点也会被感染。一旦某个节点被感染，那么它也会向其他节点传播消息，试图感染更多的节点。最终，每一个节点都会被感染，即消息被同步给了所有节点： 可扩展性 Gossip协议是可扩展的，因为它只需要O(logN) 个周期就能把消息传播给所有节点。某个节点在往固定数量节点传播消息过程中，并不需要等待确认（ack），并且，即使某条消息传播过程中丢失，它也不需要做任何补偿措施。打个比方，某个节点本来需要将消息传播给4个节点，但是由于网络或者其他原因，只有3个消息接收到消息，即使这样，这对最终所有节点接收到消息是没有任何影响的。 如下表格所示，假定fanout=4，那么在节点数分别是20、40、80、160时，消息传播到所有节点需要的循环次数对比，在节点成倍扩大的情况下，循环次数并没有增加很多。所以，Gossip协议具备可扩展性： 失败容错 Gossip也具备失败容错的能力，即使网络故障等一些问题，Gossip协议依然能很好的运行。因为一个节点会多次分享某个需要传播的信息，即使不能连通某个节点，其他被感染的节点也会尝试向这个节点传播信息。 健壮性 Gossip协议下，没有任何扮演特殊角色的节点（比如leader等）。任何一个节点无论什么时候下线或者加入，并不会破坏整个系统的服务质量。 然而，Gossip协议也有不完美的地方，例如，拜占庭问题（Byzantine）。即，如果有一个恶意传播消息的节点，Gossip协议的分布式系统就会出问题。 作者：阿飞的博客 原文地址：https://www.jianshu.com/p/54eab117e6ae ","link":"https://tianxiawuhao.github.io/BmWrFaJAQ/"},{"title":"CAP/BASE","content":"CAP原则 CAP原则又称CAP定理，指的是在一个分布式系统中， Consistency（一致性）、 Availability（可用性）、Partition tolerance（分区容错性），三者不可得兼。 CAP原则是NOSQL数据库的基石。 分布式系统的CAP理论：理论首先把分布式系统中的三个特性进行了如下归纳： 一致性（C）：在分布式系统中的所有数据备份，在同一时刻是否同样的值。（等同于所有节点访问同一份最新的数据副本） 可用性（A）：在集群中一部分节点故障后，集群整体是否还能响应客户端的读写请求。（对数据更新具备高可用性） 分区容忍性（P）：以实际效果而言，分区相当于对通信的时限要求。系统如果不能在时限内达成数据正常返回，就意味着发生了分区的情况，必须就当前操作在C和A之间做出选择。 一致性与可用性的决择编辑 CAP理论就是说在分布式存储系统中，最多只能实现上面的两点。而由于当前的网络硬件肯定会出现延迟丢包等问题，所以分区容忍性是我们必须需要实现的。所以我们只能在一致性和可用性之间进行权衡，没有NoSQL系统能同时保证这三点。 对于web2.0网站来说，关系数据库的很多主要特性却往往无用武之地 数据库事务一致性需求 很多web实时系统并不要求严格的数据库事务，对读一致性的要求很低，有些场合对写一致性要求并不高。允许实现最终一致性。 数据库的写实时性和读实时性需求 对关系数据库来说，插入一条数据之后立刻查询，是肯定可以读出来这条数据的，但是对于很多web应用来说，并不要求这么高的实时性，比方说发一条消息之 后，过几秒乃至十几秒之后，我的订阅者才看到这条动态是完全可以接受的。 对复杂的SQL查询，特别是多表关联查询的需求 任何大数据量的web系统，都非常忌讳多个大表的关联查询，以及复杂的数据分析类型的报表查询，特别是SNS类型的网站，从需求以及产品设计角 度，就避免了这种情况的产生。往往更多的只是单表的主键查询，以及单表的简单条件分页查询，SQL的功能被极大的弱化了。 CAP定理的证明 现在我们就来证明一下，为什么不能同时满足三个特性？ 假设有两台服务器，一台放着应用A和数据库V，一台放着应用B和数据库V，他们之间的网络可以互通，也就相当于分布式系统的两个部分。 在满足一致性的时候，两台服务器 N1和N2，一开始两台服务器的数据是一样的，DB0=DB0。在满足可用性的时候，用户不管是请求N1或者N2，都会得到立即响应。在满足分区容错性的情况下，N1和N2有任何一方宕机，或者网络不通的时候，都不会影响N1和N2彼此之间的正常运作。 当用户通过N1中的A应用请求数据更新到服务器DB0后，这时N1中的服务器DB0变为DB1，通过分布式系统的数据同步更新操作，N2服务器中的数据库V0也更新为了DB1，这时，用户通过B向数据库发起请求得到的数据就是即时更新后的数据DB1。 上面是正常运作的情况，但分布式系统中，最大的问题就是网络传输问题，现在假设一种极端情况，N1和N2之间的网络断开了，但我们仍要支持这种网络异常，也就是满足分区容错性，那么这样能不能同时满足一致性和可用性呢？ 假设N1和N2之间通信的时候网络突然出现故障，有用户向N1发送数据更新请求，那N1中的数据DB0将被更新为DB1，由于网络是断开的，N2中的数据库仍旧是DB0； 如果这个时候，有用户向N2发送数据读取请求，由于数据还没有进行同步，应用程序没办法立即给用户返回最新的数据DB1，怎么办呢？有二种选择，第一，牺牲数据一致性，响应旧的数据DB0给用户；第二，牺牲可用性，阻塞等待，直到网络连接恢复，数据更新操作完成之后，再给用户响应最新的数据DB1。 上面的过程比较简单，但也说明了要满足分区容错性的分布式系统，只能在一致性和可用性两者中，选择其中一个。也就是说分布式系统不可能同时满足三个特性。这就需要我们在搭建系统时进行取舍了，那么，怎么取舍才是更好的策略呢? 取舍策略 CAP三个特性只能满足其中两个，那么取舍的策略就共有三种： CA without P： 如果不要求P（不允许分区），则C（强一致性）和A（可用性）是可以保证的。但放弃P的同时也就意味着放弃了系统的扩展性，也就是分布式节点受限，没办法部署子节点，这是违背分布式系统设计的初衷的。传统的关系型数据库RDBMS：Oracle、MySQL就是CA。 CP without A： 如果不要求A（可用），相当于每个请求都需要在服务器之间保持强一致，而P（分区）会导致同步时间无限延长(也就是等待数据同步完才能正常访问服务)，一旦发生网络故障或者消息丢失等情况，就要牺牲用户的体验，等待所有数据全部一致了之后再让用户访问系统。设计成CP的系统其实不少，最典型的就是分布式数据库，如Redis、HBase等。对于这些分布式数据库来说，数据的一致性是最基本的要求，因为如果连这个标准都达不到，那么直接采用关系型数据库就好，没必要再浪费资源来部署分布式数据库。 AP wihtout C： 要高可用并允许分区，则需放弃一致性。一旦分区发生，节点之间可能会失去联系，为了高可用，每个节点只能用本地数据提供服务，而这样会导致全局数据的不一致性。典型的应用就如某米的抢购手机场景，可能前几秒你浏览商品的时候页面提示是有库存的，当你选择完商品准备下单的时候，系统提示你下单失败，商品已售完。这其实就是先在 A（可用性）方面保证系统可以正常的服务，然后在数据的一致性方面做了些牺牲，虽然多少会影响一些用户体验，但也不至于造成用户购物流程的严重阻塞。 BASE理论 BASE是Basically Available（基本可用）、Soft state（软状态）和Eventually consistent（最终一致性）三个短语的简写，BASE是对CAP中一致性和可用性权衡的结果，其来源于对大规模互联网系统分布式实践的结论，是基于CAP定理逐步演化而来的，其核心思想是即使无法做到强一致性（Strong consistency），但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性（Eventual consistency）。接下来我们着重对BASE中的三要素进行详细讲解。 基本可用 基本可用是指分布式系统在出现不可预知故障的时候，允许损失部分可用性——但请注意，这绝不等价于系统不可用，以下两个就是“基本可用”的典型例子。 响应时间上的损失：正常情况下，一个在线搜索引擎需要0.5秒内返回给用户相应的查询结果，但由于出现异常（比如系统部分机房发生断电或断网故障），查询结果的响应时间增加到了1~2秒。 功能上的损失：正常情况下，在一个电子商务网站上进行购物，消费者几乎能够顺利地完成每一笔订单，但是在一些节日大促购物高峰的时候，由于消费者的购物行为激增，为了保护购物系统的稳定性，部分消费者可能会被引导到一个降级页面。 弱状态 弱状态也称为软状态，和硬状态相对，是指允许系统中的数据存在中间状态，并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时。 最终一致性 最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性 亚马逊首席技术官Werner Vogels在于2008年发表的一篇文章中对最终一致性进行了非常详细的介绍。他认为最终一致性是一种特殊的弱一致性：系统能够保证在没有其他新的更新操作的情况下，数据最终一定能够达到一致的状态，因此所有客户端对系统的数据访问都能够获取到最新的值。同时，在没有发生故障的前提下，数据达到一致状态的时间延迟，取决于网络延迟，系统负载和数据复制方案设计等因素。 在实际工程实践中，最终一致性存在以下五类主要变种。 因果一致性： 因果一致性是指，如果进程A在更新完某个数据项后通知了进程B，那么进程B之后对该数据项的访问都应该能够获取到进程A更新后的最新值，并且如果进程B要对该数据项进行更新操作的话，务必基于进程A更新后的最新值，即不能发生丢失更新情况。与此同时，与进程A无因果关系的进程C的数据访问则没有这样的限制。 读己之所写： 读己之所写是指，进程A更新一个数据项之后，它自己总是能够访问到更新过的最新值，而不会看到旧值。也就是说，对于单个数据获取者而言，其读取到的数据一定不会比自己上次写入的值旧。因此，读己之所写也可以看作是一种特殊的因果一致性。 会话一致性： 会话一致性将对系统数据的访问过程框定在了一个会话当中：系统能保证在同一个有效的会话中实现“读己之所写”的一致性，也就是说，执行更新操作之后，客户端能够在同一个会话中始终读取到该数据项的最新值。 单调读一致性： 单调读一致性是指如果一个进程从系统中读取出一个数据项的某个值后，那么系统对于该进程后续的任何数据访问都不应该返回更旧的值。 单调写一致性： 单调写一致性是指，一个系统需要能够保证来自同一个进程的写操作被顺序地执行。 以上就是最终一致性的五类常见的变种，在时间系统实践中，可以将其中的若干个变种互相结合起来，以构建一个具有最终一致性的分布式系统。事实上，可以将其中的若干个变种相互结合起来，以构建一个具有最终一致性特性的分布式系统。事实上，最终一致性并不是只有那些大型分布式系统才设计的特性，许多现代的关系型数据库都采用了最终一致性模型。在现代关系型数据库中，大多都会采用同步和异步方式来实现主备数据复制技术。在同步方式中，数据的复制国耻鞥通常是更新事务的一部分，因此在事务完成后，主备数据库的数据就会达到一致。而在异步方式中，备库的更新往往存在延时，这取决于事务日志在主备数据库之间传输的时间长短，如果传输时间过长或者甚至在日志传输过程中出现异常导致无法及时将事务应用到备库上，那么狠显然，从备库中读取的的数据将是旧的，因此就出现了不一致的情况。当然，无论是采用多次重试还是认为数据订正，关系型数据库还是能搞保证最终数据达到一致——这就是系统提供最终一致性保证的经典案例。 总的来说，BASE理论面向的是大型高可用可扩展的分布式系统，和传统事务的ACID特性使相反的，它完全不同于ACID的强一致性模型，而是提出通过牺牲强一致性来获得可用性，并允许数据在一段时间内是不一致的，但最终达到一致状态。但同时，在实际的分布式场景中，不同业务单元和组件对数据一致性的要求是不同的，因此在具体的分布式系统架构设计过程中，ACID特性与BASE理论往往又会结合在一起使用。 ","link":"https://tianxiawuhao.github.io/cxyiXFW2S/"},{"title":"MyBatis 缓存详解","content":"缓存是一般的ORM 框架都会提供的功能，目的就是提升查询的效率和减少数据库的压力。跟Hibernate 一样，MyBatis 也有一级缓存和二级缓存，并且预留了集成第三方缓存的接口。 缓存体系结构： MyBatis 跟缓存相关的类都在cache 包里面，其中有一个Cache 接口，只有一个默认的实现类 PerpetualCache，它是用HashMap 实现的。 所有的缓存实现类总体上可分为三类：基本缓存、淘汰算法缓存、装饰器缓存。 缓存实现类 描述 作用 装饰条件 基本缓存 缓存基本实现类 默认是PerpetualCache，也可以自定义比如RedisCache等，具备基本功能的缓存类 无 LruCache LRU策略的缓存 当缓存达到上限时，删除最近最少使用的缓存 eviction=“LRU” （默认） FifoCache FIFO策略的缓存 当缓存达到上限时，删除最先入队的缓存 eviction=“FIFO” SoftCache/WeakCache 带清理策略的缓存 通过JVM的软引用和弱引用来实现缓存，当JVM内存不足时，会自动清理掉这些缓存 eviction=“SOFT”/eviction=“WEAK” LoggingCache 带日志功能的缓存 比如输出缓存命中率 基本 SynchronizedCache 同步缓存 基于Synchronized关键字实现，解决并发问题 基本 BlockingCache 阻塞缓存 通过在get/put方式中加锁，保证只有一个线程操作缓存，基于Java重入锁实现 blocking=true SerializedCache 支持序列化的缓存 将对象序列化以后存到缓存中，取出是反序列化 readOnly=false(默认) ScheduledCache 定时调度的缓存 在进行 get/put/remove/getSize 等操作前，判断 缓存时间是否超过了设置的最长缓存时间（默认是 一小时），如果是则清空缓存–即每隔一段时间清 空一次缓存 flushInterval不为空 TransactionalCache 事务缓存 在二级缓存中使用，可以一次存入多个缓存，删除多个缓存 在TransactionalCacheManager中用Map维护对应关系 一级缓存（本地缓存） 一级缓存也叫本地缓存，MyBatis 的一级缓存是在会话（SqlSession）层面进行缓存的。MyBatis 的一级缓存是默认开启的，不需要任何的配置。首先我们必须去弄清楚一个问题，在MyBatis 执行的流程里面，涉及到这么多的对象，那么缓存PerpetualCache 应该放在哪个对象里面去维护？如果要在同一个会话里面共享一级缓存，这个对象肯定是在SqlSession 里面创建的，作为SqlSession 的一个属性。 DefaultSqlSession 里面只有两个属性，Configuration 是全局的，所以缓存只可能放在Executor 里面维护——SimpleExecutor/ReuseExecutor/BatchExecutor 的父类BaseExecutor 的构造函数中持有了PerpetualCache。在同一个会话里面，多次执行相同的SQL 语句，会直接从内存取到缓存的结果，不会再发送SQL 到数据库。但是不同的会话里面，即使执行的SQL 一模一样（通过一个Mapper 的同一个方法的相同参数调用），也不能使用到一级缓存。 如下图所示，MyBatis会在一次会话的表示----一个SqlSession对象中创建一个本地缓存(local cache)，对于每一次查询，都会尝试根据查询的条件去本地缓存中查找是否在缓存中，如果在缓存中，就直接从缓存中取出，然后返回给用户；否则，从数据库读取数据，将查询结果存入缓存并返回给用户。 一级缓存的生命周期有多长？ MyBatis在开启一个数据库会话时，会 创建一个新的SqlSession对象，SqlSession对象中会有一个新的Executor对象，Executor对象中持有一个新的PerpetualCache对象；当会话结束时，SqlSession对象及其内部的Executor对象还有PerpetualCache对象也一并释放掉。 如果SqlSession调用了close()方法，会释放掉一级缓存PerpetualCache对象，一级缓存将不可用； 如果SqlSession调用了clearCache()，会清空PerpetualCache对象中的数据，但是该对象仍可使用； SqlSession中执行了任何一个update操作(update()、delete()、insert()) ，都会清空PerpetualCache对象的数据，但是该对象可以继续使用； SqlSession 一级缓存的工作流程： 对于某个查询，根据statementId,params,rowBounds来构建一个key值，根据这个key值去缓存Cache中取出对应的key值存储的缓存结果 判断从Cache中根据特定的key值取的数据数据是否为空，即是否命中； 如果命中，则直接将缓存结果返回； 如果没命中： 去数据库中查询数据，得到查询结果； 将key和查询到的结果分别作为key,value对存储到Cache中； 将查询结果返回； 接下来我们来验证一下，MyBatis 的一级缓存到底是不是只能在一个会话里面共享，以及跨会话（不同session）操作相同的数据会产生什么问题。判断是否命中缓存：如果再次发送SQL 到数据库执行，说明没有命中缓存；如果直接打印对象，说明是从内存缓存中取到了结果。 在同一个session 中共享（不同session 不能共享） 同一个会话中，update（包括delete）会导致一级缓存被清空 其他会话更新了数据，导致读取到脏数据（一级缓存不能跨会话共享） 一级缓存的不足： 使用一级缓存的时候，因为缓存不能跨会话共享，不同的会话之间对于相同的数据可能有不一样的缓存。在有多个会话或者分布式环境下，会存在脏数据的问题。如果要解决这个问题，就要用到二级缓存。MyBatis 一级缓存（MyBaits 称其为 Local Cache）无法关闭，但是有两种级别可选： session 级别的缓存，在同一个 sqlSession 内，对同样的查询将不再查询数据库，直接从缓存中。 statement 级别的缓存，避坑： 为了避免这个问题，可以将一级缓存的级别设为 statement 级别的，这样每次查询结束都会清掉一级缓存。 二级缓存 二级缓存是用来解决一级缓存不能跨会话共享的问题的，范围是namespace 级别的，可以被多个SqlSession 共享（只要是同一个接口里面的相同方法，都可以共享），生命周期和应用同步。如果你的MyBatis使用了二级缓存，并且你的Mapper和select语句也配置使用了二级缓存，那么在执行select查询的时候，MyBatis会先从二级缓存中取输入，其次才是一级缓存，即MyBatis查询数据的顺序是：二级缓存 —&gt; 一级缓存 —&gt; 数据库。 作为一个作用范围更广的缓存，它肯定是在SqlSession 的外层，否则不可能被多个SqlSession 共享。而一级缓存是在SqlSession 内部的，所以第一个问题，肯定是工作在一级缓存之前，也就是只有取不到二级缓存的情况下才到一个会话中去取一级缓存。第二个问题，二级缓存放在哪个对象中维护呢？ 要跨会话共享的话，SqlSession 本身和它里面的BaseExecutor 已经满足不了需求了，那我们应该在BaseExecutor 之外创建一个对象。 实际上MyBatis 用了一个装饰器的类来维护，就是CachingExecutor。如果启用了二级缓存，MyBatis 在创建Executor 对象的时候会对Executor 进行装饰。CachingExecutor 对于查询请求，会判断二级缓存是否有缓存结果，如果有就直接返回，如果没有委派交给真正的查询器Executor 实现类，比如SimpleExecutor 来执行查询，再走到一级缓存的流程。最后会把结果缓存起来，并且返回给用户。 开启二级缓存的方法 第一步：配置 mybatis.configuration.cache-enabled=true，只要没有显式地设置cacheEnabled=false，都会用CachingExecutor 装饰基本的执行器。 第二步：在Mapper.xml 中配置标签： &lt;cache type=&quot;org.apache.ibatis.cache.impl.PerpetualCache&quot; size=&quot;1024&quot; eviction=&quot;LRU&quot; flushInterval=&quot;120000&quot; readOnly=&quot;false&quot;/&gt; 基本上就是这样。这个简单语句的效果如下: 映射语句文件中的所有 select 语句的结果将会被缓存。 映射语句文件中的所有 insert、update 和 delete 语句会刷新缓存。 缓存会使用最近最少使用算法（LRU, Least Recently Used）算法来清除不需要的缓存。 缓存不会定时进行刷新（也就是说，没有刷新间隔）。 缓存会保存列表或对象（无论查询方法返回哪种）的 1024 个引用。 缓存会被视为读/写缓存，这意味着获取到的对象并不是共享的，可以安全地被调用者修改，而不干扰其他调用者或线程所做的潜在修改。 这个更高级的配置创建了一个 FIFO 缓存，每隔 60 秒刷新，最多可以存储结果对象或列表的 512 个引用，而且返回的对象被认为是只读的，因此对它们进行修改可能会在不同线程中的调用者产生冲突。可用的清除策略有： LRU – 最近最少使用：移除最长时间不被使用的对象。 FIFO – 先进先出：按对象进入缓存的顺序来移除它们。 SOFT – 软引用：基于垃圾回收器状态和软引用规则移除对象。 WEAK – 弱引用：更积极地基于垃圾收集器状态和弱引用规则移除对象。 默认的清除策略是 LRU。 flushInterval（刷新间隔）属性可以被设置为任意的正整数，设置的值应该是一个以毫秒为单位的合理时间量。 默认情况是不设置，也就是没有刷新间隔，缓存仅仅会在调用语句时刷新。 size（引用数目）属性可以被设置为任意正整数，要注意欲缓存对象的大小和运行环境中可用的内存资源。默认值是 1024。 readOnly（只读）属性可以被设置为 true 或 false。只读的缓存会给所有调用者返回缓存对象的相同实例。 因此这些对象不能被修改。这就提供了可观的性能提升。而可读写的缓存会（通过序列化）返回缓存对象的拷贝。 速度上会慢一些，但是更安全，因此默认值是 false。 注：二级缓存是事务性的。这意味着，当 SqlSession 完成并提交时，或是完成并回滚，但没有执行 flushCache=true 的 insert/delete/update 语句时，缓存会获得更新。 Mapper.xml 配置了之后，select()会被缓存。update()、delete()、insert()会刷新缓存。：如果cacheEnabled=true，Mapper.xml 没有配置标签，还有二级缓存吗？（没有）还会出现CachingExecutor 包装对象吗？（会） 只要cacheEnabled=true 基本执行器就会被装饰。有没有配置，决定了在启动的时候会不会创建这个mapper 的Cache 对象，只是最终会影响到CachingExecutorquery 方法里面的判断。如果某些查询方法对数据的实时性要求很高，不需要二级缓存，怎么办？我们可以在单个Statement ID 上显式关闭二级缓存（默认是true）： &lt;select id=&quot;selectBlog&quot; resultMap=&quot;BaseResultMap&quot; useCache=&quot;false&quot;&gt; 第三方缓存做二级缓存 除了MyBatis 自带的二级缓存之外，我们也可以通过实现Cache 接口来自定义二级缓存。MyBatis 官方提供了一些第三方缓存集成方式，比如ehcache 和redis：https://github.com/mybatis/redis-cache ,这里就不过多介绍了。当然，我们也可以使用独立的缓存服务，不使用MyBatis 自带的二级缓存。 pom 文件引入依赖： &lt;dependency&gt; &lt;groupId&gt;org.mybatis.caches&lt;/groupId&gt; &lt;artifactId&gt;mybatis-redis&lt;/artifactId&gt; &lt;version&gt;1.0.0-beta2&lt;/version&gt; &lt;/dependency&gt; MybatisRedisCache import com.xxx.util.JsonUtils; import org.apache.ibatis.cache.Cache; import org.apache.logging.log4j.LogManager; import org.apache.logging.log4j.Logger; import org.springframework.data.redis.core.RedisTemplate; import java.util.concurrent.locks.ReadWriteLock; import java.util.concurrent.locks.ReentrantReadWriteLock; /** * Mybatis - redis二级缓存 * */ public final class MybatisRedisCache implements Cache { /** * 日志工具类 */ private static final Logger logger = LogManager.getLogger(MybatisRedisCache.class); /** * 读写锁 */ private final ReadWriteLock readWriteLock = new ReentrantReadWriteLock(); /** * ID */ private String id; /** * 集成redisTemplate */ private static RedisTemplate redisTemplate; public MybatisRedisCache() { } public MybatisRedisCache(String id) { if (id == null) { throw new IllegalArgumentException(&quot;Cache instances require an ID&quot;); } else { logger.debug(&quot;MybatisRedisCache.id={}&quot;, id); this.id = id; } } @Override public String getId() { return this.id; } @Override public int getSize() { try { Long size = redisTemplate.opsForHash().size(this.id.toString()); logger.debug(&quot;MybatisRedisCache.getSize: {}-&gt;{}&quot;, id, size); return size.intValue(); } catch (Exception e) { e.printStackTrace(); } return 0; } @Override public void putObject(final Object key, final Object value) { try { logger.debug(&quot;MybatisRedisCache.putObject: {}-&gt;{}-&gt;{}&quot;, id, key, JsonUtils.toJson(value)); redisTemplate.opsForHash().put(this.id.toString(), key.toString(), value); } catch (Exception e) { e.printStackTrace(); } } @Override public Object getObject(final Object key) { try { Object hashVal = redisTemplate.opsForHash().get(this.id.toString(), key.toString()); logger.debug(&quot;MybatisRedisCache.getObject: {}-&gt;{}-&gt;{}&quot;, id, key, JsonUtils.toJson(hashVal)); return hashVal; } catch (Exception e) { e.printStackTrace(); return null; } } @Override public Object removeObject(final Object key) { try { redisTemplate.opsForHash().delete(this.id.toString(), key.toString()); logger.debug(&quot;MybatisRedisCache.removeObject: {}-&gt;{}-&gt;{}&quot;, id, key); } catch (Exception e) { e.printStackTrace(); } return null; } @Override public void clear() { try { redisTemplate.delete(this.id.toString()); logger.debug(&quot;MybatisRedisCache.clear: {}&quot;, id); } catch (Exception e) { e.printStackTrace(); } } @Override public ReadWriteLock getReadWriteLock() { return this.readWriteLock; } @Override public String toString() { return &quot;MybatisRedisCache {&quot; + this.id + &quot;}&quot;; } /** * 设置redisTemplate * * @param redisTemplate */ public void setRedisTemplate(RedisTemplate redisTemplate) { MybatisRedisCache.redisTemplate = redisTemplate; } } RedisConfig import com.fasterxml.jackson.annotation.JsonAutoDetect; import com.fasterxml.jackson.annotation.PropertyAccessor; import com.fasterxml.jackson.databind.ObjectMapper; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.data.redis.connection.RedisConnectionFactory; import org.springframework.data.redis.core.RedisTemplate; import org.springframework.data.redis.serializer.Jackson2JsonRedisSerializer; import org.springframework.data.redis.serializer.StringRedisSerializer; /** * Redis配置 * */ @Configuration public class RedisConfig { /** * 配置RedisTemplate * * @param factory * @return */ @Bean public RedisTemplate redisTemplate(RedisConnectionFactory factory, Jackson2JsonRedisSerializer redisJsonSerializer) { RedisTemplate&lt;Object, Object&gt; template = new RedisTemplate&lt;&gt;(); //redis连接工厂 template.setConnectionFactory(factory); StringRedisSerializer stringRedisSerializer = new StringRedisSerializer(); //redis.key序列化器 template.setKeySerializer(stringRedisSerializer); //redis.value序列化器 template.setValueSerializer(redisJsonSerializer); //redis.hash.key序列化器 template.setHashKeySerializer(stringRedisSerializer); //redis.hash.value序列化器 template.setHashValueSerializer(redisJsonSerializer); //调用其他初始化逻辑 template.afterPropertiesSet(); //这里设置redis事务一致 template.setEnableTransactionSupport(true); return template; } /** * 配置redis Json序列化器 * * @return */ @Bean public Jackson2JsonRedisSerializer redisJsonSerializer() { //使用Jackson2JsonRedisSerializer来序列化和反序列化redis的value值（默认使用JDK的序列化方式） Jackson2JsonRedisSerializer serializer = new Jackson2JsonRedisSerializer(Object.class); ObjectMapper mapper = new ObjectMapper(); mapper.setVisibility(PropertyAccessor.ALL, JsonAutoDetect.Visibility.ANY); mapper.enableDefaultTyping(ObjectMapper.DefaultTyping.NON_FINAL); serializer.setObjectMapper(mapper); return serializer; } } 开启Mybatis二级缓存设置 方式1：mybatis-config.xml &lt;configuration&gt; &lt;settings&gt; &lt;!-- 开启二级缓存 --&gt; &lt;setting name=&quot;cacheEnabled&quot; value=&quot;true&quot;/&gt; &lt;/settings&gt; ... &lt;/configuration&gt; 方式2：Springboot - application.properties #使全局的映射器启用或禁用缓存。 mybatis.configuration.cache-enabled=true Mapper.xml 配置，type 使用RedisCache： &lt;cache type=&quot;org.mybatis.caches.redis.RedisCache&quot; eviction=&quot;FIFO&quot; flushInterval=&quot;60000&quot; size=&quot;512&quot; readOnly=&quot;true&quot;/&gt; redis.properties 配置： host=localhost port=6379 connectionTimeout=5000 soTimeout=5000 database=0 ","link":"https://tianxiawuhao.github.io/P-yYZmx9j/"},{"title":"springBoot概述","content":"Spring Boot 优点 容易上手，提升开发效率，为 Spring 开发提供一个更快、更广泛的入门体验。 开箱即用，远离繁琐的配置。 提供了一系列大型项目通用的非业务性功能，例如：内嵌服务器、安全管理、运行数据监控、运行状况检查和外部化配置等。 没有代码生成，也不需要XML配置。 避免大量的 Maven 导入和各种版本冲突。 Spring Boot 的核心注解是哪个？它主要由哪几个注解组成的？ 启动类上面的注解是@SpringBootApplication，它也是 Spring Boot 的核心注解，主要组合包含了以下 3 个注解： @SpringBootConfiguration：组合了 @Configuration 注解，实现配置文件的功能。 @EnableAutoConfiguration：打开自动配置的功能，也可以关闭某个自动配置的选项，如关闭数据源自动配置功能： @SpringBootApplication(exclude = { DataSourceAutoConfiguration.class })。 @ComponentScan：Spring组件扫描。 所有其它 Spring 组件(如Controller层、Service层、Dao层、定时器等)都必须放在应用 @SpringBootApplication 注解所在类的同包或者其子包下面，因为应用从启动类开始启动，然后会扫描启动类同包及其子包下面的组件，如果放在其它地方则会因为扫描不到而加载不了 JavaConfig Spring JavaConfig 是 Spring 社区的产品，它提供了配置 Spring IoC 容器的纯Java 方法。因此它有助于避免使用 XML 配置。使用 JavaConfig 的优点在于： （1）面向对象的配置。由于配置被定义为 JavaConfig 中的类，因此用户可以充分利用 Java 中的面向对象功能。一个配置类可以继承另一个，重写它的@Bean 方法等。 （2）减少或消除 XML 配置。基于依赖注入原则的外化配置的好处已被证明。但是，许多开发人员不希望在 XML 和 Java 之间来回切换。JavaConfig 为开发人员提供了一种纯 Java 方法来配置与 XML 配置概念相似的 Spring 容器。从技术角度来讲，只使用 JavaConfig 配置类来配置容器是可行的，但实际上很多人认为将JavaConfig 与 XML 混合匹配是理想的。 （3）类型安全和重构友好。JavaConfig 提供了一种类型安全的方法来配置 Spring容器。由于 Java 5.0 对泛型的支持，现在可以按类型而不是按名称检索 bean，不需要任何强制转换或基于字符串的查找。 /** * @ConfigurationProperties 表示 告诉 SpringBoot 将本类中的所有属性和配置文件中相关的配置进行绑定； * prefix = &quot;user&quot; 表示 将配置文件中 key 为 user 的下面所有的属性与本类属性进行一一映射注入值，如果配置文件中 * 不存在 &quot;user&quot; 的 key，则不会为 POJO 注入值，属性值仍然为默认值 * @Component 将本来标识为一个 Spring 组件，因为只有是容器中的组件，容器才会为 @ConfigurationProperties 提供此注入功能 * @PropertySource (value = { &quot; classpath : user.properties &quot; }) 指明加载类路径下的哪个配置文件来注入值 */ @PropertySource(value = {&quot;classpath:user.properties&quot;}) @Component @ConfigurationProperties(prefix = &quot;user&quot;) public class User { private Integer id; private String lastName; private Integer age; private Date birthday; private List&lt;String&gt; colorList; private Map&lt;String, String&gt; cityMap; } /** * 文中的@Configuration 可以替换为@Component运行结果是一样的，但是两者是有不同的，@Configuration会为配置类生成CGLIB代理Class，@Component不会 */ @PropertySource(value = {&quot;classpath:user.properties&quot;}) @Configuration public class User { @Value(${user.id}) private Integer id; @Value(${user.lastName}) private String lastName; @Value(${user.age}) private Integer age; @Value(${user.birthday}) private Date birthday; @Value(${user.colorList}) private List&lt;String&gt; colorList; @Value(${user.maps}) private Map&lt;String, String&gt; cityMap; } user.properties user.id=111 user.lastName=张无忌 user.age=120 user.birthday=2018/07/11 user.colorList=red,yellow,green,blacnk user.cityMap.mapK1=长沙市 user.cityMap.mapK2=深圳市 user.maps=&quot;{mapK1: '长沙市', mapK2: '深圳市'}&quot; Spring Boot 自动配置 注解 @EnableAutoConfiguration, @Configuration, @ConditionalOnClass 就是自动配置的核心， @EnableAutoConfiguration 给容器导入META-INF/spring.factories 里定义的自动配置类。 筛选有效的自动配置类。 每一个自动配置类结合对应的 xxxProperties.java 读取配置文件进行自动配置功能 Spring Boot 配置加载顺序 Spring Boot 支持多种外部配置方式，如下所示，从上往下加载优先级由高到低，内容相同时覆盖，不相同时累加。 如果在不同的目录中存在多个配置文件，它的读取顺序是： 1、config/application.properties（项目根目录中config目录下） 2、config/application.yml 3、application.properties（项目根目录下） 4、application.yml 5、resources/config/application.properties（项目resources目录中config目录下） 6、resources/config/application.yml 7、resources/application.properties（项目的resources目录下） 8、resources/application.yml YAML 配置 YAML 现在可以算是非常流行的一种配置文件格式了，无论是前端还是后端，都可以见到 YAML 配置。那么 YAML 配置和传统的 properties 配置相比到底有哪些优势呢？ 配置有序，在一些特殊的场景下，配置有序很关键 支持数组，数组中的元素可以是基本数据类型也可以是对象 简洁 相比 properties 配置文件，YAML 还有一个缺点，就是不支持 @PropertySource 注解导入自定义的 YAML 配置。 Spring Boot 是否可以使用 XML 配置 Spring Boot 推荐使用 Java 配置而非 XML 配置，但是 Spring Boot 中也可以使用 XML 配置，通过 @ImportResource 注解可以引入一个 XML 配置。 spring boot 核心配置文件bootstrap.properties和 application.properties 有何区别 单纯做 Spring Boot 开发，可能不太容易遇到 bootstrap.properties 配置文件，但是在结合 Spring Cloud 时，这个配置就会经常遇到了，特别是在需要加载一些远程配置文件的时侯。 spring boot 核心的两个配置文件： bootstrap (. yml 或者 . properties)：boostrap 由父 ApplicationContext 加载的，比 applicaton 优先加载，配置在应用程序上下文的引导阶段生效。一般来说我们在 Spring Cloud Config 或者 Nacos 中会用到它。且 boostrap 里面的属性不能被覆盖； application (. yml 或者 . properties)： 由ApplicatonContext 加载，用于 spring boot 项目的自动化配置。 Spring Profiles spring: profiles: active: devel #指定激活哪个环境配置，激活后，第一个文档内容失效;不指定时，以第一个文档为准 server: port: 8083 --- #&quot;---&quot;用于分隔不同的profiles（）文档块 spring: profiles: devel #指定环境标识为&quot;devel&quot;,相当于&quot;application-{profile}.properties/yml&quot;中的profile server: port: 8081 --- spring: profiles: deploy #指定环境标识为&quot;deploy&quot;,相当于&quot;application-{profile}.properties/yml&quot;中的profile server: port: 8082 比较一下 Spring Security 和 Shiro 各自的优缺点 由于 Spring Boot 官方提供了大量的非常方便的开箱即用的 Starter ，包括 Spring Security 的 Starter ，使得在 Spring Boot 中使用 Spring Security 变得更加容易，甚至只需要添加一个依赖就可以保护所有的接口，所以，如果是 Spring Boot 项目，一般选择 Spring Security 。当然这只是一个建议的组合，单纯从技术上来说，无论怎么组合，都是没有问题的。Shiro 和 Spring Security 相比，主要有如下一些特点： Spring Security 是一个重量级的安全管理框架；Shiro 则是一个轻量级的安全管理框架 Spring Security 概念复杂，配置繁琐；Shiro 概念简单、配置简单 Spring Security 功能强大；Shiro 功能简单 Spring Boot 中如何解决跨域问题 跨域可以在前端通过 JSONP 来解决，但是 JSONP 只可以发送 GET 请求，无法发送其他类型的请求，在 RESTful 风格的应用中，就显得非常鸡肋，因此我们推荐在后端通过 CORS，(Cross-origin resource sharing） 来解决跨域问题。这种解决方案并非 Spring Boot 特有的，在传统的 SSM 框架中，就可以通过 CORS 来解决跨域问题，只不过之前我们是在 XML 文件中配置 CORS ，现在可以通过实现WebMvcConfigurer接口然后重写addCorsMappings方法解决跨域问题。 @Configuration public class CorsConfig implements WebMvcConfigurer { @Override public void addCorsMappings(CorsRegistry registry) { registry.addMapping(&quot;/**&quot;) .allowedOrigins(&quot;*&quot;) .allowCredentials(true) .allowedMethods(&quot;GET&quot;, &quot;POST&quot;, &quot;PUT&quot;, &quot;DELETE&quot;, &quot;OPTIONS&quot;) .maxAge(3600); } } 项目中前后端分离部署，所以需要解决跨域的问题。 我们使用cookie存放用户登录的信息，在spring拦截器进行权限控制，当权限不符合时，直接返回给用户固定的json结果。 当用户登录以后，正常使用；当用户退出登录状态时或者token过期时，由于拦截器和跨域的顺序有问题，出现了跨域的现象。 我们知道一个http请求，先走filter，到达servlet后才进行拦截器的处理，如果我们把cors放在filter里，就可以优先于权限拦截器执行。 @Configuration public class CorsConfig { @Bean public CorsFilter corsFilter() { CorsConfiguration corsConfiguration = new CorsConfiguration(); corsConfiguration.addAllowedOrigin(&quot;*&quot;); corsConfiguration.addAllowedHeader(&quot;*&quot;); corsConfiguration.addAllowedMethod(&quot;*&quot;); corsConfiguration.setAllowCredentials(true); UrlBasedCorsConfigurationSource urlBasedCorsConfigurationSource = new UrlBasedCorsConfigurationSource(); urlBasedCorsConfigurationSource.registerCorsConfiguration(&quot;/**&quot;, corsConfiguration); return new CorsFilter(urlBasedCorsConfigurationSource); } } Spring Boot 中的监视器 Spring boot actuator 是 spring 启动框架中的重要功能之一。Spring boot 监视器可帮助您访问生产环境中正在运行的应用程序的当前状态。有几个指标必须在生产环境中进行检查和监控。即使一些外部应用程序可能正在使用这些服务来向相关人员触发警报消息。监视器模块公开了一组可直接作为 HTTP URL 访问的REST 端点来检查状态。 注册 Servlet 三大组件 Servlet、Filter、Listener 继承，接口实现方式 ServletRegistrationBean 注册 Servlet 1、自定义类继承 javax.servlet.http.HttpServlet，然后重写其 doGet 与 doPost 方法，在方法中编写控制代码； 2、第二步将 ServletRegistrationBean 组件添加到 Spring 容器中 import javax.servlet.ServletException; import javax.servlet.http.HttpServlet; import javax.servlet.http.HttpServletRequest; import javax.servlet.http.HttpServletResponse; import java.io.IOException; /** * Created by Administrator * 标准的 Servlet 实现 HttpServlet；重写其 doGet 、doPost 方法 */ public class BookServlet extends HttpServlet { @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { this.doPost(req, resp); } @Override protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { System.out.println(&quot;:com.lct.servlet.BookServlet:&quot; + req.getRequestURL()); /**讲求转发到后台的 user/users 请求去，即会进入*/ req.getRequestDispatcher(&quot;user/users&quot;).forward(req, resp); } } 3、上面 Serlvet 转发到下面的 UserControllr 控制器中 4、@Configuration 配置类相当于以前的 beans.xml 中的配置，将 ServletRegistrationBean 也添加到 Spring 容器中来 import com.lct.component.MyLocaleResolve; import com.lct.servlet.BookServlet; import org.springframework.boot.web.server.WebServerFactoryCustomizer; import org.springframework.boot.web.servlet.ServletRegistrationBean; import org.springframework.boot.web.servlet.server.ConfigurableServletWebServerFactory; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.web.servlet.LocaleResolver; /** * Created by Administrator * 自定义配置类 */ @Configuration public class MyMvcConfig { /** * 注册 Servlet 三大组件 之 Servlet * 添加 ServletRegistrationBean ，就相当于以前在 web.xml 中配置的 &lt;servlet&gt;&lt;/servlet&gt;标签 */ @Bean public ServletRegistrationBean myServlet() { /**第二个参数是个不定参数组，可以配置映射多个请求 * 相当于以前在 web.xml中配置的 &lt;servlet-mapptin&gt;&lt;/servlet-mapptin&gt;*/ ServletRegistrationBean registrationBean = new ServletRegistrationBean(new BookServlet(), &quot;/bookServlet&quot;); return registrationBean; } } 5、运行测试： FilterRegistrationBean 注册 Filter 1、Filter(过滤器) 是 Servlet 技术中最实用的技术之一 2、Web 开发人员通过 Filter 技术，对 web 服务器管理的所有 web 资源(如动态的 Jsp、 Servlet，以及静态的 image、 html、CSS、JS 文件等) 进行过滤拦截，从而实现一些特殊的功能(如实现 URL 级别的权限访问控制、过滤敏感词汇、压缩响应信息等) 3、Filter 主要用于对用户请求进行预处理，也可以对 HttpServletResponse 进行后期处理(如编码设置，返回时禁用浏览器缓存等) 4、Filter 使用完整流程：Filter 对用户请求进行预处理，接着将请求交给 Servlet 进行处理并生成响应，最后 Filter 再对服务器响应进行后处理。 5、Servlet 的 Filter 经常会拿来与 Spring MVC 的 Interceptor(拦截器) 做对比 ​ 1）拦截器是基于 Java 的反射机制的，而过滤器是基于函数回调 ​ 2）拦截器不依赖与 servle t容器，过滤器依赖与 servlet 容器 ​ 3）拦截器可以访问 action 上下文、值栈里的对象，而过滤器不能访问 ​ 4）在 action 的生命周期中，拦截器可以多次被调用，而过滤器只能在容器初始化时被调用一次 ​ 5）拦截器可以获取 IOC 容器中的各个 bean，而过滤器就不行，这点很重要，在拦截器里注入一个 service，可以调用业务逻辑 ​ 6）SpringMVC 有自己的拦截器 import javax.servlet.*; import javax.servlet.http.HttpServletRequest; import java.io.IOException; /** * Created by Administrator * 标准 Servlet 过滤器，实现 javax.servlet.Filter 接口 * 并重写它的 三个方法 */ public class SystemFilter implements Filter { @Override public void init(FilterConfig filterConfig) throws ServletException { System.out.println(&quot;javax.servlet.Filter：：服务器启动....&quot;); } @Override public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException { /** * 转为 HttpServletRequest 输出请求路径 容易查看 请求地址 */ HttpServletRequest request = (HttpServletRequest) servletRequest; System.out.println(&quot;javax.servlet.Filter：：过滤器放行前....&quot; + request.getRequestURL()); filterChain.doFilter(servletRequest, servletResponse); System.out.println(&quot;javax.servlet.Filter：：过滤器返回后....&quot; + request.getRequestURL()); } @Override public void destroy() { System.out.println(&quot;javax.servlet.Filter：：服务器关闭....&quot;); } } 6、使用 FilterRegistrationBean 添加 FIlter ： import com.lct.component.MyLocaleResolve; import com.lct.filter.SystemFilter; import com.lct.servlet.BookServlet; import org.springframework.boot.web.server.WebServerFactoryCustomizer; import org.springframework.boot.web.servlet.FilterRegistrationBean; import org.springframework.boot.web.servlet.ServletRegistrationBean; import org.springframework.boot.web.servlet.server.ConfigurableServletWebServerFactory; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.web.servlet.LocaleResolver; import javax.servlet.DispatcherType; import java.util.Arrays; /** * Created by Administrator * 自定义配置类 */ @Configuration public class MyMvcConfig { /** * 注册 Servlet 三大组件 之 Filter (过滤器) * 添加 FilterRegistrationBean ，就相当于以前在 web.xml 中配置的 &lt;filter&gt;&lt;/filter&gt; 标签 */ @Bean public FilterRegistrationBean myFilter() { FilterRegistrationBean registrationBean = new FilterRegistrationBean(); /**同样添加自定义的 Filter*/ registrationBean.setFilter(new SystemFilter()); /**然后设置过滤的路径，参数是个集合 ,相当于 web.xml中配置的 &lt;filter-mapptin&gt;&lt;/filter-mapptin&gt; * &quot;/*&quot;: 表示过滤所有 get 与 post 请求*/ registrationBean.setUrlPatterns(Arrays.asList(&quot;/*&quot;)); /** * setDispatcherTypes 相当于 web.xml 配置中 &lt;filter-mapptin&gt; 下的 &lt;dispatcher&gt; 标签 * 用于过滤非常规的 get 、post 请求 * REQUEST：默认方式，写了之后会过滤所有静态资源的请求 * FORWARD：过滤所有的转发请求，无论是 jsp 中的 &lt;jsp:forward&lt;/&gt;、&lt;%@ page errorPage= %&gt;、还是后台的转发 * INCLUDE：过滤 jsp 中的动态包含&lt;jsp:include 请求 * ERROR：过滤在 web.xml 配置的全局错误页面 * 了解即可，实际中也很少这么做 */ registrationBean.setDispatcherTypes(DispatcherType.REQUEST); return registrationBean; } } ServletListenerRegistrationBean 注册 Listener 1、自定义监听器： import javax.servlet.ServletContextEvent; import javax.servlet.ServletContextListener; /** * Created by Administrator on 2018/8/11 0011. * 标准 Servlet 监听器，实现 javax.servlet.ServletContextListener 接口 * 然后实现方法 * ServletContextListener：属于 Servlet 应用启动关闭监听器，监听容器初始化与销毁 */ public class SystemListener implements ServletContextListener { @Override public void contextInitialized(ServletContextEvent servletContextEvent) { System.out.println(&quot;com.lct.listener.SystemListener::服务器启动.....&quot;); } @Override public void contextDestroyed(ServletContextEvent servletContextEvent) { System.out.println(&quot;com.lct.listener.SystemListener::服务器关闭.....&quot;); } } 2、注册 ServletListenerRegistrationBean： import com.lct.component.MyLocaleResolve; import com.lct.filter.SystemFilter; import com.lct.listener.SystemListener; import com.lct.servlet.BookServlet; import org.springframework.boot.web.server.WebServerFactoryCustomizer; import org.springframework.boot.web.servlet.FilterRegistrationBean; import org.springframework.boot.web.servlet.ServletListenerRegistrationBean; import org.springframework.boot.web.servlet.ServletRegistrationBean; import org.springframework.boot.web.servlet.server.ConfigurableServletWebServerFactory; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.web.servlet.LocaleResolver; import java.util.Arrays; /** * Created by Administrator * 自定义配置类 */ @Configuration public class MyMvcConfig { /** * 注册 Servlet 三大组件 之 Listner * 添加 ServletListenerRegistrationBean ，就相当于以前在 web.xml 中配置的 &lt;listener&gt;&lt;/listener&gt;标签 */ @Bean public ServletListenerRegistrationBean myListener() { /**ServletListenerRegistrationBean&lt;T extends EventListener&gt; 属于的是泛型，可以注册常见的任意监听器 * 将自己的监听器注册进来*/ ServletListenerRegistrationBean registrationBean = new ServletListenerRegistrationBean(new SystemListener()); return registrationBean; } } 注解方式 1、Servlet 三大组件 Servlet、Filter、Listener 在传统项目中需要在 web.xml 中进行相应的配置。Servlet 3.0 开始在 javax.servlet.annotation 包下提供 3 个对应 的 @WebServlet、@WebFilter、@WebListener 注解来简化操作。 2、Spring Boot 应用中这三个注解默认是不被扫描的，需要在项目启动类上添加 @ServletComponentScan 注解, 表示对 Servlet 组件扫描。 @WebServlet import javax.servlet.ServletException; import javax.servlet.annotation.WebServlet; import javax.servlet.http.HttpServlet; import javax.servlet.http.HttpServletRequest; import javax.servlet.http.HttpServletResponse; import java.io.IOException; /** * 标准的 Servlet ，实现 javax.servlet.http.HttpServlet. 重写其 doGet 、doPost 方法 * name :表示 servlet 名称，可以不写，默认为空 * urlPatterns: 表示请求的路径，如 http://ip:port/context-path/userServlet */ @WebServlet(name = &quot;UserServlet&quot;, urlPatterns = {&quot;/userServlet&quot;}) public class UserServlet extends HttpServlet { @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { this.doPost(req, resp); } @Override protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { StringBuffer requestURL = req.getRequestURL(); System.out.println(&quot;com.wmx.servlet.UserServlet -- &quot; + requestURL); resp.sendRedirect(&quot;/index.html&quot;);//浏览器重定向到服务器下的 index.html 页面 } } @WebFilter import javax.servlet.*; import javax.servlet.annotation.WebFilter; import javax.servlet.http.HttpServletRequest; import java.io.IOException; /** * 标准 Servlet 过滤器，实现 javax.servlet.Filter 接口，并重现它的 3 个方法 * filterName：表示过滤器名称，可以不写 * value：配置请求过滤的规则，如 &quot;/*&quot; 表示过滤所有请求，包括静态资源，如 &quot;/user/*&quot; 表示 /user 开头的所有请求 */ @WebFilter(filterName = &quot;SystemFilter&quot;, value = {&quot;/*&quot;}) public class SystemFilter implements Filter { @Override public void init(FilterConfig filterConfig) throws ServletException { System.out.println(&quot;com.wmx.servlet.SystemFilter -- 系统启动...&quot;); } @Override public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException { //转为 HttpServletRequest 输出请求路径 HttpServletRequest request = (HttpServletRequest) servletRequest; System.out.println(&quot;com.wmx.servlet.SystemFilter -- 过滤器放行前....&quot; + request.getRequestURL()); filterChain.doFilter(servletRequest, servletResponse); System.out.println(&quot;com.wmx.servlet.SystemFilter -- 过滤器返回后....&quot; + request.getRequestURL()); } @Override public void destroy() { System.out.println(&quot;com.wmx.servlet.SystemFilter -- 系统关闭...&quot;); } } @WebListener import javax.servlet.ServletContextEvent; import javax.servlet.ServletContextListener; import javax.servlet.annotation.WebListener; /** * 标准 Servlet 监听器，实现 javax.servlet.ServletContextListener 接口，并重写方法 * ServletContextListener 属于 Servlet 应用启动关闭监听器，监听容器初始化与销毁。常用的监听器还有： * ServletRequestListener：HttpServletRequest 对象的创建和销毁监听器 * HttpSessionListener：HttpSession 数据对象创建和销毁监听器 * HttpSessionAttributeListener 监听HttpSession中属性变化 * ServletRequestAttributeListener 监听ServletRequest中属性变化 */ @WebListener public class SystemListener implements ServletContextListener { @Override public void contextInitialized(ServletContextEvent sce) { System.out.println(&quot;com.wmx.servlet.SystemListener -- 服务器启动.&quot;); } @Override public void contextDestroyed(ServletContextEvent sce) { System.out.println(&quot;com.wmx.servlet.SystemListener -- 服务器关闭.&quot;); } } @ServletComponentScan Spring Boot 应用中这三个注解默认是不被扫描的，需要在项目启动类上添加 @ServletComponentScan 注解, 表示对 Servlet 组件扫描。 import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; import org.springframework.boot.web.servlet.ServletComponentScan; @SpringBootApplication @ServletComponentScan //对 servlet 注解进行扫描 public class RedisStuWebApplication { public static void main(String[] args) { SpringApplication.run(RedisStuWebApplication.class, args); } } ","link":"https://tianxiawuhao.github.io/O58auJO44/"},{"title":"SpringMVC工作原理","content":"SpringMVC的工作原理图： SpringMVC流程 1、 用户发送请求至前端控制器DispatcherServlet。 2、 DispatcherServlet收到请求调用HandlerMapping处理器映射器。 3、 处理器映射器找到具体的处理器(可以根据xml配置、注解进行查找)，生成处理器对象及处理器拦截器(如果有则生成)一并返回给DispatcherServlet。 4、 DispatcherServlet调用HandlerAdapter处理器适配器。 5、 HandlerAdapter经过适配调用具体的处理器(Controller，也叫后端控制器)。 6、 Controller执行完成返回ModelAndView。 7、 HandlerAdapter将controller执行结果ModelAndView返回给DispatcherServlet。 8、 DispatcherServlet将ModelAndView传给ViewReslover视图解析器。 9、 ViewReslover解析后返回具体View。 10、DispatcherServlet根据View进行渲染视图（即将模型数据填充至视图中）。 11、 DispatcherServlet响应用户。 组件说明： 以下组件通常使用框架提供实现： DispatcherServlet：作为前端控制器，整个流程控制的中心，控制其它组件执行，统一调度，降低组件之间的耦合性，提高每个组件的扩展性。 HandlerMapping：通过扩展处理器映射器实现不同的映射方式，例如：配置文件方式，实现接口方式，注解方式等。 HandlAdapter：通过扩展处理器适配器，支持更多类型的处理器。 ViewResolver：通过扩展视图解析器，支持更多类型的视图解析，例如：jsp、freemarker、pdf、excel等。 组件： 1、前端控制器DispatcherServlet（不需要工程师开发）,由框架提供 作用：接收请求，响应结果，相当于转发器，中央处理器。有了dispatcherServlet减少了其它组件之间的耦合度。 用户请求到达前端控制器，它就相当于mvc模式中的c，dispatcherServlet是整个流程控制的中心，由它调用其它组件处理用户的请求，dispatcherServlet的存在降低了组件之间的耦合性。 2、处理器映射器HandlerMapping(不需要工程师开发),由框架提供 作用：根据请求的url查找Handler HandlerMapping负责根据用户请求找到Handler即处理器，springmvc提供了不同的映射器实现不同的映射方式，例如：配置文件方式，实现接口方式，注解方式等。 3、处理器适配器HandlerAdapter 作用：按照特定规则（HandlerAdapter要求的规则）去执行Handler 通过HandlerAdapter对处理器进行执行，这是适配器模式的应用，通过扩展适配器可以对更多类型的处理器进行执行。 4、处理器Handler(需要工程师开发) 注意：编写Handler时按照HandlerAdapter的要求去做，这样适配器才可以去正确执行Handler Handler 是继DispatcherServlet前端控制器的后端控制器，在DispatcherServlet的控制下Handler对具体的用户请求进行处理。 由于Handler涉及到具体的用户业务请求，所以一般情况需要工程师根据业务需求开发Handler。 5、视图解析器View resolver(不需要工程师开发),由框架提供 作用：进行视图解析，根据逻辑视图名解析成真正的视图（view） View Resolver负责将处理结果生成View视图，View Resolver首先根据逻辑视图名解析成物理视图名即具体的页面地址，再生成View视图对象，最后对View进行渲染将处理结果通过页面展示给用户。 springmvc框架提供了很多的View视图类型，包括：jstlView、freemarkerView、pdfView等。 一般情况下需要通过页面标签或页面模版技术将模型数据通过页面展示给用户，需要由工程师根据业务需求开发具体的页面。 6、视图View(需要工程师开发jsp...) View是一个接口，实现类支持不同的View类型（jsp、freemarker、pdf...） 核心架构的具体流程步骤如下： 1、首先用户发送请求——&gt;DispatcherServlet，前端控制器收到请求后自己不进行处理，而是委托给其他的解析器进行处理，作为统一访问点，进行全局的流程控制； 2、DispatcherServlet——&gt;HandlerMapping， HandlerMapping 将会把请求映射为HandlerExecutionChain 对象（包含一个Handler 处理器（页面控制器）对象、多个HandlerInterceptor 拦截器）对象，通过这种策略模式，很容易添加新的映射策略； 3、DispatcherServlet——&gt;HandlerAdapter，HandlerAdapter 将会把处理器包装为适配器，从而支持多种类型的处理器，即适配器设计模式的应用，从而很容易支持很多类型的处理器； 4、HandlerAdapter——&gt;处理器功能处理方法的调用，HandlerAdapter 将会根据适配的结果调用真正的处理器的功能处理方法，完成功能处理；并返回一个ModelAndView 对象（包含模型数据、逻辑视图名）； 5、ModelAndView的逻辑视图名——&gt; ViewResolver， ViewResolver 将把逻辑视图名解析为具体的View，通过这种策略模式，很容易更换其他视图技术； 6、View——&gt;渲染，View会根据传进来的Model模型数据进行渲染，此处的Model实际是一个Map数据结构，因此很容易支持其他视图技术； 7、返回控制权给DispatcherServlet，由DispatcherServlet返回响应给用户，到此一个流程结束。 下边两个组件通常情况下需要开发： Handler：处理器，即后端控制器用controller表示。 View：视图，即展示给用户的界面，视图中通常需要标签语言展示模型数据。 在讲SpringMVC之前我们先来看一下什么是MVC模式 MVC：MVC是一种设计模式 MVC的原理图： 分析： M-Model 模型（完成业务逻辑：有javaBean构成，service+dao+entity） V-View 视图（做界面的展示 jsp，html……） C-Controller 控制器（接收请求—&gt;调用模型—&gt;根据结果派发页面） springMVC是什么： springMVC是一个MVC的开源框架，springMVC=struts2+spring，springMVC就相当于是Struts2加上sring的整合，但是这里有一个疑惑就是，springMVC和spring是什么样的关系呢？这个在百度百科上有一个很好的解释：意思是说，springMVC是spring的一个后续产品，其实就是spring在原有基础上，又提供了web应用的MVC模块，可以简单的把springMVC理解为是spring的一个模块（类似AOP，IOC这样的模块），网络上经常会说springMVC和spring无缝集成，其实springMVC就是spring的一个子模块，所以根本不需要同spring进行整合。 SpringMVC的原理图： 看到这个图大家可能会有很多的疑惑，现在我们来看一下这个图的步骤：（可以对比MVC的原理图进行理解） 第一步:用户发起请求到前端控制器（DispatcherServlet） 第二步：前端控制器请求处理器映射器（HandlerMappering）去查找处理器（Handle）：通过xml配置或者注解进行查找 第三步：找到以后处理器映射器（HandlerMappering）像前端控制器返回执行链（HandlerExecutionChain） 第四步：前端控制器（DispatcherServlet）调用处理器适配器（HandlerAdapter）去执行处理器（Handler） 第五步：处理器适配器去执行Handler 第六步：Handler执行完给处理器适配器返回ModelAndView 第七步：处理器适配器向前端控制器返回ModelAndView 第八步：前端控制器请求视图解析器（ViewResolver）去进行视图解析 第九步：视图解析器像前端控制器返回View 第十步：前端控制器对视图进行渲染 第十一步：前端控制器向用户响应结果 看到这些步骤我相信大家很感觉非常的乱，这是正常的，但是这里主要是要大家理解springMVC中的几个组件： 前端控制器（DispatcherServlet）：接收请求，响应结果，相当于电脑的CPU。 处理器映射器（HandlerMapping）：根据URL去查找处理器 处理器（Handler）：（需要程序员去写代码处理逻辑的） 处理器适配器（HandlerAdapter）：会把处理器包装成适配器，这样就可以支持多种类型的处理器，类比笔记本的适配器（适配器模式的应用） 视图解析器（ViewResovler）：进行视图解析，多返回的字符串，进行处理，可以解析成对应的页面 ","link":"https://tianxiawuhao.github.io/7cW_XVJ41/"},{"title":"elasticSearch基础概念汇总","content":"1.什么是ElasticSearch？ Elasticsearch是一个基于Lucene的搜索引擎。它提供了具有HTTP Web界面和无架构JSON文档的分布式，多租户能力的全文搜索引擎。Elasticsearch是用Java开发的，根据Apache许可条款作为开源发布。它可以用于全文搜索，结构化搜索以及分析，当然你也可以将这三者进行组合。 2.为什么要使用Elasticsearch? 用数据库，也可以实现搜索的功能，为什么还需要搜索引擎呢？ 就像 Stackoverflow 的网友说的： A relational database can store data and also index it. A search engine can index data but also store it. 数据库（理论上来讲，ES 也是数据库，这里的数据库，指的是关系型数据库），首先是存储，搜索只是顺便提供的功能， 而搜索引擎，首先是搜索，但是不把数据存下来就搜不了，所以只好存一存。 术业有专攻，专攻搜索的搜索引擎，自然会提供更强大的搜索能力。。 Elasticsearch是分布式的。不需要其他组件，分发是实时的，被叫做”Push replication”。 Elasticsearch 完全支持 Apache Lucene 的接近实时的搜索。 处理多租户（multitenancy）不需要特殊配置，而Solr则需要更多的高级设置。 Elasticsearch 采用 Gateway 的概念，使得完备份更加简单。 各节点组成对等的网络结构，某些节点出现故障时会自动分配其他节点代替其进行工作。 3.Elasticsearch是如何实现Master选举的？ Elasticsearch的选主是ZenDiscovery模块负责的，主要包含Ping（节点之间通过这个RPC来发现彼此）和Unicast（单播模块包含一个主机列表以控制哪些节点需要ping通）这两部分； 对所有可以成为master的节点（node.master: true）根据nodeId字典排序，每次选举每个节点都把自己所知道节点排一次序，然后选出第一个（第0位）节点，暂且认为它是master节点。 如果对某个节点的投票数达到一定的值（可以成为master节点数n/2+1）并且该节点自己也选举自己，那这个节点就是master。否则重新选举一直到满足上述条件。 补充：master节点的职责主要包括集群、节点和索引的管理，不负责文档级别的管理；data节点可以关闭http功能。 4.Elasticsearch中如何避免脑裂？ 为了避免产生脑裂，ES采用了常见的分布式系统思路，保证选举出的master被多数派(quorum)的master-eligible node认可，以此来保证只有一个master。这个quorum通过以下配置进行配置： conf/elasticsearch.yml: discovery.zen.minimum_master_nodes: 2 5.Elasticsearch中的倒排索引 为什么叫倒排索引 在没有搜索引擎时，我们是直接输入一个网址，然后获取网站内容，这时我们的行为是：document -&gt; to -&gt; words 通过文章，获取里面的单词，此谓「正向索引」，forward index.后来，我们希望能够输入一个单词，找到含有这个单词，或者和这个单词有关系的文章：word -&gt; to -&gt; documents于是我们把这种索引，成为inverted index，直译过来，应该叫「反向索引」，国内翻译成「倒排索引」。 倒排索引的内部结构 首先，在数据生成的时候，比如爬虫爬到一篇文章，这时我们需要对这篇文章进行分析，将文本拆解成一个个单词。 这个过程很复杂，比如“生存还是死亡”，你要如何让分词器自动将它分解为“生存”、“还是”、“死亡”三个词语，然后把“还是”这个无意义的词语干掉。这里不展开，感兴趣的同学可以查看关于「分析器」的内容。 接着，把这两个词语以及它对应的文档id存下来： word documentId 生存 1 死亡 1 接着爬虫继续爬，又爬到一个含有“生存”的文档，于是索引变成： word documentId 生存 1，2 死亡 1 下次搜索“生存”，就会返回文档ID是 1、2两份文档。然而上面这套索引的实现，给小孩子当玩具玩还行，要上生产环境，那还远着。想想看，这个世界上那么多单词，中文、英文、日文、韩文 … 你每次搜索一个单词，我都要全局遍历一遍，很明显不行。于是有了排序，我们需要对单词进行排序，像 B+ 树一样，可以在页里实现二分查找。光排序还不行，你单词都放在磁盘呢，磁盘 IO 慢的不得了，所以 Mysql 特意把索引缓存到了内存。你说好，我也学 Mysql 的，放内存，3，2，1，放，哐当，内存爆了。哪本字典，会把所有单词都贴在目录里的？ 所以，上图： Lucene 的倒排索，增加了最左边的一层「字典树」term index，它不存储所有的单词，只存储单词前缀，通过字典树找到单词所在的块，也就是单词的大概位置，再在块里二分查找，找到对应的单词，再找到单词对应的文档列表。 当然，内存寸土寸金，能省则省，所以 Lucene 还用了 FST（Finite State Transducers）对它进一步压缩。 最右边的 Posting List ，别看它只是存一个文档 ID 数组，但是它在设计时，遇到的问题可不少。 Frame Of Reference 原生的 Posting List 有两个痛点： 如何压缩以节省磁盘空间 如何快速求交并集（intersections and unions） 先来聊聊压缩。我们来简化下 Lucene 要面对的问题，假设有这样一个数组：[73, 300, 302, 332, 343, 372] Lucene 里，数据是按 Segment 存储的，每个 Segment 最多存 65536 个文档 ID， 所以文档 ID 的范围，从 0 到 2^16-1，所以如果不进行任何处理，那么每个元素都会占用 2 bytes ，对应上面的数组，就是 6 * 2 = 12 bytes. 怎么压缩呢？ 压缩，就是尽可能降低每个数据占用的空间，同时又能让信息不失真，能够还原回来。 Step 1：Delta-encode —— 增量编码 我们只记录元素与元素之间的增量，于是数组变成了：[73, 227, 2, 30, 11, 29] Step 2：Split into blocks —— 分割成块 Lucene里每个块是 256 个文档 ID，这样可以保证每个块，增量编码后，每个元素都不会超过 256（1 byte）.为了方便演示，我们假设每个块是 3 个文档 ID： [73, 227, 2], [30, 11, 29] Step 3：Bit packing —— 按需分配空间 对于第一个块，[73, 227, 2]，最大元素是227，需要 8 bits，好，那我给你这个块的每个元素，都分配 8 bits的空间。但是对于第二个块，[30, 11, 29]，最大的元素才30，只需要 5 bits，那我就给你每个元素，只分配 5 bits 的空间，足矣。这一步，可以说是把吝啬发挥到极致，精打细算，按需分配。 以上三个步骤，共同组成了一项编码技术，Frame Of Reference（FOR）： Roaring bitmaps 接着来聊聊 Posting List 的第二个痛点 —— 如何快速求交并集（intersections and unions）。 在 Lucene 中查询，通常不只有一个查询条件，比如我们想搜索： 含有“生存”相关词语的文档 文档发布时间在最近一个月 文档发布者是平台的特约作者 这样就需要根据三个字段，去三棵倒排索引里去查，当然，磁盘里的数据，上一节提到过，用了 FOR 进行压缩，所以我们要把数据进行反向处理，即解压，才能还原成原始的文档 ID，然后把这三个文档 ID 数组在内存中做一个交集。 即使没有多条件查询， Lucene 也需要频繁求并集，因为 Lucene 是分片存储的。 同样，我们把 Lucene 遇到的问题，简化成一道算法题。 假设有下面三个数组： [64, 300, 303, 343] [73, 300, 302, 303, 343, 372] [303, 311, 333, 343] 求它们的交集。 Option 1: Integer 数组 直接用原始的文档 ID ，可能你会说，那就逐个数组遍历一遍吧，遍历完就知道交集是什么了。 其实对于有序的数组，用跳表（skip table）可以更高效，这里就不展开了，因为不管是从性能，还是空间上考虑，Integer 数组都不靠谱，假设有100M 个文档 ID，每个文档 ID 占 2 bytes，那已经是 200 MB，而这些数据是要放到内存中进行处理的，把这么大量的数据，从磁盘解压后丢到内存，内存肯定撑不住。 Option 2: Bitmap 假设有这样一个数组：[3,6,7,10] 那么我们可以这样来表示：[0,0,1,0,0,1,1,0,0,1] 看出来了么，对，我们用 0 表示角标对应的数字不存在，用 1 表示存在。 这样带来了两个好处： 节省空间：既然我们只需要0和1，那每个文档 ID 就只需要 1 bit，还是假设有 100M 个文档，那只需要 100M bits = 100M * 1/8 bytes = 12.5 MB，比之前用 Integer 数组 的 200 MB，优秀太多 运算更快：0 和 1，天然就适合进行位运算，求交集，「与」一下，求并集，「或」一下，一切都回归到计算机的起点 Option 3: Roaring Bitmaps 细心的你可能发现了，bitmap 有个硬伤，就是不管你有多少个文档，你占用的空间都是一样的，之前说过，Lucene Posting List 的每个 Segement 最多放 65536 个文档ID，举一个极端的例子，有一个数组，里面只有两个文档 ID：[0, 65535]用 Bitmap，要怎么表示？[1,0,0,0,….(超级多个0),…,0,0,1] 你需要 65536 个 bit，也就是 65536/8 = 8192 bytes，而用 Integer 数组，你只需要 2 * 2 bytes = 4 bytes 呵呵，死板的 bitmap。可见在文档数量不多的时候，使用 Integer 数组更加节省内存。 我们来算一下临界值，很简单，无论文档数量多少，bitmap都需要 8192 bytes，而 Integer 数组则和文档数量成线性相关，每个文档 ID 占 2 bytes，所以：8192 / 2 = 4096当文档数量少于 4096 时，用 Integer 数组，否则，用 bitmap. 这里补充一下 Roaring bitmaps 和 之前讲的 Frame Of Reference 的关系。 Frame Of Reference 是压缩数据，减少磁盘占用空间，所以当我们从磁盘取数据时，也需要一个反向的过程，即解压，解压后才有我们上面看到的这样子的文档ID数组：[73, 300, 302, 303, 343, 372] ，接着我们需要对数据进行处理，求交集或者并集，这时候数据是需要放到内存进行处理的，我们有三个这样的数组，这些数组可能很大，而内存空间比磁盘还宝贵，于是需要更强有力的压缩算法，同时还要有利于快速的求交并集，于是有了Roaring Bitmaps 算法。 另外，Lucene 还会把从磁盘取出来的数据，通过 Roaring bitmaps 处理后，缓存到内存中，Lucene 称之为 filter cache. 6.ElasticSearch中的集群、节点、索引、文档、类型是什么？ 群集 是一个或多个节点（服务器）的集合，它们共同保存您的整个数据，并提供跨所有节点的联合索引和搜索功能。群集由唯一名称标识，默认情况下为“elasticsearch”。此名称很重要，因为如果节点设置为按名称加入群集，则该节点只能是群集的一部分。 节点 是属于集群一部分的单个服务器。它存储数据并参与群集索引和搜索功能。 索引 就像关系数据库中的“数据库”。它有一个定义多种类型的映射。索引是逻辑名称空间，映射到一个或多个主分片，并且可以有零个或多个副本分片。 MySQL =&gt;数据库 ElasticSearch =&gt;索引 文档 类似于关系数据库中的一行。不同之处在于索引中的每个文档可以具有不同的结构（字段），但是对于通用字段应该具有相同的数据类型。 MySQL =&gt; Databases =&gt;Tables =&gt; Columns / Rows ElasticSearch =&gt; Indices =&gt; Types =&gt;具有属性的文档 类型 是索引的逻辑类别/分区，其语义完全取决于用户。 7.ElasticSearch中的分片是什么? 在大多数环境中，每个节点都在单独的盒子或虚拟机上运行。 索引 - 在Elasticsearch中，索引是文档的集合。 分片 -因为Elasticsearch是一个分布式搜索引擎，所以索引通常被分割成分布在多个节点上的被称为分片的元素。 Segment -每个shard（分片）包含多个segment（段），每一个segment都是一个倒排索引 在查询的时，会把所有的segment查询结果汇总归并后最为最终的分片查询结果返回 1.segment是不可变的，物理上你并不能从中删除信息，所以在删除文档的时候，是在文档上面打上一个删除的标记，然后在执行段合并的时候，进行删除 2.索引segment段的个数越多，搜索性能越低且消耗内存更多 ​ 副本 -一个索引被分解成碎片以便于分发和扩展。副本是分片的副本。 ​ 分析器 -在ElasticSearch中索引数据时，数据由为索引定义的Analyzer在内部进行转换。 分析器由一个Tokenizer和零个或多个TokenFilter组成。编译器可以在一个或多个CharFilter之前。分析模块允许您在逻辑名称下注册分析器，然后可以在映射定义或某些API中引用它们。Elasticsearch附带了许多可以随时使用的预建分析器。或者，您可以组合内置的字符过滤器，编译器和过滤器器来创建自定义分析器。 ​ 编译器 -编译器用于将字符串分解为术语或标记流。一个简单的编译器可能会将字符串拆分为任何遇到空格或标点的地方。Elasticsearch有许多内置标记器，可用于构建自定义分析器。 8.详细描述一下Elasticsearch索引文档的过程。 协调节点默认使用文档ID参与计算（也支持通过routing），以便为路由提供合适的分片。 shard = hash(document_id) % (num_of_primary_shards) 当分片所在的节点接收到来自协调节点的请求后，会将请求写入到Memory Buffer，然后定时（默认是每隔1秒）写入到Filesystem Cache，这个从Momery Buffer到Filesystem Cache的过程就叫做refresh； 当然在某些情况下，存在Momery Buffer和Filesystem Cache的数据可能会丢失，ES是通过translog的机制来保证数据的可靠性的。其实现机制是接收到请求后，同时也会写入到translog中，当Filesystem cache中的数据写入到磁盘中时，才会清除掉，这个过程叫做flush； 在flush过程中，内存中的缓冲将被清除，内容被写入一个新段，段的fsync将创建一个新的提交点，并将内容刷新到磁盘，旧的translog将被删除并开始一个新的translog。 flush触发的时机是定时触发（默认30分钟）或者translog变得太大（默认为512M）时； 补充：关于Lucene的Segement： Lucene索引是由多个段组成，段本身是一个功能齐全的倒排索引。 段是不可变的，允许Lucene将新的文档增量地添加到索引中，而不用从头重建索引。 对于每一个搜索请求而言，索引中的所有段都会被搜索，并且每个段会消耗CPU的时钟周、文件句柄和内存。这意味着段的数量越多，搜索性能会越低。 为了解决这个问题，Elasticsearch会合并小段到一个较大的段，提交新的合并段到磁盘，并删除那些旧的小段。 9.详细描述一下Elasticsearch更新和删除文档的过程 删除和更新也都是写操作，但是Elasticsearch中的文档是不可变的，因此不能被删除或者改动以展示其变更； 磁盘上的每个段都有一个相应的.del文件。当删除请求发送后，文档并没有真的被删除，而是在.del文件中被标记为删除。该文档依然能匹配查询，但是会在结果中被过滤掉。当段合并时，在.del文件中被标记为删除的文档将不会被写入新段。 在新的文档被创建时，Elasticsearch会为该文档指定一个版本号，当执行更新时，旧版本的文档在.del文件中被标记为删除，新版本的文档被索引到一个新段。旧版本的文档依然能匹配查询，但是会在结果中被过滤掉。 10.详细描述一下Elasticsearch搜索的过程 搜索被执行成一个两阶段过程，我们称之为 Query Then Fetch； 在初始查询阶段时，查询会广播到索引中每一个分片拷贝（主分片或者副本分片）。 每个分片在本地执行搜索并构建一个匹配文档的大小为 from + size 的优先队列。PS：在搜索的时候是会查询Filesystem Cache的，但是有部分数据还在Memory Buffer，所以搜索是近实时的。 每个分片返回各自优先队列中 所有文档的 ID 和排序值 给协调节点，它合并这些值到自己的优先队列中来产生一个全局排序后的结果列表。 接下来就是 取回阶段，协调节点辨别出哪些文档需要被取回并向相关的分片提交多个 GET 请求。每个分片加载并 丰富 文档，如果有需要的话，接着返回文档给协调节点。一旦所有的文档都被取回了，协调节点返回结果给客户端。 补充：Query Then Fetch的搜索类型在文档相关性打分的时候参考的是本分片的数据，这样在文档数量较少的时候可能不够准确，DFS Query Then Fetch增加了一个预查询的处理，询问Term和Document frequency，这个评分更准确，但是性能会变差。 11.Elasticsearch对于大数据量（上亿量级）的聚合如何实现？ Elasticsearch 提供的首个近似聚合是cardinality 度量。它提供一个字段的基数，即该字段的distinct或者unique值的数目。它是基于HLL算法的。HLL 会先对我们的输入作哈希运算，然后根据哈希运算的结果中的 bits 做概率估算从而得到基数。其特点是：可配置的精度，用来控制内存的使用（更精确 ＝ 更多内存）；小的数据集精度是非常高的；我们可以通过配置参数，来设置去重需要的固定内存使用量。无论数千还是数十亿的唯一值，内存使用量只与你配置的精确度相关。 12.在并发情况下，Elasticsearch如果保证读写一致？ 可以通过版本号使用乐观并发控制，以确保新版本不会被旧版本覆盖，由应用层来处理具体的冲突； 另外对于写操作，一致性级别支持quorum/one/all，默认为quorum，即只有当大多数分片可用时才允许写操作。但即使大多数可用，也可能存在因为网络等原因导致写入副本失败，这样该副本被认为故障，分片将会在一个不同的节点上重建。 对于读操作，可以设置replication为sync(默认)，这使得操作在主分片和副本分片都完成后才会返回；如果设置replication为async时，也可以通过设置搜索请求参数_preference为primary来查询主分片，确保文档是最新版本。 ","link":"https://tianxiawuhao.github.io/w_tIMVx00/"},{"title":"elasticSearch查询语句","content":" 原文：https://distributedbytes.timojo.com/2016/07/23-useful-elasticsearch-example-queries.html 作者：Tim Ojo 为了演示不同类型的 ElasticSearch 的查询，我们将使用以下字段搜索书籍文档的集合（ title（标题）, authors（作者）, summary（摘要）, publish_date（发布日期）和 num_reviews（浏览数））。 在这之前，首先我们应该先创建一个新的索引（index），并批量导入一些文档： 创建索引： PUT /bookdb_index { &quot;settings&quot;: { &quot;number_of_shards&quot;: 1 }} 批量上传文档： POST /bookdb_index/book/_bulk { &quot;index&quot;: { &quot;_id&quot;: 1 }} { &quot;title&quot;: &quot;Elasticsearch: The Definitive Guide&quot;, &quot;authors&quot;: [&quot;clinton gormley&quot;, &quot;zachary tong&quot;], &quot;summary&quot; : &quot;A distibuted real-time search and analytics engine&quot;, &quot;publish_date&quot; : &quot;2015-02-07&quot;, &quot;num_reviews&quot;: 20, &quot;publisher&quot;: &quot;oreilly&quot; } { &quot;index&quot;: { &quot;_id&quot;: 2 }} { &quot;title&quot;: &quot;Taming Text: How to Find, Organize, and Manipulate It&quot;, &quot;authors&quot;: [&quot;grant ingersoll&quot;, &quot;thomas morton&quot;, &quot;drew farris&quot;], &quot;summary&quot; : &quot;organize text using approaches such as full-text search, proper name recognition, clustering, tagging, information extraction, and summarization&quot;, &quot;publish_date&quot; : &quot;2013-01-24&quot;, &quot;num_reviews&quot;: 12, &quot;publisher&quot;: &quot;manning&quot; } { &quot;index&quot;: { &quot;_id&quot;: 3 }} { &quot;title&quot;: &quot;Elasticsearch in Action&quot;, &quot;authors&quot;: [&quot;radu gheorge&quot;, &quot;matthew lee hinman&quot;, &quot;roy russo&quot;], &quot;summary&quot; : &quot;build scalable search applications using Elasticsearch without having to do complex low-level programming or understand advanced data science algorithms&quot;, &quot;publish_date&quot; : &quot;2015-12-03&quot;, &quot;num_reviews&quot;: 18, &quot;publisher&quot;: &quot;manning&quot; } { &quot;index&quot;: { &quot;_id&quot;: 4 }} { &quot;title&quot;: &quot;Solr in Action&quot;, &quot;authors&quot;: [&quot;trey grainger&quot;, &quot;timothy potter&quot;], &quot;summary&quot; : &quot;Comprehensive guide to implementing a scalable search engine using Apache Solr&quot;, &quot;publish_date&quot; : &quot;2014-04-05&quot;, &quot;num_reviews&quot;: 23, &quot;publisher&quot;: &quot;manning&quot; } 例子 1.基本比对查询 执行基本的全文本（匹配）查询有两种方式：使用Search Lite API（它希望所有搜索参数都作为URL的一部分传入），或使用完整的JSON请求正文（允许您使用完整的Elasticsearch DSL)。 这是一个基本的匹配查询，它在所有字段中搜索字符串“ guide” GET /bookdb_index/book/_search?q=guide [Results] &quot;hits&quot;: [ { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.28168046, &quot;_source&quot;: { &quot;title&quot;: &quot;Elasticsearch: The Definitive Guide&quot;, &quot;authors&quot;: [ &quot;clinton gormley&quot;, &quot;zachary tong&quot; ], &quot;summary&quot;: &quot;A distibuted real-time search and analytics engine&quot;, &quot;publish_date&quot;: &quot;2015-02-07&quot;, &quot;num_reviews&quot;: 20, &quot;publisher&quot;: &quot;manning&quot; } }, { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;4&quot;, &quot;_score&quot;: 0.24144039, &quot;_source&quot;: { &quot;title&quot;: &quot;Solr in Action&quot;, &quot;authors&quot;: [ &quot;trey grainger&quot;, &quot;timothy potter&quot; ], &quot;summary&quot;: &quot;Comprehensive guide to implementing a scalable search engine using Apache Solr&quot;, &quot;publish_date&quot;: &quot;2014-04-05&quot;, &quot;num_reviews&quot;: 23, &quot;publisher&quot;: &quot;manning&quot; } } ] 该查询的完整版本如下所示，其产生的结果与上述搜索精简版相同。 { &quot;query&quot;: { &quot;multi_match&quot; : { &quot;query&quot; : &quot;guide&quot;, &quot;fields&quot; : [&quot;_all&quot;] } } } 使用multi_match关键字代替关键字match是对多个字段运行相同查询的便捷快捷方式。该fields属性指定要查询的字段，在这种情况下，我们要查询文档中的所有字段。 这两个API均允许您指定要搜索的字段。例如，要在标题字段中搜索带有“in action”字样的图书，请执行以下操作： GET /bookdb_index/book/_search?q=title:in action [Results] &quot;hits&quot;: [ { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;4&quot;, &quot;_score&quot;: 0.6259885, &quot;_source&quot;: { &quot;title&quot;: &quot;Solr in Action&quot;, &quot;authors&quot;: [ &quot;trey grainger&quot;, &quot;timothy potter&quot; ], &quot;summary&quot;: &quot;Comprehensive guide to implementing a scalable search engine using Apache Solr&quot;, &quot;publish_date&quot;: &quot;2014-04-05&quot;, &quot;num_reviews&quot;: 23, &quot;publisher&quot;: &quot;manning&quot; } }, { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 0.5975345, &quot;_source&quot;: { &quot;title&quot;: &quot;Elasticsearch in Action&quot;, &quot;authors&quot;: [ &quot;radu gheorge&quot;, &quot;matthew lee hinman&quot;, &quot;roy russo&quot; ], &quot;summary&quot;: &quot;build scalable search applications using Elasticsearch without having to do complex low-level programming or understand advanced data science algorithms&quot;, &quot;publish_date&quot;: &quot;2015-12-03&quot;, &quot;num_reviews&quot;: 18, &quot;publisher&quot;: &quot;manning&quot; } } ] 但是，全身DSL在创建更复杂的查询（如我们将在后面看到）以及指定您希望如何返回结果方面给您更大的灵活性。在下面的示例中，我们指定要返回的结果数，开始的偏移量（用于分页），要返回的文档字段以及术语突出显示。 POST /bookdb_index/book/_search { &quot;query&quot;: { &quot;match&quot; : { &quot;title&quot; : &quot;in action&quot; } }, &quot;size&quot;: 2, &quot;from&quot;: 0, &quot;_source&quot;: [ &quot;title&quot;, &quot;summary&quot;, &quot;publish_date&quot; ], &quot;highlight&quot;: { &quot;fields&quot; : { &quot;title&quot; : {} } } } [Results] &quot;hits&quot;: { &quot;total&quot;: 2, &quot;max_score&quot;: 0.9105287, &quot;hits&quot;: [ { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 0.9105287, &quot;_source&quot;: { &quot;summary&quot;: &quot;build scalable search applications using Elasticsearch without having to do complex low-level programming or understand advanced data science algorithms&quot;, &quot;title&quot;: &quot;Elasticsearch in Action&quot;, &quot;publish_date&quot;: &quot;2015-12-03&quot; }, &quot;highlight&quot;: { &quot;title&quot;: [ &quot;Elasticsearch &lt;em&gt;in&lt;/em&gt; &lt;em&gt;Action&lt;/em&gt;&quot; ] } }, { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;4&quot;, &quot;_score&quot;: 0.9105287, &quot;_source&quot;: { &quot;summary&quot;: &quot;Comprehensive guide to implementing a scalable search engine using Apache Solr&quot;, &quot;title&quot;: &quot;Solr in Action&quot;, &quot;publish_date&quot;: &quot;2014-04-05&quot; }, &quot;highlight&quot;: { &quot;title&quot;: [ &quot;Solr &lt;em&gt;in&lt;/em&gt; &lt;em&gt;Action&lt;/em&gt;&quot; ] } } ] } 注意：对于多字查询，该match查询使您可以指定是否使用and运算符而不是默认or运算符。您还可以指定minimum_should_match选项来调整返回结果的相关性。可以在这里找到详细信息。 2.多字段搜索 正如我们已经看到的，要在搜索中查询多个文档字段（例如，在标题和摘要中搜索相同的查询字符串），则可以使用该multi_match查询。 POST /bookdb_index/book/_search { &quot;query&quot;: { &quot;multi_match&quot; : { &quot;query&quot; : &quot;elasticsearch guide&quot;, &quot;fields&quot;: [&quot;title&quot;, &quot;summary&quot;] } } } [Results] &quot;hits&quot;: { &quot;total&quot;: 3, &quot;max_score&quot;: 0.9448582, &quot;hits&quot;: [ { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.9448582, &quot;_source&quot;: { &quot;title&quot;: &quot;Elasticsearch: The Definitive Guide&quot;, &quot;authors&quot;: [ &quot;clinton gormley&quot;, &quot;zachary tong&quot; ], &quot;summary&quot;: &quot;A distibuted real-time search and analytics engine&quot;, &quot;publish_date&quot;: &quot;2015-02-07&quot;, &quot;num_reviews&quot;: 20, &quot;publisher&quot;: &quot;manning&quot; } }, { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 0.17312013, &quot;_source&quot;: { &quot;title&quot;: &quot;Elasticsearch in Action&quot;, &quot;authors&quot;: [ &quot;radu gheorge&quot;, &quot;matthew lee hinman&quot;, &quot;roy russo&quot; ], &quot;summary&quot;: &quot;build scalable search applications using Elasticsearch without having to do complex low-level programming or understand advanced data science algorithms&quot;, &quot;publish_date&quot;: &quot;2015-12-03&quot;, &quot;num_reviews&quot;: 18, &quot;publisher&quot;: &quot;manning&quot; } }, { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;4&quot;, &quot;_score&quot;: 0.14965448, &quot;_source&quot;: { &quot;title&quot;: &quot;Solr in Action&quot;, &quot;authors&quot;: [ &quot;trey grainger&quot;, &quot;timothy potter&quot; ], &quot;summary&quot;: &quot;Comprehensive guide to implementing a scalable search engine using Apache Solr&quot;, &quot;publish_date&quot;: &quot;2014-04-05&quot;, &quot;num_reviews&quot;: 23, &quot;publisher&quot;: &quot;manning&quot; } } ] } 请注意，命中数字3相匹配，因为在摘要中找到了“指南”一词。 3.提高字段重要性 由于我们正在多个字段中进行搜索，因此我们可能希望提高特定字段中的得分。在以下人为设计的示例中，我们将摘要字段的得分提高了3倍，以提高摘要字段的重要性，这反过来又会增加文档_id 4的相关性。 POST /bookdb_index/book/_search { &quot;query&quot;: { &quot;multi_match&quot; : { &quot;query&quot; : &quot;elasticsearch guide&quot;, &quot;fields&quot;: [&quot;title&quot;, &quot;summary^3&quot;] } }, &quot;_source&quot;: [&quot;title&quot;, &quot;summary&quot;, &quot;publish_date&quot;] } [Results] &quot;hits&quot;: [ { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.31495273, &quot;_source&quot;: { &quot;summary&quot;: &quot;A distibuted real-time search and analytics engine&quot;, &quot;title&quot;: &quot;Elasticsearch: The Definitive Guide&quot;, &quot;publish_date&quot;: &quot;2015-02-07&quot; } }, { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;4&quot;, &quot;_score&quot;: 0.14965448, &quot;_source&quot;: { &quot;summary&quot;: &quot;Comprehensive guide to implementing a scalable search engine using Apache Solr&quot;, &quot;title&quot;: &quot;Solr in Action&quot;, &quot;publish_date&quot;: &quot;2014-04-05&quot; } }, { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 0.13094766, &quot;_source&quot;: { &quot;summary&quot;: &quot;build scalable search applications using Elasticsearch without having to do complex low-level programming or understand advanced data science algorithms&quot;, &quot;title&quot;: &quot;Elasticsearch in Action&quot;, &quot;publish_date&quot;: &quot;2015-12-03&quot; } } ] 注意 ：Boosting不仅仅意味着计算出的分数会乘以Boosting系数。实际应用的升压值经过归一化和一些内部优化。有关增强工作原理的更多信息，请参见 Elasticsearch指南。 4.布尔查询 AND / OR / NOT运算符可用于微调我们的搜索查询，以提供更相关或更具体的结果。这是在搜索API中作为bool查询实现的。该bool查询接受一个must参数（等同于AND），一个must_not参数（等同于NOT）和一个should参数（等同于OR）。例如，如果我要搜索书名中带有“ Elasticsearch”或“ Solr”字样的书，则AND由“克林顿·戈姆利”（clinton gormley）创作，而不由“ radu gheorge”（radu gheorge）创作： POST /bookdb_index/book/_search { &quot;query&quot;: { &quot;bool&quot;: { &quot;must&quot;: { &quot;bool&quot; : { &quot;should&quot;: [ { &quot;match&quot;: { &quot;title&quot;: &quot;Elasticsearch&quot; }}, { &quot;match&quot;: { &quot;title&quot;: &quot;Solr&quot; }} ] } }, &quot;must&quot;: { &quot;match&quot;: { &quot;authors&quot;: &quot;clinton gormely&quot; }}, &quot;must_not&quot;: { &quot;match&quot;: {&quot;authors&quot;: &quot;radu gheorge&quot; }} } } } [Results] &quot;hits&quot;: [ { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.3672021, &quot;_source&quot;: { &quot;title&quot;: &quot;Elasticsearch: The Definitive Guide&quot;, &quot;authors&quot;: [ &quot;clinton gormley&quot;, &quot;zachary tong&quot; ], &quot;summary&quot;: &quot;A distibuted real-time search and analytics engine&quot;, &quot;publish_date&quot;: &quot;2015-02-07&quot;, &quot;num_reviews&quot;: 20, &quot;publisher&quot;: &quot;oreilly&quot; } } ] 注意：如您所见，布尔查询可以包装任何其他查询类型，包括其他布尔查询，以创建任意复杂或深度嵌套的查询。 5.模糊查询 可以在“匹配”和“多匹配”查询中启用模糊匹配，以捕获拼写错误。模糊程度是根据距原始单词的Levenshtein距离指定的。 POST /bookdb_index/book/_search { &quot;query&quot;: { &quot;multi_match&quot; : { &quot;query&quot; : &quot;comprihensiv guide&quot;, &quot;fields&quot;: [&quot;title&quot;, &quot;summary&quot;], &quot;fuzziness&quot;: &quot;AUTO&quot; } }, &quot;_source&quot;: [&quot;title&quot;, &quot;summary&quot;, &quot;publish_date&quot;], &quot;size&quot;: 1 } [Results] &quot;hits&quot;: [ { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;4&quot;, &quot;_score&quot;: 0.5961596, &quot;_source&quot;: { &quot;summary&quot;: &quot;Comprehensive guide to implementing a scalable search engine using Apache Solr&quot;, &quot;title&quot;: &quot;Solr in Action&quot;, &quot;publish_date&quot;: &quot;2014-04-05&quot; } } ] 注意：模糊度值&quot;AUTO&quot;等于指定2术语长度大于5时的值。但是，设置80％的人类拼写错误的编辑距离为1，并将模糊度设置为1可以改善整体搜索性能。有关更多信息，请参见《Elasticsearch最终指南》的“错别字和拼写错误”一章。 6.通配符查询 通配符查询使您可以指定要匹配的模式，而不是整个术语。?匹配任何字符并*匹配零个或多个字符。例如，要查找所有作者姓名以字母“ t”开头的记录 POST /bookdb_index/book/_search { &quot;query&quot;: { &quot;wildcard&quot; : { &quot;authors&quot; : &quot;t*&quot; } }, &quot;_source&quot;: [&quot;title&quot;, &quot;authors&quot;], &quot;highlight&quot;: { &quot;fields&quot; : { &quot;authors&quot; : {} } } } [Results] &quot;hits&quot;: [ { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: { &quot;title&quot;: &quot;Elasticsearch: The Definitive Guide&quot;, &quot;authors&quot;: [ &quot;clinton gormley&quot;, &quot;zachary tong&quot; ] }, &quot;highlight&quot;: { &quot;authors&quot;: [ &quot;zachary &lt;em&gt;tong&lt;/em&gt;&quot; ] } }, { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: { &quot;title&quot;: &quot;Taming Text: How to Find, Organize, and Manipulate It&quot;, &quot;authors&quot;: [ &quot;grant ingersoll&quot;, &quot;thomas morton&quot;, &quot;drew farris&quot; ] }, &quot;highlight&quot;: { &quot;authors&quot;: [ &quot;&lt;em&gt;thomas&lt;/em&gt; morton&quot; ] } }, { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;4&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: { &quot;title&quot;: &quot;Solr in Action&quot;, &quot;authors&quot;: [ &quot;trey grainger&quot;, &quot;timothy potter&quot; ] }, &quot;highlight&quot;: { &quot;authors&quot;: [ &quot;&lt;em&gt;trey&lt;/em&gt; grainger&quot;, &quot;&lt;em&gt;timothy&lt;/em&gt; potter&quot; ] } } ] 7.正则表达式查询 正则表达式查询使您可以指定比通配符查询更复杂的模式。 POST /bookdb_index/book/_search { &quot;query&quot;: { &quot;regexp&quot; : { &quot;authors&quot; : &quot;t[a-z]*y&quot; } }, &quot;_source&quot;: [&quot;title&quot;, &quot;authors&quot;], &quot;highlight&quot;: { &quot;fields&quot; : { &quot;authors&quot; : {} } } } [Results] &quot;hits&quot;: [ { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;4&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: { &quot;title&quot;: &quot;Solr in Action&quot;, &quot;authors&quot;: [ &quot;trey grainger&quot;, &quot;timothy potter&quot; ] }, &quot;highlight&quot;: { &quot;authors&quot;: [ &quot;&lt;em&gt;trey&lt;/em&gt; grainger&quot;, &quot;&lt;em&gt;timothy&lt;/em&gt; potter&quot; ] } } ] 8.匹配词组查询 匹配词组查询要求查询字符串中的所有术语都存在于文档中，并按照查询字符串中指定的顺序并且彼此接近。默认情况下，术语必须彼此完全平行，但是您可以指定一个slop值，该值指示在仍将文档视为匹配项时允许相隔多远的术语。 POST /bookdb_index/book/_search { &quot;query&quot;: { &quot;multi_match&quot; : { &quot;query&quot;: &quot;search engine&quot;, &quot;fields&quot;: [&quot;title&quot;, &quot;summary&quot;], &quot;type&quot;: &quot;phrase&quot;, &quot;slop&quot;: 3 } }, &quot;_source&quot;: [ &quot;title&quot;, &quot;summary&quot;, &quot;publish_date&quot; ] } [Results] &quot;hits&quot;: [ { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;4&quot;, &quot;_score&quot;: 0.22327082, &quot;_source&quot;: { &quot;summary&quot;: &quot;Comprehensive guide to implementing a scalable search engine using Apache Solr&quot;, &quot;title&quot;: &quot;Solr in Action&quot;, &quot;publish_date&quot;: &quot;2014-04-05&quot; } }, { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.16113183, &quot;_source&quot;: { &quot;summary&quot;: &quot;A distibuted real-time search and analytics engine&quot;, &quot;title&quot;: &quot;Elasticsearch: The Definitive Guide&quot;, &quot;publish_date&quot;: &quot;2015-02-07&quot; } } ] 注意：在上面的示例中，对于非短语类型查询，文档的_id 1得分通常更高，并且_id 4由于其字段长度较短而出现在文档的前面。但是，在进行短语查询时，会考虑到术语的接近度，因此文档_id 4得分会更高。 9.匹配词组前缀 匹配词组前缀查询可在查询时提供“按需输入”或“穷人”版本的自动完成功能，而无需以任何方式准备数据。像match_phrase查询一样，它接受一个slop参数以使单词顺序和相对位置的刚性降低一些。我还接受该max_expansions参数来限制匹配项的数量，以降低资源强度。 POST /bookdb_index/book/_search { &quot;query&quot;: { &quot;match_phrase_prefix&quot; : { &quot;summary&quot;: { &quot;query&quot;: &quot;search en&quot;, &quot;slop&quot;: 3, &quot;max_expansions&quot;: 10 } } }, &quot;_source&quot;: [ &quot;title&quot;, &quot;summary&quot;, &quot;publish_date&quot; ] } [Results] &quot;hits&quot;: [ { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;4&quot;, &quot;_score&quot;: 0.5161346, &quot;_source&quot;: { &quot;summary&quot;: &quot;Comprehensive guide to implementing a scalable search engine using Apache Solr&quot;, &quot;title&quot;: &quot;Solr in Action&quot;, &quot;publish_date&quot;: &quot;2014-04-05&quot; } }, { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.37248808, &quot;_source&quot;: { &quot;summary&quot;: &quot;A distibuted real-time search and analytics engine&quot;, &quot;title&quot;: &quot;Elasticsearch: The Definitive Guide&quot;, &quot;publish_date&quot;: &quot;2015-02-07&quot; } } ] 注意：“按类型查询时搜索”会降低性能。更好的解决方案是按类型进行索引时间搜索。请查看完成提示API或使用Edge-Ngram过滤器以获取更多信息。 10.请求参数 该query_string查询提供了一种以简洁的速记语法执行多重匹配查询，布尔查询，增强查询，模糊匹配，通配符，正则表达式和范围查询的方法。在下面的示例中，我们对术语“ saerch算法”执行模糊搜索，其中作者之一是“ grant ingersoll”或“ tom morton”。我们搜索所有字段，但对摘要字段加2。 POST /bookdb_index/book/_search { &quot;query&quot;: { &quot;query_string&quot; : { &quot;query&quot;: &quot;(saerch~1 algorithm~1) AND (grant ingersoll) OR (tom morton)&quot;, &quot;fields&quot;: [&quot;_all&quot;, &quot;summary^2&quot;] } }, &quot;_source&quot;: [ &quot;title&quot;, &quot;summary&quot;, &quot;authors&quot; ], &quot;highlight&quot;: { &quot;fields&quot; : { &quot;summary&quot; : {} } } } [Results] &quot;hits&quot;: [ { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.14558059, &quot;_source&quot;: { &quot;summary&quot;: &quot;organize text using approaches such as full-text search, proper name recognition, clustering, tagging, information extraction, and summarization&quot;, &quot;title&quot;: &quot;Taming Text: How to Find, Organize, and Manipulate It&quot;, &quot;authors&quot;: [ &quot;grant ingersoll&quot;, &quot;thomas morton&quot;, &quot;drew farris&quot; ] }, &quot;highlight&quot;: { &quot;summary&quot;: [ &quot;organize text using approaches such as full-text &lt;em&gt;search&lt;/em&gt;, proper name recognition, clustering, tagging, information extraction, and summarization&quot; ] } } ] 11.简单查询字符串 该simple_query_string查询是该查询的一种版本query_string，它更适合在暴露给用户的单个搜索框中使用。它分别用+ / | /-代替了AND / OR / NOT的使用，并且丢弃了查询的无效部分，而不是在用户犯错时抛出异常。 POST /bookdb_index/book/_search { &quot;query&quot;: { &quot;simple_query_string&quot; : { &quot;query&quot;: &quot;(saerch~1 algorithm~1) + (grant ingersoll) | (tom morton)&quot;, &quot;fields&quot;: [&quot;_all&quot;, &quot;summary^2&quot;] } }, &quot;_source&quot;: [ &quot;title&quot;, &quot;summary&quot;, &quot;authors&quot; ], &quot;highlight&quot;: { &quot;fields&quot; : { &quot;summary&quot; : {} } } } 12.术语查询 以上示例是全文搜索的示例。有时，我们对结构化搜索更感兴趣，在结构化搜索中我们希望找到完全匹配并返回结果。在term与terms查询帮助我们在这里。在下面的示例中，我们正在搜索Manning Publications出版的索引中的所有书籍。 POST /bookdb_index/book/_search { &quot;query&quot;: { &quot;term&quot; : { &quot;publisher&quot;: &quot;manning&quot; } }, &quot;_source&quot; : [&quot;title&quot;,&quot;publish_date&quot;,&quot;publisher&quot;] } [Results] &quot;hits&quot;: [ { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 1.2231436, &quot;_source&quot;: { &quot;publisher&quot;: &quot;manning&quot;, &quot;title&quot;: &quot;Taming Text: How to Find, Organize, and Manipulate It&quot;, &quot;publish_date&quot;: &quot;2013-01-24&quot; } }, { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 1.2231436, &quot;_source&quot;: { &quot;publisher&quot;: &quot;manning&quot;, &quot;title&quot;: &quot;Elasticsearch in Action&quot;, &quot;publish_date&quot;: &quot;2015-12-03&quot; } }, { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;4&quot;, &quot;_score&quot;: 1.2231436, &quot;_source&quot;: { &quot;publisher&quot;: &quot;manning&quot;, &quot;title&quot;: &quot;Solr in Action&quot;, &quot;publish_date&quot;: &quot;2014-04-05&quot; } } ] 可以使用terms关键字代替并传递搜索词数组来指定多个词。 { &quot;query&quot;: { &quot;terms&quot; : { &quot;publisher&quot;: [&quot;oreilly&quot;, &quot;packt&quot;] } } } 13.字词查询-排序 术语查询结果（与任何其他查询结果一样）可以轻松地进行排序。也允许多级排序 POST /bookdb_index/book/_search { &quot;query&quot;: { &quot;term&quot; : { &quot;publisher&quot;: &quot;manning&quot; } }, &quot;_source&quot; : [&quot;title&quot;,&quot;publish_date&quot;,&quot;publisher&quot;], &quot;sort&quot;: [ { &quot;publish_date&quot;: {&quot;order&quot;:&quot;desc&quot;}}, { &quot;title&quot;: { &quot;order&quot;: &quot;desc&quot; }} ] } [Results] &quot;hits&quot;: [ { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: null, &quot;_source&quot;: { &quot;publisher&quot;: &quot;manning&quot;, &quot;title&quot;: &quot;Elasticsearch in Action&quot;, &quot;publish_date&quot;: &quot;2015-12-03&quot; }, &quot;sort&quot;: [ 1449100800000, &quot;in&quot; ] }, { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;4&quot;, &quot;_score&quot;: null, &quot;_source&quot;: { &quot;publisher&quot;: &quot;manning&quot;, &quot;title&quot;: &quot;Solr in Action&quot;, &quot;publish_date&quot;: &quot;2014-04-05&quot; }, &quot;sort&quot;: [ 1396656000000, &quot;solr&quot; ] }, { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: null, &quot;_source&quot;: { &quot;publisher&quot;: &quot;manning&quot;, &quot;title&quot;: &quot;Taming Text: How to Find, Organize, and Manipulate It&quot;, &quot;publish_date&quot;: &quot;2013-01-24&quot; }, &quot;sort&quot;: [ 1358985600000, &quot;to&quot; ] } ] 14.范围查询 另一个结构化查询示例是范围查询。在此示例中，我们搜索2015年出版的图书。 POST /bookdb_index/book/_search { &quot;query&quot;: { &quot;range&quot; : { &quot;publish_date&quot;: { &quot;gte&quot;: &quot;2015-01-01&quot;, &quot;lte&quot;: &quot;2015-12-31&quot; } } }, &quot;_source&quot; : [&quot;title&quot;,&quot;publish_date&quot;,&quot;publisher&quot;] } [Results] &quot;hits&quot;: [ { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: { &quot;publisher&quot;: &quot;oreilly&quot;, &quot;title&quot;: &quot;Elasticsearch: The Definitive Guide&quot;, &quot;publish_date&quot;: &quot;2015-02-07&quot; } }, { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: { &quot;publisher&quot;: &quot;manning&quot;, &quot;title&quot;: &quot;Elasticsearch in Action&quot;, &quot;publish_date&quot;: &quot;2015-12-03&quot; } } ] 注意：范围查询适用于日期，数字和字符串类型字段。 15.过滤查询 筛选查询允许您筛选查询结果。对于我们的示例，我们正在查询标题或摘要中带有“ Elasticsearch”一词的图书，但我们希望将搜索结果过滤为仅包含20条或更多评论的图书。 POST /bookdb_index/book/_search { &quot;query&quot;: { &quot;filtered&quot;: { &quot;query&quot; : { &quot;multi_match&quot;: { &quot;query&quot;: &quot;elasticsearch&quot;, &quot;fields&quot;: [&quot;title&quot;,&quot;summary&quot;] } }, &quot;filter&quot;: { &quot;range&quot; : { &quot;num_reviews&quot;: { &quot;gte&quot;: 20 } } } } }, &quot;_source&quot; : [&quot;title&quot;,&quot;summary&quot;,&quot;publisher&quot;, &quot;num_reviews&quot;] } [Results] &quot;hits&quot;: [ { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.5955761, &quot;_source&quot;: { &quot;summary&quot;: &quot;A distibuted real-time search and analytics engine&quot;, &quot;publisher&quot;: &quot;oreilly&quot;, &quot;num_reviews&quot;: 20, &quot;title&quot;: &quot;Elasticsearch: The Definitive Guide&quot; } } ] 注意：筛选查询并不要求存在要对其进行筛选的查询。如果未指定match_all查询，则运行查询，该查询基本上返回索引中的所有文档，然后对其进行过滤。实际上，首先运行过滤器，以减少需要查询的表面积。此外，过滤器在首次使用后会被缓存，这使其性能非常好。 更新：已过滤的查询已从即将推出的Elasticsearch 5.0中删除，以支持bool查询。这是与上面相同的示例，改写为使用bool查询。返回的结果完全相同。 POST /bookdb_index/book/_search { &quot;query&quot;: { &quot;bool&quot;: { &quot;must&quot; : { &quot;multi_match&quot;: { &quot;query&quot;: &quot;elasticsearch&quot;, &quot;fields&quot;: [&quot;title&quot;,&quot;summary&quot;] } }, &quot;filter&quot;: { &quot;range&quot; : { &quot;num_reviews&quot;: { &quot;gte&quot;: 20 } } } } }, &quot;_source&quot; : [&quot;title&quot;,&quot;summary&quot;,&quot;publisher&quot;, &quot;num_reviews&quot;] } 在下面的示例中，这也适用于多个过滤器。 16.多个过滤器 可以通过使用过滤器来组合多个过滤bool器。在下一个示例中，过滤器确定返回的结果必须至少具有20条评论，不得在2015年之前发布，而应由oreilly发布。 POST /bookdb_index/book/_search { &quot;query&quot;: { &quot;filtered&quot;: { &quot;query&quot; : { &quot;multi_match&quot;: { &quot;query&quot;: &quot;elasticsearch&quot;, &quot;fields&quot;: [&quot;title&quot;,&quot;summary&quot;] } }, &quot;filter&quot;: { &quot;bool&quot;: { &quot;must&quot;: { &quot;range&quot; : { &quot;num_reviews&quot;: { &quot;gte&quot;: 20 } } }, &quot;must_not&quot;: { &quot;range&quot; : { &quot;publish_date&quot;: { &quot;lte&quot;: &quot;2014-12-31&quot; } } }, &quot;should&quot;: { &quot;term&quot;: { &quot;publisher&quot;: &quot;oreilly&quot; } } } } } }, &quot;_source&quot; : [&quot;title&quot;,&quot;summary&quot;,&quot;publisher&quot;, &quot;num_reviews&quot;, &quot;publish_date&quot;] } [Results] &quot;hits&quot;: [ { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.5955761, &quot;_source&quot;: { &quot;summary&quot;: &quot;A distibuted real-time search and analytics engine&quot;, &quot;publisher&quot;: &quot;oreilly&quot;, &quot;num_reviews&quot;: 20, &quot;title&quot;: &quot;Elasticsearch: The Definitive Guide&quot;, &quot;publish_date&quot;: &quot;2015-02-07&quot; } } ] 17.功能评分：字段值因子 在某些情况下，您可能希望将文档中特定字段的值纳入相关性分数的计算中。在您希望根据文档的受欢迎程度提高其相关性的情况下，这是典型的情况。在我们的示例中，我们希望增加受欢迎的书籍（根据评论数判断）。使用field_value_factor功能评分可以做到这一点。 POST /bookdb_index/book/_search { &quot;query&quot;: { &quot;function_score&quot;: { &quot;query&quot;: { &quot;multi_match&quot; : { &quot;query&quot; : &quot;search engine&quot;, &quot;fields&quot;: [&quot;title&quot;, &quot;summary&quot;] } }, &quot;field_value_factor&quot;: { &quot;field&quot; : &quot;num_reviews&quot;, &quot;modifier&quot;: &quot;log1p&quot;, &quot;factor&quot; : 2 } } }, &quot;_source&quot;: [&quot;title&quot;, &quot;summary&quot;, &quot;publish_date&quot;, &quot;num_reviews&quot;] } [Results] &quot;hits&quot;: [ { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.44831306, &quot;_source&quot;: { &quot;summary&quot;: &quot;A distibuted real-time search and analytics engine&quot;, &quot;num_reviews&quot;: 20, &quot;title&quot;: &quot;Elasticsearch: The Definitive Guide&quot;, &quot;publish_date&quot;: &quot;2015-02-07&quot; } }, { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;4&quot;, &quot;_score&quot;: 0.3718407, &quot;_source&quot;: { &quot;summary&quot;: &quot;Comprehensive guide to implementing a scalable search engine using Apache Solr&quot;, &quot;num_reviews&quot;: 23, &quot;title&quot;: &quot;Solr in Action&quot;, &quot;publish_date&quot;: &quot;2014-04-05&quot; } }, { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 0.046479136, &quot;_source&quot;: { &quot;summary&quot;: &quot;build scalable search applications using Elasticsearch without having to do complex low-level programming or understand advanced data science algorithms&quot;, &quot;num_reviews&quot;: 18, &quot;title&quot;: &quot;Elasticsearch in Action&quot;, &quot;publish_date&quot;: &quot;2015-12-03&quot; } }, { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.041432835, &quot;_source&quot;: { &quot;summary&quot;: &quot;organize text using approaches such as full-text search, proper name recognition, clustering, tagging, information extraction, and summarization&quot;, &quot;num_reviews&quot;: 12, &quot;title&quot;: &quot;Taming Text: How to Find, Organize, and Manipulate It&quot;, &quot;publish_date&quot;: &quot;2013-01-24&quot; } } ] 注意1：我们本来可以运行常规multi_match查询并按num_reviews字段进行排序，但是这样就失去了进行相关性评分的好处。 注意2：还有许多其他参数可以调整对原始相关性得分的增强效果，例如“修饰符”，“因子”，“ boost_mode”等。这些参数在Elasticsearch指南中进行了详细探讨。 18.作用分值：衰减功能 假设您不想以某个字段的值递增，而希望拥有理想的目标值，并且希望该增益因子随着距离该值的增加而衰减。这通常在基于纬度/经度，价格或日期等数字字段的提升中很有用。在我们精心设计的示例中，我们正在搜索理想情况下于2014年6月左右出版的“搜索引擎”书籍。 POST /bookdb_index/book/_search { &quot;query&quot;: { &quot;function_score&quot;: { &quot;query&quot;: { &quot;multi_match&quot; : { &quot;query&quot; : &quot;search engine&quot;, &quot;fields&quot;: [&quot;title&quot;, &quot;summary&quot;] } }, &quot;functions&quot;: [ { &quot;exp&quot;: { &quot;publish_date&quot; : { &quot;origin&quot;: &quot;2014-06-15&quot;, &quot;offset&quot;: &quot;7d&quot;, &quot;scale&quot; : &quot;30d&quot; } } } ], &quot;boost_mode&quot; : &quot;replace&quot; } }, &quot;_source&quot;: [&quot;title&quot;, &quot;summary&quot;, &quot;publish_date&quot;, &quot;num_reviews&quot;] } [Results] &quot;hits&quot;: [ { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;4&quot;, &quot;_score&quot;: 0.27420625, &quot;_source&quot;: { &quot;summary&quot;: &quot;Comprehensive guide to implementing a scalable search engine using Apache Solr&quot;, &quot;num_reviews&quot;: 23, &quot;title&quot;: &quot;Solr in Action&quot;, &quot;publish_date&quot;: &quot;2014-04-05&quot; } }, { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.005920768, &quot;_source&quot;: { &quot;summary&quot;: &quot;A distibuted real-time search and analytics engine&quot;, &quot;num_reviews&quot;: 20, &quot;title&quot;: &quot;Elasticsearch: The Definitive Guide&quot;, &quot;publish_date&quot;: &quot;2015-02-07&quot; } }, { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.000011564, &quot;_source&quot;: { &quot;summary&quot;: &quot;organize text using approaches such as full-text search, proper name recognition, clustering, tagging, information extraction, and summarization&quot;, &quot;num_reviews&quot;: 12, &quot;title&quot;: &quot;Taming Text: How to Find, Organize, and Manipulate It&quot;, &quot;publish_date&quot;: &quot;2013-01-24&quot; } }, { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 0.0000059171475, &quot;_source&quot;: { &quot;summary&quot;: &quot;build scalable search applications using Elasticsearch without having to do complex low-level programming or understand advanced data science algorithms&quot;, &quot;num_reviews&quot;: 18, &quot;title&quot;: &quot;Elasticsearch in Action&quot;, &quot;publish_date&quot;: &quot;2015-12-03&quot; } } ] 19.功能评分：脚本评分 如果内置的评分功能无法满足您的需求，则可以选择指定Groovy脚本进行评分。在我们的示例中，我们希望指定一个脚本，该脚本在决定要考虑评论数量的因素之前，先考虑publish_date。较新的书可能没有那么多的评论，因此不应因此受到惩罚。 评分脚本如下所示： publish_date = doc['publish_date'].value num_reviews = doc['num_reviews'].value if (publish_date &gt; Date.parse('yyyy-MM-dd', threshold).getTime()) { my_score = Math.log(2.5 + num_reviews) } else { my_score = Math.log(1 + num_reviews) } return my_score 要动态使用评分脚本，我们使用script_score参数 POST /bookdb_index/book/_search { &quot;query&quot;: { &quot;function_score&quot;: { &quot;query&quot;: { &quot;multi_match&quot; : { &quot;query&quot; : &quot;search engine&quot;, &quot;fields&quot;: [&quot;title&quot;, &quot;summary&quot;] } }, &quot;functions&quot;: [ { &quot;script_score&quot;: { &quot;params&quot; : { &quot;threshold&quot;: &quot;2015-07-30&quot; }, &quot;script&quot;: &quot;publish_date = doc['publish_date'].value; num_reviews = doc['num_reviews'].value; if (publish_date &gt; Date.parse('yyyy-MM-dd', threshold).getTime()) { return log(2.5 + num_reviews) }; return log(1 + num_reviews);&quot; } } ] } }, &quot;_source&quot;: [&quot;title&quot;, &quot;summary&quot;, &quot;publish_date&quot;, &quot;num_reviews&quot;] } [Results] &quot;hits&quot;: { &quot;total&quot;: 4, &quot;max_score&quot;: 0.8463001, &quot;hits&quot;: [ { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.8463001, &quot;_source&quot;: { &quot;summary&quot;: &quot;A distibuted real-time search and analytics engine&quot;, &quot;num_reviews&quot;: 20, &quot;title&quot;: &quot;Elasticsearch: The Definitive Guide&quot;, &quot;publish_date&quot;: &quot;2015-02-07&quot; } }, { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;4&quot;, &quot;_score&quot;: 0.7067348, &quot;_source&quot;: { &quot;summary&quot;: &quot;Comprehensive guide to implementing a scalable search engine using Apache Solr&quot;, &quot;num_reviews&quot;: 23, &quot;title&quot;: &quot;Solr in Action&quot;, &quot;publish_date&quot;: &quot;2014-04-05&quot; } }, { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 0.08952084, &quot;_source&quot;: { &quot;summary&quot;: &quot;build scalable search applications using Elasticsearch without having to do complex low-level programming or understand advanced data science algorithms&quot;, &quot;num_reviews&quot;: 18, &quot;title&quot;: &quot;Elasticsearch in Action&quot;, &quot;publish_date&quot;: &quot;2015-12-03&quot; } }, { &quot;_index&quot;: &quot;bookdb_index&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.07602123, &quot;_source&quot;: { &quot;summary&quot;: &quot;organize text using approaches such as full-text search, proper name recognition, clustering, tagging, information extraction, and summarization&quot;, &quot;num_reviews&quot;: 12, &quot;title&quot;: &quot;Taming Text: How to Find, Organize, and Manipulate It&quot;, &quot;publish_date&quot;: &quot;2013-01-24&quot; } } ] } 注意1：要使用动态脚本，必须为config/elasticsearch.yaml文件中的Elasticsearch实例启用动态脚本。也可以使用存储在Elasticsearch服务器上的脚本。查阅Elasticsearch参考文档了解更多信息。 注意2 ：JSON无法包含嵌入的换行符，因此分号用于分隔语句。 ","link":"https://tianxiawuhao.github.io/AdeCmc8BU/"},{"title":"docker安装ELK","content":"Docker部署ElasticSearch 搜索ElasticSearch镜像 docker search elasticsearch 拉取镜像 拉取镜像的时候，可以指定版本，如果不指定，默认使用latest。 docker pull elasticsearch:7.12.0 查看镜像 docker images docker 启动 elasticsearch # --name : 为 elasticsearch 容器起个别名 # -e : 指定为单节点集群模式 # -i：表示运行容器 # -t：表示容器启动后会进入其命令行。加入这两个参数后，容器创建就能登录进去。即分配一个伪终端。 # -v：表示目录映射关系（前者是宿主机目录，后者是映射到宿主机上的目录），可以使用多个－v做多个目录或文件映射。注意：最好做目录映射，在宿主机上做修改，然后共享到容器上。 # -d：在run后面加上-d参数,则会创建一个守护式容器在后台运行（这样创建容器后不会自动登录容器，如果只加-i -t两个参数，创建后就会自动进去容器）。 # -p：表示端口映射，前者是宿主机端口，后者是容器内的映射端口。可以使用多个-p做多个端口映射 docker run -di --name elasticsearch -p 9200:9200 -p 9300:9300 -e &quot;discovery.type=single-node&quot; elasticsearch:7.12.0 elasticsearch配置 创建elasticsearch滚动策略 # 定义审计日志管理策略 curl -X PUT &quot;${host}/_ilm/policy/audit_policy&quot; -H 'Content-Type: application/json' -d' { &quot;policy&quot;: { &quot;phases&quot;: { &quot;hot&quot;: { &quot;actions&quot;: { &quot;rollover&quot;: { &quot;max_size&quot;: &quot;30GB&quot;, &quot;max_age&quot;: &quot;180d&quot; } } }, &quot;delete&quot;: { &quot;min_age&quot;: &quot;180d&quot;, &quot;actions&quot;: { &quot;delete&quot;: {} } } } } } 热数据最大30G,最多180天，数据最少保持180天后删除 创建索引模板 # 导出日志索引模板 curl -X PUT &quot;${host}/_template/export_log_index_template&quot; -H 'Content-Type: application/json' -d' { &quot;index_patterns&quot;: [ &quot;export_log_index*&quot; ], &quot;settings&quot;: { &quot;number_of_shards&quot;: 1, &quot;number_of_replicas&quot;: 1, &quot;index.lifecycle.name&quot;: &quot;audit_policy&quot;, &quot;index.lifecycle.rollover_alias&quot;: &quot;export_log_index&quot;, &quot;index.max_result_window&quot;: &quot;100000&quot; }, &quot;mappings&quot;: { &quot;properties&quot;: { &quot;applicationSide&quot;: { &quot;type&quot;: &quot;integer&quot; }, &quot;exportComment&quot;: { &quot;type&quot;: &quot;keyword&quot; }, &quot;exportFileSize&quot;: { &quot;type&quot;: &quot;integer&quot; }, &quot;exportType&quot;: { &quot;type&quot;: &quot;integer&quot; }, &quot;id&quot;: { &quot;type&quot;: &quot;keyword&quot; }, &quot;operationTime&quot;: { &quot;type&quot;: &quot;date&quot;, &quot;store&quot;: true, &quot;format&quot;: &quot;yyyy-MM-dd HH:mm:ss&quot; }, &quot;operationUser&quot;: { &quot;type&quot;: &quot;keyword&quot; }, &quot;operationUserName&quot;: { &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot;, &quot;fields&quot;: { &quot;keyword&quot;: { &quot;type&quot;: &quot;keyword&quot; } } }, &quot;remarks&quot;: { &quot;type&quot;: &quot;keyword&quot; }, &quot;successful&quot;: { &quot;type&quot;: &quot;boolean&quot; } } } } 创建索引 # 创建导出日志索引，按日期命名 curl -X PUT &quot;${host}/%3Cexport_log_index-%7Bnow%2Fd%7D-1%3E&quot; -H 'Content-Type: application/json' -d' { &quot;aliases&quot;: { &quot;export_log_index&quot;: { &quot;is_write_index&quot;: true } } } docker 安装 kibana 拉取镜像 拉取镜像的时候，需要注意的是, kibana 的版本最好与 elasticsearch 保持一致, 避免发生不必要的错误 docker pull kibana:7.12.0 查看镜像 docker images docker 启动 kibana # -e : 指定环境变量配置, 提供汉化 # --like : 建立两个容器之间的关联, kibana 关联到 es docker run -di --name kibana --link elasticsearch:elasticsearch -e &quot;I18N_LOCALE=zh-CN&quot; -p 5601:5601 kibana:7.12.0 # kibana 的汉化我感觉做的并不好 # 如果不习惯汉化, 可以把条件去除 docker run -di --name kibana --link elasticsearch:elasticsearch -p 5601:5601 kibana:7.12.0 Docker 安装 Logstash 拉取镜像 拉取镜像的时候，需要注意的是, Logstash 的版本最好与 elasticsearch 保持一致, 避免发生不必要的错误 docker pull logstash:7.12.0 查看镜像 docker images 文件映射 在本机建立配置文件和目录,用来存放所有配置的映射 /usr/local/logstash/config/logstash.yml /usr/local/logstash/conf.d/ logstash.yml (文件内容) path.config: /usr/share/logstash/conf.d/*.conf path.logs: /var/log/logstash conf.d/configuration.conf (文件内容) Filebeat和elasticsearch的交互 input { beats { port =&gt; 5044 codec =&gt; &quot;json&quot; } } output { elasticsearch { hosts =&gt; [&quot;elasticsearch:9200&quot;]， index =&gt; &quot;export_log_index&quot; } stdout { codec =&gt; rubydebug } } mysql和elasticsearch的交互 input { jdbc { jdbc_driver_class =&gt; &quot;com.mysql.jdbc.Driver&quot; jdbc_connection_string =&gt; &quot;jdbc:mysql://localhost:3306/db_example&quot; jdbc_user =&gt; root jdbc_password =&gt; ymruan123 #启用追踪，如果为true，则需要指定tracking_column use_column_value =&gt; true #指定追踪的字段， tracking_column =&gt; &quot;last_updated&quot; #追踪字段的类型，目前只有数字(numeric)和时间类型(timestamp)，默认是数字类型 tracking_column_type =&gt; &quot;numeric&quot; #记录最后一次运行的结果 record_last_run =&gt; true #上面运行结果的保存位置 last_run_metadata_path =&gt; &quot;jdbc-position.txt&quot; statement =&gt; &quot;SELECT * FROM user where last_updated &gt;:sql_last_value;&quot; schedule =&gt; &quot; * * * * * *&quot; } } output { elasticsearch { document_id =&gt; &quot;%{id}&quot; document_type =&gt; &quot;_doc&quot; index =&gt; &quot;export_log_index&quot; hosts =&gt; [&quot;http://localhost:9200&quot;] } stdout{ codec =&gt; rubydebug } } kafka和elasticsearch的交互 input { kafka{ topics =&gt; &quot;topic_export&quot; #kafka中topic名称，记得创建该topic group_id =&gt; &quot;group_export&quot; #默认为“logstash” codec =&gt; &quot;json&quot; #与Shipper端output配置项一致 consumer_threads =&gt; 1 #消费线程数，集群中所有logstash相加最好等于 topic 分区数 bootstrap_servers =&gt; &quot;kafka:9092&quot; decorate_events =&gt; true #在输出消息的时候回输出自身的信息，包括：消费消息的大小、topic来源以及consumer的group信息。 type =&gt; &quot;topic_export&quot; tags =&gt; [&quot;canal&quot;] # 标签，额外使用该参数可以在elastci中创建不同索引 } } filter { # 把默认的data字段重命名为message字段，方便在elastic中显示 mutate { rename =&gt; [&quot;data&quot;, &quot;message&quot;] } # 还可以使用其他的处理方式，在此就不再列出来了 } output { elasticsearch { hosts =&gt; [&quot;http://172.17.107.187:9203&quot;, &quot;http://172.17.107.187:9201&quot;,&quot;http://172.17.107.187:9202&quot;] index =&gt; &quot;export_log_index&quot; # decorate_events=true的作用，可以使用metadata中的数据 #user =&gt; &quot;elastic&quot; #password =&gt; &quot;escluter123456&quot; } } logback和和elasticsearch的交互 引入依赖 &lt;dependency&gt; &lt;groupId&gt;net.logstash.logback&lt;/groupId&gt; &lt;artifactId&gt;logstash-logback-encoder&lt;/artifactId&gt; &lt;version&gt;4.10&lt;/version&gt; &lt;/dependency&gt; 参数配置 input { # 应用日志 tcp{ type =&gt; &quot;app&quot; mode =&gt; &quot;server&quot; host =&gt; &quot;0.0.0.0&quot; port =&gt; 4560 codec =&gt; json } } output { elasticsearch { hosts =&gt; &quot;http://127.0.0.1:9200&quot; index =&gt; &quot;export_log_index&quot; } } 应用日志入口端口为4560，需要配置java客户端logstash入口 &lt;!-- 这个是控制台日志输出格式 方便调试对比--&gt; &lt;appender name=&quot;console&quot; class=&quot;ch.qos.logback.core.ConsoleAppender&quot;&gt; &lt;encoder&gt; &lt;pattern&gt;%d{yyyy-MM-dd HH:mm:ss} %contextName %-5level %logger{50} -%msg%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;!--开启tcp格式的logstash传输，通过TCP协议连接Logstash--&gt; &lt;appender name=&quot;STASH&quot; class=&quot;net.logstash.logback.appender.LogstashTcpSocketAppender&quot;&gt; &lt;destination&gt;127.0.0.1:9600&lt;/destination&gt; &lt;encoder class=&quot;net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder&quot;&gt; &lt;!--中文序列化--&gt; &lt;jsonFactoryDecorator class=&quot;net.logstash.logback.decorate.CharacterEscapesJsonFactoryDecorator&quot;&gt; &lt;escape&gt; &lt;targetCharacterCode&gt;10&lt;/targetCharacterCode&gt; &lt;escapeSequence&gt;\\u2028&lt;/escapeSequence&gt; &lt;/escape&gt; &lt;/jsonFactoryDecorator&gt; &lt;providers&gt; &lt;pattern&gt; &lt;pattern&gt; &lt;!--{ &quot;timestamp&quot;:&quot;%date{ISO8601}&quot;, &quot;user&quot;:&quot;test&quot;, &quot;message&quot;:&quot;[%d{yyyy-MM-dd HH:mm:ss.SSS}][%p][%t][%l{80}|%L]%m&quot;}%n }--&gt; { &quot;timestamp&quot;: &quot;%date{\\&quot;yyyy-MM-dd' 'HH:mm:ss,SSSZ\\&quot;}&quot;, &quot;level&quot;: &quot;%level&quot;, &quot;thread&quot;: &quot;%thread&quot;, &quot;class_name&quot;: &quot;%class&quot;, &quot;line_number&quot;: &quot;%line&quot;, &quot;message&quot;: &quot;%message&quot;, &quot;stack_trace&quot;: &quot;%exception{5}&quot;, &quot;req_id&quot;: &quot;%X{reqId}&quot;, &quot;elapsed_time&quot;: &quot;#asLong{%X{elapsedTime}}&quot; } &lt;/pattern&gt; &lt;/pattern&gt; &lt;/providers&gt; &lt;!--格式化输出：%d表示日期，%thread表示线程名，%-5level：级别从左显示5个字符宽度%msg：日志消息，%n是换行符--&gt; &lt;/encoder&gt; &lt;keepAliveDuration&gt;5 minutes&lt;/keepAliveDuration&gt; &lt;/appender&gt; &lt;root level=&quot;INFO&quot;&gt; &lt;appender-ref ref=&quot;STASH&quot;/&gt; &lt;appender-ref ref=&quot;console&quot;/&gt; &lt;/root&gt; docker 启动 Logstash #docker容器互访 运行容器的时候加上参数link docker run -id -p 5044:5044 --name logstash --link elasticsearch --link beats -v /usr/local/logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml -v /usr/local/logstash/conf.d/:/usr/share/logstash/conf.d/ logstash:7.4.1 Docker 安装 Filebeat 拉取镜像 拉取镜像的时候，需要注意的是, Filebeat 的版本最好与 elasticsearch 保持一致, 避免发生不必要的错误 docker pull store/elastic/filebeat:7.12.0 查看镜像 docker images 文件映射 下载默认官方配置文件 wget https://raw.githubusercontent.com/elastic/beats/7.12/deploy/docker/filebeat.docker.yml 注意：文件放在宿主机/data/elk/filebeat目录下 打开配置文件 vim filebeat.docker.yml，内容如下： # 日志输入配置 filebeat.inputs: - type: log enabled: true paths: # 需要收集的日志所在的位置，可使用通配符进行配置 #- /data/elk/*.log - /logs/*/*.log #日志输出配置(采用 logstash 收集日志，5044为logstash端口) output.logstash: hosts: ['192.168.12.183:5044'] docker运行Filebeat docker run --name filebeat --user=root -d --net somenetwork --volume=&quot;/usr/local/filebeat/log/nginx/:/var/log/nginx/&quot; --volume=&quot;/data/elk/filebeat/filebeat.docker.yml:/usr/share/filebeat/filebeat.yml&quot; --volume=&quot;/var/lib/docker/containers:/var/lib/docker/containers:ro&quot; --volume=&quot;/var/run/docker.sock:/var/run/docker.sock:ro&quot; store/elastic/filebeat:7.12.0 ","link":"https://tianxiawuhao.github.io/3D5Sr6L0c/"},{"title":"springBoot,mybatis,druid整合","content":"springBoot,mybatis,druid单数据源整合 项目框架 pom.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.4.5&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;groupId&gt;com.example&lt;/groupId&gt; &lt;artifactId&gt;datasources&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;datasources&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot&lt;/description&gt; &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.45&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.1.10&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.baomidou&lt;/groupId&gt; &lt;artifactId&gt;mybatis-plus-boot-starter&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;excludes&gt; &lt;exclude&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;/exclude&gt; &lt;/excludes&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/project&gt; application.yml spring: datasource: username: root password: 123456 url: jdbc:mysql://localhost:3306/datatest?characterEncoding=utf-8&amp;useSSl=false driver-class-name: com.mysql.jdbc.Driver type: com.alibaba.druid.pool.DruidDataSource druid: initialSize: 10 # 初始化时建立物理连接的个数。初始化发生在显示调用init方法，或者第一次getConnection时 minIdle: 10 # 最小连接池数量 maxActive: 200 # 最大连接池数量 maxWait: 60000 # 获取连接时最大等待时间，单位毫秒。配置了maxWait之后，缺省启用公平锁，并发效率会有所下降，如果需要可以通过配置 timeBetweenEvictionRunsMillis: 60000 # 关闭空闲连接的检测时间间隔.Destroy线程会检测连接的间隔时间，如果连接空闲时间大于等于minEvictableIdleTimeMillis则关闭物理连接。 minEvictableIdleTimeMillis: 300000 # 连接的最小生存时间.连接保持空闲而不被驱逐的最小时间 validationQuery: SELECT 1 FROM DUAL # 验证数据库服务可用性的sql.用来检测连接是否有效的sql 因数据库方言而差, 例如 oracle 应该写成 SELECT 1 FROM DUAL testWhileIdle: true # 申请连接时检测空闲时间，根据空闲时间再检测连接是否有效.建议配置为true，不影响性能，并且保证安全性。申请连接的时候检测，如果空闲时间大于timeBetweenEvictionRun testOnBorrow: false # 申请连接时直接检测连接是否有效.申请连接时执行validationQuery检测连接是否有效，做了这个配置会降低性能。 testOnReturn: false # 归还连接时检测连接是否有效.归还连接时执行validationQuery检测连接是否有效，做了这个配置会降低性能。 poolPreparedStatements: true # 开启PSCache maxPoolPreparedStatementPerConnectionSize: 20 #设置PSCache值 connectionErrorRetryAttempts: 3 # 连接出错后再尝试连接三次 breakAfterAcquireFailure: true # 数据库服务宕机自动重连机制 timeBetweenConnectErrorMillis: 300000 # 连接出错后重试时间间隔 asyncInit: true # 异步初始化策略 remove-abandoned: true # 是否自动回收超时连接 remove-abandoned-timeout: 1800 # 超时时间(以秒数为单位) transaction-query-timeout: 6000 # 事务超时时间 filters: stat,wall,log4j2 connectionProperties: druid.stat.mergeSql=true;druid.stat.slowSqlMillis=500 #mybatis是独立节点，需要单独配置 mybatis-plus: mapper-locations: classpath*:mapper/*.xml type-aliases-package: com.example.datasources.entity configuration: map-underscore-to-camel-case: true DruidConfig @Configuration public class DruidConfig { @Bean public ServletRegistrationBean statViewServlet(){ ServletRegistrationBean&lt;StatViewServlet&gt; bean = new ServletRegistrationBean&lt;StatViewServlet&gt;( new StatViewServlet(), &quot;/druid/*&quot; ); Map&lt;String,String&gt; iniParms=new HashMap&lt;&gt;( ); iniParms.put( &quot;loginUsername&quot;,&quot;admin&quot; );//登录druid的用户名 iniParms.put( &quot;loginPassword&quot;,&quot;123456&quot; );//登录druid的密码 iniParms.put(&quot;allow&quot;,&quot;&quot;);//默认允许所有 //iniParms.put( &quot;deny&quot;,&quot;192.168.***.***&quot; );//拒绝的ip地址 bean.setInitParameters( iniParms ); return bean; } @Bean public FilterRegistrationBean webStatFilter(){ FilterRegistrationBean bean= new FilterRegistrationBean(); bean.setFilter(new WebStatFilter()); Map&lt;String,String&gt; iniParms=new HashMap&lt;&gt;(); iniParms.put( &quot;excliusions&quot;, &quot;*.js,*.gif,*.jpg,*.bmp,*.png,*.css,*.ico,/druid/*&quot;);//使静态文件访问，还有/druid/* 的访问不被拦截 bean.setInitParameters( iniParms ); bean.setUrlPatterns( Arrays.asList(&quot;/*&quot;)); return bean; } } 访问Druid http://localhost:8080/druid/login.html springBoot,mybatis,druid多数据源整合 项目架构 pom.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.4.5&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;groupId&gt;com.example&lt;/groupId&gt; &lt;artifactId&gt;datasources&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;datasources&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot&lt;/description&gt; &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.45&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.1.10&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.baomidou&lt;/groupId&gt; &lt;artifactId&gt;mybatis-plus-boot-starter&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;excludes&gt; &lt;exclude&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;/exclude&gt; &lt;/excludes&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/project&gt; application.yml spring: datasource: #使用druid连接池 type: com.alibaba.druid.pool.DruidDataSource # 自定义的主数据源配置信息 primary: datasource: #druid相关配置 druid: #监控统计拦截的filters filters: stat driverClassName: com.mysql.jdbc.Driver #配置基本属性 url: jdbc:mysql://127.0.0.1:3306/primary_database?useUnicode=true&amp;characterEncoding=UTF-8&amp;allowMultiQueries=true&amp;autoReconnect=true&amp;useSSL=false username: root password: 123456 #配置初始化大小/最小/最大 initialSize: 1 minIdle: 1 maxActive: 20 #获取连接等待超时时间 maxWait: 60000 #间隔多久进行一次检测，检测需要关闭的空闲连接 timeBetweenEvictionRunsMillis: 60000 #一个连接在池中最小生存的时间 minEvictableIdleTimeMillis: 300000 validationQuery: SELECT 'x' testWhileIdle: true testOnBorrow: false testOnReturn: false #打开PSCache，并指定每个连接上PSCache的大小。oracle设为true，mysql设为false。分库分表较多推荐设置为false poolPreparedStatements: false maxPoolPreparedStatementPerConnectionSize: 20 # 自定义的从数据源配置信息 back: datasource: #druid相关配置 druid: #监控统计拦截的filters filters: stat driverClassName: com.mysql.jdbc.Driver #配置基本属性 url: jdbc:mysql://127.0.0.1:3306/back_database?useUnicode=true&amp;characterEncoding=UTF-8&amp;allowMultiQueries=true&amp;autoReconnect=true&amp;useSSL=false username: root password: 123456 #配置初始化大小/最小/最大 initialSize: 1 minIdle: 1 maxActive: 20 #获取连接等待超时时间 maxWait: 60000 #间隔多久进行一次检测，检测需要关闭的空闲连接 timeBetweenEvictionRunsMillis: 60000 #一个连接在池中最小生存的时间 minEvictableIdleTimeMillis: 300000 validationQuery: SELECT 'x' testWhileIdle: true testOnBorrow: false testOnReturn: false #打开PSCache，并指定每个连接上PSCache的大小。oracle设为true，mysql设为false。分库分表较多推荐设置为false poolPreparedStatements: false maxPoolPreparedStatementPerConnectionSize: 20 DruidConfig @Configuration public class DruidConfig { @Bean public ServletRegistrationBean statViewServlet(){ ServletRegistrationBean&lt;StatViewServlet&gt; bean = new ServletRegistrationBean&lt;StatViewServlet&gt;( new StatViewServlet(), &quot;/druid/*&quot; ); Map&lt;String,String&gt; iniParms=new HashMap&lt;&gt;( ); iniParms.put( &quot;loginUsername&quot;,&quot;admin&quot; );//登录druid的用户名 iniParms.put( &quot;loginPassword&quot;,&quot;123456&quot; );//登录druid的密码 iniParms.put(&quot;allow&quot;,&quot;&quot;);//默认允许所有 //iniParms.put( &quot;deny&quot;,&quot;192.168.***.***&quot; );//拒绝的ip地址 bean.setInitParameters( iniParms ); return bean; } @Bean public FilterRegistrationBean webStatFilter(){ FilterRegistrationBean bean= new FilterRegistrationBean(); bean.setFilter(new WebStatFilter()); Map&lt;String,String&gt; iniParms=new HashMap&lt;&gt;(); iniParms.put( &quot;excliusions&quot;, &quot;*.js,*.gif,*.jpg,*.bmp,*.png,*.css,*.ico,/druid/*&quot;);//使静态文件访问，还有/druid/* 的访问不被拦截 bean.setInitParameters( iniParms ); bean.setUrlPatterns( Arrays.asList(&quot;/*&quot;)); return bean; } } PrimaryDataBaseConfig /** * @Description: 主数据源配置类 */ @Data @Configuration // 前缀为primary.datasource.druid的配置信息 @ConfigurationProperties(prefix = &quot;primary.datasource.druid&quot;) @MapperScan(basePackages = PrimaryDataBaseConfig.PACKAGE, sqlSessionFactoryRef = &quot;primarySqlSessionFactory&quot;) public class PrimaryDataBaseConfig { /** * dao层的包路径 */ static final String PACKAGE = &quot;com.example.datasources.dao.primary&quot;; /** * mapper文件的相对路径 */ private static final String MAPPER_LOCATION = &quot;classpath:mapper/primary/*.xml&quot;; private String filters; private String url; private String username; private String password; private String driverClassName; private int initialSize; private int minIdle; private int maxActive; private long maxWait; private long timeBetweenEvictionRunsMillis; private long minEvictableIdleTimeMillis; private String validationQuery; private boolean testWhileIdle; private boolean testOnBorrow; private boolean testOnReturn; private boolean poolPreparedStatements; private int maxPoolPreparedStatementPerConnectionSize; // 主数据源使用@Primary注解进行标识 @Primary @Bean(name = &quot;primaryDataSource&quot;) public DataSource primaryDataSource() throws SQLException { DruidDataSource druid = new DruidDataSource(); // 监控统计拦截的filters druid.setFilters(filters); // 配置基本属性 druid.setDriverClassName(driverClassName); druid.setUsername(username); druid.setPassword(password); druid.setUrl(url); //初始化时建立物理连接的个数 druid.setInitialSize(initialSize); //最大连接池数量 druid.setMaxActive(maxActive); //最小连接池数量 druid.setMinIdle(minIdle); //获取连接时最大等待时间，单位毫秒。 druid.setMaxWait(maxWait); //间隔多久进行一次检测，检测需要关闭的空闲连接 druid.setTimeBetweenEvictionRunsMillis(timeBetweenEvictionRunsMillis); //一个连接在池中最小生存的时间 druid.setMinEvictableIdleTimeMillis(minEvictableIdleTimeMillis); //用来检测连接是否有效的sql druid.setValidationQuery(validationQuery); //建议配置为true，不影响性能，并且保证安全性。 druid.setTestWhileIdle(testWhileIdle); //申请连接时执行validationQuery检测连接是否有效 druid.setTestOnBorrow(testOnBorrow); druid.setTestOnReturn(testOnReturn); //是否缓存preparedStatement，也就是PSCache，oracle设为true，mysql设为false。分库分表较多推荐设置为false druid.setPoolPreparedStatements(poolPreparedStatements); // 打开PSCache时，指定每个连接上PSCache的大小 druid.setMaxPoolPreparedStatementPerConnectionSize(maxPoolPreparedStatementPerConnectionSize); return druid; } // 创建该数据源的事务管理 @Primary @Bean(name = &quot;primaryTransactionManager&quot;) public DataSourceTransactionManager primaryTransactionManager() throws SQLException { return new DataSourceTransactionManager(primaryDataSource()); } // 创建Mybatis的连接会话工厂实例 @Primary @Bean(name = &quot;primarySqlSessionFactory&quot;) public SqlSessionFactory primarySqlSessionFactory(@Qualifier(&quot;primaryDataSource&quot;) DataSource primaryDataSource) throws Exception { final SqlSessionFactoryBean sessionFactory = new SqlSessionFactoryBean(); sessionFactory.setDataSource(primaryDataSource); // 设置数据源bean sessionFactory.setMapperLocations(new PathMatchingResourcePatternResolver() .getResources(PrimaryDataBaseConfig.MAPPER_LOCATION)); // 设置mapper文件路径 return sessionFactory.getObject(); } } BackDataBaseConfig /** * @Description: 从数据源配置类 */ @Data @Configuration @ConfigurationProperties(prefix = &quot;back.datasource.druid&quot;) @MapperScan(basePackages = BackDataBaseConfig.PACKAGE, sqlSessionFactoryRef = &quot;backSqlSessionFactory&quot;) public class BackDataBaseConfig { /** * dao层的包路径 */ static final String PACKAGE = &quot;com.example.datasources.dao.back&quot;; /** * mapper文件的相对路径 */ private static final String MAPPER_LOCATION = &quot;classpath:mapper/back/*.xml&quot;; private String filters; private String url; private String username; private String password; private String driverClassName; private int initialSize; private int minIdle; private int maxActive; private long maxWait; private long timeBetweenEvictionRunsMillis; private long minEvictableIdleTimeMillis; private String validationQuery; private boolean testWhileIdle; private boolean testOnBorrow; private boolean testOnReturn; private boolean poolPreparedStatements; private int maxPoolPreparedStatementPerConnectionSize; @Bean(name = &quot;backDataSource&quot;) public DataSource backDataSource() throws SQLException { DruidDataSource druid = new DruidDataSource(); // 监控统计拦截的filters druid.setFilters(filters); // 配置基本属性 druid.setDriverClassName(driverClassName); druid.setUsername(username); druid.setPassword(password); druid.setUrl(url); //初始化时建立物理连接的个数 druid.setInitialSize(initialSize); //最大连接池数量 druid.setMaxActive(maxActive); //最小连接池数量 druid.setMinIdle(minIdle); //获取连接时最大等待时间，单位毫秒。 druid.setMaxWait(maxWait); //间隔多久进行一次检测，检测需要关闭的空闲连接 druid.setTimeBetweenEvictionRunsMillis(timeBetweenEvictionRunsMillis); //一个连接在池中最小生存的时间 druid.setMinEvictableIdleTimeMillis(minEvictableIdleTimeMillis); //用来检测连接是否有效的sql druid.setValidationQuery(validationQuery); //建议配置为true，不影响性能，并且保证安全性。 druid.setTestWhileIdle(testWhileIdle); //申请连接时执行validationQuery检测连接是否有效 druid.setTestOnBorrow(testOnBorrow); druid.setTestOnReturn(testOnReturn); //是否缓存preparedStatement，也就是PSCache，oracle设为true，mysql设为false。分库分表较多推荐设置为false druid.setPoolPreparedStatements(poolPreparedStatements); // 打开PSCache时，指定每个连接上PSCache的大小 druid.setMaxPoolPreparedStatementPerConnectionSize(maxPoolPreparedStatementPerConnectionSize); return druid; } @Bean(name = &quot;backTransactionManager&quot;) public DataSourceTransactionManager backTransactionManager() throws SQLException { return new DataSourceTransactionManager(backDataSource()); } @Bean(name = &quot;backSqlSessionFactory&quot;) public SqlSessionFactory backSqlSessionFactory(@Qualifier(&quot;backDataSource&quot;) DataSource backDataSource) throws Exception { final SqlSessionFactoryBean sessionFactory = new SqlSessionFactoryBean(); sessionFactory.setDataSource(backDataSource); sessionFactory.setMapperLocations(new PathMatchingResourcePatternResolver() .getResources(BackDataBaseConfig.MAPPER_LOCATION)); return sessionFactory.getObject(); } } 访问Druid http://localhost:8080/druid/login.html springBoot,mybatis,druid主从备份 项目架构 pom.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.4.5&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;groupId&gt;com.example&lt;/groupId&gt; &lt;artifactId&gt;datasources&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;datasources&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot&lt;/description&gt; &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-aop&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.45&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.1.10&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.baomidou&lt;/groupId&gt; &lt;artifactId&gt;mybatis-plus-boot-starter&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;excludes&gt; &lt;exclude&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;/exclude&gt; &lt;/excludes&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/project&gt; application.yml spring: datasource: #使用druid连接池 type: com.alibaba.druid.pool.DruidDataSource # 自定义的主数据源配置信息 primary: datasource: #druid相关配置 druid: #监控统计拦截的filters filters: stat driverClassName: com.mysql.jdbc.Driver #配置基本属性 url: jdbc:mysql://127.0.0.1:3306/primary_database?useUnicode=true&amp;characterEncoding=UTF-8&amp;allowMultiQueries=true&amp;autoReconnect=true&amp;useSSL=false username: root password: 123456 #配置初始化大小/最小/最大 initialSize: 1 minIdle: 1 maxActive: 20 #获取连接等待超时时间 maxWait: 60000 #间隔多久进行一次检测，检测需要关闭的空闲连接 timeBetweenEvictionRunsMillis: 60000 #一个连接在池中最小生存的时间 minEvictableIdleTimeMillis: 300000 validationQuery: SELECT 'x' testWhileIdle: true testOnBorrow: false testOnReturn: false #打开PSCache，并指定每个连接上PSCache的大小。oracle设为true，mysql设为false。分库分表较多推荐设置为false poolPreparedStatements: false maxPoolPreparedStatementPerConnectionSize: 20 # 自定义的从数据源配置信息 back: datasource: #druid相关配置 druid: #监控统计拦截的filters filters: stat driverClassName: com.mysql.jdbc.Driver #配置基本属性 url: jdbc:mysql://127.0.0.1:3306/back_database?useUnicode=true&amp;characterEncoding=UTF-8&amp;allowMultiQueries=true&amp;autoReconnect=true&amp;useSSL=false username: root password: 123456 #配置初始化大小/最小/最大 initialSize: 1 minIdle: 1 maxActive: 20 #获取连接等待超时时间 maxWait: 60000 #间隔多久进行一次检测，检测需要关闭的空闲连接 timeBetweenEvictionRunsMillis: 60000 #一个连接在池中最小生存的时间 minEvictableIdleTimeMillis: 300000 validationQuery: SELECT 'x' testWhileIdle: true testOnBorrow: false testOnReturn: false #打开PSCache，并指定每个连接上PSCache的大小。oracle设为true，mysql设为false。分库分表较多推荐设置为false poolPreparedStatements: false maxPoolPreparedStatementPerConnectionSize: 20 DruidConfig @Configuration public class DruidConfig { public final static String MAPPER_XML_PATH = &quot;classpath:mapper/*.xml&quot;; @ConfigurationProperties(prefix = &quot;master.datasource.druid&quot;) @Bean(name = &quot;masterDataSource&quot;) public DataSource masterDataSource() { return new DruidDataSource(); } @Bean public PlatformTransactionManager txManager(DataSource dynamicDataSource) { return new DataSourceTransactionManager(dynamicDataSource); } @ConfigurationProperties(prefix = &quot;slave.datasource.druid&quot;) @Bean public DataSource slaveDataSource(){ return new DruidDataSource(); } @Bean public DynamicDataSource dynamicDataSource(){ DynamicDataSource dynamicDataSource=new DynamicDataSource(); Map&lt;Object,Object&gt; map=new HashMap&lt;&gt;(); map.put(DbUtil.master,masterDataSource()); map.put(DbUtil.slave,slaveDataSource()); dynamicDataSource.setDefaultTargetDataSource(masterDataSource()); dynamicDataSource.setTargetDataSources(map); return dynamicDataSource; } @Bean public SqlSessionFactoryBean sqlSessionFactoryBean(DataSource dynamicDataSource) throws IOException { SqlSessionFactoryBean sqlSessionFactory = new SqlSessionFactoryBean(); sqlSessionFactory.setDataSource(dynamicDataSource); sqlSessionFactory.setMapperLocations(new PathMatchingResourcePatternResolver().getResources(MAPPER_XML_PATH)); return sqlSessionFactory; } @Bean public SqlSessionTemplate sqlSessionTemplate(SqlSessionFactoryBean sqlSessionFactoryBean) throws Exception { SqlSessionTemplate sqlSessionTemplate = new SqlSessionTemplate(sqlSessionFactoryBean.getObject()); return sqlSessionTemplate; } @Bean public ServletRegistrationBean statViewServlet(){ ServletRegistrationBean&lt;StatViewServlet&gt; bean = new ServletRegistrationBean&lt;StatViewServlet&gt;( new StatViewServlet(), &quot;/druid/*&quot; ); Map&lt;String,String&gt; iniParms=new HashMap&lt;&gt;( ); iniParms.put( &quot;loginUsername&quot;,&quot;admin&quot; );//登录druid的用户名 iniParms.put( &quot;loginPassword&quot;,&quot;123456&quot; );//登录druid的密码 iniParms.put(&quot;allow&quot;,&quot;&quot;);//默认允许所有 //iniParms.put( &quot;deny&quot;,&quot;192.168.***.***&quot; );//拒绝的ip地址 bean.setInitParameters( iniParms ); return bean; } @Bean public FilterRegistrationBean webStatFilter(){ FilterRegistrationBean bean= new FilterRegistrationBean(); bean.setFilter(new WebStatFilter()); Map&lt;String,String&gt; iniParms=new HashMap&lt;&gt;(); iniParms.put( &quot;excliusions&quot;, &quot;*.js,*.gif,*.jpg,*.bmp,*.png,*.css,*.ico,/druid/*&quot;);//使静态文件访问，还有/druid/* 的访问不被拦截 bean.setInitParameters( iniParms ); bean.setUrlPatterns( Arrays.asList(&quot;/*&quot;)); return bean; } }@Configuration public class DruidConfig { @Bean public ServletRegistrationBean statViewServlet(){ ServletRegistrationBean&lt;StatViewServlet&gt; bean = new ServletRegistrationBean&lt;StatViewServlet&gt;( new StatViewServlet(), &quot;/druid/*&quot; ); Map&lt;String,String&gt; iniParms=new HashMap&lt;&gt;( ); iniParms.put( &quot;loginUsername&quot;,&quot;admin&quot; );//登录druid的用户名 iniParms.put( &quot;loginPassword&quot;,&quot;123456&quot; );//登录druid的密码 iniParms.put(&quot;allow&quot;,&quot;&quot;);//默认允许所有 //iniParms.put( &quot;deny&quot;,&quot;192.168.***.***&quot; );//拒绝的ip地址 bean.setInitParameters( iniParms ); return bean; } @Bean public FilterRegistrationBean webStatFilter(){ FilterRegistrationBean bean= new FilterRegistrationBean(); bean.setFilter(new WebStatFilter()); Map&lt;String,String&gt; iniParms=new HashMap&lt;&gt;(); iniParms.put( &quot;excliusions&quot;, &quot;*.js,*.gif,*.jpg,*.bmp,*.png,*.css,*.ico,/druid/*&quot;);//使静态文件访问，还有/druid/* 的访问不被拦截 bean.setInitParameters( iniParms ); bean.setUrlPatterns( Arrays.asList(&quot;/*&quot;)); return bean; } } MasterDataSource /** * 自定义主数据库注解 */ @Target(ElementType.METHOD) @Retention(RetentionPolicy.RUNTIME) public @interface MasterDataSource { String value() default &quot;&quot;; } DatabaseAOP /** * aop从dao层判断使用哪个数据库 * @MasterDataSource标识使用主数据库 * 不标识使用从数据库 */ @Aspect @Component public class DatabaseAOP { @Pointcut(value = &quot;execution(* com.example.datasources.dao..*.*(..))&quot;) public void pointCut() { } @Before(&quot;pointCut()&quot;) public void before(JoinPoint joinPoint) { MethodSignature methodSignature = (MethodSignature) joinPoint.getSignature(); Method method = methodSignature.getMethod(); boolean isExist = method.isAnnotationPresent(MasterDataSource.class); if (!isExist) { DbUtil.setDb(DbUtil.slave); return; } DbUtil.setDb(DbUtil.master); } } DbUtil /** * 存储当前线程使用数据库标识 */ public class DbUtil { public static String master=&quot;master&quot;; public static String slave=&quot;slave&quot;; private static final ThreadLocal&lt;String&gt; threadLocal=new ThreadLocal(); public static void setDb(String db){ threadLocal.set(db); } public static String getDb(){ return threadLocal.get(); } } DynamicDataSource /** * spring的jdbc提供了动态数据源的入口 * 继承AbstractRoutingDataSource覆盖determineCurrentLookupKey()方法 返回当前使用数据库 */ @Slf4j public class DynamicDataSource extends AbstractRoutingDataSource { @Override protected Object determineCurrentLookupKey() { log.info(&quot;当前使用数据库：{}&quot;, DbUtil.getDb()); return DbUtil.getDb(); } } CityMapper /** * springBoot启动入口DatasourcesApplication注解@MapperScan(basePackages = &quot;com.example.datasources.dao&quot;) * 起到和@Mapper一样的作用 * 添加了@Mapper注解之后这个接口在编译时会生成相应的实现类，不再需要写mapper映射文件，可以按下方@Select(&quot;select * from city where city_name like CONCAT('%', #{cityName},'%')&quot;)实现功能，@Autowired注解也需要通过@Mapper注解实现接口的动态代理实现类 */ @Mapper @Component public interface CityMapper { /** * @MasterDataSource指定使用主数据库新增 * 不指定默认使用从数据库 */ @MasterDataSource void insertCity(City city); /** * 根据城市名称，查询城市信息 * * @param cityName 城市名 * 默认使用从数据库 */ //@Select(&quot;select * from city where city_name like CONCAT('%', #{cityName},'%')&quot;) List&lt;City&gt; selectByName(@Param(&quot;cityName&quot;) String cityName); } CityMapper.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt; &lt;mapper namespace=&quot;com.example.datasources.dao.CityMapper&quot;&gt; &lt;resultMap id=&quot;BaseResultMap&quot; type=&quot;com.example.datasources.entity.City&quot;&gt; &lt;id column=&quot;id&quot; jdbcType=&quot;INTEGER&quot; property=&quot;id&quot;/&gt; &lt;result column=&quot;city_name&quot; jdbcType=&quot;VARCHAR&quot; property=&quot;cityName&quot;/&gt; &lt;/resultMap&gt; &lt;sql id=&quot;Base_Column_List&quot;&gt; id,city_name &lt;/sql&gt; &lt;insert id=&quot;insertCity&quot; parameterType=&quot;com.example.datasources.entity.City&quot;&gt; INSERT into city (id,city_name) VALUES (#{id,jdbcType=INTEGER}, #{cityName,jdbcType=VARCHAR}); &lt;/insert&gt; &lt;select id=&quot;selectByName&quot; resultMap=&quot;BaseResultMap&quot;&gt; select &lt;include refid=&quot;Base_Column_List&quot;/&gt; from city where city_name like CONCAT('%', #{cityName},'%') &lt;/select&gt; &lt;/mapper&gt; ","link":"https://tianxiawuhao.github.io/Ari4Sf8A0/"},{"title":"SpringBoot基础","content":"一、SpringBoot简介 1.1 原有Spring优缺点分析 1.1.1 Spring的优点分析 Spring是Java企业版（Java Enterprise Edition，JEE，也称J2EE）的轻量级代替品。无需开发重量级的Enterprise JavaBean（EJB），Spring为企业级Java开发提供了一种相对简单的方法，通过依赖注入和面向切面编程，用简单的Java对象（Plain Old Java Object，POJO）实现了EJB的功能。 1.1.2 Spring的缺点分析 虽然Spring的组件代码是轻量级的，但它的配置却是重量级的。一开始，Spring用XML配置，而且是很多XML配置。Spring 2.5引入了基于注解的组件扫描，这消除了大量针对应用程序自身组件的显式XML配置。Spring 3.0引入了基于Java的配置，这是一种类型安全的可重构配置方式，可以代替XML。 所有这些配置都代表了开发时的损耗。因为在思考Spring特性配置和解决业务问题之间需要进行思维切换，所以编写配置挤占了编写应用程序逻辑的时间。和所有框架一样，Spring实用，但与此同时它要求的回报也不少。 除此之外，项目的依赖管理也是一件耗时耗力的事情。在环境搭建时，需要分析要导入哪些库的坐标，而且还需要分析导入与之有依赖关系的其他库的坐标，一旦选错了依赖的版本，随之而来的不兼容问题就会严重阻碍项目的开发进度。 1.2 SpringBoot的概述 1.2.1 SpringBoot解决上述Spring的缺点 SpringBoot对上述Spring的缺点进行的改善和优化，基于约定优于配置的思想，可以让开发人员不必在配置与逻辑业务之间进行思维的切换，全身心的投入到逻辑业务的代码编写中，从而大大提高了开发的效率，一定程度上缩短了项目周期。 1.2.2 SpringBoot的特点 为基于Spring的开发提供更快的入门体验 开箱即用，没有代码生成，也无需XML配置。同时也可以修改默认值来满足特定的需求 提供了一些大型项目中常见的非功能性特性，如嵌入式服务器、安全、指标，健康检测、外部配置等 SpringBoot不是对Spring功能上的增强，而是提供了一种快速使用Spring的方式 1.2.3 SpringBoot的核心功能 起步依赖 起步依赖本质上是一个Maven项目对象模型（Project Object Model，POM），定义了对其他库的传递依赖，这些东西加在一起即支持某项功能。 简单的说，起步依赖就是将具备某种功能的坐标打包到一起，并提供一些默认的功能。 自动配置 Spring Boot的自动配置是一个运行时（更准确地说，是应用程序启动时）的过程，考虑了众多因素，才决定Spring配置应该用哪个，不该用哪个。该过程是Spring自动完成的。 ​ 注意：起步依赖和自动配置的原理剖析会在第三章《SpringBoot原理分析》进行详细讲解 二、SpringBoot快速入门 2.1 代码实现 2.1.1 创建Maven工程 使用idea工具创建一个maven工程，该工程为普通的java工程即可 2.1.2 添加SpringBoot的起步依赖 SpringBoot要求，项目要继承SpringBoot的起步依赖spring-boot-starter-parent &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.0.1.RELEASE&lt;/version&gt; &lt;/parent&gt; SpringBoot要集成SpringMVC进行Controller的开发，所以项目要导入web的启动依赖 &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 2.1.3 编写SpringBoot引导类 要通过SpringBoot提供的引导类起步SpringBoot才可以进行访问 package com.itheima; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; @SpringBootApplication public class MySpringBootApplication { public static void main(String[] args) { SpringApplication.run(MySpringBootApplication.class); } } 2.1.4 编写Controller 在引导类MySpringBootApplication同级包或者子级包中创建QuickStartController package com.itheima.controller; import org.springframework.stereotype.Controller; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.ResponseBody; @Controller public class QuickStartController { @RequestMapping(&quot;/quick&quot;) @ResponseBody public String quick(){ return &quot;springboot 访问成功!&quot;; } } 2.1.5 测试 执行SpringBoot起步类的主方法，控制台打印日志如下： . ____ _ __ _ _ /\\\\ / ___'_ __ _ _(_)_ __ __ _ \\ \\ \\ \\ ( ( )\\___ | '_ | '_| | '_ \\/ _` | \\ \\ \\ \\ \\\\/ ___)| |_)| | | | | || (_| | ) ) ) ) ' |____| .__|_| |_|_| |_\\__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot :: (v2.0.1.RELEASE) 2018-05-08 14:29:59.714 INFO 5672 --- [ main] com.itheima.MySpringBootApplication : Starting MySpringBootApplication on DESKTOP-RRUNFUH with PID 5672 (C:\\Users\\muzimoo\\IdeaProjects\\IdeaTest\\springboot_quick\\target\\classes started by muzimoo in C:\\Users\\muzimoo\\IdeaProjects\\IdeaTest) ... ... ... o.s.w.s.handler.SimpleUrlHandlerMapping : Mapped URL path [/**] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler] 2018-05-08 14:30:03.126 INFO 5672 --- [ main] o.s.j.e.a.AnnotationMBeanExporter : Registering beans for JMX exposure on startup 2018-05-08 14:30:03.196 INFO 5672 --- [ main] o.s.b.w.embedded.tomcat.TomcatWebServer : Tomcat started on port(s): 8080 (http) with context path '' 2018-05-08 14:30:03.206 INFO 5672 --- [ main] com.itheima.MySpringBootApplication : Started MySpringBootApplication in 4.252 seconds (JVM running for 5.583) 通过日志发现，Tomcat started on port(s): 8080 (http) with context path '' tomcat已经起步，端口监听8080，web应用的虚拟工程名称为空 打开浏览器访问url地址为：http://localhost:8080/quick 2.2 快速入门解析 2.2.2 SpringBoot代码解析 @SpringBootApplication：标注SpringBoot的启动类，该注解具备多种功能（后面详细剖析） SpringApplication.run(MySpringBootApplication.class) 代表运行SpringBoot的启动类，参数为SpringBoot启动类的字节码对象 2.2.3 SpringBoot工程热部署 我们在开发中反复修改类、页面等资源，每次修改后都是需要重新启动才生效，这样每次启动都很麻烦，浪费了大量的时间，我们可以在修改代码后不重启就能生效，在 pom.xml 中添加如下配置就可以实现这样的功能，我们称之为热部署。 &lt;!--热部署配置--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;/dependency&gt; 注意：IDEA进行SpringBoot热部署失败原因 出现这种情况，并不是热部署配置问题，其根本原因是因为Intellij IEDA默认情况下不会自动编译，需要对IDEA进行自动编译的设置，如下： 然后 Shift+Ctrl+Alt+/，选择Registry 2.2.4 使用idea快速创建SpringBoot项目 通过idea快速创建的SpringBoot项目的pom.xml中已经导入了我们选择的web的起步依赖的坐标 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.itheima&lt;/groupId&gt; &lt;artifactId&gt;springboot_quick2&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;springboot_quick2&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot&lt;/description&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.0.1.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;9&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/project&gt; 可以使用快速入门的方式创建Controller进行访问，此处不再赘述 三、SpringBoot原理分析 3.1 起步依赖原理分析 3.1.1 分析spring-boot-starter-parent 按住Ctrl点击pom.xml中的spring-boot-starter-parent，跳转到了spring-boot-starter-parent的pom.xml，xml配置如下（只摘抄了部分重点配置）： &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt; &lt;version&gt;2.0.1.RELEASE&lt;/version&gt; &lt;relativePath&gt;../../spring-boot-dependencies&lt;/relativePath&gt; &lt;/parent&gt; 按住Ctrl点击pom.xml中的spring-boot-starter-dependencies，跳转到了spring-boot-starter-dependencies的pom.xml，xml配置如下（只摘抄了部分重点配置）： &lt;properties&gt; &lt;activemq.version&gt;5.15.3&lt;/activemq.version&gt; &lt;antlr2.version&gt;2.7.7&lt;/antlr2.version&gt; &lt;appengine-sdk.version&gt;1.9.63&lt;/appengine-sdk.version&gt; &lt;artemis.version&gt;2.4.0&lt;/artemis.version&gt; &lt;aspectj.version&gt;1.8.13&lt;/aspectj.version&gt; &lt;assertj.version&gt;3.9.1&lt;/assertj.version&gt; &lt;atomikos.version&gt;4.0.6&lt;/atomikos.version&gt; &lt;bitronix.version&gt;2.1.4&lt;/bitronix.version&gt; &lt;build-helper-maven-plugin.version&gt;3.0.0&lt;/build-helper-maven-plugin.version&gt; &lt;byte-buddy.version&gt;1.7.11&lt;/byte-buddy.version&gt; ... ... ... &lt;/properties&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot&lt;/artifactId&gt; &lt;version&gt;2.0.1.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-test&lt;/artifactId&gt; &lt;version&gt;2.0.1.RELEASE&lt;/version&gt; &lt;/dependency&gt; ... ... ... &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;build&gt; &lt;pluginManagement&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.jetbrains.kotlin&lt;/groupId&gt; &lt;artifactId&gt;kotlin-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${kotlin.version}&lt;/version&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.jooq&lt;/groupId&gt; &lt;artifactId&gt;jooq-codegen-maven&lt;/artifactId&gt; &lt;version&gt;${jooq.version}&lt;/version&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;version&gt;2.0.1.RELEASE&lt;/version&gt; &lt;/plugin&gt; ... ... ... &lt;/plugins&gt; &lt;/pluginManagement&gt; &lt;/build&gt; 从上面的spring-boot-starter-dependencies的pom.xml中我们可以发现，一部分坐标的版本、依赖管理、插件管理已经定义好，所以我们的SpringBoot工程继承spring-boot-starter-parent后已经具备版本锁定等配置了。所以起步依赖的作用就是进行依赖的传递。 3.1.2 分析spring-boot-starter-web 按住Ctrl点击pom.xml中的spring-boot-starter-web，跳转到了spring-boot-starter-web的pom.xml，xml配置如下（只摘抄了部分重点配置）： &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot; xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starters&lt;/artifactId&gt; &lt;version&gt;2.0.1.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;version&gt;2.0.1.RELEASE&lt;/version&gt; &lt;name&gt;Spring Boot Web Starter&lt;/name&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;2.0.1.RELEASE&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-json&lt;/artifactId&gt; &lt;version&gt;2.0.1.RELEASE&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-tomcat&lt;/artifactId&gt; &lt;version&gt;2.0.1.RELEASE&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.hibernate.validator&lt;/groupId&gt; &lt;artifactId&gt;hibernate-validator&lt;/artifactId&gt; &lt;version&gt;6.0.9.Final&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-web&lt;/artifactId&gt; &lt;version&gt;5.0.5.RELEASE&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;5.0.5.RELEASE&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/project&gt; 从上面的spring-boot-starter-web的pom.xml中我们可以发现，spring-boot-starter-web就是将web开发要使用的spring-web、spring-webmvc等坐标进行了“打包”，这样我们的工程只要引入spring-boot-starter-web起步依赖的坐标就可以进行web开发了，同样体现了依赖传递的作用。 3.2 自动配置原理解析 按住Ctrl点击查看启动类MySpringBootApplication上的注解@SpringBootApplication @SpringBootApplication public class MySpringBootApplication { public static void main(String[] args) { SpringApplication.run(MySpringBootApplication.class); } } 注解@SpringBootApplication的源码 @Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) @Documented @Inherited @SpringBootConfiguration @EnableAutoConfiguration @ComponentScan(excludeFilters = { @Filter(type = FilterType.CUSTOM, classes = TypeExcludeFilter.class), @Filter(type = FilterType.CUSTOM, classes = AutoConfigurationExcludeFilter.class) }) public @interface SpringBootApplication { /** * Exclude specific auto-configuration classes such that they will never be applied. * @return the classes to exclude */ @AliasFor(annotation = EnableAutoConfiguration.class) Class&lt;?&gt;[] exclude() default {}; ... ... ... } @SpringBootConfiguration：等同与@Configuration，既标注该类是Spring的一个配置类 @EnableAutoConfiguration：SpringBoot自动配置功能开启 按住Ctrl点击查看注解@EnableAutoConfiguration @Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) @Documented @Inherited @AutoConfigurationPackage @Import(AutoConfigurationImportSelector.class) public @interface EnableAutoConfiguration { ... ... ... } @Import(AutoConfigurationImportSelector.class) 导入了AutoConfigurationImportSelector类 按住Ctrl点击查看AutoConfigurationImportSelector源码 public String[] selectImports(AnnotationMetadata annotationMetadata) { ... ... ... List&lt;String&gt; configurations = getCandidateConfigurations(annotationMetadata, attributes); configurations = removeDuplicates(configurations); Set&lt;String&gt; exclusions = getExclusions(annotationMetadata, attributes); checkExcludedClasses(configurations, exclusions); configurations.removeAll(exclusions); configurations = filter(configurations, autoConfigurationMetadata); fireAutoConfigurationImportEvents(configurations, exclusions); return StringUtils.toStringArray(configurations); } protected List&lt;String&gt; getCandidateConfigurations(AnnotationMetadata metadata, AnnotationAttributes attributes) { List&lt;String&gt; configurations = SpringFactoriesLoader.loadFactoryNames( getSpringFactoriesLoaderFactoryClass(), getBeanClassLoader()); return configurations; } SpringFactoriesLoader.loadFactoryNames 方法的作用就是从META-INF/spring.factories文件中读取指定类对应的类名称列表 spring.factories 文件中有关自动配置的配置信息如下： ... ... ... org.springframework.boot.autoconfigure.web.reactive.function.client.WebClientAutoConfiguration,\\ org.springframework.boot.autoconfigure.web.servlet.DispatcherServletAutoConfiguration,\\ org.springframework.boot.autoconfigure.web.servlet.ServletWebServerFactoryAutoConfiguration,\\ org.springframework.boot.autoconfigure.web.servlet.error.ErrorMvcAutoConfiguration,\\ org.springframework.boot.autoconfigure.web.servlet.HttpEncodingAutoConfiguration,\\ org.springframework.boot.autoconfigure.web.servlet.MultipartAutoConfiguration,\\ ... ... ... 上面配置文件存在大量的以Configuration为结尾的类名称，这些类就是存有自动配置信息的类，而SpringApplication在获取这些类名后再加载 我们以ServletWebServerFactoryAutoConfiguration为例来分析源码： @Configuration @AutoConfigureOrder(Ordered.HIGHEST_PRECEDENCE) @ConditionalOnClass(ServletRequest.class) @ConditionalOnWebApplication(type = Type.SERVLET) @EnableConfigurationProperties(ServerProperties.class) @Import({ ServletWebServerFactoryAutoConfiguration.BeanPostProcessorsRegistrar.class, ServletWebServerFactoryConfiguration.EmbeddedTomcat.class, ServletWebServerFactoryConfiguration.EmbeddedJetty.class, ServletWebServerFactoryConfiguration.EmbeddedUndertow.class }) public class ServletWebServerFactoryAutoConfiguration { ... ... ... } @EnableConfigurationProperties(ServerProperties.class) 代表加载ServerProperties服务器配置属性类 进入ServerProperties.class源码如下： @ConfigurationProperties(prefix = &quot;server&quot;, ignoreUnknownFields = true) public class ServerProperties { /** * Server HTTP port. */ private Integer port; /** * Network address to which the server should bind. */ private InetAddress address; ... ... ... } prefix = &quot;server&quot; 表示SpringBoot配置文件中的前缀，SpringBoot会将配置文件中以server开始的属性映射到该类的字段中。映射关系如下： 四、SpringBoot的配置文件 4.1 SpringBoot配置文件类型 4.1.1 SpringBoot配置文件类型和作用 SpringBoot是基于约定的，所以很多配置都有默认值，但如果想使用自己的配置替换默认配置的话，就可以使用application.properties或者application.yml（application.yaml）进行配置。 SpringBoot默认会从Resources目录下加载application.properties或application.yml（application.yaml）文件 其中，application.properties文件是键值对类型的文件，之前一直在使用，所以此处不在对properties文件的格式进行阐述。除了properties文件外，SpringBoot还可以使用yml文件进行配置，下面对yml文件进行讲解。 4.1.2 application.yml配置文件 4.1.2.1 yml配置文件简介 YML文件格式是YAML (YAML Aint Markup Language)编写的文件格式，YAML是一种直观的能够被电脑识别的的数据数据序列化格式，并且容易被人类阅读，容易和脚本语言交互的，可以被支持YAML库的不同的编程语言程序导入，比如： C/C++, Ruby, Python, Java, Perl, C#, PHP等。YML文件是以数据为核心的，比传统的xml方式更加简洁。 YML文件的扩展名可以使用.yml或者.yaml。 4.1.2.2 yml配置文件的语法 4.1.2.2.1 配置普通数据 语法： key: value 示例代码： name: haohao 注意：value之前有一个空格 4.1.2.2.2 配置对象数据 语法： ​ key: ​ key1: value1 ​ key2: value2 ​ 或者： ​ key: {key1: value1,key2: value2} 示例代码： person: name: haohao age: 31 addr: beijing #或者 person: {name: haohao,age: 31,addr: beijing} 注意：key1前面的空格个数不限定，在yml语法中，相同缩进代表同一个级别 4.1.2.2.3 配置数组（List、Set）数据 语法： ​ key: ​ - value1 ​ - value2 或者： ​ key: [value1,value2] 示例代码： city: - beijing - tianjin - shanghai - chongqing #或者 city: [beijing,tianjin,shanghai,chongqing] #集合中的元素是对象形式 student: - name: zhangsan age: 18 score: 100 - name: lisi age: 28 score: 88 - name: wangwu age: 38 score: 90 注意：value1与之间的 - 之间存在一个空格 4.1.3 SpringBoot配置信息的查询 上面提及过，SpringBoot的配置文件，主要的目的就是对配置信息进行修改的，但在配置时的key从哪里去查询呢？我们可以查阅SpringBoot的官方文档,文档URL：https://docs.spring.io/spring-boot/docs/2.0.1.RELEASE/reference/htmlsingle/#common-application-properties 常用的配置摘抄如下： # QUARTZ SCHEDULER (QuartzProperties) spring.quartz.jdbc.initialize-schema=embedded # Database schema initialization mode. spring.quartz.jdbc.schema=classpath:org/quartz/impl/jdbcjobstore/tables_@@platform@@.sql # Path to the SQL file to use to initialize the database schema. spring.quartz.job-store-type=memory # Quartz job store type. spring.quartz.properties.*= # Additional Quartz Scheduler properties. # ---------------------------------------- # WEB PROPERTIES # ---------------------------------------- # EMBEDDED SERVER CONFIGURATION (ServerProperties) server.port=8080 # Server HTTP port. server.servlet.context-path= # Context path of the application. server.servlet.path=/ # Path of the main dispatcher servlet. # HTTP encoding (HttpEncodingProperties) spring.http.encoding.charset=UTF-8 # Charset of HTTP requests and responses. Added to the &quot;Content-Type&quot; header if not set explicitly. # JACKSON (JacksonProperties) spring.jackson.date-format= # Date format string or a fully-qualified date format class name. For instance, `yyyy-MM-dd HH:mm:ss`. # SPRING MVC (WebMvcProperties) spring.mvc.servlet.load-on-startup=-1 # Load on startup priority of the dispatcher servlet. spring.mvc.static-path-pattern=/** # Path pattern used for static resources. spring.mvc.view.prefix= # Spring MVC view prefix. spring.mvc.view.suffix= # Spring MVC view suffix. # DATASOURCE (DataSourceAutoConfiguration &amp; DataSourceProperties) spring.datasource.driver-class-name= # Fully qualified name of the JDBC driver. Auto-detected based on the URL by default. spring.datasource.password= # Login password of the database. spring.datasource.url= # JDBC URL of the database. spring.datasource.username= # Login username of the database. # JEST (Elasticsearch HTTP client) (JestProperties) spring.elasticsearch.jest.password= # Login password. spring.elasticsearch.jest.proxy.host= # Proxy host the HTTP client should use. spring.elasticsearch.jest.proxy.port= # Proxy port the HTTP client should use. spring.elasticsearch.jest.read-timeout=3s # Read timeout. spring.elasticsearch.jest.username= # Login username. 我们可以通过配置application.poperties 或者 application.yml 来修改SpringBoot的默认配置 例如： application.properties文件 server.port=8888 server.servlet.context-path=demo application.yml文件 server: port: 8888 servlet: context-path: /demo 4.2 配置文件与配置类的属性映射方式 4.2.1 使用注解@Value映射 我们可以通过@Value注解将配置文件中的值映射到一个Spring管理的Bean的字段上 例如： application.properties配置如下： person: name: zhangsan age: 18 或者，application.yml配置如下： person: name: zhangsan age: 18 实体Bean代码如下： @Controller public class QuickStartController { @Value(&quot;${person.name}&quot;) private String name; @Value(&quot;${person.age}&quot;) private Integer age; @RequestMapping(&quot;/quick&quot;) @ResponseBody public String quick(){ return &quot;springboot 访问成功! name=&quot;+name+&quot;,age=&quot;+age; } } 浏览器访问地址：http://localhost:8080/quick 结果如下： 4.2.2 使用注解@ConfigurationProperties映射 通过注解@ConfigurationProperties(prefix=&quot;配置文件中的key的前缀&quot;)可以将配置文件中的配置自动与实体进行映射 application.properties配置如下： person: name: zhangsan age: 18 或者，application.yml配置如下： person: name: zhangsan age: 18 实体Bean代码如下： @Controller @ConfigurationProperties(prefix = &quot;person&quot;) public class QuickStartController { private String name; private Integer age; @RequestMapping(&quot;/quick&quot;) @ResponseBody public String quick(){ return &quot;springboot 访问成功! name=&quot;+name+&quot;,age=&quot;+age; } public void setName(String name) { this.name = name; } public void setAge(Integer age) { this.age = age; } } 浏览器访问地址：http://localhost:8080/quick 结果如下： 注意：使用@ConfigurationProperties方式可以进行配置文件与实体字段的自动映射，但需要字段必须提供set方法才可以，而使用@Value注解修饰的字段不需要提供set方法 五、SpringBoot与整合其他技术 5.1 SpringBoot整合Mybatis 5.1.1 添加Mybatis的起步依赖 &lt;!--mybatis起步依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.1.1&lt;/version&gt; &lt;/dependency&gt; 5.1.2 添加数据库驱动坐标 &lt;!-- MySQL连接驱动 --&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;/dependency&gt; 5.1.3 添加数据库连接信息 在application.properties中添加数据量的连接信息 #DB Configuration: spring.datasource.driverClassName=com.mysql.jdbc.Driver spring.datasource.url=jdbc:mysql://127.0.0.1:3306/test?useUnicode=true&amp;characterEncoding=utf8 spring.datasource.username=root spring.datasource.password=root 5.1.4 创建user表 在test数据库中创建user表 -- ---------------------------- -- Table structure for `user` -- ---------------------------- DROP TABLE IF EXISTS `user`; CREATE TABLE `user` ( `id` int(11) NOT NULL AUTO_INCREMENT, `username` varchar(50) DEFAULT NULL, `password` varchar(50) DEFAULT NULL, `name` varchar(50) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=10 DEFAULT CHARSET=utf8; -- ---------------------------- -- Records of user -- ---------------------------- INSERT INTO `user` VALUES ('1', 'zhangsan', '123', '张三'); INSERT INTO `user` VALUES ('2', 'lisi', '123', '李四'); 5.1.5 创建实体Bean public class User { // 主键 private Long id; // 用户名 private String username; // 密码 private String password; // 姓名 private String name; //此处省略getter和setter方法 .. .. } 5.1.6 编写Mapper @Mapper public interface UserMapper { public List&lt;User&gt; queryUserList(); } 注意：@Mapper标记该类是一个mybatis的mapper接口，可以被spring boot自动扫描到spring上下文中 5.1.7 配置Mapper映射文件 在src\\main\\resources\\mapper路径下加入UserMapper.xml配置文件&quot; &lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot; ?&gt; &lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot; &gt; &lt;mapper namespace=&quot;com.itheima.mapper.UserMapper&quot;&gt; &lt;select id=&quot;queryUserList&quot; resultType=&quot;user&quot;&gt; select * from user &lt;/select&gt; &lt;/mapper&gt; 5.1.8 在application.properties中添加mybatis的信息 #spring集成Mybatis环境 #pojo别名扫描包 mybatis.type-aliases-package=com.itheima.domain #加载Mybatis映射文件 mybatis.mapper-locations=classpath:mapper/*Mapper.xml 5.1.9 编写测试Controller @Controller public class MapperController { @Autowired private UserMapper userMapper; @RequestMapping(&quot;/queryUser&quot;) @ResponseBody public List&lt;User&gt; queryUser(){ List&lt;User&gt; users = userMapper.queryUserList(); return users; } } 5.1.10 测试 5.2 SpringBoot整合Junit 5.2.1 添加Junit的起步依赖 &lt;!--测试的起步依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; 5.2.2 编写测试类 package com.itheima.test; import com.itheima.MySpringBootApplication; import com.itheima.domain.User; import com.itheima.mapper.UserMapper; import org.junit.Test; import org.junit.runner.RunWith; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.boot.test.context.SpringBootTest; import org.springframework.test.context.junit4.SpringRunner; import java.util.List; @RunWith(SpringRunner.class) @SpringBootTest(classes = MySpringBootApplication.class) public class MapperTest { @Autowired private UserMapper userMapper; @Test public void test() { List&lt;User&gt; users = userMapper.queryUserList(); System.out.println(users); } } 其中， SpringRunner继承自SpringJUnit4ClassRunner，使用哪一个Spring提供的测试测试引擎都可以 public final class SpringRunner extends SpringJUnit4ClassRunner @SpringBootTest的属性指定的是引导类的字节码对象 5.2.3 控制台打印信息 5.3 SpringBoot整合Spring Data JPA 5.3.1 添加Spring Data JPA的起步依赖 &lt;!-- springBoot JPA的起步依赖 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt; &lt;/dependency&gt; 5.3.2 添加数据库驱动依赖 &lt;!-- MySQL连接驱动 --&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;/dependency&gt; 5.3.3 在application.properties中配置数据库和jpa的相关属性 #DB Configuration: spring.datasource.driverClassName=com.mysql.jdbc.Driver spring.datasource.url=jdbc:mysql://127.0.0.1:3306/test?useUnicode=true&amp;characterEncoding=utf8 spring.datasource.username=root spring.datasource.password=root #JPA Configuration: spring.jpa.database=MySQL spring.jpa.show-sql=true spring.jpa.generate-ddl=true spring.jpa.hibernate.ddl-auto=update spring.jpa.hibernate.naming_strategy=org.hibernate.cfg.ImprovedNamingStrategy 5.3.4 创建实体配置实体 @Entity public class User { // 主键 @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; // 用户名 private String username; // 密码 private String password; // 姓名 private String name; //此处省略setter和getter方法... ... } 5.3.5 编写UserRepository public interface UserRepository extends JpaRepository&lt;User,Long&gt;{ public List&lt;User&gt; findAll(); } 5.3.6 编写测试类 @RunWith(SpringRunner.class) @SpringBootTest(classes=MySpringBootApplication.class) public class JpaTest { @Autowired private UserRepository userRepository; @Test public void test(){ List&lt;User&gt; users = userRepository.findAll(); System.out.println(users); } } 5.3.7 控制台打印信息 注意：如果是jdk9，执行报错如下： 原因：jdk缺少相应的jar 解决方案：手动导入对应的maven坐标，如下： &lt;!--jdk9需要导入如下坐标--&gt; &lt;dependency&gt; &lt;groupId&gt;javax.xml.bind&lt;/groupId&gt; &lt;artifactId&gt;jaxb-api&lt;/artifactId&gt; &lt;version&gt;2.3.0&lt;/version&gt; &lt;/dependency&gt; 5.4 SpringBoot整合Redis 5.4.1 添加redis的起步依赖 &lt;!-- 配置使用redis启动器 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt; &lt;/dependency&gt; 5.4.2 配置redis的连接信息 #Redis spring.redis.host=127.0.0.1 spring.redis.port=6379 5.4.3 注入RedisTemplate测试redis操作 @RunWith(SpringRunner.class) @SpringBootTest(classes = SpringbootJpaApplication.class) public class RedisTest { @Autowired private UserRepository userRepository; @Autowired private RedisTemplate&lt;String, String&gt; redisTemplate; @Test public void test() throws JsonProcessingException { //从redis缓存中获得指定的数据 String userListData = redisTemplate.boundValueOps(&quot;user.findAll&quot;).get(); //如果redis中没有数据的话 if(null==userListData){ //查询数据库获得数据 List&lt;User&gt; all = userRepository.findAll(); //转换成json格式字符串 ObjectMapper om = new ObjectMapper(); userListData = om.writeValueAsString(all); //将数据存储到redis中，下次在查询直接从redis中获得数据，不用在查询数据库 redisTemplate.boundValueOps(&quot;user.findAll&quot;).set(userListData); System.out.println(&quot;===============从数据库获得数据===============&quot;); }else{ System.out.println(&quot;===============从redis缓存中获得数据===============&quot;); } System.out.println(userListData); } } ","link":"https://tianxiawuhao.github.io/RosSxJXYD/"},{"title":"SpringBoot自动装配原理","content":"SpringBoot自动配置 从代码里看项目SpringBoot的项目启动类只有一个注解@SpringBootApplication和一个run方法。 @SpringBootApplication public class Application { public static void main(String[] args) { SpringApplication.run(Application.class, args); } } 直接看@SpringBootApplication的代码： @Target({ElementType.TYPE}) @Retention(RetentionPolicy.RUNTIME) @Documented @Inherited @SpringBootConfiguration @EnableAutoConfiguration @ComponentScan( excludeFilters = {@Filter( type = FilterType.CUSTOM, classes = {TypeExcludeFilter.class} ), @Filter( type = FilterType.CUSTOM, classes = {AutoConfigurationExcludeFilter.class} )} ) public @interface SpringBootApplication { @AliasFor( annotation = EnableAutoConfiguration.class, attribute = &quot;exclude&quot; ) Class&lt;?&gt;[] exclude() default {}; @AliasFor( annotation = EnableAutoConfiguration.class, attribute = &quot;excludeName&quot; ) String[] excludeName() default {}; @AliasFor( annotation = ComponentScan.class, attribute = &quot;basePackages&quot; ) String[] scanBasePackages() default {}; @AliasFor( annotation = ComponentScan.class, attribute = &quot;basePackageClasses&quot; ) Class&lt;?&gt;[] scanBasePackageClasses() default {}; } @SpringBootApplication：包含了@SpringBootConfiguration（打开是@Configuration），@EnableAutoConfiguration，@ComponentScan注解。 @Configuration JavaConfig形式的Spring Ioc容器的配置类使用的那个@Configuration，SpringBoot社区推荐使用基于JavaConfig的配置形式，所以，这里的启动类标注了@Configuration之后，本身其实也是一个IoC容器的配置类。 对比一下传统XML方式和config配置方式的区别： XML声明和定义配置方式： &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-3.0.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-3.0.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-3.0.xsd &quot;&gt; &lt;bean id=&quot;app&quot; class=&quot;com...&quot; /&gt; 用一个过滤器举例，JavaConfig的配置方式是这样： @Configuration public class DruidConfiguration { @Bean public FilterRegistrationBean statFilter(){ //创建过滤器 FilterRegistrationBean filterRegistrationBean = new FilterRegistrationBean(new WebStatFilter()); //设置过滤器过滤路径 filterRegistrationBean.addUrlPatterns(&quot;/*&quot;); //忽略过滤的形式 filterRegistrationBean.addInitParameter(&quot;exclusions&quot;,&quot;*.js,*.gif,*.jpg,*.png,*.css,*.ico,/druid/*&quot;); return filterRegistrationBean; } } 任何一个标注了@Configuration的Java类定义都是一个JavaConfig配置类。 任何一个标注了@Bean的方法，其返回值将作为一个bean定义注册到Spring的IoC容器，方法名将默认成该bean定义的id。 @ComponentScan @ComponentScan对应XML配置中的元素，@ComponentScan的功能其实就是自动扫描并加载符合条件的组件（比如@Component和@Repository等）或者bean定义，最终将这些bean定义加载到IoC容器中。 我们可以通过basePackages等属性来细粒度的定制@ComponentScan自动扫描的范围，如果不指定，则默认Spring框架实现会从声明@ComponentScan所在类的package进行扫描。 注：所以SpringBoot的启动类最好是放在root package下，因为默认不指定basePackages。 @EnableAutoConfiguration （核心内容）看英文意思就是自动配置，概括一下就是，借助@Import的帮助，将所有符合自动配置条件的bean定义加载到IoC容器。 @Target({ElementType.TYPE}) @Retention(RetentionPolicy.RUNTIME) @Documented @Inherited @AutoConfigurationPackage @Import({EnableAutoConfigurationImportSelector.class}) public @interface EnableAutoConfiguration { String ENABLED_OVERRIDE_PROPERTY = &quot;spring.boot.enableautoconfiguration&quot;; Class&lt;?&gt;[] exclude() default {}; String[] excludeName() default {}; } 里面最关键的是@Import(EnableAutoConfigurationImportSelector.class)，借助EnableAutoConfigurationImportSelector，@EnableAutoConfiguration可以帮助SpringBoot应用将所有符合条件的@Configuration配置都加载到当前SpringBoot创建并使用的IoC容器。该配置模块的主要使用到了SpringFactoriesLoader。 SpringFactoriesLoader详解 SpringFactoriesLoader为Spring工厂加载器，该对象提供了loadFactoryNames方法，入参为factoryClass和classLoader即需要传入工厂类名称和对应的类加载器，方法会根据指定的classLoader，加载该类加器搜索路径下的指定文件，即spring.factories文件，传入的工厂类为接口，而文件中对应的类则是接口的实现类，或最终作为实现类。 public abstract class SpringFactoriesLoader { private static final Log logger = LogFactory.getLog(SpringFactoriesLoader.class); public static final String FACTORIES_RESOURCE_LOCATION = &quot;META-INF/spring.factories&quot;; public SpringFactoriesLoader() { } public static &lt;T&gt; List&lt;T&gt; loadFactories(Class&lt;T&gt; factoryClass, ClassLoader classLoader) { Assert.notNull(factoryClass, &quot;'factoryClass' must not be null&quot;); ClassLoader classLoaderToUse = classLoader; if (classLoader == null) { classLoaderToUse = SpringFactoriesLoader.class.getClassLoader(); } List&lt;String&gt; factoryNames = loadFactoryNames(factoryClass, classLoaderToUse); if (logger.isTraceEnabled()) { logger.trace(&quot;Loaded [&quot; + factoryClass.getName() + &quot;] names: &quot; + factoryNames); } List&lt;T&gt; result = new ArrayList(factoryNames.size()); Iterator var5 = factoryNames.iterator(); while(var5.hasNext()) { String factoryName = (String)var5.next(); result.add(instantiateFactory(factoryName, factoryClass, classLoaderToUse)); } AnnotationAwareOrderComparator.sort(result); return result; } public static List&lt;String&gt; loadFactoryNames(Class&lt;?&gt; factoryClass, ClassLoader classLoader) { String factoryClassName = factoryClass.getName(); try { Enumeration&lt;URL&gt; urls = classLoader != null ? classLoader.getResources(&quot;META-INF/spring.factories&quot;) : ClassLoader.getSystemResources(&quot;META-INF/spring.factories&quot;); ArrayList result = new ArrayList(); while(urls.hasMoreElements()) { URL url = (URL)urls.nextElement(); Properties properties = PropertiesLoaderUtils.loadProperties(new UrlResource(url)); String factoryClassNames = properties.getProperty(factoryClassName); result.addAll(Arrays.asList(StringUtils.commaDelimitedListToStringArray(factoryClassNames))); } return result; } catch (IOException var8) { throw new IllegalArgumentException(&quot;Unable to load [&quot; + factoryClass.getName() + &quot;] factories from location [&quot; + &quot;META-INF/spring.factories&quot; + &quot;]&quot;, var8); } } private static &lt;T&gt; T instantiateFactory(String instanceClassName, Class&lt;T&gt; factoryClass, ClassLoader classLoader) { try { Class&lt;?&gt; instanceClass = ClassUtils.forName(instanceClassName, classLoader); if (!factoryClass.isAssignableFrom(instanceClass)) { throw new IllegalArgumentException(&quot;Class [&quot; + instanceClassName + &quot;] is not assignable to [&quot; + factoryClass.getName() + &quot;]&quot;); } else { Constructor&lt;?&gt; constructor = instanceClass.getDeclaredConstructor(); ReflectionUtils.makeAccessible(constructor); return constructor.newInstance(); } } catch (Throwable var5) { throw new IllegalArgumentException(&quot;Unable to instantiate factory class: &quot; + factoryClass.getName(), var5); } } } 所以文件中一般为如下图这种一对多的类名集合，获取到这些实现类的类名后，loadFactoryNames方法返回类名集合，方法调用方得到这些集合后，再通过反射获取这些类的类对象、构造方法，最终生成实例。 下图有助于我们形象理解自动配置流程（盗个图） AutoConfigurationImportSelector 继续上面讲的AutoConfigurationImportSelector.class。该类主要关注selectImports方法 public String[] selectImports(AnnotationMetadata annotationMetadata) { if (!this.isEnabled(annotationMetadata)) { return NO_IMPORTS; } else { try { AutoConfigurationMetadata autoConfigurationMetadata = AutoConfigurationMetadataLoader.loadMetadata(this.beanClassLoader); AnnotationAttributes attributes = this.getAttributes(annotationMetadata); List&lt;String&gt; configurations = this.getCandidateConfigurations(annotationMetadata, attributes); configurations = this.removeDuplicates(configurations); configurations = this.sort(configurations, autoConfigurationMetadata); Set&lt;String&gt; exclusions = this.getExclusions(annotationMetadata, attributes); this.checkExcludedClasses(configurations, exclusions); configurations.removeAll(exclusions); configurations = this.filter(configurations, autoConfigurationMetadata); this.fireAutoConfigurationImportEvents(configurations, exclusions); return (String[])configurations.toArray(new String[configurations.size()]); } catch (IOException var6) { throw new IllegalStateException(var6); } } } 该方法在springboot启动流程——bean实例化前被执行，返回要实例化的类信息列表。如果获取到类信息，spring可以通过类加载器将类加载到jvm中，现在我们已经通过spring-boot的starter依赖方式依赖了我们需要的组件，那么这些组件的类信息在select方法中就可以被获取到。 protected List&lt;String&gt; getCandidateConfigurations(AnnotationMetadata metadata, AnnotationAttributes attributes) { List&lt;String&gt; configurations = SpringFactoriesLoader.loadFactoryNames(this.getSpringFactoriesLoaderFactoryClass(), this.getBeanClassLoader()); Assert.notEmpty(configurations, &quot;No auto configuration classes found in META-INF/spring.factories. If you are using a custom packaging, make sure that file is correct.&quot;); return configurations; } 方法中的getCandidateConfigurations方法，其返回一个自动配置类的类名列表，方法调用了loadFactoryNames方法，查看该方法 public static List&lt;String&gt; loadFactoryNames(Class&lt;?&gt; factoryClass, ClassLoader classLoader) { String factoryClassName = factoryClass.getName(); try { Enumeration&lt;URL&gt; urls = classLoader != null ? classLoader.getResources(&quot;META-INF/spring.factories&quot;) : ClassLoader.getSystemResources(&quot;META-INF/spring.factories&quot;); ArrayList result = new ArrayList(); while(urls.hasMoreElements()) { URL url = (URL)urls.nextElement(); Properties properties = PropertiesLoaderUtils.loadProperties(new UrlResource(url)); String factoryClassNames = properties.getProperty(factoryClassName); result.addAll(Arrays.asList(StringUtils.commaDelimitedListToStringArray(factoryClassNames))); } return result; } catch (IOException var8) { throw new IllegalArgumentException(&quot;Unable to load [&quot; + factoryClass.getName() + &quot;] factories from location [&quot; + &quot;META-INF/spring.factories&quot; + &quot;]&quot;, var8); } } 自动配置器会跟根据传入的factoryClass.getName()到项目系统路径下所有的spring.factories文件中找到相应的key，从而加载里面的类。我们就选取这个mybatis-spring-boot-autoconfigure下的spring.factories文件 Auto Configure org.springframework.boot.autoconfigure.EnableAutoConfiguration= org.mybatis.spring.boot.autoconfigure.MybatisAutoConfiguration 进入org.mybatis.spring.boot.autoconfigure.MybatisAutoConfiguration中，又是一堆注解 @org.springframework.context.annotation.Configuration @ConditionalOnClass({SqlSessionFactory.class, SqlSessionFactoryBean.class}) @ConditionalOnBean({DataSource.class}) @EnableConfigurationProperties({MybatisProperties.class}) @AutoConfigureAfter({DataSourceAutoConfiguration.class}) public class MybatisAutoConfiguration { private static final Logger logger = LoggerFactory.getLogger(MybatisAutoConfiguration.class); private final MybatisProperties properties; private final Interceptor[] interceptors; private final ResourceLoader resourceLoader; private final DatabaseIdProvider databaseIdProvider; private final List&lt;ConfigurationCustomizer&gt; configurationCustomizers; @Configuration是一个通过注解标注的springBean， @ConditionalOnClass({ SqlSessionFactory.class, SqlSessionFactoryBean.class})这个注解的意思是：当存在SqlSessionFactory.class, SqlSessionFactoryBean.class这两个类时才解析MybatisAutoConfiguration配置类,否则不解析这一个配置类。我们需要mybatis为我们返回会话对象，就必须有会话工厂相关类 @CondtionalOnBean(DataSource.class)只处理已经被声明为bean的dataSource @ConditionalOnMissingBean(MapperFactoryBean.class)这个注解的意思是如果容器中不存在name指定的bean则创建bean注入，否则不执行以上配置可以保证sqlSessionFactory、sqlSessionTemplate、dataSource等mybatis所需的组件均可被自动配置，@Configuration注解已经提供了Spring的上下文环境，所以以上组件的配置方式与Spring启动时通过mybatis.xml文件进行配置起到一个效果。 只要一个基于SpringBoot项目的类路径下存在SqlSessionFactory.class, SqlSessionFactoryBean.class，并且容器中已经注册了dataSourceBean，就可以触发自动化配置，意思说我们只要在maven的项目中加入了mybatis所需要的若干依赖，就可以触发自动配置，但引入mybatis原生依赖的话，每集成一个功能都要去修改其自动化配置类，那就得不到开箱即用的效果了。所以Spring-boot为我们提供了统一的starter可以直接配置好相关的类，触发自动配置所需的依赖(mybatis)如下： &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; 因为maven依赖的传递性，我们只要依赖starter就可以依赖到所有需要自动配置的类，实现开箱即用的功能。也体现出Springboot简化了Spring框架带来的大量XML配置以及复杂的依赖管理，让开发人员可以更加关注业务逻辑的开发。 再贴个盗的图SpringBoot的启动结构图 ","link":"https://tianxiawuhao.github.io/YkvDN4zR3/"},{"title":"mysql概述六","content":"数据库优化 为什么要优化 系统的吞吐量瓶颈往往出现在数据库的访问速度上 随着应用程序的运行，数据库的中的数据会越来越多，处理时间会相应变慢 数据是存放在磁盘上的，读写速度无法和内存相比 优化原则：减少系统瓶颈，减少资源占用，增加系统的反应速度。 数据库结构优化 一个好的数据库设计方案对于数据库的性能往往会起到事半功倍的效果。需要考虑数据冗余、查询和更新的速度、字段的数据类型是否合理等多方面的内容。 将字段很多的表分解成多个表 对于字段较多的表，如果有些字段的使用频率很低，可以将这些字段分离出来形成新表。因为当一个表的数据量很大时，会由于使用频率低的字段的存在而变慢。 增加中间表 对于需要经常联合查询的表，可以建立中间表以提高查询效率。通过建立中间表，将需要通过联合查询的数据插入到中间表中，然后将原来的联合查询改为对中间表的查询。 增加冗余字段 设计数据表时应尽量遵循范式理论的规约，尽可能的减少冗余字段，让数据库设计看起来精致、优雅。但是，合理的加入冗余字段可以提高查询速度。表的规范化程度越高，表和表之间的关系越多，需要连接查询的情况也就越多，性能也就越差。 注意：冗余字段的值在一个表中修改了，就要想办法在其他表中更新，否则就会导致数据不一致的问题。 MySQL数据库cpu飙满的话怎么处理 当 cpu 飙满时，先用操作系统命令 top 命令观察是不是 mysqld 占用导致的，如果不是，找出占用高的进程，并进行相关处理。 如果是 mysqld 造成的， show processlist，看看里面跑的 session 情况，是不是有消耗资源的 sql 在运行。找出消耗高的 sql，看看执行计划是否准确， index 是否缺失，或者实在是数据量太大造成。 一般来说，肯定要 kill 掉这些线程(同时观察 cpu 使用率是否下降)，等进行相应的调整(比如说加索引、改 sql、改内存参数)之后，再重新跑这些 SQL。 也有可能是每个 sql 消耗资源并不多，但是突然之间，有大量的 session 连进来导致 cpu 飙升，这种情况就需要跟应用一起来分析为何连接数会激增，再做出相应的调整，比如说限制连接数等 大表优化 当MySQL单表记录数过大时，数据库的CRUD性能会明显下降，一些常见的优化措施如下： 限定数据的范围： 务必禁止不带任何限制数据范围条件的查询语句。比如：我们当用户在查询订单历史的时候，我们可以控制在一个月的范围内。； 读/写分离： 经典的数据库拆分方案，主库负责写，从库负责读； 缓存： 使用MySQL的缓存，另外对重量级、更新少的数据可以考虑使用应用级别的缓存； 还有就是通过分库分表的方式进行优化，主要有垂直分表和水平分表 垂直分表： 根据数据库里面数据表的相关性进行拆分。 例如，用户表中既有用户的登录信息又有用户的基本信息，可以将用户表拆分成两个单独的表，甚至放到单独的库做分库。 简单来说垂直拆分是指数据表列的拆分，把一张列比较多的表拆分为多张表。 如下图所示，这样来说大家应该就更容易理解了。 垂直拆分的优点： 可以使得行数据变小，在查询时减少读取的Block数，减少I/O次数。此外，垂直分区可以简化表的结构，易于维护。 垂直拆分的缺点： 主键会出现冗余，需要管理冗余列，并会引起Join操作，可以通过在应用层进行Join来解决。此外，垂直分区会让事务变得更加复杂； 适用场景 1、如果一个表中某些列常用，另外一些列不常用 2、可以使数据行变小，一个数据页能存储更多数据，查询时减少I/O次数 缺点 有些分表的策略基于应用层的逻辑算法，一旦逻辑算法改变，整个分表逻辑都会改变，扩展性较差 对于应用层来说，逻辑算法增加开发成本 管理冗余列，查询所有数据需要join操作 水平分表： 保持数据表结构不变，通过某种策略存储数据分片。这样每一片数据分散到不同的表或者库中，达到了分布式的目的。 水平拆分可以支撑非常大的数据量。 水平拆分是指数据表行的拆分，表的行数超过200万行时，就会变慢，这时可以把一张的表的数据拆成多张表来存放。举个例子：我们可以将用户信息表拆分成多个用户信息表，这样就可以避免单一表数据量过大对性能造成影响。 水品拆分可以支持非常大的数据量。需要注意的一点是:分表仅仅是解决了单一表数据过大的问题，但由于表的数据还是在同一台机器上，其实对于提升MySQL并发能力没有什么意义，所以 水平拆分最好分库 。 水平拆分能够 支持非常大的数据量存储，应用端改造也少，但 分片事务难以解决 ，跨界点Join性能较差，逻辑复杂。 《Java工程师修炼之道》的作者推荐 尽量不要对数据进行分片，因为拆分会带来逻辑、部署、运维的各种复杂度 ，一般的数据表在优化得当的情况下支撑千万以下的数据量是没有太大问题的。如果实在要分片，尽量选择客户端分片架构，这样可以减少一次和中间件的网络I/O。 适用场景 1、表中的数据本身就有独立性，例如表中分表记录各个地区的数据或者不同时期的数据，特别是有些数据常用，有些不常用。 2、需要把数据存放在多个介质上。 水平切分的缺点 1、给应用增加复杂度，通常查询时需要多个表名，查询所有数据都需UNION操作 2、在许多数据库应用中，这种复杂度会超过它带来的优点，查询时会增加读一个索引层的磁盘次数 数据库分片两种常见方案 客户端代理： 分片逻辑在应用端，封装在jar包中，通过修改或者封装JDBC层来实现。 当当网的 Sharding-JDBC 、阿里的TDDL是两种比较常用的实现。 中间件代理： 在应用和数据中间加了一个代理层。分片逻辑统一维护在中间件服务中。 我们现在谈的 Mycat 、360的Atlas、网易的DDB等等都是这种架构的实现。 分库分表后面临的问题 事务支持 分库分表后，就成了分布式事务了。如果依赖数据库本身的分布式事务管理功能去执行事务，将付出高昂的性能代价； 如果由应用程序去协助控制，形成程序逻辑上的事务，又会造成编程方面的负担。 跨库join 只要是进行切分，跨节点Join的问题是不可避免的。但是良好的设计和切分却可以减少此类情况的发生。解决这一问题的普遍做法是分两次查询实现。在第一次查询的结果集中找出关联数据的id,根据这些id发起第二次请求得到关联数据。 分库分表方案产品 跨节点的count,order by,group by以及聚合函数问题 这些是一类问题，因为它们都需要基于全部数据集合进行计算。多数的代理都不会自动处理合并工作。解决方案：与解决跨节点join问题的类似，分别在各个节点上得到结果后在应用程序端进行合并。和join不同的是每个结点的查询可以并行执行，因此很多时候它的速度要比单一大表快很多。但如果结果集很大，对应用程序内存的消耗是一个问题。 数据迁移，容量规划，扩容等问题 来自淘宝综合业务平台团队，它利用对2的倍数取余具有向前兼容的特性（如对4取余得1的数对2取余也是1）来分配数据，避免了行级别的数据迁移，但是依然需要进行表级别的迁移，同时对扩容规模和分表数量都有限制。总得来说，这些方案都不是十分的理想，多多少少都存在一些缺点，这也从一个侧面反映出了Sharding扩容的难度。 ID问题 一旦数据库被切分到多个物理结点上，我们将不能再依赖数据库自身的主键生成机制。一方面，某个分区数据库自生成的ID无法保证在全局上是唯一的；另一方面，应用程序在插入数据之前需要先获得ID,以便进行SQL路由. 一些常见的主键生成策略 UUID 使用UUID作主键是最简单的方案，但是缺点也是非常明显的。由于UUID非常的长，除占用大量存储空间外，最主要的问题是在索引上，在建立索引和基于索引进行查询时都存在性能问题。 Twitter的分布式自增ID算法Snowflake 在分布式系统中，需要生成全局UID的场合还是比较多的，twitter的snowflake解决了这种需求，实现也还是很简单的，除去配置信息，核心代码就是毫秒级时间41位 机器ID 10位 毫秒内序列12位。 跨分片的排序分页 般来讲，分页时需要按照指定字段进行排序。当排序字段就是分片字段的时候，我们通过分片规则可以比较容易定位到指定的分片，而当排序字段非分片字段的时候，情况就会变得比较复杂了。为了最终结果的准确性，我们需要在不同的分片节点中将数据进行排序并返回，并将不同分片返回的结果集进行汇总和再次排序，最后再返回给用户。如下图所示： MySQL复制原理以及流程 主从复制：将主数据库中的DDL和DML操作通过二进制日志（BINLOG）传输到从数据库上，然后将这些日志重新执行（重做）；从而使得从数据库的数据与主数据库保持一致。 主从复制的作用 主数据库出现问题，可以切换到从数据库。 可以进行数据库层面的读写分离。 可以在从数据库上进行日常备份。 MySQL主从复制解决的问题 数据分布：随意开始或停止复制，并在不同地理位置分布数据备份 负载均衡：降低单个服务器的压力 高可用和故障切换：帮助应用程序避免单点失败 升级测试：可以用更高版本的MySQL作为从库 MySQL主从复制工作原理 在主库上把数据更高记录到二进制日志 从库将主库的日志复制到自己的中继日志 从库读取中继日志的事件，将其重放到从库数据中 基本原理流程，3个线程以及之间的关联 主：binlog线程——记录下所有改变了数据库数据的语句，放进master上的binlog中； 从：io线程——在使用start slave 之后，负责从master上拉取 binlog 内容，放进自己的relay log中； 从：sql执行线程——执行relay log中的语句； 复制过程 Binary log：主数据库的二进制日志 Relay log：从服务器的中继日志 第一步：master在每个事务更新数据完成之前，将该操作记录串行地写入到binlog文件中。 第二步：salve开启一个I/O Thread，该线程在master打开一个普通连接，主要工作是binlog dump process。如果读取的进度已经跟上了master，就进入睡眠状态并等待master产生新的事件。I/O线程最终的目的是将这些事件写入到中继日志中。 第三步：SQL Thread会读取中继日志，并顺序执行该日志中的SQL事件，从而与主数据库中的数据保持一致。 读写分离解决方案 读写分离是依赖于主从复制，而主从复制又是为读写分离服务的。因为主从复制要求slave不能写只能读（如果对slave执行写操作，那么show slave status将会呈现Slave_SQL_Running=NO，此时你需要按照前面提到的手动同步一下slave）。 方案一 使用mysql-proxy代理 优点：直接实现读写分离和负载均衡，不用修改代码，master和slave用一样的帐号，mysql官方不建议实际生产中使用 缺点：降低性能， 不支持事务 方案二 使用AbstractRoutingDataSource+aop+annotation在dao层决定数据源。 如果采用了mybatis， 可以将读写分离放在ORM层，比如mybatis可以通过mybatis plugin拦截sql语句，所有的insert/update/delete都访问master库，所有的select 都访问salve库，这样对于dao层都是透明。 plugin实现时可以通过注解或者分析语句是读写方法来选定主从库。不过这样依然有一个问题， 也就是不支持事务， 所以我们还需要重写一下DataSourceTransactionManager， 将read-only的事务扔进读库， 其余的有读有写的扔进写库。 方案三 使用AbstractRoutingDataSource+aop+annotation在service层决定数据源，可以支持事务. 缺点：类内部方法通过this.xx()方式相互调用时，aop不会进行拦截，需进行特殊处理。 备份计划，mysqldump以及xtranbackup的实现原理 (1)备份计划 视库的大小来定，一般来说 100G 内的库，可以考虑使用 mysqldump 来做，因为 mysqldump更加轻巧灵活，备份时间选在业务低峰期，可以每天进行都进行全量备份(mysqldump 备份出来的文件比较小，压缩之后更小)。 100G 以上的库，可以考虑用 xtranbackup 来做，备份速度明显要比 mysqldump 要快。一般是选择一周一个全备，其余每天进行增量备份，备份时间为业务低峰期。 (2)备份恢复时间 物理备份恢复快，逻辑备份恢复慢 这里跟机器，尤其是硬盘的速率有关系，以下列举几个仅供参考 20G的2分钟（mysqldump） 80G的30分钟(mysqldump) 111G的30分钟（mysqldump) 288G的3小时（xtra) 3T的4小时（xtra) 逻辑导入时间一般是备份时间的5倍以上 (3)备份恢复失败如何处理 首先在恢复之前就应该做足准备工作，避免恢复的时候出错。比如说备份之后的有效性检查、权限检查、空间检查等。如果万一报错，再根据报错的提示来进行相应的调整。 (4)mysqldump和xtrabackup实现原理 mysqldump mysqldump 属于逻辑备份。加入–single-transaction 选项可以进行一致性备份。后台进程会先设置 session 的事务隔离级别为 RR(SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ)，之后显式开启一个事务(START TRANSACTION /*!40100 WITH CONSISTENTSNAPSHOT */)，这样就保证了该事务里读到的数据都是事务事务时候的快照。之后再把表的数据读取出来。如果加上–master-data=1 的话，在刚开始的时候还会加一个数据库的读锁(FLUSH TABLES WITH READ LOCK),等开启事务后，再记录下数据库此时 binlog 的位置(showmaster status)，马上解锁，再读取表的数据。等所有的数据都已经导完，就可以结束事务 Xtrabackup xtrabackup 属于物理备份，直接拷贝表空间文件，同时不断扫描产生的 redo 日志并保存下来。最后完成 innodb 的备份后，会做一个 flush engine logs 的操作(老版本在有 bug，在5.6 上不做此操作会丢数据)，确保所有的 redo log 都已经落盘(涉及到事务的两阶段提交概念，因为 xtrabackup 并不拷贝 binlog，所以必须保证所有的 redo log 都落盘，否则可能会丢最后一组提交事务的数据)。这个时间点就是 innodb 完成备份的时间点，数据文件虽然不是一致性的，但是有这段时间的 redo 就可以让数据文件达到一致性(恢复的时候做的事情)。然后还需要flush tables with read lock，把 myisam 等其他引擎的表给备份出来，备份完后解锁。这样就做到了完美的热备。 数据表损坏的修复方式 使用 myisamchk 来修复，具体步骤： 1）修复前将mysql服务停止。 2）打开命令行方式，然后进入到mysql的/bin目录。 3）执行myisamchk –recover 数据库所在路径/*.MYI 使用repair table 或者 OPTIMIZE table命令来修复，REPAIR TABLE table_name 修复表OPTIMIZE TABLE table_name 优化表 REPAIR TABLE 用于修复被破坏的表。 OPTIMIZE TABLE 用于回收闲置的数据库空间，当表上的数据行被删除时，所占据的磁盘空间并没有立即被回收，使用了OPTIMIZE TABLE命令后这些空间将被回收，并且对磁盘上的数据行进行重排（注意：是磁盘上，而非数据库） ","link":"https://tianxiawuhao.github.io/jytwMqgV3/"},{"title":"mysql概述五","content":"SQL优化 对于低性能的SQL语句的定位，最重要也是最有效的方法就是使用执行计划，MySQL提供了explain命令来查看语句的执行计划。 我们知道，不管是哪种数据库，或者是哪种数据库引擎，在对一条SQL语句进行执行的过程中都会做很多相关的优化，对于查询语句，最重要的优化方式就是使用索引。 而执行计划，就是显示数据库引擎对于SQL语句的执行的详细情况，其中包含了是否使用索引，使用什么索引，使用的索引的相关信息等。 执行计划包含的信息 id 有一组数字组成。表示一个查询中各个子查询的执行顺序; id相同执行顺序由上至下。 id不同，id值越大优先级越高，越先被执行。 id为null时表示一个结果集，不需要使用它查询，常出现在包含union等查询语句中。 select_type 每个子查询的查询类型，一些常见的查询类型。 id select_type description 1 SIMPLE 不包含任何子查询或union等查询 2 PRIMARY 包含子查询最外层查询就显示为 PRIMARY 3 SUBQUERY 在select或 where字句中包含的查询 4 DERIVED from字句中包含的查询 5 UNION 出现在union后的查询语句中 6 UNION RESULT 从UNION中获取结果集，例如上文的第三个例子 type(非常重要，可以看到有没有走索引) 访问类型 ALL 扫描全表数据 index 遍历索引 range 索引范围查找 index_subquery 在子查询中使用 ref unique_subquery 在子查询中使用 eq_ref ref_or_null 对Null进行索引的优化的 ref fulltext 使用全文索引 ref 使用非唯一索引查找数据 eq_ref 在join查询中使用PRIMARY KEY or UNIQUE NOT NULL索引关联。 possible_keys 可能使用的索引，注意不一定会使用。查询涉及到的字段上若存在索引，则该索引将被列出来。当该列为 NULL时就要考虑当前的SQL是否需要优化了。 key 显示MySQL在查询中实际使用的索引，若没有使用索引，显示为NULL。 TIPS:查询中若使用了覆盖索引(覆盖索引：索引的数据覆盖了需要查询的所有数据)，则该索引仅出现在key列表中 key_length 索引长度 ref 表示上述表的连接匹配条件，即哪些列或常量被用于查找索引列上的值 rows 返回估算的结果集数目，并不是一个准确的值。 extra 的信息非常丰富，常见的有： Using index 使用覆盖索引 Using where 使用了用where子句来过滤结果集 Using filesort 使用文件排序，使用非索引列进行排序时出现，非常消耗性能，尽量优化。 Using temporary 使用了临时表 sql优化的目标可以参考阿里开发手册 【推荐】SQL性能优化的目标：至少要达到 range 级别，要求是ref级别，如果可以是consts最好。 说明： 1） consts 单表中最多只有一个匹配行（主键或者唯一索引），在优化阶段即可读取到数据。 2） ref 指的是使用普通的索引（normal index）。 3） range 对索引进行范围检索。 反例：explain表的结果，type=index，索引物理文件全扫描，速度非常慢，这个index级别比较range还低，与全表扫描是小巫见大巫。 SQL执行顺序 mysql执行sql的顺序从 From 开始，以下是执行的顺序流程 FROM FROM table1 left join table2 on 将table1和table2中的数据产生笛卡尔积，生成Temp1 JOIN JOIN table2 所以先是确定表，再确定关联条件 ON ON table1.column = table2.columu 确定表的绑定条件 由Temp1产生中间表Temp2 WHERE 对中间表Temp2产生的结果进行过滤 产生中间表Temp3 GROUP BY 对中间表Temp3进行分组，产生中间表Temp4 HAVING 对分组后的记录进行聚合 产生中间表Temp5 SELECT 对中间表Temp5进行列筛选，产生中间表 Temp6 DISTINCT 对中间表 Temp6进行去重，产生中间表 Temp7 ORDER BY 对Temp7中的数据进行排序，产生中间表Temp8 LIMIT 对中间表Temp8进行分页，产生中间表Temp9 SQL的生命周期 与服务器建立连接，客户端发送一条查询给服务器 服务器先检查查询缓存，如果命中了缓存则立刻返回存储在缓存中的结果，否则执行下一步 服务端进行sql解析，预处理，再由查询优化器生成对应的查询执行计划 mysql根据优化器提供的执行计划，调用存储引擎的api来执行查询 通过步骤一的连接，发送结果到客户端 关掉连接，释放资源 优化大表数据查询 优化shema、sql语句+索引； 加缓存，memcached, redis； 主从复制，读写分离； 垂直拆分，根据你模块的耦合度，将一个大的系统分为多个小的系统，也就是分布式系统； 水平切分，针对数据量大的表，这一步最麻烦，最能考验技术水平，要选择一个合理的sharding key, 为了有好的查询效率，表结构也要改动，做一定的冗余，应用也要改，sql中尽量带sharding key，将数据定位到限定的表上去查，而不是扫描全部的表； 超大分页怎么处理 超大的分页一般从两个方向上来解决. 数据库层面,这也是我们主要集中关注的(虽然收效没那么大),类似于select * from table where age &gt; 20 limit 1000000,10这种查询其实也是有可以优化的余地的. 这条语句需要load1000000数据然后基本上全部丢弃,只取10条当然比较慢. 当时我们可以修改为select * from table where id in (select id from table where age &gt; 20 limit 1000000,10).这样虽然也load了一百万的数据,但是由于索引覆盖,要查询的所有字段都在索引中,所以速度会很快. 同时如果ID连续的好,我们还可以select * from table where id &gt; 1000000 limit 10,效率也是不错的,优化的可能性有许多种,但是核心思想都一样,就是减少load的数据. 从需求的角度减少这种请求…主要是不做类似的需求(直接跳转到几百万页之后的具体某一页.只允许逐页查看或者按照给定的路线走,这样可预测,可缓存)以及防止ID泄漏且连续被人恶意攻击. 解决超大分页,其实主要是靠缓存,可预测性的提前查到内容,缓存至redis等k-V数据库中,直接返回即可. 在阿里巴巴《Java开发手册》中,对超大分页的解决办法是类似于上面提到的第一种. 【推荐】利用延迟关联或者子查询优化超多分页场景。 说明：MySQL并不是跳过offset行，而是取offset+N行，然后返回放弃前offset行，返回N行，那当offset特别大的时候，效率就非常的低下，要么控制返回的总页数，要么对超过特定阈值的页数进行SQL改写。 正例：先快速定位需要获取的id段，然后再关联： SELECT a.* FROM 表1 a, (select id from 表1 where 条件 LIMIT 100000,20 ) b where a.id=b.id mysql 分页 LIMIT 子句可以被用于强制 SELECT 语句返回指定的记录数。LIMIT 接受一个或两个数字参数。参数必须是一个整数常量。如果给定两个参数，第一个参数指定第一个返回记录行的偏移量，第二个参数指定返回记录行的最大数目。初始记录行的偏移量是 0(而不是 1) mysql&gt; SELECT * FROM table LIMIT 5,10; // 检索记录行 6-15 为了检索从某一个偏移量到记录集的结束所有的记录行，可以指定第二个参数为 -1： mysql&gt; SELECT * FROM table LIMIT 95,-1; // 检索记录行 96-last. 如果只给定一个参数，它表示返回最大的记录行数目： mysql&gt; SELECT * FROM table LIMIT 5; //检索前 5 个记录行 换句话说，LIMIT n 等价于 LIMIT 0,n。 慢查询日志 用于记录执行时间超过某个临界值的SQL日志，用于快速定位慢查询，为我们的优化做参考。 开启慢查询日志 配置项：slow_query_log 可以使用show variables like ‘slov_query_log’查看是否开启，如果状态值为OFF，可以使用set GLOBAL slow_query_log = on来开启，它会在datadir下产生一个xxx-slow.log的文件。 设置临界时间 配置项：long_query_time 查看：show VARIABLES like 'long_query_time'，单位秒 设置：set long_query_time=0.5 实操时应该从长时间设置到短的时间，即将最慢的SQL优化掉 查看日志，一旦SQL超过了我们设置的临界时间就会被记录到xxx-slow.log中 慢查询处理 在业务系统中，除了使用主键进行的查询，其他的会在测试库上测试其耗时，慢查询的统计主要由运维在做，会定期将业务中的慢查询反馈给我们。 慢查询的优化首先要搞明白慢的原因是什么？ 是查询条件没有命中索引？是load了不需要的数据列？还是数据量太大？ 所以优化也是针对这三个方向来的， 首先分析语句，看看是否load了额外的数据，可能是查询了多余的行并且抛弃掉了，可能是加载了许多结果中并不需要的列，对语句进行分析以及重写。 分析语句的执行计划，然后获得其使用索引的情况，之后修改语句或者修改索引，使得语句可以尽可能的命中索引。 如果对语句的优化已经无法进行，可以考虑表中的数据量是否太大，如果是的话可以进行横向或者纵向的分表。 主键必要性 主键是数据库确保数据行在整张表唯一性的保障，即使业务上本张表没有主键，也建议添加一个自增长的ID列作为主键。设定了主键之后，在后续的删改查的时候可能更加快速以及确保操作数据范围安全。 主键使用自增ID还是UUID 推荐使用自增ID，不要使用UUID。 因为在InnoDB存储引擎中，主键索引是作为聚簇索引存在的，也就是说，主键索引的B+树叶子节点上存储了主键索引以及全部的数据(按照顺序)，如果主键索引是自增ID，那么只需要不断向后排列即可，如果是UUID，由于到来的ID与原来的大小不确定，会造成非常多的数据插入，数据移动，然后导致产生很多的内存碎片，进而造成插入性能的下降。 总之，在数据量大一些的情况下，用自增主键性能会好一些。 由于主键是聚簇索引，如果没有主键，InnoDB会选择一个唯一键来作为聚簇索引，如果没有唯一键，会生成一个隐式的主键。 字段为什么要求定义为not null null值会占用更多的字节，且会在程序中造成很多与预期不符的情况。 如果要存储用户的密码散列，应该使用什么字段进行存储？ 密码散列，盐，用户身份证号等固定长度的字符串应该使用char而不是varchar来存储，这样可以节省空间且提高检索效率。 优化查询过程中的数据访问 访问数据太多导致查询性能下降 确定应用程序是否在检索大量超过需要的数据，可能是太多行或列 确认MySQL服务器是否在分析大量不必要的数据行 避免犯如下SQL语句错误 查询不需要的数据。解决办法：使用limit解决 多表关联返回全部列。解决办法：指定列名 总是返回全部列。解决办法：避免使用SELECT * 重复查询相同的数据。解决办法：可以缓存数据，下次直接读取缓存 是否在扫描额外的记录。解决办法： 使用explain进行分析，如果发现查询需要扫描大量的数据，但只返回少数的行，可以通过如下技巧去优化： 使用索引覆盖扫描，把所有的列都放到索引中，这样存储引擎不需要回表获取对应行就可以返回结果。 改变数据库和表的结构，修改数据表范式 重写SQL语句，让优化器可以以更优的方式执行查询。 优化长难的查询语句 一个复杂查询还是多个简单查询 MySQL内部每秒能扫描内存中上百万行数据，相比之下，响应数据给客户端就要慢得多 使用尽可能小的查询是好的，但是有时将一个大的查询分解为多个小的查询是很有必要的。 切分查询 将一个大的查询分为多个小的相同的查询 一次性删除1000万的数据要比一次删除1万，暂停一会的方案更加损耗服务器开销。 分解关联查询，让缓存的效率更高。 执行单个查询可以减少锁的竞争。 在应用层做关联更容易对数据库进行拆分。 查询效率会有大幅提升。 较少冗余记录的查询。 优化特定类型的查询语句 count(*)会忽略所有的列，直接统计所有列数，不要使用count(列名) MyISAM中，没有任何where条件的count(*)非常快。 当有where条件时，MyISAM的count统计不一定比其它引擎快。 可以使用explain查询近似值，用近似值替代count(*) 增加汇总表 使用缓存 优化关联查询 确定ON或者USING子句中是否有索引。 确保GROUP BY和ORDER BY只有一个表中的列，这样MySQL才有可能使用索引。 优化子查询 用关联查询替代 优化GROUP BY和DISTINCT 这两种查询可以使用索引来优化，是最有效的优化方法 关联查询中，使用标识列分组的效率更高 如果不需要ORDER BY，进行GROUP BY时加ORDER BY NULL，MySQL不会再进行文件排序。 WITH ROLLUP超级聚合，可以挪到应用程序处理 优化LIMIT分页 LIMIT偏移量大的时候，查询效率较低 可以记录上次查询的最大ID，下次查询时直接根据该ID来查询 优化UNION查询 UNION ALL的效率高于UNION 优化WHERE子句 对于此类问题，先说明如何定位低效SQL语句，然后根据SQL语句可能低效的原因做排查，先从索引着手，如果索引没有问题，考虑以上几个方面，数据访问的问题，长难查询句的问题还是一些特定类型优化的问题，逐一回答。 SQL语句优化的一些方法？ 1.对查询进行优化，应尽量避免全表扫描，首先应考虑在 where 及 order by 涉及的列上建立索引。 2.应尽量避免在 where 子句中对字段进行 null 值判断，否则将导致引擎放弃使用索引而进行全表扫描，如： select id from t where num is null -- 可以在num上设置默认值0，确保表中num列没有null值，然后这样查询： select id from t where num=0 3.应尽量避免在 where 子句中使用!=或&lt;&gt;操作符，否则引擎将放弃使用索引而进行全表扫描。 4.应尽量避免在 where 子句中使用or 来连接条件，否则将导致引擎放弃使用索引而进行全表扫描，如： select id from t where num=10 or num=20 -- 可以这样查询： select id from t where num=10 union all select id from t where num=20 5.in 和 not in 也要慎用，否则会导致全表扫描，如： select id from t where num in(1,2,3) -- 对于连续的数值，能用 between 就不要用 in 了： select id from t where num between 1 and 3 6.下面的查询也将导致全表扫描： select id from t where name like ‘%李%’ -- 若要提高效率，可以考虑全文检索。 7.如果在 where 子句中使用参数，也会导致全表扫描。因为SQL只有在运行时才会解析局部变量，但优化程序不能将访问计划的选择推迟到运行时；它必须在编译时进行选择。然 而，如果在编译时建立访问计划，变量的值还是未知的，因而无法作为索引选择的输入项。如下面语句将进行全表扫描： select id from t where num=@num -- 可以改为强制查询使用索引： select id from t with(index(索引名)) where num=@num 8.应尽量避免在 where 子句中对字段进行表达式操作，这将导致引擎放弃使用索引而进行全表扫描。如： select id from t where num/2=100 -- 应改为: select id from t where num=100*2 9.应尽量避免在where子句中对字段进行函数操作，这将导致引擎放弃使用索引而进行全表扫描。如： select id from t where substring(name,1,3)=’abc’ -- name以abc开头的id应改为: select id from t where name like ‘abc%’ 10.不要在 where 子句中的“=”左边进行函数、算术运算或其他表达式运算，否则系统将可能无法正确使用索引。 ","link":"https://tianxiawuhao.github.io/lbX34toJL/"},{"title":"mysql概述四","content":"视图 为了提高复杂SQL语句的复用性和表操作的安全性，MySQL数据库管理系统提供了视图特性。所谓视图，本质上是一种虚拟表，在物理上是不存在的，其内容与真实的表相似，包含一系列带有名称的列和行数据。但是，视图并不在数据库中以储存的数据值形式存在。行和列数据来自定义视图的查询所引用基本表，并且在具体引用视图时动态生成。 视图使开发者只关心感兴趣的某些特定数据和所负责的特定任务，只能看到视图中所定义的数据，而不是视图所引用表中的数据，从而提高了数据库中数据的安全性。 特点 视图的特点如下: 视图的列可以来自不同的表，是表的抽象和在逻辑意义上建立的新关系。 视图是由基本表(实表)产生的表(虚表)。 视图的建立和删除不影响基本表。 对视图内容的更新(添加，删除和修改)直接影响基本表。 当视图来自多个基本表时，不允许添加和删除数据。 视图的操作包括创建视图，查看视图，删除视图和修改视图。 使用场景 视图根本用途：简化sql查询，提高开发效率。如果说还有另外一个用途那就是兼容老的表结构。 下面是视图的常见使用场景： 重用SQL语句； 简化复杂的SQL操作。在编写查询后，可以方便的重用它而不必知道它的基本查询细节； 使用表的组成部分而不是整个表； 保护数据。可以给用户授予表的特定部分的访问权限而不是整个表的访问权限； 更改数据格式和表示。视图可返回与底层表的表示和格式不同的数据。 优点 查询简单化。视图能简化用户的操作 数据安全性。视图使用户能以多种角度看待同一数据，能够对机密数据提供安全保护 逻辑数据独立性。视图对重构数据库提供了一定程度的逻辑独立性 缺点 性能。数据库必须把视图的查询转化成对基本表的查询，如果这个视图是由一个复杂的多表查询所定义，那么，即使是视图的一个简单查询，数据库也把它变成一个复杂的结合体，需要花费一定的时间。 修改限制。当用户试图修改视图的某些行时，数据库必须把它转化为对基本表的某些行的修改。事实上，当从视图中插入或者删除时，情况也是这样。对于简单视图来说，这是很方便的，但是，对于比较复杂的视图，可能是不可修改的 这些视图有如下特征： 有UNIQUE等集合操作符的视图。 有GROUP BY子句的视图。 有诸如AVG\\SUM\\MAX等聚合函数的视图。 使用DISTINCT关键字的视图。 连接表的视图（其中有些例外） 游标 游标是系统为用户开设的一个数据缓冲区，存放SQL语句的执行结果，每个游标区都有一个名字。用户可以通过游标逐一获取记录并赋给主变量，交由主语言进一步处理。 存储过程与函数 存储过程是一个预编译的SQL语句，优点是允许模块化的设计，就是说只需要创建一次，以后在该程序中就可以调用多次。如果某次操作需要执行多次SQL，使用存储过程比单纯SQL语句执行要快。 优点 存储过程是预编译过的，执行效率高。 存储过程的代码直接存放于数据库中，通过存储过程名直接调用，减少网络通讯。 安全性高，执行存储过程需要有一定权限的用户。 存储过程可以重复使用，减少数据库开发人员的工作量。 缺点 1）调试麻烦，但是用 PL/SQL Developer 调试很方便！弥补这个缺点。 2）移植问题，数据库端代码当然是与数据库相关的。但是如果是做工程型项目，基本不存在移植问题。 3）重新编译问题，因为后端代码是运行前编译的，如果带有引用关系的对象发生改变时，受影响的存储过程、包将需要重新编译（不过也可以设置成运行时刻自动编译）。 4）如果在一个程序系统中大量的使用存储过程，到程序交付使用的时候随着用户需求的增加会导致数据结构的变化，接着就是系统的相关问题了，最后如果用户想维护该系统可以说是很难很难、而且代价是空前的，维护起来更麻烦。 触发器 触发器是用户定义在关系表上的一类由事件驱动的特殊的存储过程。触发器是指一段代码，当触发某个事件时，自动执行这些代码。 使用场景 可以通过数据库中的相关表实现级联更改。 实时监控某张表中的某个字段的更改而需要做出相应的处理。 例如可以生成某些业务的编号。 注意不要滥用，否则会造成数据库及应用程序的维护困难。 大家需要牢记以上基础知识点，重点是理解数据类型CHAR和VARCHAR的差异，表存储引擎InnoDB和MyISAM的区别。 MySQL中都有哪些触发器 在MySQL数据库中有如下六种触发器： Before Insert After Insert Before Update After Update Before Delete After Delete SQL语句 SQL语句分类 数据定义语言DDL（Data Ddefinition Language）CREATE，DROP，ALTER 主要为以上操作 即对逻辑结构等有操作的，其中包括表结构，视图和索引。 数据查询语言DQL（Data Query Language）SELECT 这个较为好理解 即查询操作，以select关键字。各种简单查询，连接查询等 都属于DQL。 数据操纵语言DML（Data Manipulation Language）INSERT，UPDATE，DELETE 主要为以上操作 即对数据进行操作的，对应上面所说的查询操作 DQL与DML共同构建了多数初级程序员常用的增删改查操作。而查询是较为特殊的一种 被划分到DQL中。 数据控制功能DCL（Data Control Language）GRANT，REVOKE，COMMIT，ROLLBACK 主要为以上操作 即对数据库安全性完整性等有操作的，可以简单的理解为权限控制等。 超键、候选键、主键、外键 超键：在关系中能唯一标识元组的属性集称为关系模式的超键。一个属性可以为作为一个超键，多个属性组合在一起也可以作为一个超键。超键包含候选键和主键。 候选键：是最小超键，即没有冗余元素的超键。 主键：数据库表中对储存数据对象予以唯一和完整标识的数据列或属性的组合。一个数据列只能有一个主键，且主键的取值不能缺失，即不能为空值（Null）。 外键：在一个表中存在的另一个表的主键称此表的外键。 SQL 约束 NOT NULL: 用于控制字段的内容一定不能为空（NULL）。 UNIQUE: 控件字段内容不能重复，一个表允许有多个 Unique 约束。 PRIMARY KEY: 也是用于控件字段内容不能重复，但它在一个表只允许出现一个。 FOREIGN KEY: 用于预防破坏表之间连接的动作，也能防止非法数据插入外键列，因为它必须是它指向的那个表中的值之一。 CHECK: 用于控制字段的值范围。 六种关联查询 交叉连接（CROSS JOIN） 内连接（INNER JOIN） 外连接（LEFT JOIN/RIGHT JOIN） 联合查询（UNION与UNION ALL） 全连接（FULL JOIN） 交叉连接（CROSS JOIN） SELECT * FROM A,B(,C)或者SELECT * FROM A CROSS JOIN B (CROSS JOIN C)#没有任何关联条件，结果是笛卡尔积，结果集会很大，没有意义，很少使用内连接（INNER JOIN）SELECT * FROM A,B WHERE A.id=B.id或者SELECT * FROM A INNER JOIN B ON A.id=B.id多表中同时符合某种条件的数据记录的集合，INNER JOIN可以缩写为JOIN 内连接分为三类 等值连接：ON A.id=B.id 不等值连接：ON A.id &gt; B.id 自连接：SELECT * FROM A T1 INNER JOIN A T2 ON T1.id=T2.pid 外连接（LEFT JOIN/RIGHT JOIN） 左外连接：LEFT OUTER JOIN, 以左表为主，先查询出左表，按照ON后的关联条件匹配右表，没有匹配到的用NULL填充，可以简写成LEFT JOIN 右外连接：RIGHT OUTER JOIN, 以右表为主，先查询出右表，按照ON后的关联条件匹配左表，没有匹配到的用NULL填充，可以简写成RIGHT JOIN 联合查询（UNION与UNION ALL） SELECT * FROM A UNION SELECT * FROM B UNION ... 就是把多个结果集集中在一起，UNION前的结果为基准，需要注意的是联合查询的列数要相等，相同的记录行会合并 如果使用UNION ALL，不会合并重复的记录行 效率 UNION ALL 高于 UNION 全连接（FULL JOIN） MySQL不支持全连接 可以使用LEFT JOIN 和UNION和RIGHT JOIN联合使用 SELECT * FROM A LEFT JOIN B ON A.id=B.id UNIONSELECT * FROM A RIGHT JOIN B ON A.id=B.id 表连接展示 有2张表，1张R、1张S，R表有ABC三列，S表有CD两列，表中各有三条记录。 R表 A B C a1 b1 c1 a2 b2 c2 a3 b3 c3 S表 C D c1 d1 c2 d2 c4 d3 交叉连接(笛卡尔积): select r.*,s.* from r,s A B C C D a1 b1 c1 c1 d1 a2 b2 c2 c1 d1 a3 b3 c3 c1 d1 a1 b1 c1 c2 d2 a2 b2 c2 c2 d2 a3 b3 c3 c2 d2 a1 b1 c1 c4 d3 a2 b2 c2 c4 d3 a3 b3 c3 c4 d3 内连接结果： select r.*,s.* from r inner join s on r.c=s.c A B C C D a1 b1 c1 c1 d1 a2 b2 c2 c2 d2 左连接结果： select r.*,s.* from r left join s on r.c=s.c A B C C D a1 b1 c1 c1 d1 a2 b2 c2 c2 d2 a3 b3 c3 右连接结果： select r.*,s.* from r right join s on r.c=s.c A B C C D a1 b1 c1 c1 d1 a2 b2 c2 c2 d2 c4 d3 全表连接的结果（MySql不支持，Oracle支持）： select r.*,s.* from r full join s on r.c=s.c A B C C D a1 b1 c1 c1 d1 a2 b2 c2 c2 d2 a3 b3 c3 c4 d3 子查询 条件：一条SQL语句的查询结果做为另一条查询语句的条件或查询结果 嵌套：多条SQL语句嵌套使用，内部的SQL查询语句称为子查询。 子查询的三种情况 子查询是单行单列的情况：结果集是一个值，父查询使用：=、 &lt;、 &gt; 等运算符 -- 查询工资最高的员工是谁？ select * from employee where salary=(select max(salary) from employee); 子查询是多行单列的情况：结果集类似于一个数组，父查询使用：in 运算符 -- 查询工资最高的员工是谁？ select * from employee where salary=(select max(salary) from employee); 子查询是多行多列的情况：结果集类似于一张虚拟表，不能用于where条件，用于select子句中做为子表 -- 1) 查询出2011年以后入职的员工信息 -- 2) 查询所有的部门信息，与上面的虚拟表中的信息比对，找出所有部门ID相等的员工。 SELECT * FROM dept d, ( SELECT * FROM employee WHERE join_date &gt; '2011-1-1' ) e WHERE e.dept_id = d.id; -- 使用表连接： SELECT d.*, e.* FROM dept d INNER JOIN employee e ON d.id = e.dept_id WHERE e.join_date &gt; '2011-1-1' mysql中 in 和 exists 区别 mysql中的in语句是把外表和内表作hash 连接，而exists语句是对外表作loop循环，每次loop循环再对内表进行查询。一直大家都认为exists比in语句的效率要高，这种说法其实是不准确的。这个是要区分环境的。 如果查询的两个表大小相当，那么用in和exists差别不大。 如果两个表中一个较小，一个是大表，则子查询表大的用exists，子查询表小的用in。 not in 和not exists：如果查询语句使用了not in，那么内外表都进行全表扫描，没有用到索引；而not extsts的子查询依然能用到表上的索引。所以无论那个表大，用not exists都比not in要快。 varchar与char的区别 char的特点 char表示定长字符串，长度是固定的； 如果插入数据的长度小于char的固定长度时，则用空格填充； 因为长度固定，所以存取速度要比varchar快很多，甚至能快50%，但正因为其长度固定，所以会占据多余的空间，是空间换时间的做法； 对于char来说，最多能存放的字符个数为255，和编码无关 varchar的特点 varchar表示可变长字符串，长度是可变的； 插入的数据是多长，就按照多长来存储； varchar在存取方面与char相反，它存取慢，因为长度不固定，但正因如此，不占据多余的空间，是时间换空间的做法； 对于varchar来说，最多能存放的字符个数为65532 总之，结合性能角度（char更快）和节省磁盘空间角度（varchar更小），具体情况还需具体来设计数据库才是妥当的做法。 varchar(50)中50的涵义 最多存放50个字符，varchar(50)和(200)存储hello所占空间一样，但后者在排序时会消耗更多内存，因为order by col采用fixed_length计算col长度(memory引擎也一样)。在早期 MySQL 版本中， 50 代表字节数，现在代表字符数。 int(20)中20的涵义 是指显示字符的长度。20表示最大显示宽度为20，但仍占4字节存储，存储范围不变；不影响内部存储，只是影响带 zerofill 定义的 int 时，前面补多少个 0，易于报表展示 mysql为什么这么设计字段 对大多数应用没有意义，只是规定一些工具用来显示字符的个数；int(1)和int(20)存储和计算均一样； mysql中int(10)和char(10)以及varchar(10)的区别 int(10)的10表示显示的数据的长度，不是存储数据的大小；chart(10)和varchar(10)的10表示存储数据的大小，即表示存储多少个字符。 int(10) 10位的数据长度 9999999999，占32个字节，int型4位 char(10) 10位固定字符串，不足补空格 最多10个字符 varchar(10) 10位可变字符串，不足补空格 最多10个字符 char(10)表示存储定长的10个字符，不足10个就用空格补齐，占用更多的存储空间 varchar(10)表示存储10个变长的字符，存储多少个就是多少个，空格也按一个字符存储，这一点是和char(10)的空格不同的，char(10)的空格表示占位不算一个字符 FLOAT和DOUBLE的区别 FLOAT类型数据可以存储至多8位十进制数，并在内存中占4字节。 DOUBLE类型数据可以存储至多18位十进制数，并在内存中占8字节。 drop、delete与truncate的区别 三者都表示删除，但是三者有一些差别： Delete Truncate Drop 类型 属于DML 属于DDL 属于DDL 回滚 可回滚 不可回滚 不可回滚 删除内容 表结构还在，删除表的全部或者一部分数据行 表结构还在，删除表中的所有数据 从数据库中删除表，所有的数据行，索引和权限也会被删除 删除速度 删除速度慢，需要逐行删除 删除速度快 删除速度最快 因此，在不再需要一张表的时候，用drop；在想删除部分数据行时候，用delete；在保留表而删除所有数据的时候用truncate。 UNION与UNION ALL的区别 如果使用UNION ALL，不会合并重复的记录行 效率 UNION 高于 UNION ALL ","link":"https://tianxiawuhao.github.io/QKdNIC7pG/"},{"title":"mysql概述三","content":"事务 事务是一个不可分割的数据库操作序列，也是数据库并发控制的基本单位，其执行的结果必须使数据库从一种一致性状态变到另一种一致性状态。事务是逻辑上的一组操作，要么都执行，要么都不执行。 事务最经典也经常被拿出来说例子就是转账了。 假如小明要给小红转账1000元，这个转账会涉及到两个关键操作就是：将小明的余额减少1000元，将小红的余额增加1000元。万一在这两个操作之间突然出现错误比如银行系统崩溃，导致小明余额减少而小红的余额没有增加，这样就不对了。事务就是保证这两个关键操作要么都成功，要么都要失败。 事物的四大特性(ACID) 关系性数据库需要遵循ACID规则，具体内容如下： 原子性： 事务是最小的执行单位，不允许分割。事务的原子性确保动作要么全部完成，要么完全不起作用； 一致性： 执行事务前后，数据保持一致，多个事务对同一个数据读取的结果是相同的； 隔离性： 并发访问数据库时，一个用户的事务不被其他事务所干扰，各并发事务之间数据库是独立的； 持久性： 一个事务被提交之后。它对数据库中数据的改变是持久的，即使数据库发生故障也不应该对其有任何影响。 脏读？幻读？不可重复读？ 脏读(Drity Read)：某个事务已更新一份数据，另一个事务在此时读取了同一份数据，由于某些原因，前一个RollBack了操作，则后一个事务所读取的数据就会是不正确的。 不可重复读(Non-repeatable read):在一个事务的两次查询之中数据不一致，这可能是两次查询过程中间插入了一个事务更新了原有的数据。 幻读(Phantom Read):在一个事务的两次查询中数据笔数不一致，例如有一个事务查询了几列(Row)数据，而另一个事务却在此时插入了新的几列数据，先前的事务在接下来的查询中，就会发现有几列数据是它先前所没有的。 事务的隔离级别 为了达到事务的四大特性，数据库定义了4种不同的事务隔离级别，由低到高依次为Read uncommitted、Read committed、Repeatable read、Serializable，这四个级别可以逐个解决脏读、不可重复读、幻读这几类问题。 隔离级别 脏读 不可重复读 幻影读 READ-UNCOMMITTED √ √ √ READ-COMMITTED × √ √ REPEATABLE-READ × × √ SERIALIZABLE × × × SQL 标准定义了四个隔离级别： READ-UNCOMMITTED(读未提交)： 最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读。 READ-COMMITTED(读已提交)： 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生。 REPEATABLE-READ(可重复读)： 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。 SERIALIZABLE(串行化)： 最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。 这里需要注意的是：Mysql 默认采用的 REPEATABLE_READ(可重复读)隔离级别 Oracle 默认采用的 READ_COMMITTED(读已提交)隔离级别 事务隔离机制的实现基于锁机制和并发调度。其中并发调度使用的是MVVC（多版本并发控制），通过保存修改的旧版本信息来支持并发一致性读和回滚等特性。 因为隔离级别越低，事务请求的锁越少，所以大部分数据库系统的隔离级别都是READ-COMMITTED(读已提交):，但是你要知道的是InnoDB 存储引擎默认使用 REPEATABLE-READ(可重读)并不会有任何性能损失。 InnoDB 存储引擎在 分布式事务 的情况下一般会用到SERIALIZABLE(可串行化)隔离级别。 锁 当数据库有并发事务的时候，可能会产生数据的不一致，这时候需要一些机制来保证访问的次序，锁机制就是这样的一个机制。 就像酒店的房间，如果大家随意进出，就会出现多人抢夺同一个房间的情况，而在房间上装上锁，申请到钥匙的人才可以入住并且将房间锁起来，其他人只有等他使用完毕才可以再次使用。 隔离级别与锁的关系 在Read Uncommitted级别下，读取数据不需要加共享锁，这样就不会跟被修改的数据上的排他锁冲突 在Read Committed级别下，读操作需要加共享锁，但是在语句执行完以后释放共享锁； 在Repeatable Read级别下，读操作需要加共享锁，但是在事务提交之前并不释放共享锁，也就是必须等待事务执行完毕以后才释放共享锁。 SERIALIZABLE 是限制性最强的隔离级别，因为该级别锁定整个范围的键，并一直持有锁，直到事务完成。 按照锁的粒度分数据库锁 在关系型数据库中，可以按照锁的粒度把数据库锁分为行级锁(INNODB引擎)、表级锁(MYISAM引擎)和页级锁(BDB引擎 )。 MyISAM和InnoDB存储引擎使用的锁 MyISAM采用表级锁(table-level locking)。 InnoDB支持行级锁(row-level locking)和表级锁，默认为行级锁 行级锁，表级锁和页级锁对比 行级锁 行级锁是Mysql中锁定粒度最细的一种锁，表示只针对当前操作的行进行加锁。行级锁能大大减少数据库操作的冲突。其加锁粒度最小，但加锁的开销也最大。行级锁分为共享锁 和 排他锁。 特点：开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高。 表级锁 表级锁是MySQL中锁定粒度最大的一种锁，表示对当前操作的整张表加锁，它实现简单，资源消耗较少，被大部分MySQL引擎支持。最常使用的MYISAM与INNODB都支持表级锁定。表级锁定分为表共享读锁（共享锁）与表独占写锁（排他锁）。 特点：开销小，加锁快；不会出现死锁；锁定粒度大，发出锁冲突的概率最高，并发度最低。 页级锁 页级锁是MySQL中锁定粒度介于行级锁和表级锁中间的一种锁。表级锁速度快，但冲突多，行级冲突少，但速度慢。所以取了折中的页级，一次锁定相邻的一组记录。 特点：开销和加锁时间介于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般 按照锁的类别分数据库锁 从锁的类别上来讲，有共享锁和排他锁。 共享锁: 又叫做读锁。 当用户要进行数据的读取时，对数据加上共享锁。共享锁可以同时加上多个。 排他锁: 又叫做写锁。 当用户要进行数据的写入时，对数据加上排他锁。排他锁只可以加一个，他和其他的排他锁，共享锁都相斥。 用上面的例子来说就是用户的行为有两种，一种是来看房，多个用户一起看房是可以接受的。 一种是真正的入住一晚，在这期间，无论是想入住的还是想看房的都不可以。 锁的粒度取决于具体的存储引擎，InnoDB实现了行级锁，页级锁，表级锁。 他们的加锁开销从大到小，并发能力也是从大到小。 MySQL中InnoDB引擎的行锁实现 答：InnoDB是基于索引来完成行锁 例: select * from tab_with_index where id = 1 for update; for update 可以根据条件来完成行锁锁定，并且 id 是有索引键的列，如果 id 不是索引键那么InnoDB将完成表锁，并发将无从谈起 InnoDB存储引擎的锁的算法有三种 Record lock：单个行记录上的锁 Gap lock：间隙锁，锁定一个范围，不包括记录本身 Next-key lock：record+gap 锁定一个范围，包含记录本身 相关知识点： innodb对于行的查询使用next-key lock Next-locking keying为了解决Phantom Problem幻读问题 当查询的索引含有唯一属性时，将next-key lock降级为record key Gap锁设计的目的是为了阻止多个事务将记录插入到同一范围内，而这会导致幻读问题的产生 有两种方式显式关闭gap锁：（除了外键约束和唯一性检查外，其余情况仅使用record lock） A. 将事务隔离级别设置为RC B. 将参数innodb_locks_unsafe_for_binlog设置为1 死锁 死锁是指两个或多个事务在同一资源上相互占用，并请求锁定对方的资源，从而导致恶性循环的现象。 常见的解决死锁的方法 如果不同程序会并发存取多个表，尽量约定以相同的顺序访问表，可以大大降低死锁机会。 在同一个事务中，尽可能做到一次锁定所需要的所有资源，减少死锁产生概率； 对于非常容易产生死锁的业务部分，可以尝试使用升级锁定颗粒度，通过表级锁定来减少死锁产生的概率； 如果业务处理不好可以用分布式事务锁或者使用乐观锁 数据库的乐观锁和悲观锁 数据库管理系统（DBMS）中的并发控制的任务是确保在多个事务同时存取数据库中同一数据时不破坏事务的隔离性和统一性以及数据库的统一性。乐观并发控制（乐观锁）和悲观并发控制（悲观锁）是并发控制主要采用的技术手段。 悲观锁：假定会发生并发冲突，屏蔽一切可能违反数据完整性的操作。在查询完数据的时候就把事务锁起来，直到提交事务。实现方式：使用数据库中的锁机制 乐观锁：假设不会发生并发冲突，只在提交操作时检查是否违反数据完整性。在修改数据的时候把事务锁起来，通过version的方式来进行锁定。实现方式：乐一般会使用版本号机制或CAS算法实现。 两种锁的使用场景 从上面对两种锁的介绍，我们知道两种锁各有优缺点，不可认为一种好于另一种，像乐观锁适用于写比较少的情况下（多读场景），即冲突真的很少发生的时候，这样可以省去了锁的开销，加大了系统的整个吞吐量。 但如果是多写的情况，一般会经常产生冲突，这就会导致上层应用会不断的进行retry，这样反倒是降低了性能，所以一般多写的场景下用悲观锁就比较合适。 ","link":"https://tianxiawuhao.github.io/5ppmKteEb/"},{"title":"mysql概述二","content":"索引 索引是一种特殊的文件(InnoDB数据表上的索引是表空间的一个组成部分)，它们包含着对数据表里所有记录的引用指针。 索引是一种数据结构。数据库索引，是数据库管理系统中一个排序的数据结构，以协助快速查询、更新数据库表中数据。索引的实现通常使用B树及其变种B+树。 更通俗的说，索引就相当于目录。为了方便查找书中的内容，通过对内容建立索引形成目录。索引是一个文件，它是要占据物理空间的。 索引的基本原理 索引用来快速地寻找那些具有特定值的记录。如果没有索引，一般来说执行查询时遍历整张表。 索引的原理很简单，就是把无序的数据变成有序的查询 把创建了索引的列的内容进行排序 对排序结果生成倒排表 在倒排表内容上拼上数据地址链 在查询的时候，先拿到倒排表内容，再取出数据地址链，从而拿到具体数据 索引优缺点 优点 可以大大加快数据的检索速度，这也是创建索引的最主要的原因。 通过使用索引，可以在查询的过程中，使用优化隐藏器，提高系统的性能。 缺点 时间方面：创建索引和维护索引要耗费时间，具体地，当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，会降低增/改/删的执行效率； 空间方面：索引需要占物理空间。 使用场景（重点） where 上图中，根据id查询记录，因为id字段仅建立了主键索引，因此此SQL执行可选的索引只有主键索引，如果有多个，最终会选一个较优的作为检索的依据。 -- 增加一个没有建立索引的字段 alter table innodb1 add sex char(1); -- 按sex检索时可选的索引为null EXPLAIN SELECT * from innodb1 where sex='男'; 可以尝试在一个字段未建立索引时，根据该字段查询的效率，然后对该字段建立索引alter table 表名 add index(字段名)，同样的SQL执行的效率，你会发现查询效率会有明显的提升（数据量越大越明显）。 order by 当我们使用order by将查询结果按照某个字段排序时，如果该字段没有建立索引，那么执行计划会将查询出的所有数据使用外部排序（将数据从硬盘分批读取到内存使用内部排序，最后合并排序结果），这个操作是很影响性能的，因为需要将查询涉及到的所有数据从磁盘中读到内存（如果单条数据过大或者数据量过多都会降低效率），更无论读到内存之后的排序了。 但是如果我们对该字段建立索引alter table 表名 add index(字段名)，那么由于索引本身是有序的，因此直接按照索引的顺序和映射关系逐条取出数据即可。而且如果分页的，那么只用取出索引表某个范围内的索引对应的数据，而不用像上述那取出所有数据进行排序再返回某个范围内的数据。（从磁盘取数据是最影响性能的） join 对join语句匹配关系（on）涉及的字段建立索引能够提高效率 索引覆盖 如果要查询的字段都建立过索引，那么引擎会直接在索引表中查询而不会访问原始数据（否则只要有一个字段没有建立索引就会做全表扫描），这叫索引覆盖。因此我们需要尽可能的在select后只写必要的查询字段，以增加索引覆盖的几率。 这里值得注意的是不要想着为每个字段建立索引，因为优先使用索引的优势就在于其体积小。 索引类型 主键索引: 数据列不允许重复，不允许为NULL，一个表只能有一个主键。 唯一索引: 数据列不允许重复，允许为NULL值，一个表允许多个列创建唯一索引。 可以通过 ALTER TABLE table_name ADD UNIQUE (column); 创建唯一索引 可以通过 ALTER TABLE table_name ADD UNIQUE (column1,column2); 创建唯一组合索引 普通索引: 基本的索引类型，没有唯一性的限制，允许为NULL值。 可以通过ALTER TABLE table_name ADD INDEX index_name (column);创建普通索引 可以通过ALTER TABLE table_name ADD INDEX index_name(column1, column2, column3);创建组合索引 全文索引： 是目前搜索引擎使用的一种关键技术。 可以通过ALTER TABLE table_name ADD FULLTEXT (column);创建全文索引 索引设计的原则 适合索引的列是出现在where子句中的列，或者连接子句中指定的列 基数较小的类，索引效果较差，没有必要在此列建立索引 使用短索引，如果对长字符串列进行索引，应该指定一个前缀长度，这样能够节省大量索引空间 不要过度索引。索引需要额外的磁盘空间，并降低写操作的性能。在修改表内容的时候，索引会进行更新甚至重构，索引列越多，这个时间就会越长。所以只保持需要的索引有利于查询即可。 创建索引的原则（重中之重） 索引虽好，但也不是无限制的使用，最好符合一下几个原则 1） 最左前缀匹配原则，组合索引非常重要的原则，mysql会一直向右匹配直到遇到范围查询(&gt;、&lt;、between、like)就停止匹配，比如a = 1 and b = 2 and c &gt; 3 and d = 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。 2）较频繁作为查询条件的字段才去创建索引 3）更新频繁字段不适合创建索引 4）若是不能有效区分数据的列不适合做索引列(如性别，男女未知，最多也就三种，区分度实在太低) 5）尽量的扩展索引，不要新建索引。比如表中已经有a的索引，现在要加(a,b)的索引，那么只需要修改原来的索引即可。 6）定义有外键的数据列一定要建立索引。 7）对于那些查询中很少涉及的列，重复值比较多的列不要建立索引。 8）对于定义为text、image和bit的数据类型的列不要建立索引。 创建索引的三种方式 第一种方式：在执行CREATE TABLE时创建索引 CREATE TABLE user_index2 ( id INT auto_increment PRIMARY KEY, first_name VARCHAR ( 16 ), last_name VARCHAR ( 16 ), id_card VARCHAR ( 18 ), information text, KEY NAME ( first_name, last_name ), FULLTEXT KEY ( information ), UNIQUE KEY ( id_card ) ); 第二种方式：使用ALTER TABLE命令去增加索引 //ALTER TABLE用来创建普通索引、UNIQUE索引或PRIMARY KEY索引。 ALTER TABLE table_name ADD INDEX index_name (column_list); 其中table_name是要增加索引的表名，column_list指出对哪些列进行索引，多列时各列之间用逗号分隔。 索引名index_name可自己命名，缺省时，MySQL将根据第一个索引列赋一个名称。另外，ALTER TABLE允许在单个语句中更改多个表，因此可以在同时创建多个索引。 第三种方式：使用CREATE INDEX命令创建 //CREATE INDEX可对表增加普通索引或UNIQUE索引。（但是，不能创建PRIMARY KEY索引） CREATE INDEX index_name ON table_name (column_list); 删除索引 根据索引名删除普通索引、唯一索引、全文索引：alter table 表名 drop KEY 索引名 ALTER TABLE user_index DROP KEY NAME; ALTER TABLE user_index DROP KEY id_card; ALTER TABLE user_index DROP KEY information; 删除主键索引：alter table 表名 drop primary key（因为主键只有一个）。这里值得注意的是，如果主键自增长，那么不能直接执行此操作（自增长依赖于主键索引）： 需要取消自增长再行删除： alter table user_index -- 重新定义字段 MODIFY id int, drop PRIMARY KEY 但通常不会删除主键，因为设计主键一定与业务逻辑无关。 创建索引时需要注意什么？ 非空字段：应该指定列为NOT NULL，除非你想存储NULL。在mysql中，含有空值的列很难进行查询优化，因为它们使得索引、索引的统计信息以及比较运算更加复杂。你应该用0、一个特殊的值或者一个空串代替空值； 取值离散大的字段：（变量各个取值之间的差异程度）的列放到联合索引的前面，可以通过count()函数查看字段的差异值，返回值越大说明字段的唯一值越多字段的离散程度高； 索引字段越小越好：数据库的数据存储以页为单位一页存储的数据越多一次IO操作获取的数据越大效率越高。 使用索引查询一定能提高查询的性能吗？为什么 通常，通过索引查询数据比全表扫描要快。但是我们也必须注意到它的代价。 索引需要空间来存储，也需要定期维护， 每当有记录在表中增减或索引列被修改时，索引本身也会被修改。 这意味着每条记录的INSERT，DELETE，UPDATE将为此多付出4，5 次的磁盘I/O。 因为索引需要额外的存储空间和处理，那些不必要的索引反而会使查询反应时间变慢。使用索引查询不一定能提高查询性能，索引范围查询(INDEX RANGE SCAN)适用于两种情况: 基于一个范围的检索，一般查询返回结果集小于表中记录数的30% 基于非唯一性索引的检索 百万级别或以上的数据如何删除 关于索引：由于索引需要额外的维护成本，因为索引文件是单独存在的文件,所以当我们对数据的增加,修改,删除,都会产生额外的对索引文件的操作,这些操作需要消耗额外的IO,会降低增/改/删的执行效率。所以，在我们删除数据库百万级别数据的时候，查询MySQL官方手册得知删除数据的速度和创建的索引数量是成正比的。 所以我们想要删除百万数据的时候可以先删除索引（此时大概耗时三分多钟） 然后删除其中无用数据（此过程需要不到两分钟） 删除完成后重新创建索引(此时数据较少了)创建索引也非常快，约十分钟左右。 与之前的直接删除绝对是要快速很多，更别说万一删除中断,一切删除会回滚。那更是坑了。 前缀索引 语法：index(field(10))，使用字段值的前10个字符建立索引，默认是使用字段的全部内容建立索引。 前提：前缀的标识度高。比如密码就适合建立前缀索引，因为密码几乎各不相同。 实操的难度：在于前缀截取的长度。 我们可以利用select count(*)/count(distinct left(password,prefixLen));，通过从调整prefixLen的值（从1自增）查看不同前缀长度的一个平均匹配度，接近1时就可以了（表示一个密码的前prefixLen个字符几乎能确定唯一一条记录） 什么是最左前缀原则？什么是最左匹配原则 顾名思义，就是最左优先，在创建多列索引时，要根据业务需求，where子句中使用最频繁的一列放在最左边。 最左前缀匹配原则，非常重要的原则，mysql会一直向右匹配直到遇到范围查询(&gt;、&lt;、between、like)就停止匹配，比如a = 1 and b = 2 and c &gt; 3 and d = 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。 =和in可以乱序，比如a = 1 and b = 2 and c = 3 建立(a,b,c)索引可以任意顺序，mysql的查询优化器会帮你优化成索引可以识别的形式 索引算法 索引算法有 BTree算法和Hash算法 BTree算法 BTree是最常用的mysql数据库索引算法，也是mysql默认的算法。因为它不仅可以被用在=,&gt;,&gt;=,&lt;,&lt;=和between这些比较操作符上，而且还可以用于like操作符，只要它的查询条件是一个不以通配符开头的常量， 例如： -- 只要它的查询条件是一个不以通配符开头的常量 select * from user where name like 'jack%'; -- 如果一通配符开头，或者没有使用常量，则不会使用索引，例如： select * from user where name like '%jack'; Hash算法 Hash Hash索引只能用于对等比较，例如=,&lt;=&gt;（相当于=）操作符。由于是一次定位数据，不像BTree索引需要从根节点到枝节点，最后才能访问到页节点这样多次IO访问，所以检索效率远高于BTree索引。 索引的数据结构（b树，hash） 索引的数据结构和具体存储引擎的实现有关，在MySQL中使用较多的索引有Hash索引，B+树索引等，而我们经常使用的InnoDB存储引擎的默认索引实现为：B+树索引。对于哈希索引来说，底层的数据结构就是哈希表，因此在绝大多数需求为单条记录查询的时候，可以选择哈希索引，查询性能最快；其余大部分场景，建议选择BTree索引。 B树索引 mysql通过存储引擎取数据，基本上90%的人用的就是InnoDB了，按照实现方式分，InnoDB的索引类型目前只有两种：BTREE（B树）索引和HASH索引。B树索引是Mysql数据库中使用最频繁的索引类型，基本所有存储引擎都支持BTree索引。通常我们说的索引不出意外指的就是（B树）索引（实际是用B+树实现的，因为在查看表索引时SHOW INDEX FROM &lt;表名&gt; [ FROM &lt;数据库名&gt;]，mysql一律打印BTREE，所以简称为B树索引） 查询方式： 主键索引区:PI(关联保存的数据的地址)按主键查询, 普通索引区:si(关联的id的地址,然后再到达上面的地址)。所以按主键查询,速度最快 B+tree性质： 1.）n棵子tree的节点包含n个关键字，不用来保存数据而是保存数据的索引。 2.）所有的叶子结点中包含了全部关键字的信息，及指向含这些关键字记录的指针，且叶子结点本身依关键字的大小自小而大顺序链接。 3.）所有的非终端结点可以看成是索引部分，结点中仅含其子树中的最大（或最小）关键字。 4.）B+ 树中，数据对象的插入和删除仅在叶节点上进行。 5.）B+树有2个头指针，一个是树的根节点，一个是最小关键码的叶节点。 哈希索引 简要说下，类似于数据结构中简单实现的HASH表（散列表）一样，当我们在mysql中用哈希索引时，主要就是通过Hash算法（常见的Hash算法有直接定址法、平方取中法、折叠法、除数取余法、随机数法），将数据库字段数据转换成定长的Hash值，与这条数据的行指针一并存入Hash表的对应位置；如果发生Hash碰撞（两个不同关键字的Hash值相同），则在对应Hash键下以链表形式存储。当然这只是简略模拟图。 B树和B+树的区别 在B树中，你可以将键和值存放在内部节点和叶子节点；但在B+树中，内部节点都是键，没有值，叶子节点同时存放键和值。 B+树的叶子节点有一条链相连，而B树的叶子节点各自独立。 使用B树的好处 B树可以在内部节点同时存储键和值，因此，把频繁访问的数据放在靠近根节点的地方将会大大提高热点数据的查询效率。这种特性使得B树在特定数据重复多次查询的场景中更加高效。 使用B+树的好处 由于B+树的内部节点只存放键，不存放值，因此，一次读取，可以在内存页中获取更多的键，有利于更快地缩小查找范围。 B+树的叶节点由一条链相连，因此，当需要进行一次全数据遍历的时候，B+树只需要使用O(logN)时间找到最小的一个节点，然后通过链进行O(N)的顺序遍历即可。而B树则需要对树的每一层进行遍历，这会需要更多的内存置换次数，因此也就需要花费更多的时间 Hash索引和B+树对比 首先要知道Hash索引和B+树索引的底层实现原理： hash索引底层就是hash表，进行查找时，调用一次hash函数就可以获取到相应的键值，之后进行回表查询获得实际数据。B+树底层实现是多路平衡查找树。对于每一次的查询都是从根节点出发，查找到叶子节点方可以获得所查键值，然后根据查询判断是否需要回表查询数据。 那么可以看出他们有以下的不同： hash索引进行等值查询更快(一般情况下)，但是却无法进行范围查询。 因为在hash索引中经过hash函数建立索引之后，索引的顺序与原顺序无法保持一致，不能支持范围查询。而B+树的的所有节点皆遵循(左节点小于父节点，右节点大于父节点，多叉树也类似)，天然支持范围。 hash索引不支持使用索引进行排序，原理同上。 hash索引不支持模糊查询以及多列索引的最左前缀匹配。原理也是因为hash函数的不可预测。AAAA和AAAAB的索引没有相关性。 hash索引任何时候都避免不了回表查询数据，而B+树在符合某些条件(聚簇索引，覆盖索引等)的时候可以只通过索引完成查询。 hash索引虽然在等值查询上较快，但是不稳定。性能不可预测，当某个键值存在大量重复的时候，发生hash碰撞，此时效率可能极差。而B+树的查询效率比较稳定，对于所有的查询都是从根节点到叶子节点，且树的高度较低。 因此，在大多数情况下，直接选择B+树索引可以获得稳定且较好的查询速度。而不需要使用hash索引。 数据库为什么使用B+树而不是B树 B树只适合随机检索，而B+树同时支持随机检索和顺序检索； B+树空间利用率更高，可减少I/O次数，磁盘读写代价更低。一般来说，索引本身也很大，不可能全部存储在内存中，因此索引往往以索引文件的形式存储的磁盘上。这样的话，索引查找过程中就要产生磁盘I/O消耗。B+树的内部结点并没有指向关键字具体信息的指针，只是作为索引使用，其内部结点比B树小，盘块能容纳的结点中关键字数量更多，一次性读入内存中可以查找的关键字也就越多，相对的，IO读写次数也就降低了。而IO读写次数是影响索引检索效率的最大因素； B+树的查询效率更加稳定。B树搜索有可能会在非叶子结点结束，越靠近根节点的记录查找时间越短，只要找到关键字即可确定记录的存在，其性能等价于在关键字全集内做一次二分查找。而在B+树中，顺序检索比较明显，随机检索时，任何关键字的查找都必须走一条从根节点到叶节点的路，所有关键字的查找路径长度相同，导致每一个关键字的查询效率相当。 B-树在提高了磁盘IO性能的同时并没有解决元素遍历的效率低下的问题。B+树的叶子节点使用指针顺序连接在一起，只要遍历叶子节点就可以实现整棵树的遍历。而且在数据库中基于范围的查询是非常频繁的，而B树不支持这样的操作。 增删文件（节点）时，效率更高。因为B+树的叶子节点包含所有关键字，并以有序的链表结构存储，这样可很好提高增删效率。 B+树在满足聚簇索引和覆盖索引的时候不需要回表查询数据 在B+树的索引中，叶子节点可能存储了当前的key值，也可能存储了当前的key值以及整行的数据，这就是聚簇索引和非聚簇索引。 在InnoDB中，只有主键索引是聚簇索引，如果没有主键，则挑选一个唯一键建立聚簇索引。如果没有唯一键，则隐式的生成一个键来建立聚簇索引。 当查询使用聚簇索引时，在对应的叶子节点，可以获取到整行数据，因此不用再次进行回表查询。 什么是聚簇索引？何时使用聚簇索引与非聚簇索引 聚簇索引：将数据存储与索引放到了一块，找到索引也就找到了数据 非聚簇索引：将数据存储于索引分开结构，索引结构的叶子节点指向了数据的对应行，myisam通过key_buffer把索引先缓存到内存中，当需要访问数据时（通过索引访问数据），在内存中直接搜索索引，然后通过索引找到磁盘相应数据，这也就是为什么索引不在key buffer命中时，速度慢的原因 澄清一个概念：innodb中，在聚簇索引之上创建的索引称之为辅助索引，辅助索引访问数据总是需要二次查找，非聚簇索引都是辅助索引，像复合索引、前缀索引、唯一索引，辅助索引叶子节点存储的不再是行的物理位置，而是主键值 何时使用聚簇索引与非聚簇索引 非聚簇索引一定会回表查询吗？ 不一定，这涉及到查询语句所要求的字段是否全部命中了索引，如果全部命中了索引，那么就不必再进行回表查询。 举个简单的例子，假设我们在员工表的年龄上建立了索引，那么当进行select age from employee where age &lt; 20的查询时，在索引的叶子节点上，已经包含了age信息，不会再次进行回表查询。 联合索引 MySQL可以使用多个字段同时建立一个索引，叫做联合索引。 在联合索引中，如果想要命中索引，需要按照建立索引时的字段顺序挨个使用，否则无法命中索引。 具体原因为: MySQL使用索引时需要索引有序，假设现在建立了&quot;name，age，school&quot;的联合索引，那么索引的排序为: 先按照name排序，如果name相同，则按照age排序，如果age的值也相等，则按照school进行排序。 当进行查询时，此时索引仅仅按照name严格有序，因此必须首先使用name字段进行等值查询，之后对于匹配到的列而言，其按照age字段严格有序，此时可以使用age字段用做索引查找，以此类推。因此在建立联合索引的时候应该注意索引列的顺序，一般情况下，将查询需求频繁或者字段选择性高的列放在前面。此外可以根据特例的查询或者表结构进行单独的调整。 漫画：什么是B-树？https://tinaxiawuhao.github.io/post/LhRxiTagh/ 漫画：什么是B+树？https://tinaxiawuhao.github.io/post/jVTlR3ljT/ ","link":"https://tianxiawuhao.github.io/QzFPAEzVT/"},{"title":"mysql概述一","content":"为什么要使用数据库 数据保存在内存 优点： 存取速度快 缺点： 数据不能永久保存 数据保存在文件 优点： 数据永久保存 缺点：1）速度比内存操作慢，频繁的IO操作。 ​ 2）查询数据不方便 数据保存在数据库 1）数据永久保存 2）使用SQL语句，查询方便效率高。 3）管理数据方便 SQL 结构化查询语言(Structured Query Language)简称SQL，是一种数据库查询语言。 作用：用于存取数据、查询、更新和管理关系数据库系统。 MySQL MySQL是一个关系型数据库管理系统，由瑞典MySQL AB 公司开发，属于 Oracle 旗下产品。MySQL 是最流行的关系型数据库管理系统之一，在 WEB 应用方面，MySQL是最好的 RDBMS (Relational Database Management System，关系数据库管理系统) 应用软件之一。在Java企业级开发中非常常用，因为 MySQL 是开源免费的，并且方便扩展。 数据库三大范式是什么 第一范式：每个列都不可以再拆分。 第二范式：在第一范式的基础上，非主键列完全依赖于主键，而不能是依赖于主键的一部分。 第三范式：在第二范式的基础上，非主键列只依赖于主键，不依赖于其他非主键。 在设计数据库结构的时候，要尽量遵守三范式，如果不遵守，必须有足够的理由。比如性能。事实上我们经常会为了性能而妥协数据库的设计。 mysql有关权限的表 MySQL服务器通过权限表来控制用户对数据库的访问，权限表存放在mysql数据库里，由mysql_install_db脚本初始化。这些权限表分别user，db，table_priv，columns_priv和host。下面分别介绍一下这些表的结构和内容： user权限表：记录允许连接到服务器的用户帐号信息，里面的权限是全局级的。 db权限表：记录各个帐号在各个数据库上的操作权限。 table_priv权限表：记录数据表级的操作权限。 columns_priv权限表：记录数据列级的操作权限。 host权限表：配合db权限表对给定主机上数据库级操作权限作更细致的控制。这个权限表不受GRANT和REVOKE语句的影响。 MySQL的binlog有几种录入格式？分别有什么区别？ 有三种格式，statement，row和mixed。 statement模式下，每一条会修改数据的sql都会记录在binlog中。不需要记录每一行的变化，减少了binlog日志量，节约了IO，提高性能。由于sql的执行是有上下文的，因此在保存的时候需要保存相关的信息，同时还有一些使用了函数之类的语句无法被记录复制。 row级别下，不记录sql语句上下文相关信息，仅保存哪条记录被修改。记录单元为每一行的改动，基本是可以全部记下来但是由于很多操作，会导致大量行的改动(比如alter table)，因此这种模式的文件保存的信息太多，日志量太大。 mixed，一种折中的方案，普通操作使用statement记录，当无法使用statement的时候使用row。 此外，新版的MySQL中对row级别也做了一些优化，当表结构发生变化的时候，会记录语句而不是逐行记录。 数据类型 分类 类型名称 说明 整数类型 tinyInt 很小的整数(8位二进制) smallint 小的整数(16位二进制) mediumint 中等大小的整数(24位二进制) int(integer) 普通大小的整数(32位二进制) 小数类型 float 单精度浮点数 double 双精度浮点数 decimal(m,d) 压缩严格的定点数 日期类型 year YYYY 1901~2155 time HH:MM:SS -838:59:59~838:59:59 date YYYY-MM-DD 1000-01-01~9999-12-3 datetime YYYY-MM-DD HH:MM:SS 1000-01-01 00:00:00~ 9999-12-31 23:59:59 timestamp YYYY-MM-DD HH:MM:SS 19700101 00:00:01 UTC~2038-01-19 03:14:07UTC 文本、二进制类型 CHAR(M) M为0~255之间的整数 VARCHAR(M) M为0~65535之间的整数 TINYBLOB 允许长度0~255字节 BLOB 允许长度0~65535字节 MEDIUMBLOB 允许长度0~167772150字节 LONGBLOB 允许长度0~4294967295字节 TINYTEXT 允许长度0~255字节 TEXT 允许长度0~65535字节 MEDIUMTEXT 允许长度0~167772150字节 LONGTEXT 允许长度0~4294967295字节 VARBINARY(M) 允许长度0~M个字节的变长字节字符串 BINARY(M) 允许长度0~M个字节的定长字节字符串 1、整数类型，包括TINYINT、SMALLINT、MEDIUMINT、INT、BIGINT，分别表示1字节、2字节、3字节、4字节、8字节整数。任何整数类型都可以加上UNSIGNED属性，表示数据是无符号的，即非负整数。 长度：整数类型可以被指定长度，例如：INT(11)表示长度为11的INT类型。长度在大多数场景是没有意义的，它不会限制值的合法范围，只会影响显示字符的个数，而且需要和UNSIGNED ZEROFILL属性配合使用才有意义。 例子，假定类型设定为INT(5)，属性为UNSIGNED ZEROFILL，如果用户插入的数据为12的话，那么数据库实际存储数据为00012。 2、实数类型，包括FLOAT、DOUBLE、DECIMAL。 DECIMAL可以用于存储比BIGINT还大的整型，能存储精确的小数。 而FLOAT和DOUBLE是有取值范围的，并支持使用标准的浮点进行近似计算。 计算时FLOAT和DOUBLE相比DECIMAL效率更高一些，DECIMAL你可以理解成是用字符串进行处理。 3、字符串类型，包括VARCHAR、CHAR、TEXT、BLOB VARCHAR用于存储可变长字符串，它比定长类型更节省空间。 VARCHAR使用额外1或2个字节存储字符串长度。列长度小于255字节时，使用1字节表示，否则使用2字节表示。 VARCHAR存储的内容超出设置的长度时，内容会被截断。 CHAR是定长的，根据定义的字符串长度分配足够的空间。 CHAR会根据需要使用空格进行填充方便比较。 CHAR适合存储很短的字符串，或者所有值都接近同一个长度。 CHAR存储的内容超出设置的长度时，内容同样会被截断。 使用策略： 对于经常变更的数据来说，CHAR比VARCHAR更好，因为CHAR不容易产生碎片。 对于非常短的列，CHAR比VARCHAR在存储空间上更有效率。 使用时要注意只分配需要的空间，更长的列排序时会消耗更多内存。 尽量避免使用TEXT/BLOB类型，查询时会使用临时表，导致严重的性能开销。 4、枚举类型（ENUM），把不重复的数据存储为一个预定义的集合。 有时可以使用ENUM代替常用的字符串类型。 ENUM存储非常紧凑，会把列表值压缩到一个或两个字节。 ENUM在内部存储时，其实存的是整数。 尽量避免使用数字作为ENUM枚举的常量，因为容易混乱。 排序是按照内部存储的整数 5、日期和时间类型，尽量使用timestamp，空间效率高于datetime， 用整数保存时间戳通常不方便处理。 如果需要存储微妙，可以使用bigint存储。 MySQL存储引擎 存储引擎Storage engine：MySQL中的数据、索引以及其他对象是如何存储的，是一套文件系统的实现。 常用的存储引擎有以下： Innodb引擎：Innodb引擎提供了对数据库ACID事务的支持。并且还提供了行级锁和外键的约束。它的设计的目标就是处理大数据容量的数据库系统。 MyIASM引擎(原本Mysql的默认引擎)：不提供事务的支持，也不支持行级锁和外键。 MEMORY引擎：所有的数据都在内存中，数据的处理速度快，但是安全性不高。 MyISAM与InnoDB区别 MyISAM Innodb 存储结构 每张表被存放在三个文件：frm-表格定义、MYD(MYData)-数据文件、MYI(MYIndex)-索引文件 所有的表都保存在同一个数据文件中（也可能是多个文件，或者是独立的表空间文件），InnoDB表的大小只受限于操作系统文件的大小，一般为2GB 存储空间 MyISAM可被压缩，存储空间较小 InnoDB的表需要更多的内存和存储，它会在主内存中建立其专用的缓冲池用于高速缓冲数据和索引 可移植性、备份及恢复 由于MyISAM的数据是以文件的形式存储，所以在跨平台的数据转移中会很方便。在备份和恢复时可单独针对某个表进行操作 免费的方案可以是拷贝数据文件、备份 binlog，或者用 mysqldump，在数据量达到几十G的时候就相对痛苦了 文件格式 数据和索引是分别存储的，数据.MYD，索引.MYI 数据和索引是集中存储的，.ibd 记录存储顺序 按记录插入顺序保存 按主键大小有序插入 外键 不支持 支持 事务 不支持 支持 锁支持（锁是避免资源争用的一个机制，MySQL锁对用户几乎是透明的） 表级锁定 行级锁定、表级锁定，锁定力度小并发能力高 SELECT MyISAM更优 INSERT、UPDATE、DELETE InnoDB更优 select count(*) myisam更快，因为myisam内部维护了一个计数器，可以直接调取。 索引的实现方式 B+树索引，myisam 是堆表 B+树索引，Innodb 是索引组织表 哈希索引 不支持 支持 全文索引 支持 不支持 MyISAM索引与InnoDB索引的区别？ InnoDB索引是聚簇索引，MyISAM索引是非聚簇索引。 InnoDB的主键索引的叶子节点存储着行数据，因此主键索引非常高效。 MyISAM索引的叶子节点存储的是行数据地址，需要再寻址一次才能得到数据。 InnoDB非主键索引的叶子节点存储的是主键和其他带索引的列数据，因此查询时做到覆盖索引会非常高效。 InnoDB引擎的4大特性 插入缓冲（Insert Buffer/Change Buffer) 提升插入性能，change buffering是insert buffer的加强，insert buffer只针对insert有效，change buffering对insert、delete、update(delete+insert)、purge都有效 只对于非聚集索引（非唯一）的插入和更新有效，对于每一次的插入不是写到索引页中，而是先判断插入的非聚集索引页是否在缓冲池中，如果在则直接插入；若不在，则先放到Insert Buffer 中，再按照一定的频率进行合并操作，再写回disk。这样通常能将多个插入合并到一个操作中，目的还是为了减少随机IO带来性能损耗。 使用插入缓冲的条件： * 非聚集索引 * 非唯一索引 Change buffer是作为buffer pool中的一部分存在。Innodb_change_buffering参数缓存所对应的操作：(update会被认为是delete+insert) innodb_change_buffering，设置的值有：inserts、deletes、purges、changes（inserts和deletes）、all（默认）、none。 all: 默认值，缓存insert, delete, purges操作 none: 不缓存 inserts: 缓存insert操作 deletes: 缓存delete操作 changes: 缓存insert和delete操作 purges: 缓存后台执行的物理删除操作 可以通过参数控制其使用的大小： innodb_change_buffer_max_size，默认是25%，即缓冲池的1/4。最大可设置为50%。当MySQL实例中有大量的修改操作时，要考虑增大innodb_change_buffer_max_size 上面提过在一定频率下进行合并，那所谓的频率是什么条件？ 1）辅助索引页被读取到缓冲池中。正常的select先检查Insert Buffer是否有该非聚集索引页存在，若有则合并插入。 2）辅助索引页没有可用空间。空间小于1/32页的大小，则会强制合并操作。 3）Master Thread 每秒和每10秒的合并操作。 二次写(double write) Doublewrite缓存是位于系统表空间的存储区域，用来缓存InnoDB的数据页从innodb buffer pool中flush之后并写入到数据文件之前，所以当操作系统或者数据库进程在数据页写磁盘的过程中崩溃，Innodb可以在doublewrite缓存中找到数据页的备份而用来执行crash恢复。数据页写入到doublewrite缓存的动作所需要的IO消耗要小于写入到数据文件的消耗，因为此写入操作会以一次大的连续块的方式写入 在应用（apply）重做日志前，用户需要一个页的副本，当写入失效发生时，先通过页的副本来还原该页，再进行重做，这就是double write doublewrite组成： 内存中的doublewrite buffer,大小2M。 物理磁盘上共享表空间中连续的128个页，即2个区（extend），大小同样为2M。 对缓冲池的脏页进行刷新时，不是直接写磁盘，而是会通过memcpy()函数将脏页先复制到内存中的doublewrite buffer，之后通过doublewrite 再分两次，每次1M顺序地写入共享表空间的物理磁盘上，在这个过程中，因为doublewrite页是连续的，因此这个过程是顺序写的，开销并不是很大。在完成doublewrite页的写入后，再将doublewrite buffer 中的页写入各个 表空间文件中，此时的写入则是离散的。如果操作系统在将页写入磁盘的过程中发生了崩溃，在恢复过程中，innodb可以从共享表空间中的doublewrite中找到该页的一个副本，将其复制到表空间文件，再应用重做日志。 自适应哈希索引(ahi) Adaptive Hash index属性使得InnoDB更像是内存数据库。该属性通过innodb_adapitve_hash_index开启，也可以通过—skip-innodb_adaptive_hash_index参数 Innodb存储引擎会监控对表上二级索引的查找，如果发现某二级索引被频繁访问，二级索引成为热数据，建立哈希索引可以带来速度的提升 经常访问的二级索引数据会自动被生成到hash索引里面去(最近连续被访问三次的数据)，自适应哈希索引通过缓冲池的B+树构造而来，因此建立的速度很快。 哈希（hash）是一种非常快的等值查找方法，在一般情况下这种查找的时间复杂度为O(1),即一般仅需要一次查找就能定位数据。而B+树的查找次数，取决于B+树的高度，在生产环境中，B+树的高度一般3-4层，故需要3-4次的查询。 innodb会监控对表上个索引页的查询。如果观察到建立哈希索引可以带来速度提升，则自动建立哈希索引，称之为自适应哈希索引（Adaptive Hash Index，AHI）。 AHI有一个要求，就是对这个页的连续访问模式必须是一样的。 例如对于（a,b）访问模式情况： where a = xxx where a = xxx and b = xxx 特点 1、无序，没有树高 2、降低对二级索引树的频繁访问资源，索引树高&lt;=4，访问索引：访问树、根节点、叶子节点 3、自适应 缺陷 1、hash自适应索引会占用innodb buffer pool； 2、自适应hash索引只适合搜索等值的查询，如select * from table where index_col='xxx'，而对于其他查找类型，如范围查找，是不能使用的； 3、极端情况下，自适应hash索引才有比较大的意义，可以降低逻辑读。 预读(read ahead) InnoDB使用两种预读算法来提高I/O性能：线性预读（linear read-ahead）和随机预读（randomread-ahead） 为了区分这两种预读的方式，我们可以把线性预读放到以extent为单位，而随机预读放到以extent中的page为单位。线性预读着眼于将下一个extent提前读取到buffer pool中，而随机预读着眼于将当前extent中的剩余的page提前读取到buffer pool中。 线性预读（linear read-ahead） 线性预读方式有一个很重要的变量控制是否将下一个extent预读到buffer pool中，通过使用配置参数innodb_read_ahead_threshold，可以控制Innodb执行预读操作的时间。如果一个extent中的被顺序读取的page超过或者等于该参数变量时，Innodb将会异步的将下一个extent读取到buffer pool中，innodb_read_ahead_threshold可以设置为0-64的任何值，默认值为56，值越高，访问模式检查越严格 例如，如果将值设置为48，则InnoDB只有在顺序访问当前extent中的48个pages时才触发线性预读请求，将下一个extent读到内存中。如果值为8，InnoDB触发异步预读，即使程序段中只有8页被顺序访问。你可以在MySQL配置文件中设置此参数的值，或者使用SET GLOBAL需要该SUPER权限的命令动态更改该参数。 在没有该变量之前，当访问到extent的最后一个page的时候，Innodb会决定是否将下一个extent放入到buffer pool中。 随机预读（randomread-ahead） 随机预读方式则是表示当同一个extent中的一些page在buffer pool中发现时，Innodb会将该extent中的剩余page一并读到buffer pool中，由于随机预读方式给Innodb code带来了一些不必要的复杂性，同时在性能也存在不稳定性，在5.5中已经将这种预读方式废弃。要启用此功能，请将配置变量设置innodb_random_read_ahead为ON。 存储引擎选择 如果没有特别的需求，使用默认的Innodb即可。 MyISAM：以读写插入为主的应用程序，比如博客系统、新闻门户网站。 Innodb：更新（删除）操作频率也高，或者要保证数据的完整性；并发量高，支持事务和外键。比如OA自动化办公系统。 MySQL查询过程 ​ MySQL查询优化实质说白了就是遵循一些原则让MySQL的优化器能够按照预想的合理方式运行，从而达到不同的业务目标需要实现的效果。下图展示了MySQL的查询过程。 客户端/服务端通信协议 MySQL客户端/服务端通信协议是&quot;半双工&quot;的：在任意时刻，要么是服务器向客户端发送数据，要么是客户端向服务器发送数据，这两个动作不能同时发生。一旦一端开始发送消息，另一端要接收完整个消息才能响应它，所以我们无法也无须将一个消息切成小块独立发送，也没有办法进行流量控制。 客户端用一个单独的数据包将查询请求发送给服务器，所以当查询语句很长的时候，需要设置max_allowed_packet参数。但是需要注意的是，如果查询实在是太大，服务端会拒绝接收更多数据并抛出异常。 与之相反的是，服务器响应给用户的数据通常会很多，由多个数据包组成。但是当服务器响应客户端请求时，客户端必须完整的接收整个返回结果，而不能简单的只取前面几条结果，然后让服务器停止发送。因而在实际开发中，尽量保持查询简单且只返回必需的数据，减小通信间数据包的大小和数量是一个非常好的习惯，这也是查询中尽量避免使用SELECT *以及加上LIMIT限制的原因之一。 查询缓存 在解析一个查询语句前，如果查询缓存是打开的，那么MySQL会检查这个查询语句是否命中查询缓存中的数据。如果当前查询恰好命中查询缓存，在检查一次用户权限后直接返回缓存中的结果。这种情况下，查询不会被解析，也不会生成执行计划，更不会执行。 MySQL将缓存存放在一个引用表（类似于HashMap的数据结构）中，通过哈希值索引，这个哈希值通过查询本身、当前要查询的数据库、客户端协议版本号等一些可能影响结果的信息计算得来。所以两个查询在任何字符上的不同（例如：空格、注释），都会导致缓存不会命中。 如果查询中包含任何用户自定义函数、存储函数、用户变量、临时表、mysql库中的系统表，其查询结果都不会被缓存。比如函数NOW()或者CURRENT_DATE()会因为不同的查询时间，返回不同的查询结果，再比如包含CURRENT_USER或者CONNECION_ID()的查询语句会因为不同的用户而返回不同的结果，将这样的查询结果缓存起来没有任何的意义。 MySQL的查询缓存系统会跟踪查询中涉及的每个表，如果这些表（数据或结构）发生变化，那么和这张表相关的所有缓存数据都将失效。正因为如此，在任何的写操作时，MySQL必须将对应表的所有缓存都设置为失效。如果查询缓存非常大或者碎片很多，这个操作就可能带来很大的系统消耗，甚至导致系统僵死一会儿。而且查询缓存对系统的额外消耗也不仅仅在写操作，读操作也不例外。如果查询结果被缓存，那么执行完成后，会将结果存入缓存，也会带来额外的系统消耗。基于此，我们知道并不是什么情况下查询缓存都会提高系统性能，缓存和失效都会带来额外消耗，只有当缓存带来的资源节约大于其本身消耗的资源时，才会给系统带来性能提升。如果系统确实存在一些性能问题，可以尝试打开查询缓存，并在数据库设计上做一些优化，比如： 1）多个小表代替一个大表（注意不要过度设计） 2）批量插入代替循环单条插入，降低磁盘IO次数 3）合理控制缓存空间大小，一般来说其大小设置为几十兆比较合适 4）可以通过SQL_CACHE和SQL_NO_CACHE来控制某个查询语句是否需要进行缓存 语法解析和预处理 MySQL通过关键字将SQL语句进行解析并生成一颗对应的解析树。这个过程解析器主要通过语法规则来验证和解析，比如SQL中是否使用了错误的关键字或者关键字的顺序是否正确等等。预处理则会根据MySQL规则进一步检查解析树是否合法，比如检查要查询的数据表和数据列是否存在等等。 查询优化 经过前面的步骤生成的语法树被认为是合法的了，并且由优化器将其转化成查询计划。多数情况下一条查询可以有很多种执行方式，最后都返回相应的结果。优化器的作用就是找到这其中最好的执行计划。MySQL使用基于成本的优化器，它尝试预测一个查询使用某种执行计划时的成本，并选择其中成本最小的一个。在MySQL可以通过查询当前会话的last_query_cost的值来得到其计算当前查询的成本。 有非常多的原因会导致MySQL选择错误的执行计划，比如统计信息不准确、不会考虑不受其控制的操作成本（用户自定义函数、存储过程）、MySQL认为的最优跟我们想的不一样，我们希望执行时间尽可能短，但MySQL只选择它认为成本小的，但成本小有的时候并不是我们的预期。 查询执行引擎 在完成解析和优化阶段以后，MySQL会生成对应的执行计划，查询执行引擎根据执行计划给出的指令逐步执行得出结果。整个执行过程的大部分操作均是通过调用存储引擎实现的接口来完成，这些接口被称为handler API。查询过程中的每一张表由一个handler实例表示。实际上，MySQL在查询优化阶段就为每一张表创建了一个handler实例，优化器可以根据这些实例的接口来获取表的相关信息，包括表的所有列名、索引统计信息等。存储引擎接口提供了非常丰富的功能，但其底层仅有几十个接口，这些接口像搭积木一样完成了一次查询的大部分操作。 返回结果给客户端 查询执行的最后一个阶段就是将结果返回给客户端。即使查询不到数据，MySQL仍然会返回这个查询的相关信息，比如该查询影响到的行数以及执行时间等等。如果查询缓存被打开且这个查询可以被缓存，MySQL也会将结果存放到缓存中。结果集返回客户端是一个增量且逐步返回的过程。有可能MySQL在生成第一条结果时，就开始向客户端逐步返回结果集了。这样服务端就无须存储太多结果而消耗过多内存，也可以让客户端第一时间获得返回结果。需要注意的是，结果集中的每一行都会以一个满足客户端/服务器通信协议的数据包发送，再通过TCP协议进行传输，在传输过程中，可能对MySQL的数据包进行缓存然后批量发送。 ","link":"https://tianxiawuhao.github.io/Nq-4WaiGL/"},{"title":"redis概述五","content":"分布式问题 Redis实现分布式锁 Redis为单进程单线程模式，采用队列模式将并发访问变成串行访问，且多客户端对Redis的连接并不存在竞争关系Redis中可以使用SETNX命令实现分布式锁。 当且仅当 key 不存在，将 key 的值设为 value。 若给定的 key 已经存在，则 SETNX 不做任何动作 SETNX 是『SET if Not eXists』(如果不存在，则 SET)的简写。 返回值：设置成功，返回 1 。设置失败，返回 0 。 使用SETNX完成同步锁的流程及事项如下： 使用SETNX命令获取锁，若返回0（key已存在，锁已存在）则获取失败，反之获取成功 为了防止获取锁后程序出现异常，导致其他线程/进程调用SETNX命令总是返回0而进入死锁状态，需要为该key设置一个“合理”的过期时间 释放锁，使用DEL命令将锁数据删除 package com.example.redisstudy.template; import org.apache.commons.lang3.StringUtils; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.data.redis.core.StringRedisTemplate; import org.springframework.stereotype.Component; import java.util.concurrent.TimeUnit; /** * @Description //直接使用Redis进行分布式锁 * 这是简易版本 如果要使用Redis原生锁记得加过期时间，防止死锁 最好使用Redisson操作简单更加方便 * @Date * @Author wuhao **/ @Component public class RedisLockCommon { @Autowired private StringRedisTemplate stringRedisTemplate; private Integer EXPIRE_TIME=3000; /** * Redis加锁的操作 * * @param key * @param value * @return */ public Boolean tryLock(String key, String value) { if (stringRedisTemplate.opsForValue().setIfAbsent(key, value, EXPIRE_TIME, TimeUnit.SECONDS)) { return true; } return false; } /** * Redis解锁的操作 * * @param key * @param value */ public void unlock(String key, String value) { String currentValue = stringRedisTemplate.opsForValue().get(key); try { if (StringUtils.isNotEmpty(currentValue) &amp;&amp; currentValue.equals(value)) { stringRedisTemplate.opsForValue().getOperations().delete(key); } } catch (Exception e) { } } } 如何解决 Redis 的并发竞争 Key 问题 所谓 Redis 的并发竞争 Key 的问题也就是多个系统同时对一个 key 进行操作，但是最后执行的顺序和我们期望的顺序不同，这样也就导致了结果的不同！ 推荐一种方案：分布式锁（zookeeper 和 redis 都可以实现分布式锁）。（如果不存在 Redis 的并发竞争 Key 问题，不要使用分布式锁，这样会影响性能） 基于zookeeper临时有序节点可以实现的分布式锁。大致思想为：每个客户端对某个方法加锁时，在zookeeper上的与该方法对应的指定节点的目录下，生成一个唯一的瞬时有序节点。 判断是否获取锁的方式很简单，只需要判断有序节点中序号最小的一个。 当释放锁的时候，只需将这个瞬时节点删除即可。同时，其可以避免服务宕机导致的锁无法释放，而产生的死锁问题。完成业务流程后，删除对应的子节点释放锁。 在实践中，当然是从以可靠性为主。所以首推Zookeeper。 参考：https://www.jianshu.com/p/8bddd381de06 分布式Redis是前期做还是后期规模上来了再做好？为什么？ 既然Redis是如此的轻量（单实例只使用1M内存），为防止以后的扩容，最好的办法就是一开始就启动较多实例。即便你只有一台服务器，你也可以一开始就让Redis以分布式的方式运行，使用分区，在同一台服务器上启动多个实例。 一开始就多设置几个Redis实例，例如32或者64个实例，对大多数用户来说这操作起来可能比较麻烦，但是从长久来看做这点牺牲是值得的。 这样的话，当你的数据不断增长，需要更多的Redis服务器时，你需要做的就是仅仅将Redis实例从一台服务迁移到另外一台服务器而已（而不用考虑重新分区的问题）。一旦你添加了另一台服务器，你需要将你一半的Redis实例从第一台机器迁移到第二台机器。 什么是 RedLock Redis 官方站提出了一种权威的基于 Redis 实现分布式锁的方式名叫 Redlock，此种方式比原先的单节点的方法更安全。它可以保证以下特性： 安全特性：互斥访问，即永远只有一个 client 能拿到锁 避免死锁：最终 client 都可能拿到锁，不会出现死锁的情况，即使原本锁住某资源的 client crash 了或者出现了网络分区 容错性：只要大部分 Redis 节点存活就可以正常提供服务 package com.example.redisstudy.template; import org.springframework.data.redis.connection.RedisConnection; import org.springframework.data.redis.connection.RedisConnectionFactory; import org.springframework.data.redis.connection.ReturnType; import org.springframework.data.redis.core.RedisConnectionUtils; import org.springframework.data.redis.core.StringRedisTemplate; import org.springframework.stereotype.Repository; import java.nio.charset.Charset; import java.util.UUID; import java.util.concurrent.TimeUnit; @Repository public class RedisLock { /** * 解锁脚本，原子操作 */ private static final String unlockScript = &quot;if redis.call(\\&quot;get\\&quot;,KEYS[1]) == ARGV[1]\\n&quot; + &quot;then\\n&quot; + &quot; return redis.call(\\&quot;del\\&quot;,KEYS[1])\\n&quot; + &quot;else\\n&quot; + &quot; return 0\\n&quot; + &quot;end&quot;; private StringRedisTemplate redisTemplate; public RedisLock(StringRedisTemplate redisTemplate) { this.redisTemplate = redisTemplate; } /** * 加锁，有阻塞 * @param name * @param expire * @param timeout * @return */ public String lock(String name, long expire, long timeout){ long startTime = System.currentTimeMillis(); String token; do{ token = tryLock(name, expire); if(token == null) { if((System.currentTimeMillis()-startTime) &gt; (timeout-50)) break; try { Thread.sleep(50); //try 50 per sec } catch (InterruptedException e) { e.printStackTrace(); return null; } } }while(token==null); return token; } /** * 加锁，无阻塞 * @param name * @param expire * @return */ public String tryLock(String name, long expire) { String token = UUID.randomUUID().toString(); if (redisTemplate.opsForValue().setIfAbsent(name, token, expire, TimeUnit.SECONDS)) { return token; } return null; } /** * 解锁 * @param name * @param token * @return */ public boolean unlock(String name, String token) { byte[][] keysAndArgs = new byte[2][]; keysAndArgs[0] = name.getBytes(Charset.forName(&quot;UTF-8&quot;)); keysAndArgs[1] = token.getBytes(Charset.forName(&quot;UTF-8&quot;)); RedisConnectionFactory factory = redisTemplate.getConnectionFactory(); RedisConnection conn = factory.getConnection(); try { Long result = (Long)conn.scriptingCommands().eval(unlockScript.getBytes(Charset.forName(&quot;UTF-8&quot;)), ReturnType.INTEGER, 1, keysAndArgs); if(result!=null &amp;&amp; result&gt;0) return true; }finally { RedisConnectionUtils.releaseConnection(conn, factory); } return false; } } 缓存雪崩 缓存雪崩是指缓存同一时间大面积的失效，所以，后面的请求都会落到数据库上，造成数据库短时间内承受大量请求而崩掉。 解决方案 缓存数据的过期时间设置随机，防止同一时间大量数据过期现象发生。 一般并发量不是特别多的时候，使用最多的解决方案是加锁排队。 给每一个缓存数据增加相应的缓存标记，记录缓存的是否失效，如果缓存标记失效，则更新数据缓存。 缓存穿透 缓存穿透是指缓存和数据库中都没有的数据，导致所有的请求都落到数据库上，造成数据库短时间内承受大量请求而崩掉。 解决方案 接口层增加校验，如用户鉴权校验，id做基础校验，id&lt;=0的直接拦截； 从缓存取不到的数据，在数据库中也没有取到，这时也可以将key-value对写为key-null，缓存有效时间可以设置短点，如30秒（设置太长会导致正常情况也没法使用）。这样可以防止攻击用户反复用同一个id暴力攻击 采用布隆过滤器，将所有可能存在的数据哈希到一个足够大的 bitmap 中，一个一定不存在的数据会被这个 bitmap 拦截掉，从而避免了对底层存储系统的查询压力 附加 对于空间的利用到达了一种极致，那就是Bitmap和布隆过滤器(Bloom Filter)。 Bitmap： 典型的就是哈希表 缺点是，Bitmap对于每个元素只能记录1bit信息，如果还想完成额外的功能，恐怕只能靠牺牲更多的空间、时间来完成了。 布隆过滤器（推荐） 就是引入了k(k&gt;1)个相互独立的哈希函数，保证在给定的空间、误判率下，完成元素判重的过程。 它的优点是空间效率和查询时间都远远超过一般的算法，缺点是有一定的误识别率和删除困难。 Bloom-Filter算法的核心思想就是利用多个不同的Hash函数来解决“冲突”。 Hash存在一个冲突（碰撞）的问题，用同一个Hash得到的两个URL的值有可能相同。为了减少冲突，我们可以多引入几个Hash，如果通过其中的一个Hash值我们得出某元素不在集合中，那么该元素肯定不在集合中。只有在所有的Hash函数告诉我们该元素在集合中时，才能确定该元素存在于集合中。这便是Bloom-Filter的基本思想。 Bloom-Filter一般用于在大数据量的集合中判定某元素是否存在。 缓存击穿 缓存击穿是指缓存中没有但数据库中有的数据（一般是缓存时间到期），这时由于并发用户特别多，同时读缓存没读到数据，又同时去数据库去取数据，引起数据库压力瞬间增大，造成过大压力。和缓存雪崩不同的是，缓存击穿指并发查同一条数据，缓存雪崩是不同数据都过期了，很多数据都查不到从而查数据库。 解决方案 设置热点数据永远不过期。 加互斥锁，互斥锁 缓存预热 缓存预热就是系统上线后，将相关的缓存数据直接加载到缓存系统。这样就可以避免在用户请求的时候，先查询数据库，然后再将数据缓存的问题！用户直接查询事先被预热的缓存数据！ 解决方案 直接写个缓存刷新页面，上线时手工操作一下； 数据量不大，可以在项目启动的时候自动进行加载； 定时刷新缓存； 缓存降级 当访问量剧增、服务出现问题（如响应时间慢或不响应）或非核心服务影响到核心流程的性能时，仍然需要保证服务还是可用的，即使是有损服务。系统可以根据一些关键数据进行自动降级，也可以配置开关实现人工降级。 缓存降级的最终目的是保证核心服务可用，即使是有损的。而且有些服务是无法降级的（如加入购物车、结算）。 在进行降级之前要对系统进行梳理，看看系统是不是可以丢卒保帅；从而梳理出哪些必须誓死保护，哪些可降级；比如可以参考日志级别设置预案： 一般：比如有些服务偶尔因为网络抖动或者服务正在上线而超时，可以自动降级； 警告：有些服务在一段时间内成功率有波动（如在95~100%之间），可以自动降级或人工降级，并发送告警； 错误：比如可用率低于90%，或者数据库连接池被打爆了，或者访问量突然猛增到系统能承受的最大阀值，此时可以根据情况自动降级或者人工降级； 严重错误：比如因为特殊原因数据错误了，此时需要紧急人工降级。 服务降级的目的，是为了防止Redis服务故障，导致数据库跟着一起发生雪崩问题。因此，对于不重要的缓存数据，可以采取服务降级策略，例如一个比较常见的做法就是，Redis出现问题，不去数据库查询，而是直接返回默认值给用户。 热点数据和冷数据 热点数据，缓存才有价值 对于冷数据而言，大部分数据可能还没有再次访问到就已经被挤出内存，不仅占用内存，而且价值不大。频繁修改的数据，看情况考虑使用缓存 对于热点数据，比如我们的某IM产品，生日祝福模块，当天的寿星列表，缓存以后可能读取数十万次。再举个例子，某导航产品，我们将导航信息，缓存以后可能读取数百万次。 数据更新前至少读取两次，缓存才有意义。这个是最基本的策略，如果缓存还没有起作用就失效了，那就没有太大价值了。 那存不存在，修改频率很高，但是又不得不考虑缓存的场景呢？有！比如，这个读取接口对数据库的压力很大，但是又是热点数据，这个时候就需要考虑通过缓存手段，减少数据库的压力，比如我们的某助手产品的，点赞数，收藏数，分享数等是非常典型的热点数据，但是又不断变化，此时就需要将数据同步保存到Redis缓存，减少数据库压力。 缓存热点key 缓存中的一个Key(比如一个促销商品)，在某个时间点过期的时候，恰好在这个时间点对这个Key有大量的并发请求过来，这些请求发现缓存过期一般都会从后端DB加载数据并回设到缓存，这个时候大并发的请求可能会瞬间把后端DB压垮。 解决方案 对缓存查询加锁，如果KEY不存在，就加锁，然后查DB入缓存，然后解锁；其他进程如果发现有锁就等待，然后等解锁后返回数据或者进入DB查询 常用工具 Redis支持的Java客户端都有哪些？官方推荐用哪个？ Redisson、Jedis、lettuce等等，官方推荐使用Redisson。 Redis和Redisson有什么关系？ Redisson是一个高级的分布式协调Redis客服端，能帮助用户在分布式环境中轻松实现一些Java的对象 (Bloom filter, BitSet, Set, SetMultimap, ScoredSortedSet, SortedSet, Map, ConcurrentMap, List, ListMultimap, Queue, BlockingQueue, Deque, BlockingDeque, Semaphore, Lock, ReadWriteLock, AtomicLong, CountDownLatch, Publish / Subscribe, HyperLogLog)。 Jedis与Redisson对比有什么优缺点？ Jedis是Redis的Java实现的客户端，其API提供了比较全面的Redis命令的支持；Redisson实现了分布式和可扩展的Java数据结构，和Jedis相比，功能较为简单，不支持字符串操作，不支持排序、事务、管道、分区等Redis特性。Redisson的宗旨是促进使用者对Redis的关注分离，从而让使用者能够将精力更集中地放在处理业务逻辑上。 Redisson实现分布式锁 package com.example.redisstudy.template; import org.redisson.Redisson; import org.redisson.api.RLock; import org.redisson.api.RedissonClient; import org.redisson.config.Config; import org.springframework.beans.factory.annotation.Value; import org.springframework.stereotype.Component; /** * @author wuhao * @desc ... * @date 2020-12-09 14:57:30 */ @Component public class RedissonLock { @Value(&quot;${spring.redis.address}&quot;) public String address; public RLock lock(){ Config config = new Config(); config.useSingleServer().setAddress(address); // config.useSingleServer().setPassword(&quot;redis1234&quot;); final RedissonClient client = Redisson.create(config); RLock lock = client.getLock(&quot;redis:lock&quot;); return lock; } } 如何保证缓存与数据库双写时的数据一致性？ 你只要用缓存，就可能会涉及到缓存与数据库双存储双写，你只要是双写，就一定会有数据一致性的问题，那么你如何解决一致性问题？ 一般来说，就是如果你的系统不是严格要求缓存+数据库必须一致性的话，缓存可以稍微的跟数据库偶尔有不一致的情况，最好不要做这个方案，读请求和写请求串行化，串到一个内存队列里去，这样就可以保证一定不会出现不一致的情况 串行化之后，就会导致系统的吞吐量会大幅度的降低，用比正常情况下多几倍的机器去支撑线上的一个请求。 还有一种方式就是可能会暂时产生不一致的情况，但是发生的几率特别小，就是先更新数据库，然后再删除缓存。 问题场景 描述 解决 先写缓存，再写数据库，缓存写成功，数据库写失败 缓存写成功，但写数据库失败或者响应延迟，则下次读取（并发读）缓存时，就出现脏读 这个写缓存的方式，本身就是错误的，需要改为先写数据库，把旧缓存置为失效；读取数据的时候，如果缓存不存在，则读取数据库再写缓存 先写数据库，再写缓存，数据库写成功，缓存写失败 写数据库成功，但写缓存失败，则下次读取（并发读）缓存时，则读不到数据 缓存使用时，假如读缓存失败，先读数据库，再回写缓存的方式实现 需要缓存异步刷新 指数据库操作和写缓存不在一个操作步骤中，比如在分布式场景下，无法做到同时写缓存或需要异步刷新（补救措施）时候 确定哪些数据适合此类场景，根据经验值确定合理的数据不一致时间，用户数据刷新的时间间隔 Redis常见性能问题和解决方案？ Master最好不要做任何持久化工作，包括内存快照和AOF日志文件，特别是不要启用内存快照做持久化。如果采用了主从架构，那么建议必须开启 master node 的持久化，不建议用 slave node 作为 master node 的数据热备，因为那样的话，如果你关掉 master 的持久化，可能在 master 宕机重启的时候数据是空的，然后可能一经过复制， slave node 的数据也丢了。另外，master 的各种备份方案，也需要做。万一本地的所有文件丢失了，从备份中挑选一份 rdb 去恢复 master，这样才能确保启动的时候，是有数据的，即使采用了后续讲解的高可用机制，slave node 可以自动接管 master node，但也可能 sentinel 还没检测到 master failure，master node 就自动重启了，还是可能导致上面所有的 slave node 数据被清空。 如果数据比较关键，某个Slave开启AOF备份数据，策略为每秒同步一次。 为了主从复制的速度和连接的稳定性，Slave和Master最好在同一个局域网内。 尽量避免在压力较大的主库上增加从库 Master调用BGREWRITEAOF重写AOF文件，AOF在重写的时候会占大量的CPU和内存资源，导致服务load过高，出现短暂服务暂停现象。 为了Master的稳定性，主从复制不要用图状结构，用单向链表结构更稳定，即主从关系为：Master&lt;–Slave1&lt;–Slave2&lt;–Slave3…，这样的结构也方便解决单点故障问题，实现Slave对Master的替换，也即，如果Master挂了，可以立马启用Slave1做Master，其他不变。 ","link":"https://tianxiawuhao.github.io/Sjb64Uy0k/"},{"title":"redis概述四","content":"集群方案 1，Redis 主从架构 单机的 redis，能够承载的 QPS 大概就在上万到几万不等。对于缓存来说，一般都是用来支撑读高并发的。因此架构做成主从(master-slave)架构，一主多从，主负责写，并且将数据复制到其它的 slave 节点，从节点负责读。所有的读请求全部走从节点。这样也可以很轻松实现水平扩容，支撑读高并发。 redis replication -&gt; 主从架构 -&gt; 读写分离 -&gt; 水平扩容支撑读高并发 redis replication 的核心机制 redis 采用异步方式复制数据到 slave 节点，不过 redis2.8 开始，slave node 会周期性地确认自己每次复制的数据量； 一个 master node 是可以配置多个 slave node 的； slave node 也可以连接其他的 slave node； slave node 做复制的时候，不会 block master node 的正常工作； slave node 在做复制的时候，也不会 block 对自己的查询操作，它会用旧的数据集来提供服务；但是复制完成的时候，需要删除旧数据集，加载新数据集，这个时候就会暂停对外服务了； slave node 主要用来进行横向扩容，做读写分离，扩容的 slave node 可以提高读的吞吐量。 注意，如果采用了主从架构，那么建议必须开启 master node 的持久化，不建议用 slave node 作为 master node 的数据热备，因为那样的话，如果你关掉 master 的持久化，可能在 master 宕机重启的时候数据是空的，然后可能一经过复制， slave node 的数据也丢了。 另外，master 的各种备份方案，也需要做。万一本地的所有文件丢失了，从备份中挑选一份 rdb 去恢复 master，这样才能确保启动的时候，是有数据的，即使采用了后续讲解的高可用机制，slave node 可以自动接管 master node，但也可能 sentinel 还没检测到 master failure，master node 就自动重启了，还是可能导致上面所有的 slave node 数据被清空。 redis 主从复制的核心原理 当启动一个 slave node 的时候，它会发送一个 PSYNC 命令给 master node。 如果这是 slave node 初次连接到 master node，那么会触发一次 full resynchronization 全量复制。此时 master 会启动一个后台线程，开始生成一份 RDB 快照文件， 同时还会将从客户端 client 新收到的所有写命令缓存在内存中。RDB 文件生成完毕后， master 会将这个 RDB 发送给 slave，slave 会先写入本地磁盘，然后再从本地磁盘加载到内存中， 接着 master 会将内存中缓存的写命令发送到 slave，slave 也会同步这些数据。 slave node 如果跟 master node 有网络故障，断开了连接，会自动重连，连接之后 master node 仅会复制给 slave 部分缺少的数据。 过程原理 当从库和主库建立MS关系后，会向主数据库发送SYNC命令 主库接收到SYNC命令后会开始在后台保存快照(RDB持久化过程)，并将期间接收到的写命令缓存起来 当快照完成后，主Redis会将快照文件和所有缓存的写命令发送给从Redis 从Redis接收到后，会载入快照文件并且执行收到的缓存的命令 之后，主Redis每当接收到写命令时就会将命令发送从Redis，从而保证数据的一致 缺点 所有的slave节点数据的复制和同步都由master节点来处理，会照成master节点压力太大，使用主从从结构来解决 2，哨兵模式 哨兵的介绍 sentinel，中文名是哨兵。哨兵是 redis 集群机构中非常重要的一个组件，主要有以下功能： 集群监控：负责监控 redis master 和 slave 进程是否正常工作。 消息通知：如果某个 redis 实例有故障，那么哨兵负责发送消息作为报警通知给管理员。 故障转移：如果 master node 挂掉了，会自动转移到 slave node 上。 配置中心：如果故障转移发生了，通知 client 客户端新的 master 地址。 哨兵用于实现 redis 集群的高可用，本身也是分布式的，作为一个哨兵集群去运行，互相协同工作。 故障转移时，判断一个 master node 是否宕机了，需要大部分的哨兵都同意才行，涉及到了分布式选举的问题。 即使部分哨兵节点挂掉了，哨兵集群还是能正常工作的，因为如果一个作为高可用机制重要组成部分的故障转移系统本身是单点的，那就很坑爹了。 哨兵的核心知识 哨兵至少需要 3 个实例，来保证自己的健壮性。 哨兵 + redis 主从的部署架构，是不保证数据零丢失的，只能保证 redis 集群的高可用性。 对于哨兵 + redis 主从这种复杂的部署架构，尽量在测试环境和生产环境，都进行充足的测试和演练。 分布式和哨兵 Redis主从复制模式下， 一旦主节点出现了故障不可达， 需要人工干预进行故障转移， 无论对于Redis的应用方还是运维方都带来了很大的不便。对于应用方来说无法及时感知到主节点的变化， 必然会造成一定的写数据丢失和读数据错误， 甚至可能造成应用方服务不可用。 对于Redis的运维方来说， 整个故障转移的过程是需要人工来介入的， 故障转移实时性和准确性上都无法得到保障。考虑到这点， 有些公司把上述流程自动化了， 但是仍然存在如下问题： 第一， 判断节点不可达的机制是否健全和标准。 第二， 如果有多个从节点， 怎样保证只有一个被晋升为主节点。 第三，通知客户端新的主节点机制是否足够健壮。 Redis Sentinel正是用于解决这些问题 Redis Sentinel是一个分布式架构， 其中包含若干个Sentinel节点和Redis数据节点， 每个Sentinel节点会对数据节点和其余Sentinel节点进行监控， 当它发现节点不可达时， 会对节点做下线标识。 如果被标识的是主节点， 它还会和其他Sentinel节点进行“协商”， 当大多数Sentinel节点都认为主节点不可达时， 它们会选举出一个Sentinel节点来完成自动故障转移的工作， 同时会将这个变化实时通知给Redis应用方。 整个过程完全是自动的， 不需要人工来介入， 所以这套方案很有效地解决了Redis的高可用问题 哨兵的监控与选举 哨兵的定时监控 任务1：每个哨兵节点每10秒会向主节点和从节点发送info命令获取最拓扑结构图，哨兵配置时只要配置对主节点的监控即可，通过向主节点发送info，获取从节点的信息，并当有新的从节点加入时可以马上感知到 任务2：每个哨兵节点每隔2秒会向redis数据节点的指定频道上发送该哨兵节点对于主节点的判断以及当前哨兵节点的信息，同时每个哨兵节点也会订阅该频道，来了解其它哨兵节点的信息及对主节点的判断，其实就是通过消息publish和subscribe来完成的 任务3：每隔1秒每个哨兵会向主节点、从节点及其余哨兵节点发送一次ping命令做一次心跳检测，这个也是哨兵用来判断节点是否正常的重要依据 主观下线：所谓主观下线，就是单个sentinel认为某个服务下线（有可能是接收不到订阅，之间的网络不通等等原因）。 sentinel会以每秒一次的频率向所有与其建立了命令连接的实例（master，从服务，其他sentinel）发ping命令，通过判断ping回复是有效回复，还是无效回复来判断实例时候在线（对该sentinel来说是“主观在线”）。 sentinel配置文件中的down-after-milliseconds设置了判断主观下线的时间长度，如果实例在down-after-milliseconds毫秒内，返回的都是无效回复，那么sentinel回认为该实例已（主观）下线，修改其flags状态为SRI_S_DOWN。如果多个sentinel监视一个服务，有可能存在多个sentinel的down-after-milliseconds配置不同，这个在实际生产中要注意。 客观下线：当主观下线的节点是主节点时，此时该哨兵3节点会通过指令sentinel is-masterdown-by-addr寻求其它哨兵节点对主节点的判断，如果其他的哨兵也认为主节点主观线下了，则当认为主观下线的票数超过了quorum（选举）个数，此时哨兵节点则认为该主节点确实有问题，这样就客观下线了，大部分哨兵节点都同意下线操作，也就说是客观下线 哨兵lerder选举流程 如果主节点被判定为客观下线之后，就要选取一个哨兵节点来完成后面的故障转移工作，选举出一个leader的流程如下: a)每个在线的哨兵节点都可以成为领导者，当它确认（比如哨兵3）主节点下线时，会向其它哨兵发is-master-down-by-addr命令，征求判断并要求将自己设置为领导者，由领导者处理故障转移； b)当其它哨兵收到此命令时，可以同意或者拒绝它成为领导者； c)如果哨兵3发现自己在选举的票数大于等于num(sentinels)/2+1时，将成为领导者，如果没有超过，继续选举………… 自动故障转移机制 在从节点中选择新的主节点 sentinel状态数据结构中保存了主服务的所有从服务信息，领头sentinel按照如下的规则从从服务列表中挑选出新的主服务 过滤掉主观下线的节点 选择slave-priority最高的节点，如果由则返回没有就继续选择 选择出复制偏移量最大的系节点，因为复制便宜量越大则数据复制的越完整，如果由就返回了，没有就继续 选择run_id最小的节点 更新主从状态 通过slaveof no one命令，让选出来的从节点成为主节点；并通过slaveof命令让其他节点成为其从节点。 将已下线的主节点设置成新的主节点的从节点，当其回复正常时，复制新的主节点，变成新的主节点的从节点 同理，当已下线的服务重新上线时，sentinel会向其发送slaveof命令，让其成为新主的从 3，官方Redis Cluster 方案(服务端路由查询) 简介 Redis Cluster是一种服务端Sharding技术，3.0版本开始正式提供。Redis Cluster并没有使用一致性hash，而是采用slot(槽)的概念，一共分成16384个槽。将请求发送到任意节点，接收到请求的节点会将查询请求发送到正确的节点上执行 方案说明 通过哈希的方式，将数据分片，每个节点均分存储一定哈希槽(哈希值)区间的数据，默认分配了16384 个槽位 每份数据分片会存储在多个互为主从的多节点上 数据写入先写主节点，再同步到从节点(支持配置为阻塞同步) 同一分片多个节点间的数据不保持一致性 读取数据时，当客户端操作的key没有分配在该节点上时，redis会返回转向指令，指向正确的节点 扩容时时需要需要把旧节点的数据迁移一部分到新节点 在 redis cluster 架构下，每个 redis 要放开两个端口号，比如一个是 6379，另外一个就是 加1w 的端口号，比如 16379。 16379 端口号是用来进行节点间通信的，也就是 cluster bus 的东西，cluster bus 的通信，用来进行故障检测、配置更新、故障转移授权。cluster bus 用了另外一种二进制的协议，gossip 协议，用于节点间进行高效的数据交换，占用更少的网络带宽和处理时间。 节点间的内部通信机制 基本通信原理 集群元数据的维护有两种方式：集中式、Gossip 协议。redis cluster 节点间采用 gossip 协议进行通信。 分布式寻址算法 hash 算法（大量缓存重建） 使用特定的数据， 如Redis的键或用户ID， 再根据节点数量N使用公式：hash（key） %N计算出哈希值， 用来决定数据映射到哪一个节点上。 这种方案存在一个问题： 当节点数量变化时， 如扩容或收缩节点， 数据节点映射关系需要重新计算， 会导致数据的重新迁移。 这种方式的突出优点是简单性， 常用于数据库的分库分表规则， 一般采用预分区的方式， 提前根据数据量规划好分区数， 比如划分为512或1024张表， 保证可支撑未来一段时间的数据量， 再根据负载情况将表迁移到其他数据库中。 扩容时通常采用翻倍扩容， 避免数据映射全部被打乱导致全量迁移的情况， 如图所示。 一致性 hash 算法（自动缓存迁移）+ 虚拟节点（自动负载均衡） 一致性哈希分区（Distributed Hash Table） 实现思路是为系统中每个节点分配一个token， 范围一般在0~2^32， 这些token构成一个哈希环。 数据读写执行节点查找操作时， 先根据key计算hash值， 然后顺时针找到第一个大于等于该哈希值的token节点， 如图所示。 这种方式相比节点取余最大的好处在于加入和删除节点只影响哈希环中相邻的节点， 对其他节点无影响。 但一致性哈希分区存在几个问题： 加减节点会造成哈希环中部分数据无法命中， 需要手动处理或者忽略这部分数据， 因此一致性哈希常用于缓存场景。 当使用少量节点时， 节点变化将大范围影响哈希环中数据映射， 因此这种方式不适合少量数据节点的分布式方案。 普通的一致性哈希分区在增减节点时需要增加一倍或减去一半节点才能保证数据和负载的均衡。 redis cluster 的 hash slot 算法 虚拟槽分区巧妙地使用了哈希空间， 使用分散度良好的哈希函数把所有数据映射到一个固定范围的整数集合中， 整数定义为槽（slot）。 这个范围一般远远大于节点数， 比如Redis Cluster槽范围是0~16383。 槽是集群内数据管理和迁移的基本单位。 采用大范围槽的主要目的是为了方便数据拆分和集群扩展。 每个节点会负责一定数量的槽， 如图所示。 Redis Cluser采用虚拟槽分区， 所有的键根据哈希函数映射到0~16383整数槽内， 计算公式： slot=CRC16（key） &amp;16383。 每一个节点负责维护一部分槽以及槽所映射的键值数据， 如图所示 Redis虚拟槽分区的特点： 解耦数据和节点之间的关系， 简化了节点扩容和收缩难度。 节点自身维护槽的映射关系， 不需要客户端或者代理服务维护槽分区元数据。 支持节点、 槽、 键之间的映射查询， 用于数据路由、 在线伸缩等场景 优点 无中心架构，支持动态扩容，对业务透明 具备Sentinel的监控和自动Failover(故障转移)能力 客户端不需要连接集群所有节点，连接集群中任何一个可用节点即可 高性能，客户端直连redis服务，免去了proxy代理的损耗 缺点 运维也很复杂，数据迁移需要人工干预 只能使用0号数据库 不支持批量操作(pipeline管道操作) 分布式逻辑和存储模块耦合等 集群相关提问 Redis集群的主从复制模型是怎样的？ 为了使在部分节点失败或者大部分节点无法通信的情况下集群仍然可用，所以集群使用了主从复制模型，每个节点都会有N-1个复制品 生产环境中的 redis 是怎么部署的？ redis cluster，10 台机器，5 台机器部署了 redis 主实例，另外 5 台机器部署了 redis 的从实例，每个主实例挂了一个从实例，5 个节点对外提供读写服务，每个节点的读写高峰qps可能可以达到每秒 5 万，5 台机器最多是 25 万读写请求/s。 机器是什么配置？32G 内存+ 8 核 CPU + 1T 磁盘，但是分配给 redis 进程的是10g内存，一般线上生产环境，redis 的内存尽量不要超过 10g，超过 10g 可能会有问题。 5 台机器对外提供读写，一共有 50g 内存。 因为每个主实例都挂了一个从实例，所以是高可用的，任何一个主实例宕机，都会自动故障迁移，redis 从实例会自动变成主实例继续提供读写服务。 你往内存里写的是什么数据？每条数据的大小是多少？商品数据，每条数据是 10kb。100 条数据是 1mb，10 万条数据是 1g。常驻内存的是 200 万条商品数据，占用内存是 20g，仅仅不到总内存的 50%。目前高峰期每秒就是 3500 左右的请求量。 其实大型的公司，会有基础架构的 team 负责缓存集群的运维。 说说Redis哈希槽的概念？ Redis集群没有使用一致性hash,而是引入了哈希槽的概念，Redis集群有16384个哈希槽，每个key通过CRC16校验后对16384取模来决定放置哪个槽，slot=CRC16（key） &amp;16383，集群的每个节点负责一部分hash槽。 Redis集群会有写操作丢失吗？为什么？ Redis并不能保证数据的强一致性，这意味这在实际中集群在特定的条件下可能会丢失写操作。 以下情况可能导致写操作丢失： 过期 key 被清理 最大内存不足，导致 Redis 自动清理部分 key 以节省空间 主库故障后自动重启，从库自动同步 单独的主备方案，网络不稳定触发哨兵的自动切换主从节点，切换期间会有数据丢失 Redis集群之间是如何复制的？ 在从节点执行slaveof命令后， 复制过程便开始运作， 下面详细介绍建立复制的完整流程， 如图所示。 从图中可以看出复制过程大致分为6个过程： 保存主节点（master） 信息。执行slaveof后从节点只保存主节点的地址信息便直接返回， 这时建立复制流程还没有开始， 在从节点6380执行info replication可以看到如下信息： master_host:127.0.0.1 master_port:6379 master_link_status:down 从统计信息可以看出， 主节点的ip和port被保存下来， 但是主节点的连接状态（master_link_status） 是下线状态。 执行slaveof后Redis会打印如下日志： SLAVE OF 127.0.0.1:6379 enabled (user request from 'id=65 addr=127.0.0.1:58090 fd=5 name= age=11 idle=0 flags=N db=0 sub=0 psub=0 multi=-1 qbuf=0 qbuf-free=32768 obl=0 oll=0 omem=0 events=r cmd=slaveof') 通过该日志可以帮助运维人员定位发送slaveof命令的客户端， 方便追踪和发现问题。 从节点（slave） 内部通过每秒运行的定时任务维护复制相关逻辑，当定时任务发现存在新的主节点后， 会尝试与该节点建立网络连接， 如图所示。 从节点会建立一个socket套接字， 例如图6-8中从节点建立了一个端口为24555的套接字， 专门用于接受主节点发送的复制命令。 从节点连接成功后 打印如下日志： \\* Connecting to MASTER 127.0.0.1:6379 \\* MASTER &lt;-&gt; SLAVE sync started 如果从节点无法建立连接， 定时任务会无限重试直到连接成功或者执行slaveof no one取消复制， 如图所示。 关于连接失败， 可以在从节点执行info replication查看master_link_down_since_seconds指标， 它会记录与主节点连接失败的系统时 间。 从节点连接主节点失败时也会每秒打印如下日志， 方便运维人员发现问题： \\# Error condition on socket for SYNC: {socket_error_reason} 发送ping命令。 连接建立成功后从节点发送ping请求进行首次通信， ping请求主要目的如下： ·检测主从之间网络套接字是否可用。 ·检测主节点当前是否可接受处理命令。 如果发送ping命令后， 从节点没有收到主节点的pong回复或者超时， 比如网络超时或者主节点正在阻塞无法响应命令， 从节点会断开复制连接， 下 次定时任务会发起重连， 如图所示。 从节点发送的ping命令成功返回， Redis打印如下日志， 并继续后续复制流程： Master replied to PING, replication can continue... 权限验证。 如果主节点设置了requirepass参数， 则需要密码验证，从节点必须配置masterauth参数保证与主节点相同的密码才能通过验证； 如果验证失败复制将终止， 从节点重新发起复制流程。 同步数据集。 主从复制连接正常通信后， 对于首次建立复制的场景， 主节点会把持有的数据全部发送给从节点， 这部分操作是耗时最长的步骤。 Redis在2.8版本以后采用新复制命令psync进行数据同步， 原来的sync命令依然支持， 保证新旧版本的兼容性。 新版同步划分两种情况： 全量同步和部分同步. 命令持续复制。 当主节点把当前的数据同步给从节点后， 便完成了复制的建立流程。 接下来主节点会持续地把写命令发送给从节点， 保证主从数据一致性。 Redis集群最大节点个数是多少？ 16384个 Redis集群如何选择数据库？ Redis集群目前无法做数据库选择，默认在0数据库。 ","link":"https://tianxiawuhao.github.io/FWuQD6Kxu/"},{"title":"redis概述三","content":"Redis持久化 持久化就是把内存的数据写到磁盘中去，防止服务宕机了内存数据丢失。 Redis 的持久化机制是什么？各自的优缺点？ Redis 提供两种持久化机制 RDB（默认） 和 AOF 机制: RDB： RDB是Redis DataBase缩写快照 RDB是Redis默认的持久化方式。按照一定的时间将内存的数据以快照的形式保存到硬盘中，对应产生的数据文件为dump.rdb。通过配置文件中的save参数来定义快照的周期。 bgsave是主流的触发RDB持久化方式， 根据下图了解它的运作流程 执行bgsave命令， Redis父进程判断当前是否存在正在执行的子进程， 如RDB/AOF子进程， 如果存在bgsave命令直接返回。 父进程执行fork操作创建子进程， fork操作过程中父进程会阻塞， 通过info stats命令查看latest_fork_usec选项， 可以获取最近一个fork操作的耗时， 单位为微秒。 父进程fork完成后， bgsave命令返回“Background saving started”信息并不再阻塞父进程， 可以继续响应其他命令。 子进程创建RDB文件， 根据父进程内存生成临时快照文件， 完成后对原有文件进行原子替换。 执行lastsave命令可以获取最后一次生成RDB的时间， 对应info统计的rdb_last_save_time选项。 进程发送信号给父进程表示完成， 父进程更新统计信息， 具体见info Persistence下的rdb_*相关选 优点： 1、只有一个文件 dump.rdb，方便持久化。 2、容灾性好，一个文件可以保存到安全的磁盘。 3、性能最大化，fork 子进程来完成写操作，让主进程继续处理命令，所以是 IO 最大化。使用单独子进程来进行持久化，主进程不会进行任何 IO 操作，保证了 redis 的高性能 4.相对于数据集大时，比 AOF 的启动效率更高。 缺点： 1、数据安全性低。RDB 是间隔一段时间进行持久化，如果持久化之间 redis 发生故障，会发生数据丢失。所以这种方式更适合数据要求不严谨的时候 2、AOF（Append-only file)持久化方式： 是指所有的命令行记录以 redis 命令请 求协议的格式完全持久化存储)保存为 aof 文件。 AOF： AOF持久化(即Append Only File持久化)，则是将Redis执行的每次写命令记录到单独的日志文件中，当重启Redis会重新将持久化的日志中文件恢复数据。 当两种方式同时开启时，数据恢复Redis会优先选择AOF恢复。 开启AOF功能需要设置配置： appendonly yes， 默认不开启。 AOF文件名通过appendfilename配置设置， 默认文件名是appendonly.aof。 保存路径同RDB持久化方式一致， 通过dir配置指定。 AOF的工作流程操作： 命令写入（append） 、 文件同步（sync） 、 文件重写（rewrite） 、 重启加载 （load） ， 如图所示 流程如下： 所有的写入命令会追加到aof_buf（ 缓冲区） 中。 AOF缓冲区根据对应的策略向硬盘做同步操作。 随着AOF文件越来越大， 需要定期对AOF文件进行重写， 达到压缩的目的。 当Redis服务器重启时， 可以加载AOF文件进行数据恢复。 aof重写 AOF重新运作流程 流程说明： 执行AOF重写请求。 如果当前进程正在执行AOF重写， 请求不执行并返回如下响应：ERR Background append only file rewriting already in progress 如果当前进程正在执行bgsave操作， 重写命令延迟到bgsave完成之后再执行， 返回如下响应：Background append only file rewriting scheduled 父进程执行fork创建子进程， 开销等同于bgsave过程。 3.1. 主进程fork操作完成后， 继续响应其他命令。 所有修改命令依然写入AOF缓冲区并根据appendfsync策略同步到硬盘， 保证原有AOF机制正确性。 3.2. 由于fork操作运用写时复制技术， 子进程只能共享fork操作时的内存数据。 由于父进程依然响应命令， Redis使用“AOF重写缓冲区”保存这部分新数据， 防止新AOF文件生成期间丢失这部分数据。 子进程根据内存快照， 按照命令合并规则写入到新的AOF文件。 每次批量写入硬盘数据量由配置aof-rewrite-incremental-fsync控制， 默认为32MB， 防止单次刷盘数据过多造成硬盘阻塞。 5.1. 新AOF文件写入完成后， 子进程发送信号给父进程， 父进程更新统计信息， 具体见info persistence下的aof_*相关统计。 5.2. 父进程把AOF重写缓冲区的数据写入到新的AOF文件。 5.3. 使用新AOF文件替换老文件， 完成AOF重写。 AOF追加阻塞 当开启AOF持久化时， 常用的同步硬盘的策略是everysec， 用于平衡性能和数据安全性。 对于这种方式， Redis使用另一条线程每秒执行fsync同步硬盘。 当系统硬盘资源繁忙时， 会造成Redis主线程阻塞， 如图所示。 使用everysec做刷盘策略的流程 阻塞流程分析： 1） 主线程负责写入AOF缓冲区。 2） AOF线程负责每秒执行一次同步磁盘操作， 并记录最近一次同步时间。 3） 主线程负责对比上次AOF同步时间： ·如果距上次同步成功时间在2秒内， 主线程直接返回。 ·如果距上次同步成功时间超过2秒， 主线程将会阻塞， 直到同步操作完成。 通过对AOF阻塞流程可以发现两个问题： 1） everysec配置最多可能丢失2秒数据， 不是1秒。 2） 如果系统fsync缓慢， 将会导致Redis主线程阻塞影响效率。 AOF阻塞问题定位： 1） 发生AOF阻塞时， Redis输出如下日志， 用于记录AOF fsync阻塞导致拖慢Redis服务的行为： Asynchronous AOF fsync is taking too long (disk is busy). Writing the AOF buffer without waiting for fsync to complete, this may slow down Redis 2） 每当发生AOF追加阻塞事件发生时， 在info Persistence统计中，aof_delayed_fsync指标会累加， 查看这个指标方便定位AOF阻塞问题。 3） AOF同步最多允许2秒的延迟， 当延迟发生时说明硬盘存在高负载问题， 可以通过监控工具如iotop， 定位消耗硬盘IO资源的进程。优化AOF追加阻塞问题主要是优化系统硬盘负载 优点： 1、数据安全，aof 持久化可以配置 appendfsync 属性。 配置为always时， 每次写入都要同步AOF文件， 在一般的SATA硬盘上， Redis只能支持大约几百TPS写入， 显然跟Redis高性能特性背道而驰，不建议配置。 配置为no， 由于操作系统每次同步AOF文件的周期不可控， 而且会加大每次同步硬盘的数据量， 虽然提升了性能， 但数据安全性无法保证。 配置为everysec， 是建议的同步策略， 也是默认配置， 做到兼顾性能和数据安全性。 理论上只有在系统突然宕机的情况下丢失1秒的数据 2、通过 append 模式写文件，即使中途服务器宕机，可以通过 redis-check-aof 工具解决数据一致性问题。 3、AOF 机制的 rewrite 模式。AOF 文件没被 rewrite 之前（文件过大时会对命令 进行合并重写），可以删除其中的某些命令（比如误操作的 flushall）) 缺点： 1、AOF 文件比 RDB 文件大，且恢复速度慢。 2、数据集大的时候，比 rdb 启动效率低。 比较 AOF文件比RDB更新频率高，优先使用AOF还原数据。 AOF比RDB更安全也更大 RDB性能比AOF好 如果两个都配了优先加载AOF 如何选择合适的持久化方式 一般来说， 如果想达到足以媲美PostgreSQL的数据安全性，你应该同时使用两种持久化功能。在这种情况下，当 Redis 重启的时候会优先载入AOF文件来恢复原始的数据，因为在通常情况下AOF文件保存的数据集要比RDB文件保存的数据集要完整。 如果你非常关心你的数据， 但仍然可以承受数分钟以内的数据丢失，那么你可以只使用RDB持久化。 有很多用户都只使用AOF持久化，但并不推荐这种方式，因为定时生成RDB快照（snapshot）非常便于进行数据库备份， 并且 RDB 恢复数据集的速度也要比AOF恢复的速度要快，除此之外，使用RDB还可以避免AOF程序的bug。 如果你只希望你的数据在服务器运行的时候存在，你也可以不使用任何持久化方式。 Redis持久化数据和缓存怎么做扩容？ 如果Redis被当做缓存使用，使用一致性哈希实现动态扩容缩容。 如果Redis被当做一个持久化存储使用，必须使用固定的keys-to-nodes映射关系，节点的数量一旦确定不能变化。否则的话(即Redis节点需要动态变化的情况），必须使用可以在运行时进行数据再平衡的一套系统，而当前只有Redis集群可以做到这样。 Redis的过期键的删除策略 我们都知道，Redis是key-value数据库，我们可以设置Redis中缓存的key的过期时间。Redis的过期策略就是指当Redis中缓存的key过期了，Redis如何处理。 过期策略通常有以下三种： 定时过期：每个设置过期时间的key都需要创建一个定时器，到过期时间就会立即清除。该策略可以立即清除过期的数据，对内存很友好；但是会占用大量的CPU资源去处理过期的数据，从而影响缓存的响应时间和吞吐量。 惰性过期：只有当访问一个key时，才会判断该key是否已过期，过期则清除。该策略可以最大化地节省CPU资源，却对内存非常不友好。极端情况可能出现大量的过期key没有再次被访问，从而不会被清除，占用大量内存。 定期过期：每隔一定的时间，会扫描一定数量的数据库的expires字典中一定数量的key，并清除其中已过期的key。该策略是前两者的一个折中方案。通过调整定时扫描的时间间隔和每次扫描的限定耗时，可以在不同情况下使得CPU和内存资源达到最优的平衡效果。 (expires字典会保存所有设置了过期时间的key的过期时间数据，其中，key是指向键空间中的某个键的指针，value是该键的毫秒精度的UNIX时间戳表示的过期时间。键空间是指该Redis集群中保存的所有键。) Redis中同时使用了惰性过期和定期过期两种过期策略。 redis内存 redis内存数据集大小上升到一定大小的时候，就会施行数据淘汰策略。 Redis的内存淘汰策略有哪些 Redis的内存淘汰策略是指在Redis的用于缓存的内存不足时，怎么处理需要新写入且需要申请额外空间的数据。 全局的键空间选择性移除 noeviction：当内存不足以容纳新写入数据时，新写入操作会报错。 allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的key。（这个是最常用的） allkeys-random：当内存不足以容纳新写入数据时，在键空间中，随机移除某个key。 allkeys-lfu：从所有键中驱逐使用频率最少的键 设置过期时间的键空间选择性移除 volatile-lru：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最近最少使用的key。 volatile-random：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，随机移除某个key。 volatile-ttl：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，有更早过期时间的key优先移除。 volatile-lfu：从所有配置了过期时间的键中驱逐使用频率最少的键 总结 Redis的内存淘汰策略的选取并不会影响过期的key的处理。内存淘汰策略用于处理内存不足时的需要申请额外空间的数据；过期策略用于处理过期的缓存数据。 Redis主要消耗什么物理资源？ 内存。 Redis的内存用完了会发生什么？ 如果达到设置的上限，Redis的写命令会返回错误信息（但是读命令还可以正常返回。）或者你可以配置内存淘汰机制，当Redis达到内存上限时会冲刷掉旧的内容。 Redis如何做内存优化？ 可以好好利用Hash,list,sorted set,set等集合类型数据，因为通常情况下很多小的Key-Value可以用更紧凑的方式存放到一起。尽可能使用散列表（hashes），散列表（是说散列表里面存储的数少）使用的内存非常小，所以你应该尽可能的将你的数据模型抽象到一个散列表里面。比如你的web系统中有一个用户对象，不要为这个用户的名称，姓氏，邮箱，密码设置单独的key，而是应该把这个用户的所有信息存储到一张散列表里面 Redis线程模型 Redis基于Reactor模式开发了网络事件处理器，这个处理器被称为文件事件处理器（file event handler）。它的组成结构为4部分：多个套接字、IO多路复用程序、文件事件分派器、事件处理器。因为文件事件分派器队列的消费是单线程的，所以Redis才叫单线程模型。 文件事件处理器使用 I/O 多路复用（multiplexing）程序来同时监听多个套接字， 并根据套接字目前执行的任务来为套接字关联不同的事件处理器。 当被监听的套接字准备好执行连接应答（accept）、读取（read）、写入（write）、关闭（close）等操作时， 与操作相对应的文件事件就会产生， 这时文件事件处理器就会调用套接字之前关联好的事件处理器来处理这些事件。 虽然文件事件处理器以单线程方式运行， 但通过使用 I/O 多路复用程序来监听多个套接字， 文件事件处理器既实现了高性能的网络通信模型， 又可以很好地与 redis 服务器中其他同样以单线程方式运行的模块进行对接， 这保持了 Redis 内部单线程设计的简单性。 参考：https://www.cnblogs.com/barrywxx/p/8570821.html 事务 事务是一个单独的隔离操作：事务中的所有命令都会序列化、按顺序地执行。事务在执行的过程中，不会被其他客户端发送来的命令请求所打断.事务是一个原子操作：事务中的命令要么全部被执行，要么全部都不执行。 Redis事务的概念 Redis 事务的本质是通过MULTI、EXEC、DISCARD、WATCH等一组命令的集合。事务支持一次执行多个命令，一个事务中所有命令都会被序列化。在事务执行过程，会按照顺序串行化执行队列中的命令，其他客户端提交的命令请求不会插入到事务执行命令序列中。 总结说：redis事务就是一次性、顺序性、排他性的执行一个队列中的一系列命令。 Redis事务的三个阶段 事务开始 MULTI 命令入队 事务执行 EXEC 事务执行过程中，如果服务端收到有EXEC、DISCARD、WATCH、MULTI之外的请求，将会把请求放入队列中排队 Redis事务相关命令 Redis事务功能是通过MULTI、EXEC、DISCARD和WATCH 四个原语实现的 Redis会将一个事务中的所有命令序列化，然后按顺序执行。 redis 不支持回滚，“Redis 在事务失败时不进行回滚，而是继续执行余下的命令”， 所以 Redis 的内部可以保持简单且快速。 如果在一个事务中的命令出现错误，那么所有的命令都不会执行； 如果在一个事务中出现运行错误，那么正确的命令会被执行。 WATCH 命令是一个乐观锁，可以为 Redis 事务提供 check-and-set （CAS）行为。 可以监控一个或多个键，一旦其中有一个键被修改（或删除），之后的事务就不会执行，监控一直持续到EXEC命令。 MULTI命令用于开启一个事务，它总是返回OK。 MULTI执行之后，客户端可以继续向服务器发送任意多条命令，这些命令不会立即被执行，而是被放到一个队列中，当EXEC命令被调用时，所有队列中的命令才会被执行。 EXEC：执行所有事务块内的命令。返回事务块内所有命令的返回值，按命令执行的先后顺序排列。 当操作被打断时，返回空值 nil 。 通过调用DISCARD，客户端可以清空事务队列，并放弃执行事务， 并且客户端会从事务状态中退出。 UNWATCH命令可以取消watch对所有key的监控。 事务管理（ACID）概述 原子性（Atomicity） 原子性是指事务是一个不可分割的工作单位，事务中的操作要么都发生，要么都不发生。 一致性（Consistency） 事务前后数据的完整性必须保持一致。 隔离性（Isolation） 多个事务并发执行时，一个事务的执行不应影响其他事务的执行 持久性（Durability） 持久性是指一个事务一旦被提交，它对数据库中数据的改变就是永久性的，接下来即使数据库发生故障也不应该对其有任何影响 Redis的事务总是具有ACID中的隔离性，不具有隔离级别，其他特性是不支持的。当服务器运行在AOF持久化模式下，并且appendfsync选项的值为always时，事务也具有持久性。 Redis事务支持隔离性吗 Redis 是单进程程序，并且它保证在执行事务时，不会对事务进行中断，事务可以运行直到执行完所有事务队列中的命令为止。因此，Redis 的事务是总是带有隔离性的，因为是单线程不具有隔离级别。 Redis事务保证原子性吗，支持回滚吗 Redis中，单条命令是原子性执行的，但事务不保证原子性，且没有回滚。事务中任意命令执行失败，其余的命令仍会被执行。 Redis事务其他实现 基于Lua脚本，Redis可以保证脚本内的命令一次性、按顺序地执行， 其同时也不提供事务运行错误的回滚，执行过程中如果部分命令运行错误，剩下的命令还是会继续运行完 基于中间标记变量，通过另外的标记变量来标识事务是否执行完成，读取数据时先读取该标记变量判断是否事务执行完成。但这样会需要额外写代码实现，比较繁琐 ","link":"https://tianxiawuhao.github.io/WAG-fYk_l/"},{"title":"redis概述二","content":"Redis有哪些数据类型 Redis主要有5种数据类型，包括String，List，Set，Zset，Hash，满足大部分的使用要求 数据类型 可以存储的值 操作 应用场景 STRING 字符串、整数或者浮点数 对整个字符串或者字符串的其中一部分执行操作对整数和浮点数执行自增或者自减操作 做简单的键值对缓存 LIST 列表 从两端压入或者弹出元素对单个或者多个元素进行修剪，只保留一个范围内的元素 存储一些列表型的数据结构，类似粉丝列表、文章的评论列表之类的数据 SET 无序集合 添加、获取、移除单个元素检查一个元素是否存在于集合中计算交集、并集、差集从集合里面随机获取元素 交集、并集、差集的操作，比如交集，可以把两个人的粉丝列表整一个交集 HASH 包含键值对的无序散列表 添加、获取、移除单个键值对获取所有键值对检查某个键是否存在 结构化的数据，比如一个对象 ZSET 有序集合 添加、获取、删除元素根据分值范围或者成员来获取元素计算一个键的排名 去重但可以排序，如获取排名前几名的用户 Redis数据详解 String 设置值 //设置值 set key value [ex seconds] [px milliseconds] [nx|xx] 127.0.0.1:6379&gt; set hello world set命令有几个选项： ·ex seconds： 为键设置秒级过期时间。 ·px milliseconds： 为键设置毫秒级过期时间。 ·nx： 键必须不存在， 才可以设置成功， 用于添加。 ·xx： 与nx相反， 键必须存在， 才可以设置成功， 用于更新。 // 因为键hello已存在， 所以setnx失败， 返回结果为0 127.0.0.1:6379&gt; setnx hello redis (integer) 0 //因为键hello已存在， 所以set xx成功， 返回结果为OK： 127.0.0.1:6379&gt; set hello jedis xx OK setnx和setxx在实际使用中有什么应用场景吗？ 以setnx命令为例子， 由于Redis的单线程命令处理机制， 如果有多个客户端同时执行setnx key value，根据setnx的特性只有一个客户端能设置成功， setnx可以作为分布式锁的一种实现方案， Redis官方给出了使用setnx实现分布式锁的方法： http://redis.io/topics/distlock。 获取值 get key //下面操作获取键hello的值： 127.0.0.1:6379&gt; get hello &quot;world&quot; //如果要获取的键不存在， 则返回nil（空） ： 127.0.0.1:6379&gt; get not_exist_key (nil) 批量设置值 mset key value [key value ...] //下面操作通过mset命令一次性设置4个键值对： 127.0.0.1:6379&gt; mset a 1 b 2 c 3 d 4 OK 批量获取值 mget key [key ...] //下面操作批量获取了键a、 b、 c、 d的值： 127.0.0.1:6379&gt; mget a b c d 1) &quot;1&quot; 2) &quot;2&quot; 3) &quot;3&quot; 4) &quot;4&quot; 如果有些键不存在， 那么它的值为nil（空） ， 结果是按照传入键的顺序返回： 127.0.0.1:6379&gt; mget a b c f 1) &quot;1&quot; 2) &quot;2&quot; 3) &quot;3&quot; 4) (nil) 计数 incr key //incr命令用于对值做自增操作， 返回结果分为三种情况： //·值不是整数， 返回错误。 //·值是整数， 返回自增后的结果。 //·键不存在， 按照值为0自增， 返回结果为1。 //例如对一个不存在的键执行incr操作后， 返回结果是1： 127.0.0.1:6379&gt; exists key (integer) 0 127.0.0.1:6379&gt; incr key (integer) 1 //再次对键执行incr命令， 返回结果是2： 127.0.0.1:6379&gt; incr key (integer) 2 //如果值不是整数， 那么会返回错误： 127.0.0.1:6379&gt; set hello world OK 127.0.0.1:6379&gt; incr hello (error) ERR value is not an integer or out of range //除了incr命令， Redis提供了decr(自减)、incrby(自增指定数字)、decrby(自减指定数字)、incrbyfloat(自增浮点数)： decr key incrby key increment decrby key decrement incrbyfloat key increment //很多存储系统和编程语言内部使用CAS机制实现计数功能， 会有一定的CPU开销， 但在Redis中完全不存在这个问题， 因为Redis是单线程架构， 任何命令到了Redis服务端都要顺序执行 String使用场景 缓存功能 计数 共享Session 限速 List 添加 //从右边插入元素 rpush key value [value ...] //下面代码从右向左插入元素c、 b、 a： 127.0.0. 1:6379&gt; rpush listkey c b a (integer) 3 //lrange0-1命令可以从左到右获取列表的所有元素： 127.0.0.1:6379&gt; lrange listkey 0 -1 1) &quot;c&quot; 2) &quot;b&quot; 3) &quot;a&quot; //从左边插入元素 lpush key value [value ...] 使用方法和rpush相同， 只不过从左侧插入。 //向某个元素前或者后插入元素 linsert key before|after pivot value //linsert命令会从列表中找到等于pivot的元素， 在其前(before)或者后(after)插入一个新的元素value， 例如下面操作会在列表的元素b前插入java： 127.0.0.1:6379&gt; linsert listkey before b java (integer) 4 //返回结果为4， 代表当前命令的长度， 当前列表变为： 127.0.0.1:6379&gt; lrange listkey 0 -1 1) &quot;c&quot; 2) &quot;java&quot; 3) &quot;b&quot; 4) &quot;a&quot; 查找 //获取指定范围内的元素列表 lrange key start end //lrange操作会获取列表指定索引范围所有的元素。 索引下标有两个特点： 第一， 索引下标从左到右分别是0到N-1， 但是从右到左分别是-1到-N。 //第二， lrange中的end选项包含了自身， 这个和很多编程语言不包含end不太相同， 例如想获取列表的第2到第4个元素， 可以执行如下操作： 127.0.0.1:6379&gt; lrange listkey 1 3 1) &quot;java&quot; 2) &quot;b&quot; 3) &quot;a&quot; //获取列表指定索引下标的元素 lindex key index //例如当前列表最后一个元素为a： 127.0.0.1:6379&gt; lindex listkey -1 &quot;a&quot; // 获取列表长度 llen key //例如， 下面示例当前列表长度为4： 127.0.0.1:6379&gt; llen listkey (integer) 4 删除 //从列表左侧弹出元素 lpop key //如下操作将列表最左侧的元素c会被弹出， 弹出后列表变为java、 b、a： 127.0.0.1:6379&gt;t lpop listkey &quot;c&quot; 127.0.0.1:6379&gt; lrange listkey 0 -1 1) &quot;java&quot; 2) &quot;b&quot; 3) &quot;a&quot; //从列表右侧弹出 rpop key 它的使用方法和lpop是一样的， 只不过从列表右侧弹出 //删除指定元素 lrem key count value //lrem命令会从列表中找到等于value的元素进行删除， 根据count的不同分为三种情况： //·count&gt;0， 从左到右， 删除最多count个元素。 //·count&lt;0， 从右到左， 删除最多count绝对值个元素。 //·count=0， 删除所有。 //例如向列表从左向右插入5个a， 那么当前列表变为“a a a a a java b a”， //下面操作将从列表左边开始删除4个为a的元素： 127.0.0.1:6379&gt; lrem listkey 4 a (integer) 4 127.0.0.1:6379&gt; lrange listkey 0 -1 1) &quot;a&quot; 2) &quot;java&quot; 3) &quot;b&quot; 4) &quot;a&quot; //按照索引范围修剪列表 ltrim key start end //例如， 下面操作会只保留列表listkey第2个到第4个元素： 127.0.0.1:6379&gt; ltrim listkey 1 3 OK 127.0.0.1:6379&gt; lrange listkey 0 -1 1) &quot;java&quot; 2) &quot;b&quot; 3) &quot;a&quot; 修改 //修改指定索引下标的元素： lset key index newValue //下面操作会将列表listkey中的第3个元素设置为python： 127.0.0.1:6379&gt; lset listkey 2 python OK 127.0.0.1:6379&gt; lrange listkey 0 -1 1) &quot;java&quot; 2) &quot;b&quot; 3) &quot;python&quot; 阻塞操作 //阻塞式弹出如下： blpop key [key ...] timeout brpop key [key ...] timeout //blpop和brpop是lpop和rpop的阻塞版本， 它们除了弹出方向不同， 使用方法基本相同， 所以下面以brpop命令进行说明， brpop命令包含两个参数： //·key[key...]： 多个列表的键。 //·timeout： 阻塞时间（单位： 秒） 。 //列表为空： 如果timeout=3， 那么客户端要等到3秒后返回， 如果timeout=0， 那么客户端一直阻塞等下去： 127.0.0.1:6379&gt; brpop list:test 3 (nil) (3.10s) 127.0.0.1:6379&gt; brpop list:test 0 //...阻塞... //如果此期间添加了数据element1， 客户端立即返回： 127.0.0.1:6379&gt; brpop list:test 3 1) &quot;list:test&quot; 2) &quot;element1&quot; (2.06s) //列表不为空： 客户端会立即返回。 127.0.0.1:6379&gt; brpop list:test 0 1) &quot;list:test&quot; 2) &quot;element1&quot; //在使用brpop时， 有两点需要注意。 //第一点， 如果是多个键， 那么brpop会从左至右遍历键， 一旦有一个键能弹出元素， 客户端立即返回： 127.0.0.1:6379&gt; brpop list:1 list:2 list:3 0 //..阻塞.. //此时另一个客户端分别向list： 2和list： 3插入元素： client-lpush&gt; lpush list:2 element2 (integer) 1 client-lpush&gt; lpush list:3 element3 (integer) 1 //客户端会立即返回list： 2中的element2， 因为list： 2最先有可以弹出的元素： 127.0.0.1:6379&gt; brpop list:1 list:2 list:3 0 1) &quot;list:2&quot; 2) &quot;element2_1&quot; //第二点， 如果多个客户端对同一个键执行brpop， 那么最先执行brpop命令的客户端可以获取到弹出的值。 //客户端1： client-1&gt; brpop list:test 0 //...阻塞... //客户端2： client-2&gt; brpop list:test 0 //...阻塞... //客户端3： client-3&gt; brpop list:test 0 //...阻塞... //此时另一个客户端lpush一个元素到list： test列表中： client-lpush&gt; lpush list:test element (integer) 1 //那么客户端1最会获取到元素， 因为客户端1最先执行brpop， 而客户端2和客户端3继续阻塞： client&gt; brpop list:test 0 1) &quot;list:test&quot; 2) &quot;element&quot; List使用场景 lpush+lpop=Stack（ 栈） lpush+rpop=Queue（ 队列） lpush+ltrim=Capped Collection（ 有限集合） lpush+brpop=Message Queue（ 消息队列） Set 集合内操作 添加元素 //sadd key element [element ...] //返回结果为添加成功的元素个数， 例如： 127.0.0.1:6379&gt; exists myset (integer) 0 127.0.0.1:6379&gt; sadd myset a b c (integer) 3 127.0.0.1:6379&gt; sadd myset a b (integer) 0 删除元素 //srem key element [element ...] //返回结果为成功删除元素个数， 例如： 127.0.0.1:6379&gt; srem myset a b (integer) 2 127.0.0.1:6379&gt; srem myset hello (integer) 0 计算元素个数 //scard key //scard的时间复杂度为O（1） ， 它不会遍历集合所有元素， 而是直接用Redis内部的变量， 例如： 127.0.0.1:6379&gt; scard myset (integer) 1 判断元素是否在集合中 //sismember key element //如果给定元素element在集合内返回1， 反之返回0， 例如： 127.0.0.1:6379&gt; sismember myset c (integer) 1 随机从集合返回指定个数元素 //srandmember key [count] //[count]是可选参数， 如果不写默认为1， 例如： 127.0.0.1:6379&gt; srandmember myset 2 1) &quot;a&quot; 2) &quot;c&quot; 127.0.0.1:6379&gt; srandmember myset &quot;d&quot; 从集合随机弹出元素 //spop key //spop操作可以从集合中随机弹出一个元素， 例如下面代码是一次spop后， 集合元素变为&quot;d b a&quot;： 123 127.0.0.1:6379&gt; spop myset &quot;c&quot; 127.0.0.1:6379&gt; smembers myset 1) &quot;d&quot; 2) &quot;b&quot; 3) &quot;a&quot; //需要注意的是Redis从3.2版本开始， spop也支持[count]参数。 //srandmember和spop都是随机从集合选出元素， 两者不同的是spop命令执行后， 元素会从集合中删除， 而srandmember不会。 获取所有元素 //smembers key //下面代码获取集合myset所有元素， 并且返回结果是无序的： 127.0.0.1:6379&gt; smembers myset 1) &quot;d&quot; 2) &quot;b&quot; 3) &quot;a&quot; //smembers和lrange、 hgetall都属于比较重的命令， 如果元素过多存在阻塞Redis的可能性， 这时候可以使用sscan来完成， 有关sscan命令2.7节会介绍。 集合间操作 //现在有两个集合， 它们分别是user： 1： follow和user： 2： follow： 127.0.0.1:6379&gt; sadd user:1:follow it music his sports (integer) 4 127.0.0.1:6379&gt; sadd user:2:follow it news ent sports (integer) 4 124 求多个集合的交集 //sinter key [key ...] //例如下面代码是求user： 1： follow和user： 2： follow两个集合的交集，返回结果是sports、 it： 127.0.0.1:6379&gt; sinter user:1:follow user:2:follow 1) &quot;sports&quot; 2) &quot;it&quot; 求多个集合的并集 //suinon key [key ...] //例如下面代码是求user： 1： follow和user： 2： follow两个集合的并集，返回结果是sports、 it、 his、 news、 music、 ent： 127.0.0.1:6379&gt; sunion user:1:follow user:2:follow 1) &quot;sports&quot; 2) &quot;it&quot; 3) &quot;his&quot; 4) &quot;news&quot; 5) &quot;music&quot; 6) &quot;ent&quot; 求多个集合的差集 //sdiff key [key ...] //例如下面代码是求user： 1： follow和user： 2： follow两个集合的差集，返回结果是music和his： 127.0.0.1:6379&gt; sdiff user:1:follow user:2:follow 1) &quot;music&quot; 2) &quot;his&quot; 前面三个命令如图所示 将交集、 并集、 差集的结果保存 sinterstore destination key [key ...] suionstore destination key [key ...] sdiffstore destination key [key ...] 集合间的运算在元素较多的情况下会比较耗时， 所以Redis提供了上面三个命令（原命令+store） 将集合间交集、 并集、 差集的结果保存在destination key中， 例如下面操作将user： 1： follow和user： 2： follow两个集合的交集结果保存在user： 1_2： inter中， user： 1_2： inter本身也是集合类 型： 127.0.0.1:6379&gt; sinterstore user:1_2:inter user:1:follow user:2:follow (integer) 2 127.0.0.1:6379&gt; type user:1_2:inter set 127.0.0.1:6379&gt; smembers user:1_2:inter 1) &quot;it&quot; 2) &quot;sports&quot; Set使用场景 sadd=Tagging（标签） spop/srandmember=Random item（生成随机数， 比如抽奖） sadd+sinter=Social Graph（社交需求） Zset 集合内操作 添加成员 //zadd key score member [score member ...] //下面操作向有序集合user： ranking添加用户tom和他的分数251： 127.0.0.1:6379&gt; zadd user:ranking 251 tom (integer) 1 //返回结果代表成功添加成员的个数： 127.0.0.1:6379&gt; zadd user:ranking 1 kris 91 mike 200 frank 220 tim 250 martin (integer) 5 有关zadd命令有两点需要注意： Redis3.2为zadd命令添加了nx、 xx、 ch、 incr四个选项： nx： member必须不存在， 才可以设置成功， 用于添加。 xx： member必须存在， 才可以设置成功， 用于更新。 ch： 返回此次操作后， 有序集合元素和分数发生变化的个数 incr： 对score做增加， 相当于后面介绍的zincrby。 有序集合相比集合提供了排序字段， 但是也产生了代价， zadd的时间复杂度为O（log（n） ） ， sadd的时间复杂度为O（1） 。 计算成员个数 //zcard key //例如下面操作返回有序集合user： ranking的成员数为5， 和集合类型的scard命令一样， zcard的时间复杂度为O（1） 。 127.0.0.1:6379&gt; zcard user:ranking (integer) 5 计算某个成员的分数 //zscore key member //tom的分数为251， 如果成员不存在则返回nil： 127.0.0.1:6379&gt; zscore user:ranking tom &quot;251&quot; 127.0.0.1:6379&gt; zscore user:ranking test (nil) 计算成员的排名 //zrank key member //zrevrank key member //zrank是从分数从低到高返回排名， zrevrank反之。 例如下面操作中， tom在zrank和zrevrank分别排名第5和第0（排名从0开始计算） 。 127.0.0.1:6379&gt; zrank user:ranking tom (integer) 5 127.0.0.1:6379&gt; zrevrank user:ranking tom (integer) 0 删除成员 //zrem key member [member ...] //下面操作将成员mike从有序集合user： ranking中删除。 127.0.0.1:6379&gt; zrem user:ranking mike (integer) 1 //返回结果为成功删除的个数。 增加成员的分数 //zincrby key increment member //下面操作给tom增加了9分， 分数变为了260分： 127.0.0.1:6379&gt; zincrby user:ranking 9 tom &quot;260&quot; 返回指定排名范围的成员 //zrange key start end [withscores] //zrevrange key start end [withscores] //有序集合是按照分值排名的， zrange是从低到高返回， zrevrange反之。 //下面代码返回排名最低的是三个成员， 如果加上withscores选项， 同时会返回成员的分数： 127.0.0.1:6379&gt; zrange user:ranking 0 2 withscores 1) &quot;kris&quot; 2) &quot;1&quot; 3) &quot;frank&quot; 4) &quot;200&quot; 5) &quot;tim&quot; 6) &quot;220&quot; 127.0.0.1:6379&gt; zrevrange user:ranking 0 2 withscores 1) &quot;tom&quot; 2) &quot;260&quot; 3) &quot;martin&quot; 4) &quot;250&quot; 5) &quot;tim&quot; 6) &quot;220&quot; 返回指定分数范围的成员 //zrangebyscore key min max [withscores] [limit offset count] //zrevrangebyscore key max min [withscores] [limit offset count] //其中zrangebyscore按照分数从低到高返回， zrevrangebyscore反之。 例如下面操作从低到高返回200到221分的成员， withscores选项会同时返回每个成员的分数。 [limit offset count]选项可以限制输出的起始位置和个数： 127.0.0.1:6379&gt; zrangebyscore user:ranking 200 tinf withscores 1) &quot;frank&quot; 2) &quot;200&quot; 3) &quot;tim&quot; 4) &quot;220&quot; 127.0.0.1:6379&gt; zrevrangebyscore user:ranking 221 200 withscores 1) &quot;tim&quot; 2) &quot;220&quot; 3) &quot;frank&quot; 4) &quot;200&quot; //同时min和max还支持开区间（ 小括号） 和闭区间（ 中括号） ， -inf和+inf分别代表无限小和无限大： 127.0.0.1:6379&gt; zrangebyscore user:ranking (200 +inf withscores 1) &quot;tim&quot; 2) &quot;220&quot; 3) &quot;martin&quot; 4) &quot;250&quot; 5) &quot;tom&quot; 6) &quot;260&quot; 返回指定分数范围成员个数 //zcount key min max //下面操作返回200到221分的成员的个数： 127.0.0.1:6379&gt; zcount user:ranking 200 221 (integer) 2 删除指定排名内的升序元素 //zremrangebyrank key start end //下面操作删除第start到第end名的成员： 127.0.0.1:6379&gt; zremrangebyrank user:ranking 0 2 (integer) 3 删除指定分数范围的成员 //zremrangebyscore key min max //下面操作将250分以上的成员全部删除， 返回结果为成功删除的个数： 127.0.0.1:6379&gt; zremrangebyscore user:ranking (250 +inf (integer) 2 集合间操作 将图中的两个有序集合导入到Redis中。 127.0.0.1:6379&gt; zadd user:ranking:1 1 kris 91 mike 200 frank 220 tim 250 martin 251 tom (integer) 6 127.0.0.1:6379&gt; zadd user:ranking:2 8 james 77 mike 625 martin 888 tom (integer) 4 交集 //zinterstore destination numkeys key [key ...] [weights weight [weight ...]][aggregate sum|min|max] //这个命令参数较多， 下面分别进行说明： //·destination： 交集计算结果保存到这个键。 //·numkeys： 需要做交集计算键的个数。 //·key[key...]： 需要做交集计算的键。 //·weights weight[weight...]： 每个键的权重， 在做交集计算时， 每个键中的每个member会将自己分数乘以这个权重， 每个键的权重默认是1。 //·aggregate sum|min|max： 计算成员交集后， 分值可以按照sum（ 和） 、min（ 最小值） 、 max（ 最大值） 做汇总， 默认值是sum。 //下面操作对user： ranking： 1和user： ranking： 2做交集， weights和aggregate使用了默认配置， 可以看到目标键user： ranking： 1_inter_2对分值做了sum操作： 127.0.0.1:6379&gt; zinterstore user:ranking:1_inter_2 2 user:ranking:1 user:ranking:2 (integer) 3 127.0.0.1:6379&gt; zrange user:ranking:1_inter_2 0 -1 withscores 1) &quot;mike&quot; 2) &quot;168&quot; 3) &quot;martin&quot; 4) &quot;875&quot; 5) &quot;tom&quot; 6) &quot;1139&quot; //如果想让user： ranking： 2的权重变为0.5， 并且聚合效果使用max， 可以执行如下操作： 127.0.0.1:6379&gt; zinterstore user:ranking:1_inter_2 2 user:ranking:1 user:ranking:2 weights 1 0.5 aggregate max (integer) 3 127.0.0.1:6379&gt; zrange user:ranking:1_inter_2 0 -1 withscores 1) &quot;mike&quot; 2) &quot;91&quot; 3) &quot;martin&quot; 4) &quot;312.5&quot; 5) &quot;tom&quot; 6) &quot;444&quot; 并集 //zunionstore destination numkeys key [key ...] [weights weight [weight ...]][aggregate sum|min|max] //该命令的所有参数和zinterstore是一致的， 只不过是做并集计算， 例如下面操作是计算user： ranking： 1和user： ranking： 2的并集， weights和 //aggregate使用了默认配置， 可以看到目标键user： ranking： 1_union_2对分值做了sum操作： 127.0.0.1:6379&gt; zunionstore user:ranking:1_union_2 2 user:ranking:1 user:ranking:2 (integer) 7 127.0.0.1:6379&gt; zrange user:ranking:1_union_2 0 -1 withscores 1) &quot;kris&quot; 2) &quot;1&quot; 3) &quot;james&quot; 4) &quot;8&quot; 5) &quot;mike&quot; 6) &quot;168&quot; 7) &quot;frank&quot; 8) &quot;200&quot; 9) &quot;tim&quot; 10) &quot;220&quot; 11) &quot;martin&quot; 12) &quot;875&quot; 13) &quot;tom&quot; 14) &quot;1139&quot; Zset使用场景 有序集合比较典型的使用场景就是排行榜系统。 例如视频网站需要对用户上传的视频做排行榜， 榜单的维度可能是多个方面的： 按照时间、 按照播 放数量、 按照获得的赞数。 本节使用赞数这个维度， 记录每天用户上传视频的排行榜。 主要需要实现以下4个功能。 添加用户赞数 例如用户mike上传了一个视频， 并获得了3个赞， 可以使用有序集合的zadd和zincrby功能： zadd user:ranking:2016_03_15 mike 3 如果之后再获得一个赞， 可以使用zincrby： zincrby user:ranking:2016_03_15 mike 1 取消用户赞数 由于各种原因（例如用户注销、 用户作弊） 需要将用户删除， 此时需要将用户从榜单中删除掉， 可以使用zrem。 例如删除成员tom： zrem user:ranking:2016_03_15 mike 展示获取赞数最多的十个用户 此功能使用zrevrange命令实现： zrevrangebyrank user:ranking:2016_03_15 0 9 展示用户信息以及用户分数 此功能将用户名作为键后缀， 将用户信息保存在哈希类型中， 至于用户的分数和排名可以使用zscore和zrank两个功能： hgetall user:info:tom zscore user:ranking:2016_03_15 mike zrank user:ranking:2016_03_15 mik Hash 设置值 //hset key field value //下面为user： 1添加一对field-value： 127.0.0.1:6379&gt; hset user:1 name tom (integer) 1 //如果设置成功会返回1， 反之会返回0。 此外Redis提供了hsetnx命令， 它们的关系就像set和setnx命令一样， 只不过作用域由键变为field。 获取值 //hget key field //例如， 下面操作获取user： 1的name域（属性） 对应的值： 127.0.0.1:6379&gt; hget user:1 name &quot;tom&quot; //如果键或field不存在， 会返回nil： 127.0.0.1:6379&gt; hget user:2 name (nil) 127.0.0.1:6379&gt; hget user:1 age (nil) 删除field //hdel key field [field ...] //hdel会删除一个或多个field， 返回结果为成功删除field的个数， 例如： 127.0.0.1:6379&gt; hdel user:1 name (integer) 1 127.0.0.1:6379&gt; hdel user:1 age (integer) 0 计算field个数 //hlen key //例如user： 1有3个field： 127.0.0.1:6379&gt; hset user:1 name tom (integer) 1 127.0.0.1:6379&gt; hset user:1 age 23 (integer) 1 127.0.0.1:6379&gt; hset user:1 city tianjin (integer) 1 127.0.0.1:6379&gt; hlen user:1 (integer) 3 批量设置或获取field-value //hmget key field [field ...] //hmset key field value [field value ...] //hmset和hmget分别是批量设置和获取field-value， hmset需要的参数是key和多对field-value， hmget需要的参数是key和多个field。 例如： 127.0.0.1:6379&gt; hmset user:1 name mike age 12 city tianjin OK 127.0.0.1:6379&gt; hmget user:1 name city 1) &quot;mike&quot; 2) &quot;tianjin&quot; 判断field是否存在 //hexists key field //例如， user： 1包含name域， 所以返回结果为1， 不包含时返回0： 127.0.0.1:6379&gt; hexists user:1 name (integer) 1 获取所有field //hkeys key //hkeys命令应该叫hfields更为恰当， 它返回指定哈希键所有的field， 例如： 127.0.0.1:6379&gt; hkeys user:1 1) &quot;name&quot; 2) &quot;age&quot; 3) &quot;city&quot; 获取所有value //hvals key 下面操作获取user： 1全部value： 127.0.0.1:6379&gt; hvals user:1 1) &quot;mike&quot; 2) &quot;12&quot; 3) &quot;tianjin&quot; 获取所有的field-value //hgetall key //下面操作获取user： 1所有的field-value： 127.0.0.1:6379&gt; hgetall user:1 1) &quot;name&quot; 2) &quot;mike&quot; 3) &quot;age&quot; 4) &quot;12&quot; 5) &quot;city&quot; 6) &quot;tianjin&quot; 在使用hgetall时， 如果哈希元素个数比较多， 会存在阻塞Redis的可能。 如果开发人员只需要获取部分field， 可以使用hmget， 如果一定要获取全部field-value， 可以使用hscan命令， 该命令会渐进式遍历哈希类型. hincrby hincrbyfloat //hincrby key field //hincrbyfloat key field //hincrby和hincrbyfloat， 就像incrby和incrbyfloat命令一样， 但是它们的作用域是filed。 计算value的字符串长度（需要Redis3.2以上） //hstrlen key field //例如hget user： 1name的value是tom， 那么hstrlen的返回结果是3： 127.0.0.1:6379&gt; hstrlen user:1 name (integer) 3 Hash使用场景 保存对象信息 Redis 发布订阅 Redis 发布订阅 (pub/sub) 是一种消息通信模式：发送者 (pub) 发送消息，订阅者 (sub) 接收消息。 Redis 客户端可以订阅任意数量的频道。 下图展示了频道 channel1 ， 以及订阅这个频道的三个客户端 —— client2 、 client5 和 client1 之间的关系： 当有新消息通过 PUBLISH 命令发送给频道 channel1 时， 这个消息就会被发送给订阅它的三个客户端： 实例 以下实例演示了发布订阅是如何工作的，需要开启两个 redis-cli 客户端。 在我们实例中我们创建了订阅频道名为 runoobChat: 第一个 redis-cli 客户端 redis 127.0.0.1:6379&gt; SUBSCRIBE runoobChat Reading messages... (press Ctrl-C to quit) &quot;subscribe&quot; &quot;redisChat&quot; (integer) 1 现在，我们先重新开启个 redis 客户端，然后在同一个频道 runoobChat 发布两次消息，订阅者就能接收到消息。 第二个 redis-cli 客户端 redis 127.0.0.1:6379&gt; PUBLISH runoobChat &quot;Redis PUBLISH test&quot; (integer) 1 redis 127.0.0.1:6379&gt; PUBLISH runoobChat &quot;Learn redis by runoob.com&quot; (integer) 1 # 订阅者的客户端会显示如下消息 &quot;message&quot; &quot;runoobChat&quot; &quot;Redis PUBLISH test&quot; &quot;message&quot; &quot;runoobChat&quot; &quot;Learn redis by runoob.com&quot; springboot中实现 一.配置文件 spring: redis: host: 192.168.0.200 port: 6379 password: database: 1 pool.max-active: 8 pool.max-wait: -1 pool.max-idle: 8 pool.min-idle: 0 timeout: 5000 二.maven坐标 &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 三.redis消息监听器容器以及redis监听器注入IOC容器 package com.example.demo; import org.springframework.cache.annotation.EnableCaching; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.context.annotation.Scope; import org.springframework.data.redis.connection.RedisConnectionFactory; import org.springframework.data.redis.listener.PatternTopic; import org.springframework.data.redis.listener.RedisMessageListenerContainer; import org.springframework.data.redis.listener.adapter.MessageListenerAdapter; @Configuration @EnableCaching public class RedisConfig{ /** * Redis消息监听器容器 * @param connectionFactory * @return **/ @Bean RedisMessageListenerContainer container(RedisConnectionFactory connectionFactory) { RedisMessageListenerContainer container = new RedisMessageListenerContainer(); container.setConnectionFactory(connectionFactory); //订阅了一个叫pmp和channel 的通道，多通道 container.addMessageListener(listenerAdapter(new RedisPmpSub()),new PatternTopic(&quot;pmp&quot;)); container.addMessageListener(listenerAdapter(new RedisChannelSub()),new PatternTopic(&quot;channel&quot;)); //这个container 可以添加多个 messageListener return container; } /** * 配置消息接收处理类 * @param redisMsg 自定义消息接收类 * @return */ @Bean() @Scope(&quot;prototype&quot;) MessageListenerAdapter listenerAdapter(RedisMsg redisMsg) { //这个地方 是给messageListenerAdapter 传入一个消息接受的处理器，利用反射的方法调用“receiveMessage” //也有好几个重载方法，这边默认调用处理器的方法 叫handleMessage 可以自己到源码里面看 return new MessageListenerAdapter(redisMsg, &quot;receiveMessage&quot;);//注意2个通道调用的方法都要为receiveMessage } } 四.普通的消息处理器接口 package com.example.demo; import org.springframework.stereotype.Component; @Component public interface RedisMsg { public void receiveMessage(String message); } 五.普通的消息处理器POJO package com.example.demo; /** * @Auther: Administrator * @Date: 2018/7/9 11:01 * @Description: */ public class RedisChannelSub implements RedisMsg { @Override public void receiveMessage(String message) { //注意通道调用的方法名要和RedisConfig的listenerAdapter的MessageListenerAdapter参数2相同 System.out.println(&quot;这是RedisChannelSub&quot;+&quot;-----&quot;+message); } } package com.example.demo; public class RedisPmpSub implements RedisMsg{ /** * 接收消息的方法 * @param message 订阅消息 */ public void receiveMessage(String message){ //注意通道调用的方法名要和RedisConfig的listenerAdapter的MessageListenerAdapter参数2相同 System.out.println(message); } } 六.消息发布者 package com.example.demo; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.data.redis.core.StringRedisTemplate; import org.springframework.scheduling.annotation.EnableScheduling; import org.springframework.scheduling.annotation.Scheduled; import org.springframework.stereotype.Component; //定时器 @EnableScheduling @Component public class TestSenderController { @Autowired private StringRedisTemplate stringRedisTemplate; //向redis消息队列index通道发布消息 @Scheduled(fixedRate = 2000) public void sendMessage(){ stringRedisTemplate.convertAndSend(&quot;pmp&quot;,String.valueOf(Math.random())); stringRedisTemplate.convertAndSend(&quot;channel&quot;,String.valueOf(Math.random())); } } 七.启动类 package com.example.demo; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; @SpringBootApplication public class DemoApplication { public static void main(String[] args) { SpringApplication.run(DemoApplication.class, args); } } 3种高级数据结构 BitMap BitMap，即位图，其实也就是 byte 数组，用二进制表示，只有 0 和 1 两个数字。 如图所示： 重要 API 命令 含义 getbit key offset 对key所存储的字符串值，获取指定偏移量上的位（bit） setbit key offset value 对key所存储的字符串值，设置或清除指定偏移量上的位（bit） 1. 返回值为该位在setbit之前的值 2. value只能取0或1 3. offset从0开始，即使原位图只能10位，offset可以取1000 bitcount key [start end] 获取位图指定范围中位值为1的个数 如果不指定start与end，则取所有 bitop op destKey key1 [key2...] 做多个BitMap的and（交集）、or（并集）、not（非）、xor（异或）操作并将结果保存在destKey中 bitpos key tartgetBit [start end] 计算位图指定范围第一个偏移量对应的的值等于targetBit的位置 1. 找不到返回-1 2. start与end没有设置，则取全部 3. targetBit只能取0或者1 演示 应用场景 统计每日用户的登录数。每一位标识一个用户ID，当某个用户访问我们的网页或执行了某个操作，就在bitmap中把标识此用户的位设置为1。 这里做了一个 使用 set 和 BitMap 存储的对比。 场景1：1 亿用户，5千万独立 数据类型 每个 userid 占用空间 需要存储的用户量 全部内存量 set 32位（假设userid用的是整型，实际很多网站用的是长整型） 50,000,000 32位 * 50,000,000 = 200 MB BitMap 1 位 100,000,000 1 位 * 100,000,000 = 12.5 MB 一天 一个月 一年 set 200M 6G 72G BitMap 12.5M 375M 4.5G 场景2：只有 10 万独立用户 数据类型 每个 userid 占用空间 需要存储的用户量 全部内存量 set 32位（假设userid用的是整型，实际很多网站用的是长整型） 1,000,000 32位 * 1,000,000 = 4 MB BitMap 1 位 100,000,000 1 位 * 100,000,000 = 12.5 MB 通过上面的对比，我们可以看到，如果独立用户数量很多，使用 BitMap 明显更有优势，能节省大量的内存。但如果独立用户数量较少，还是建议使用 set 存储，BitMap 会产生多余的存储开销。 使用经验 type = string，BitMap 是 sting 类型，最大 512 MB。 注意 setbit 时的偏移量，可能有较大耗时 位图不是绝对好。 HyperLogLog HyperLogLog 是基于 HyperLogLog 算法的一种数据结构，该算法可以在极小空间完成独立数量统计。 在本质上还是字符串类型。 重要 API 命令 含义 pfadd key element1 [element2...] 向HyperLogLog中添加元素 pfcount key1 [key2...] 计算HyperLogLog的独立总数 pfmerge destKey key1 [key2...] 合并多个hyperLogLog到destKey中 演示 内存消耗 以百万独立用户为例 内存消耗 1 天 15 KB 1 个月 450 KB 1 年 15KB * 365 = 5 MB 可以看到内存消耗是非常低的，比我们之前学过的 BitMap 还要低得多。 使用经验 Q：既然 HyperLogLog 那么好，那么是不是以后用这个来存储数据就行了呢？ A：这里要考虑两个因素： hyperloglog 的错误率为：0.81%，存储的数据不能百分百准确。 hyperloglog 不能取出单条数据。api 中也没有相关操作。 如果你没有这两个方面的顾虑，那么用 HyperLogLog 来存储大规模数据，还是非常不错的。 GEO Redis 3.2添加新特性 功能：存储经纬度、计算两地距离、范围计算等 基于ZSet实现 删除操作使用 zrem key member GEO 相关命令 1. geoadd key longitude latitude member [lon lat member...] 含义：增加地理位置信息 longitude ：经度 latitude : 纬度 member : 标识信息 2. geopos key member1 [member2...] 含义：获取地理位置信息 3. geodist key member1 member2 [unit] 含义：获取两个地理位置的距离 unit取值范围 m（米，默认） km（千米） mi（英里） ft（英尺） 4. georadius key longitude latitude unit [withcoord] [withdist] [withhash] [COUNT count] [sort] [store key] [storedist key] 含义：以给定的经纬度为中心，返回包含的位置元素当中，与中心距离不超过给定最大距离的所有位置元素。 unit取值范围 m（米） km（千米） mi（英里） ft（英尺） withcoord：将位置元素的经度与纬度也一并返回 withdist：在返回位置元素的同时，将距离也一并返回。距离的单位和用户给定的范围单位保持一致 withhash：以52位的符号整数形式，返回位置元素经过geohash编码的有序集合分值。用于底层应用或调试，实际作用不大。 sort取值范围 asc：根据中心位置，按照从近到远的方式返回位置元素 desc：根据中心位置，按照从远到近的方式返回位置元素 store key：将返回结果而的地理位置信息保存到指定键 storedist key：将返回结果距离中心节点的距离保存到指定键 5. georadiusbymember key member radius unit [withcoord][withdist][withhash][COUNT count][sort][store key][storedist key] 含义：以给定的元素为中心，返回包含的位置元素当中，与中心距离不超过给定最大距离的所有位置元素。 unit取值范围 m（米） km（千米） mi（英里） ft（英尺） withcoord：将位置元素的经度与纬度也一并返回 withdist：在返回位置元素的同时，将距离也一并返回。距离的单位和用户给定的范围单位保持一致 withhash：以52位的符号整数形式，返回位置元素经过geohash编码的有序集合分值。用于底层应用或调试，实际作用不大。 sort取值范围 asc：根据中心位置，按照从近到远的方式返回位置元素 desc：根据中心位置，按照从远到近的方式返回位置元素 store key：将返回结果而的地理位置信息保存到指定键 storedist key：将返回结果距离中心节点的距离保存到指定键 演示 geo 功能是在 redis-3.2 后引入的 127.0.0.1:6381&gt; geoadd cities:locations 116.28 39.55 beijing (integer) 1 127.0.0.1:6381&gt; geoadd cities:locations 117.12 39.08 tianjin 114.29 38.02 shijiazhuang 118.01 39.38 tangshan 115.29 38.51 baoding (integer) 4 127.0.0.1:6381&gt; geopos cities:locations tianjin 1) 1) &quot;117.12000042200088501&quot; 2) &quot;39.0800000535766543&quot; 127.0.0.1:6381&gt; geodist cities:locations tianjin beijing km &quot;89.2061&quot; 127.0.0.1:6379&gt; georadiusbymember cities:locations beijing 150 km 1) &quot;beijing&quot; 2) &quot;tianjin&quot; 3) &quot;tangshan&quot; 4) &quot;baoding&quot;。 ","link":"https://tianxiawuhao.github.io/YNIBE7Yaj/"},{"title":"redis概述一","content":"什么是Redis Redis(Remote Dictionary Server) 是一个使用 C 语言编写的，开源的（BSD许可）高性能非关系型（NoSQL）的键值对数据库。 Redis 可以存储键和五种不同类型的值之间的映射。键的类型只能为字符串，值支持五种数据类型：字符串、列表、集合、散列表、有序集合。 与传统数据库不同的是 Redis 的数据是存在内存中的，所以读写速度非常快，因此 redis 被广泛应用于缓存方向，每秒可以处理超过 10万次读写操作，是已知性能最快的Key-Value DB。另外，Redis 也经常用来做分布式锁。除此之外，Redis 支持事务 、持久化、LUA脚本、LRU驱动事件、多种集群方案。 Redis有哪些优缺点 优点 读写性能优异， Redis能读的速度是110000次/s，写的速度是81000次/s。 支持数据持久化，支持AOF和RDB两种持久化方式。 支持事务，Redis的所有操作都是原子性的，同时Redis还支持对几个操作合并后的原子性执行。 数据结构丰富，除了支持string类型的value外还支持hash、set、zset、list等数据结构。 支持主从复制，主机会自动将数据同步到从机，可以进行读写分离。 缺点 数据库容量受到物理内存的限制，不能用作海量数据的高性能读写，因此Redis适合的场景主要局限在较小数据量的高性能操作和运算上。 Redis 不具备自动容错和恢复功能，主机从机的宕机都会导致前端部分读写请求失败，需要等待机器重启或者手动切换前端的IP才能恢复。 主机宕机，宕机前有部分数据未能及时同步到从机，切换IP后还会引入数据不一致的问题，降低了系统的可用性。 Redis 较难支持在线扩容，在集群容量达到上限时在线扩容会变得很复杂。为避免这一问题，运维人员在系统上线时必须确保有足够的空间，这对资源造成了很大的浪费。 为什么要用 Redis /为什么要用缓存 主要从“高性能”和“高并发”这两点来看待这个问题。 高性能： 假如用户第一次访问数据库中的某些数据。这个过程会比较慢，因为是从硬盘上读取的。将该用户访问的数据存在缓存中，这样下一次再访问这些数据的时候就可以直接从缓存中获取了。操作缓存就是直接操作内存，所以速度相当快。如果数据库中的对应数据改变的之后，同步改变缓存中相应的数据即可！ 高并发： 直接操作缓存能够承受的请求是远远大于直接访问数据库的，所以我们可以考虑把数据库中的部分数据转移到缓存中去，这样用户的一部分请求会直接到缓存这里而不用经过数据库。 为什么要用 Redis 而不用 map/guava 做缓存? 缓存分为本地缓存和分布式缓存。以 Java 为例，使用自带的 map 或者 guava 实现的是本地缓存，最主要的特点是轻量以及快速，生命周期随着 jvm 的销毁而结束，并且在多实例的情况下，每个实例都需要各自保存一份缓存，缓存不具有一致性。 使用 redis 或 memcached 之类的称为分布式缓存，在多实例的情况下，各实例共用一份缓存数据，缓存具有一致性。缺点是需要保持 redis 或 memcached服务的高可用，整个程序架构上较为复杂。 Redis为什么这么快 1、完全基于内存，绝大部分请求是纯粹的内存操作，非常快速。数据存在内存中，类似于 HashMap，HashMap 的优势就是查找和操作的时间复杂度都是O(1)； 2、数据结构简单，对数据操作也简单，Redis 中的数据结构是专门进行设计的； 3、采用单线程，避免了不必要的上下文切换和竞争条件，也不存在多进程或者多线程导致的切换而消耗 CPU，不用去考虑各种锁的问题，不存在加锁释放锁操作，没有因为可能出现死锁而导致的性能消耗； 4、使用多路 I/O 复用模型，非阻塞 IO； 5、使用底层模型不同，它们之间底层实现方式以及与客户端之间通信的应用协议不一样，Redis 直接自己构建了 VM 机制 ，因为一般的系统调用系统函数的话，会浪费一定的时间去移动和请求； ","link":"https://tianxiawuhao.github.io/7i0HRwy0e/"},{"title":"多线程基础概念四","content":"Lock 接口(Lock interface)简介 Lock 接口比同步方法和同步块提供了更具扩展性的锁操作。他们允许更灵活的结构，可以具有完全不同的性质，并且可以支持多个相关类的条件对象。 优势 它的优势有： （1）可以使锁更公平 （2）可以使线程在等待锁的时候响应中断 （3）可以让线程尝试获取锁，并在无法获取锁的时候立即返回或者等待一段时间 （4）可以在不同的范围，以不同的顺序获取和释放锁 整体上来说 Lock 是 synchronized 的扩展版，Lock 提供了无条件的、可轮询的(tryLock 方法)、定时的(tryLock 带参方法)、可中断的(lockInterruptibly)、可多条件队列的(newCondition 方法)锁操作。另外 Lock 的实现类基本都支持非公平锁(默认)和公平锁，synchronized 只支持非公平锁，当然，在大部分情况下，非公平锁是高效的选择。 乐观锁和悲观锁 悲观锁：总是假设最坏的情况，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会阻塞直到它拿到锁。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。再比如 Java 里面的同步原语 synchronized 关键字的实现也是悲观锁。 乐观锁：顾名思义，就是很乐观，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号等机制。乐观锁适用于多读的应用类型，这样可以提高吞吐量，像数据库提供的类似于 write_condition 机制，其实都是提供的乐观锁。在 Java中 java.util.concurrent.atomic 包下面的原子变量类就是使用了乐观锁的一种实现方式 CAS 实现的。 乐观锁的实现方式： 1、使用版本标识来确定读到的数据与提交时的数据是否一致。提交后修改版本标识，不一致时可以采取丢弃和再次尝试的策略。 2、java 中的 Compare and Swap 即 CAS ，当多个线程尝试使用 CAS 同时更新同一个变量时，只有其中一个线程能更新变量的值，而其它线程都失败，失败的线程并不会被挂起，而是被告知这次竞争中失败，并可以再次尝试。 CAS 操作中包含三个操作数 —— 需要读写的内存位置（V），进行比较的预期原值（A）和拟写入的新值(B)。如果内存位置 V 的值与预期原值 A 相匹配，那么处理器会自动将该位置值更新为新值 B。否则处理器不做任何操作。 CAS CAS 是 compare and swap 的缩写，即我们所说的比较交换。 CAS 是一种基于锁的操作，而且是乐观锁。在 java 中锁分为乐观锁和悲观锁。悲观锁是将资源锁住，等一个之前获得锁的线程释放锁之后，下一个线程才可以访问。而乐观锁采取了一种宽泛的态度，通过某种方式不加锁来处理资源，比如通过给记录加 version 来获取数据，性能较悲观锁有很大的提高。 CAS 操作包含三个操作数 —— 内存位置（V）、预期原值（A）和新值(B)。如果内存地址里面的值和 A 的值是一样的，那么就将内存里面的值更新成 B。CAS是通过无限循环来获取数据的，若果在第一轮循环中，a 线程获取地址里面的值被b 线程修改了，那么 a 线程需要自旋，到下次循环才有可能机会执行。 java.util.concurrent.atomic 包下的类大多是使用 CAS 操作来实现的(AtomicInteger,AtomicBoolean,AtomicLong)。 CAS产生的问题 ABA 问题： 比如说一个线程 one 从内存位置 V 中取出 A，这时候另一个线程 two 也从内存中取出 A，并且 two 进行了一些操作变成了 B，然后 two 又将 V 位置的数据变成 A，这时候线程 one 进行 CAS 操作发现内存中仍然是 A，然后 one 操作成功。尽管线程 one 的 CAS 操作成功，但可能存在潜藏的问题。从 Java1.5 开始 JDK 的 atomic包里提供了一个类 AtomicStampedReference 来解决 ABA 问题。 循环时间长开销大： 对于资源竞争严重（线程冲突严重）的情况，CAS 自旋的概率会比较大，从而浪费更多的 CPU 资源，效率低于 synchronized。 只能保证一个共享变量的原子操作： 当对一个共享变量执行操作时，我们可以使用循环 CAS 的方式来保证原子操作，但是对多个共享变量操作时，循环 CAS 就无法保证操作的原子性，这个时候就可以用锁。 公平锁锁和非公平锁 公平锁：多个线程按照申请锁的顺序去获得锁，线程会直接进入队列去排队，永远都是队列的第一位才能得到锁。 优点：所有的线程都能得到资源，不会饿死在队列中。 缺点：吞吐量会下降很多，队列里面除了第一个线程，其他的线程都会阻塞，cpu唤醒阻塞线程的开销会很大。 非公平锁：多个线程去获取锁的时候，会直接去尝试获取，获取不到，再去进入等待队列，如果能获取到，就直接获取到锁。 优点：可以减少CPU唤醒线程的开销，整体的吞吐效率会高点，CPU也不必取唤醒所有线程，会减少唤起线程的数量。 缺点：你们可能也发现了，这样可能导致队列中间的线程一直获取不到锁或者长时间获取不到锁，导致饿死。 伪共享 一、伪共享的定义： 缓存系统中是以缓存行（cache line）为单位存储的，当多线程修改互相独立的变量时，如果这些变量共享同一个缓存行，就会无意中影响彼此的性能，这就是伪共享。 二、CPU缓存机制 CPU 缓存（Cache Memory）是位于 CPU 与内存之间的临时存储器，它的容量比内存小的多但是交换速度却比内存要快得多。 高速缓存的出现主要是为了解决 CPU 运算速度与内存读写速度不匹配的矛盾，因为 CPU 运算速度要比内存读写速度快很多，这样会使 CPU 花费很长时间等待数据到来或把数据写入内存。 在缓存中的数据是内存中的一小部分，但这一小部分是短时间内 CPU 即将访问的，当 CPU 调用大量数据时，就可避开内存直接从缓存中调用，从而加快读取速度。 CPU 和主内存之间有好几层缓存，因为即使直接访问主内存也是非常慢的。如果你正在多次对一块数据做相同的运算，那么在执行运算的时候把它加载到离 CPU 很近的地方就有意义了。 按照数据读取顺序和与 CPU 结合的紧密程度，CPU 缓存可以分为一级缓存，二级缓存，部分高端 CPU 还具有三级缓存。每一级缓存中所储存的全部数据都是下一级缓存的一部分，越靠近 CPU 的缓存越快也越小。所以 L1 缓存很小但很快(译注：L1 表示一级缓存)，并且紧靠着在使用它的 CPU 内核。L2 大一些，也慢一些，并且仍然只能被一个单独的 CPU 核使用。L3 在现代多核机器中更普遍，仍然更大，更慢，并且被单个插槽上的所有 CPU 核共享。最后，你拥有一块主存，由全部插槽上的所有 CPU 核共享。拥有三级缓存的的 CPU，到三级缓存时能够达到 95% 的命中率，只有不到 5% 的数据需要从内存中查询。 多核机器的存储结构如下图所示： 当 CPU 执行运算的时候，它先去 L1 查找所需的数据，再去 L2，然后是 L3，最后如果这些缓存中都没有，所需的数据就要去主内存拿。走得越远，运算耗费的时间就越长。所以如果你在做一些很频繁的事，你要确保数据在 L1 缓存中。 Martin Thompson 给出了一些缓存未命中的消耗数据，如下所示： 三、缓存行 缓存系统中是以缓存行（cache line）为单位存储的。缓存行通常是 64 字节（译注：本文基于 64 字节，其他长度的如 32 字节等不适本文讨论的重点），并且它有效地引用主内存中的一块地址。例如一个 的 long 类型是 8 字节，因此在一个缓存行中可以存 8 个 long 类型的变量。所以，如果你访问一个 long 数组，当数组中的一个值被加载到缓存中，它会额外加载另外 7 个，以致你能非常快地遍历这个数组。事实上，你可以非常快速的遍历在连续的内存块中分配的任意数据结构。而如果你在数据结构中的项在内存中不是彼此相邻的（如链表），你将得不到免费缓存加载所带来的优势，并且在这些数据结构中的每一个项都可能会出现缓存未命中。 如果存在这样的场景，有多个线程操作不同的成员变量，但是相同的缓存行，这个时候会发生什么？。没错，伪共享（False Sharing）问题就发生了！有张 Disruptor 项目的经典示例图，如下： 上图中，一个运行在处理器 core1上的线程想要更新变量 X 的值，同时另外一个运行在处理器 core2 上的线程想要更新变量 Y 的值。但是，这两个频繁改动的变量都处于同一条缓存行。两个线程就会轮番发送 RFO 消息，占得此缓存行的拥有权。当 core1 取得了拥有权开始更新 X，则 core2 对应的缓存行需要设为 I 状态。当 core2 取得了拥有权开始更新 Y，则 core1 对应的缓存行需要设为 I 状态(失效态)。轮番夺取拥有权不但带来大量的 RFO 消息，而且如果某个线程需要读此行数据时，L1 和 L2 缓存上都是失效数据，只有 L3 缓存上是同步好的数据。从前一篇我们知道，读 L3 的数据非常影响性能。更坏的情况是跨槽读取，L3 都要 miss，只能从内存上加载。 表面上 X 和 Y 都是被独立线程操作的，而且两操作之间也没有任何关系。只不过它们共享了一个缓存行，但所有竞争冲突都是来源于共享。 因此，当两个以上CPU都要访问同一个缓存行大小的内存区域时，就会引起冲突，这种情况就叫“共享”。但是，这种情况里面又包含了“其实不是共享”的“伪共享”情况。比如，两个处理器各要访问一个word，这两个word却存在于同一个cache line大小的区域里，这时，从应用逻辑层面说，这两个处理器并没有共享内存，因为他们访问的是不同的内容（不同的word）。但是因为cache line的存在和限制，这两个CPU要访问这两个不同的word时，却一定要访问同一个cache line块，产生了事实上的“共享”。显然，由于cache line大小限制带来的这种“伪共享”是我们不想要的，会浪费系统资源。 四、如何避免伪共享？ 1）让不同线程操作的对象处于不同的缓存行。 可以进行缓存行填充（Padding） 。例如，如果一条缓存行有 64 字节，而 Java 程序的对象头固定占 8 字节(32位系统)或 12 字节( 64 位系统默认开启压缩, 不开压缩为 16 字节)，所以我们只需要填 6 个无用的长整型补上6*8=48字节，让不同的 VolatileLong 对象处于不同的缓存行，就避免了伪共享( 64 位系统超过缓存行的 64 字节也无所谓，只要保证不同线程不操作同一缓存行就可以)。 2）使用编译指示，强制使每一个变量对齐。 强制使对象按照缓存行的边界对齐。例如可以使数据按64位对齐，那么一个缓存行只有一个可操作对象，这样发生伪共享之后，也只是对应缓存行的数据变化，并不影响其他的对象。 无锁队列 import java.util.concurrent.atomic.AtomicInteger; import java.util.concurrent.atomic.AtomicReferenceArray; /** * 用数组实现无锁有界队列 */ public class LockFreeQueue { private AtomicReferenceArray atomicReferenceArray; //代表为空，没有元素 private static final Integer EMPTY = null; //头指针,尾指针 AtomicInteger head,tail; public LockFreeQueue(int size){ atomicReferenceArray = new AtomicReferenceArray(new Integer[size + 1]); head = new AtomicInteger(0); tail = new AtomicInteger(0); } /** * 入队 * @param element * @return */ public boolean add(Integer element){ int index = (tail.get() + 1) % atomicReferenceArray.length(); if( index == head.get() % atomicReferenceArray.length()){ System.out.println(&quot;当前队列已满,&quot;+ element+&quot;无法入队!&quot;); return false; } while(!atomicReferenceArray.compareAndSet(index,EMPTY,element)){ return add(element); } tail.incrementAndGet(); //移动尾指针 System.out.println(&quot;入队成功!&quot; + element); return true; } /** * 出队 * @return */ public Integer poll(){ if(head.get() == tail.get()){ System.out.println(&quot;当前队列为空&quot;); return null; } int index = (head.get() + 1) % atomicReferenceArray.length(); Integer ele = (Integer) atomicReferenceArray.get(index); if(ele == null){ //有可能其它线程也在出队 return poll(); } while(!atomicReferenceArray.compareAndSet(index,ele,EMPTY)){ return poll(); } head.incrementAndGet(); System.out.println(&quot;出队成功!&quot; + ele); return ele; } public void print(){ StringBuffer buffer = new StringBuffer(&quot;[&quot;); for(int i = 0; i &lt; atomicReferenceArray.length() ; i++){ if(i == head.get() || atomicReferenceArray.get(i) == null){ continue; } buffer.append(atomicReferenceArray.get(i) + &quot;,&quot;); } buffer.deleteCharAt(buffer.length() - 1); buffer.append(&quot;]&quot;); System.out.println(&quot;队列内容:&quot; +buffer.toString()); } } 死锁 当线程 A 持有独占锁a，并尝试去获取独占锁 b 的同时，线程 B 持有独占锁 b，并尝试获取独占锁 a 的情况下，就会发生 AB 两个线程由于互相持有对方需要的锁，而发生的阻塞现象，我们称为死锁。 1. 产生死锁的条件和防止死锁 互斥 请求与保持 不可剥夺 循环等待 产生死锁的必要条件： 互斥条件：所谓互斥就是进程在某一时间内独占资源。 请求与保持条件：一个进程因请求资源而阻塞时，对已获得的资源保持不放。 不可剥夺条件：进程已获得资源，在末使用完之前，不能强行剥夺。 循环等待条件：若干进程之间形成一种头尾相接的循环等待资源关系。 这四个条件是死锁的必要条件，只要系统发生死锁，这些条件必然成立，而只要上述条件之 一不满足，就不会发生死锁。 理解了死锁的原因，尤其是产生死锁的四个必要条件，就可以最大可能地避免、预防和 解除死锁。 防止死锁可以采用以下的方法： 尽量使用 tryLock(long timeout, TimeUnit unit)的方法(ReentrantLock、ReentrantReadWriteLock)，设置超时时间，超时可以退出防止死锁。 尽量使用 Java. util. concurrent 并发类代替自己手写锁。 尽量降低锁的使用粒度，尽量不要几个功能用同一把锁。 尽量减少同步的代码块。 import lombok.SneakyThrows; import java.util.concurrent.TimeUnit; class HoldLockThread implements Runnable{ private String lockOne; private String lockTwo; public HoldLockThread(String lockOne, String lockTwo) { this.lockOne = lockOne; this.lockTwo = lockTwo; } @Override @SneakyThrows public void run(){ synchronized (lockOne){ System.out.println(Thread.currentThread().getName()+&quot;自己获取&quot;+lockOne+&quot;尝试获取&quot;+lockTwo); TimeUnit.SECONDS.sleep(2L); synchronized (lockTwo){ System.out.println(Thread.currentThread().getName()+&quot;自己获取&quot;+lockTwo+&quot;尝试获取&quot;+lockOne); } } } } public class DeadLockDemo { public static void main(String[] args) { String lockA=&quot;lockA&quot;; String lockB=&quot;lockB&quot;; new Thread(new HoldLockThread(lockA,lockB),&quot;ThreadAAA&quot;).start(); new Thread(new HoldLockThread(lockB,lockA),&quot;ThreadBBB&quot;).start(); } } 2. 死锁与活锁与饥饿的区别 死锁：是指两个或两个以上的进程（或线程）在执行过程中，因争夺资源而造成的一种互相等待的现象，若无外力作用，它们都将无法推进下去。 活锁：任务或者执行者没有被阻塞，由于某些条件没有满足，导致一直重复尝试，失败，尝试，失败。 活锁和死锁的区别在于，处于活锁的实体是在不断的改变状态，这就是所谓的“活”， 而处于死锁的实体表现为等待；活锁有可能自行解开，死锁则不能。 饥饿：一个或者多个线程因为种种原因无法获得所需要的资源，导致一直无法执行的状态。 Java 中导致饥饿的原因： 1、高优先级线程吞噬所有的低优先级线程的 CPU 时间。 2、线程被永久堵塞在一个等待进入同步块的状态，因为其他线程总是能在它之前持续地对该同步块进行访问。 3、线程在等待一个本身也处于永久等待完成的对象(比如调用这个对象的 wait 方法)，因为其他线程总是被持续地获得唤醒。 ReentrantLock(重入锁) ReentrantLock重入锁，是实现Lock接口的一个类，也是在实际编程中使用频率很高的一个锁，支持重入性，表示能够对共享资源能够重复加锁，即当前线程获取该锁再次获取不会被阻塞。 在java关键字synchronized隐式支持重入性，synchronized通过获取自增，释放自减的方式实现重入。与此同时，ReentrantLock还支持公平锁和非公平锁两种方式。那么，要想完完全全的弄懂ReentrantLock的话，主要也就是ReentrantLock同步语义的学习： 重入性的实现原理； 公平锁和非公平锁 1. 重入性的实现原理 要想支持重入性，就要解决两个问题： 1. 在线程获取锁的时候，如果已经获取锁的线程是当前线程的话则直接再次获取成功； 2. 由于锁会被获取n次，那么只有锁在被释放同样的n次之后，该锁才算是完全释放成功。 ReentrantLock支持两种锁：公平锁和非公平锁。何谓公平性，是针对获取锁而言的，如果一个锁是公平的，那么锁的获取顺序就应该符合请求上的绝对时间顺序，满足FIFO。 2. ReentrantLock使用 class Bank{ /** * volatile实现 */ private int count=0; /** * 使用可重入锁 */ private Lock lock=new ReentrantLock(); public void getCount(){ System.out.println(&quot;账户余额为：&quot;+count); } /** * 同步方法实现存钱 * @param money */ public void save(int money){ lock.lock(); try { count+=money; System.out.println(System.currentTimeMillis()+&quot;存进：&quot;+money); } catch (Exception e) { // TODO Auto-generated catch block e.printStackTrace(); }finally { lock.unlock();//释放锁 } } /** * 同步代码块实现取钱 * @param money */ public void remove(int money){ if (count-money&lt;0) { System.err.println(&quot;余额不足。&quot;); return; } lock.lock(); try { count-=money; System.err.println(System.currentTimeMillis()+&quot;取出：&quot;+money); } catch (Exception e) { // TODO Auto-generated catch block e.printStackTrace(); }finally { lock.unlock(); } } } 读写锁ReentrantReadWriteLock 首先明确一下，不是说 ReentrantLock 不好，只是 ReentrantLock 某些时候有局限。如果使用 ReentrantLock，可能本身是为了防止线程 A 在写数据、线程 B 在读数据造成的数据不一致，但这样，如果线程 C 在读数据、线程 D 也在读数据，读数据是不会改变数据的，没有必要加锁，但是还是加锁了，降低了程序的性能。因为这个，才诞生了读写锁 ReadWriteLock。 ReadWriteLock 是一个读写锁接口，读写锁是用来提升并发程序性能的锁分离技术，ReentrantReadWriteLock 是 ReadWriteLock 接口的一个具体实现，实现了读写的分离，读锁是共享的，写锁是独占的，读和读之间不会互斥，读和写、写和读、写和写之间才会互斥，提升了读写的性能。 而读写锁有以下三个重要的特性： （1）公平选择性：支持非公平（默认）和公平的锁获取方式，吞吐量还是非公平优于公平。 （2）重进入：读锁和写锁都支持线程重进入。 并发容器 1. ConcurrentHashMap ConcurrentHashMap是Java中的一个线程安全且高效的HashMap实现。平时涉及高并发如果要用map结构，那第一时间想到的就是它。相对于hashmap来说，ConcurrentHashMap就是线程安全的map，其中利用了锁分段的思想提高了并发度。 那么它到底是如何实现线程安全的？ JDK 1.6版本关键要素： segment继承了ReentrantLock充当锁的角色，为每一个segment提供了线程安全的保障； segment维护了哈希散列表的若干个桶，每个桶由HashEntry构成的链表。 JDK1.8后，ConcurrentHashMap抛弃了原有的Segment 分段锁，而采用了 CAS + synchronized 来保证并发安全性。 1.1 ConcurrentHashMap 的并发度 ConcurrentHashMap 把实际 map 划分成若干部分来实现它的可扩展性和线程安全。这种划分是使用并发度获得的，它是 ConcurrentHashMap 类构造函数的一个可选参数，默认值为 16，这样在多线程情况下就能避免争用。 在 JDK8 后，它摒弃了 Segment（锁段）的概念，而是启用了一种全新的方式实现,利用 CAS 算法。同时加入了更多的辅助变量来提高并发度， 1.2 并发容器的实现 何为同步容器：可以简单地理解为通过 synchronized 来实现同步的容器，如果有多个线程调用同步容器的方法，它们将会串行执行。比如 Vector，Hashtable，以及 Collections.synchronizedSet，synchronizedList 等方法返回的容器。可以通过查看 Vector，Hashtable 等这些同步容器的实现代码，可以看到这些容器实现线程安全的方式就是将它们的状态封装起来，并在需要同步的方法上加上关键字 synchronized。 并发容器使用了与同步容器完全不同的加锁策略来提供更高的并发性和伸缩性，例如在 ConcurrentHashMap 中采用了一种粒度更细的加锁机制，可以称为分段锁，在这种锁机制下，允许任意数量的读线程并发地访问 map，并且执行读操作的线程和写操作的线程也可以并发的访问 map，同时允许一定数量的写操作线程并发地修改 map，所以它可以在并发环境下实现更高的吞吐量。 1.3 同步集合与并发集合区别 同步集合与并发集合都为多线程和并发提供了合适的线程安全的集合，不过并发集合的可扩展性更高。在 Java1.5 之前程序员们只有同步集合来用且在多线程并发的时候会导致争用，阻碍了系统的扩展性。Java5 介绍了并发集合像ConcurrentHashMap，不仅提供线程安全还用锁分离和内部分区等现代技术提高了可扩展性。 1.4 SynchronizedMap 和 ConcurrentHashMap 比较 SynchronizedMap 一次锁住整张表来保证线程安全，所以每次只能有一个线程来访为 map。 ConcurrentHashMap 使用分段锁来保证在多线程下的性能。 ConcurrentHashMap 中则是一次锁住一个桶。ConcurrentHashMap 默认将hash 表分为 16 个桶，诸如 get，put，remove 等常用操作只锁当前需要用到的桶。 这样，原来只能一个线程进入，现在却能同时有 16 个写线程执行，并发性能的提升是显而易见的。 另外 ConcurrentHashMap 使用了一种不同的迭代方式。在这种迭代方式中，当iterator 被创建后集合再发生改变就不再是抛出ConcurrentModificationException，取而代之的是在改变时 new 新的数据从而不影响原有的数据，iterator 完成后再将头指针替换为新的数据 ，这样 iterator线程可以使用原来老的数据，而写线程也可以并发的完成改变。 2. CopyOnWriteArrayList CopyOnWriteArrayList 是一个并发容器。有很多人称它是线程安全的，我认为这句话不严谨，缺少一个前提条件，那就是非复合场景下操作它是线程安全的。 CopyOnWriteArrayList(免锁容器)的好处之一是当多个迭代器同时遍历和修改这个列表时，不会抛出 ConcurrentModificationException。在CopyOnWriteArrayList 中，写入将导致创建整个底层数组的副本，而源数组将保留在原地，使得复制的数组在被修改时，读取操作可以安全地执行。 CopyOnWriteArrayList 的使用场景 通过源码分析，我们看出它的优缺点比较明显，所以使用场景也就比较明显。就是合适读多写少的场景。 CopyOnWriteArrayList 的缺点 由于写操作的时候，需要拷贝数组，会消耗内存，如果原数组的内容比较多的情况下，可能导致 young gc 或者 full gc。 不能用于实时读的场景，像拷贝数组、新增元素都需要时间，所以调用一个 set 操作后，读取到数据可能还是旧的，虽然CopyOnWriteArrayList 能做到最终一致性,但是还是没法满足实时性要求。 由于实际使用中可能没法保证 CopyOnWriteArrayList 到底要放置多少数据，万一数据稍微有点多，每次 add/set 都要重新复制数组，这个代价实在太高昂了。在高性能的互联网应用中，这种操作分分钟引起故障。 CopyOnWriteArrayList 的设计思想 读写分离，读和写分开 最终一致性 使用另外开辟空间的思路，来解决并发冲突 3. ThreadLocal ThreadLocal 是一个本地线程副本变量工具类，在每个线程中都创建了一个 ThreadLocalMap 对象，简单说 ThreadLocal 就是一种以空间换时间的做法，每个线程可以访问自己内部 ThreadLocalMap 对象内的 value。通过这种方式，避免资源在多线程间共享。 原理：线程局部变量是局限于线程内部的变量，属于线程自身所有，不在多个线程间共享。Java提供ThreadLocal类来支持线程局部变量，是一种实现线程安全的方式。但是在管理环境下（如 web 服务器）使用线程局部变量的时候要特别小心，在这种情况下，工作线程的生命周期比任何应用变量的生命周期都要长。任何线程局部变量一旦在工作完成后没有释放，Java 应用就存在内存泄露的风险。 经典的使用场景是为每个线程分配一个 JDBC 连接 Connection。这样就可以保证每个线程的都在各自的 Connection 上进行数据库的操作，不会出现 A 线程关了 B线程正在使用的 Connection； 还有 Session 管理 等问题。 ThreadLocal 使用例子： public class TestThreadLocal { //线程本地存储变量 private static final ThreadLocal&lt;Integer&gt; THREAD_LOCAL_NUM = new ThreadLocal&lt;Integer&gt;() { @Override protected Integer initialValue() { return 0; } }; public static void main(String[] args) { for (int i = 0; i &lt;3; i++) {//启动三个线程 Thread t = new Thread() { @Override public void run() { add10ByThreadLocal(); } }; t.start(); } } /** * 线程本地存储变量加 5 */ private static void add10ByThreadLocal() { for (int i = 0; i &lt;5; i++) { Integer n = THREAD_LOCAL_NUM.get(); n += 1; THREAD_LOCAL_NUM.set(n); System.out.println(Thread.currentThread().getName() + &quot; : ThreadLocal num=&quot; + n); } } } 打印结果：启动了 3 个线程，每个线程最后都打印到 “ThreadLocal num=5”，而不是 num 一直在累加直到值等于 15 3.1什么是线程局部变量？ 线程局部变量是局限于线程内部的变量，属于线程自身所有，不在多个线程间共享。Java 提供 ThreadLocal 类来支持线程局部变量，是一种实现线程安全的方式。但是在管理环境下（如 web 服务器）使用线程局部变量的时候要特别小心，在这种情况下，工作线程的生命周期比任何应用变量的生命周期都要长。任何线程局部变量一旦在工作完成后没有释放，Java 应用就存在内存泄露的风险。 3.2 ThreadLocal内存泄漏分析与解决方案 ThreadLocal造成内存泄漏的原因？ ThreadLocalMap 中使用的 key 为 ThreadLocal 的弱引用,而 value 是强引用。所以，如果 ThreadLocal 没有被外部强引用的情况下，在垃圾回收的时候，key 会被清理掉，而 value 不会被清理掉。这样一来，ThreadLocalMap 中就会出现key为null的Entry。假如我们不做任何措施的话，value 永远无法被GC 回收，这个时候就可能会产生内存泄露。ThreadLocalMap实现中已经考虑了这种情况，在调用 set()、get()、remove() 方法的时候，会清理掉 key 为 null 的记录。使用完 ThreadLocal方法后 最好手动调用remove()方法 ThreadLocal内存泄漏解决方案？ 每次使用完ThreadLocal，都调用它的remove()方法，清除数据。 在使用线程池的情况下，没有及时清理ThreadLocal，不仅是内存泄漏的问题，更严重的是可能导致业务逻辑出现问题。所以，使用ThreadLocal就跟加锁完要解锁一样，用完就清理。 4. BlockingQueue 阻塞队列（BlockingQueue）是一个支持两个附加操作的队列。 这两个附加的操作是：在队列为空时，获取元素的线程会等待队列变为非空。当队列满时，存储元素的线程会等待队列可用。 阻塞队列常用于生产者和消费者的场景，生产者是往队列里添加元素的线程，消费者是从队列里拿元素的线程。阻塞队列就是生产者存放元素的容器，而消费者也只从容器里拿元素。 JDK7 提供了 7 个阻塞队列。分别是： ArrayBlockingQueue ：一个由数组结构组成的有界阻塞队列。 LinkedBlockingQueue ：一个由链表结构组成的有界阻塞队列。 PriorityBlockingQueue ：一个支持优先级排序的无界阻塞队列。 DelayQueue：一个使用优先级队列实现的无界阻塞队列。 SynchronousQueue：一个不存储元素的阻塞队列。 LinkedTransferQueue：一个由链表结构组成的无界阻塞队列。 LinkedBlockingDeque：一个由链表结构组成的双向阻塞队列。 Java 5 之前实现同步存取时，可以使用普通的一个集合，然后在使用线程的协作和线程同步可以实现生产者，消费者模式，主要的技术就是用好，wait,notify,notifyAll,sychronized 这些关键字。而在 java 5 之后，可以使用阻塞队列来实现，此方式大大简少了代码量，使得多线程编程更加容易，安全方面也有保障。 BlockingQueue 接口是 Queue 的子接口，它的主要用途并不是作为容器，而是作为线程同步的的工具，因此他具有一个很明显的特性，当生产者线程试图向 BlockingQueue 放入元素时，如果队列已满，则线程被阻塞，当消费者线程试图从中取出一个元素时，如果队列为空，则该线程会被阻塞，正是因为它所具有这个特性，所以在程序中多个线程交替向 BlockingQueue 中放入元素，取出元素，它可以很好的控制线程之间的通信。 阻塞队列使用最经典的场景就是 socket 客户端数据的读取和解析，读取数据的线程不断将数据放入队列，然后解析线程不断从队列取数据解析。 ","link":"https://tianxiawuhao.github.io/b7Jq-wpXZ/"},{"title":"多线程基础概念三","content":"Java内存模型和线程相关 Java中垃圾回收有什么目的？什么时候进行垃圾回收？ 垃圾回收的目的是识别并且丢弃应用不再使用的对象来释放和重用资源。 如果对象的引用被置为null，垃圾收集器是否会立即释放对象占用的内存？ 不会，在本次垃圾回收周期中，这个对象将被标记为可回收的。也就是说并不会立即被垃圾收集器立刻回收，而是在下一次垃圾回收时对象依旧不可达才会释放其占用的内存。 finalize()方法什么时候被调用？析构函数(finalization)的目的是什么？ 1）垃圾回收器（garbage colector）决定回收某对象时，就会运行该对象的finalize()方法； finalize是Object类的一个方法，该方法在Object类中的声明protected void finalize() throws Throwable { } 在垃圾回收器执行时会调用被回收对象的finalize()方法，可以覆盖此方法来实现对其资源的回收。注意：一旦垃圾回收器准备释放对象占用的内存，将首先调用该对象的finalize()方法，并且下一次垃圾回收动作发生时，才真正回收对象占用的内存空间 2）GC本来就是内存回收了，应用还需要在finalization做什么呢？ 答案是大部分时候，什么都不用做(也就是不需要重载)。只有在某些很特殊的情况下，比如你调用了一些native的方法(一般是C写的)，可以要在finaliztion里去调用C的释放函数。 重排序与数据依赖性 为什么代码会重排序？ 在执行程序时，为了提供性能，处理器和编译器常常会对指令进行重排序，但是不能随意重排序，不是你想怎么排序就怎么排序，它需要满足以下两个条件： 在单线程环境下不能改变程序运行的结果； 存在数据依赖关系的不允许重排序 需要注意的是：重排序不会影响单线程环境的执行结果，但是会破坏多线程的执行语义。 as-if-serial规则和happens-before规则的区别 as-if-serial语义保证单线程内程序的执行结果不被改变，happens-before关系保证正确同步的多线程程序的执行结果不被改变。 as-if-serial语义给编写单线程程序的程序员创造了一个幻境：单线程程序是按程序的顺序来执行的。happens-before关系给编写正确同步的多线程程序的程序员创造了一个幻境：正确同步的多线程程序是按happens-before指定的顺序来执行的。 as-if-serial语义和happens-before这么做的目的，都是为了在不改变程序执行结果的前提下，尽可能地提高程序执行的并行度。 synchronized 在 Java 中,synchronized 关键字是用来控制线程同步的,就是在多线程的环境下,控制synchronized 代码段不被多个线程同时执行。synchronized 可以修饰类、方法、变量。 另外，在 Java 早期版本中，synchronized属于重量级锁，效率低下，因为监视器锁（monitor）是依赖于底层的操作系统的 Mutex Lock 来实现的，Java 的线程是映射到操作系统的原生线程之上的。如果要挂起或者唤醒一个线程，都需要操作系统帮忙完成，而操作系统实现线程之间的切换时需要从用户态转换到内核态，这个状态之间的转换需要相对比较长的时间，时间成本相对较高，这也是为什么早期的 synchronized 效率低的原因。庆幸的是在 Java 6 之后 Java 官方对从 JVM 层面对synchronized 较大优化，所以现在的 synchronized 锁效率也优化得很不错了。JDK1.6对锁的实现引入了大量的优化，如自旋锁、适应性自旋锁、锁消除、锁粗化、偏向锁、轻量级锁等技术来减少锁操作的开销。 synchronized关键字最主要的三种使用方式： 修饰实例方法: 作用于当前对象实例加锁，进入同步代码前要获得当前对象实例的锁 修饰静态方法: 也就是给当前类加锁，会作用于类的所有对象实例，因为静态成员不属于任何一个实例对象，是类成员（ static 表明这是该类的一个静态资源，不管new了多少个对象，只有一份）。所以如果一个线程A调用一个实例对象的非静态 synchronized 方法，而线程B需要调用这个实例对象所属类的静态 synchronized 方法，是允许的，不会发生互斥现象，因为访问静态 synchronized 方法占用的锁是当前类的锁，而访问非静态 synchronized 方法占用的锁是当前实例对象锁。 修饰代码块: 指定加锁对象，对给定对象加锁，进入同步代码库前要获得给定对象的锁。 总结： synchronized 关键字加到 static 静态方法和 synchronized(class)代码块上都是是给 Class 类上锁。synchronized 关键字加到实例方法上是给对象实例上锁。尽量不要使用 synchronized(String a) 因为JVM中，字符串常量池具有缓存功能！ 下面我以一个常见的面试题为例讲解一下 synchronized 关键字的具体使用。 面试中面试官经常会说：“单例模式了解吗？来给我手写一下！给我解释一下双重检验锁方式实现单例模式的原理呗！” 双重校验锁实现对象单例（线程安全） public class Singleton { private volatile static Singleton uniqueInstance; private Singleton() { } public static Singleton getUniqueInstance() { //先判断对象是否已经实例过，没有实例化过才进入加锁代码 if (uniqueInstance == null) { //类对象加锁 synchronized (Singleton.class) { if (uniqueInstance == null) { uniqueInstance = new Singleton(); } } } return uniqueInstance; } } 另外，需要注意 uniqueInstance 采用 volatile 关键字修饰也是很有必要。 uniqueInstance 采用 volatile 关键字修饰也是很有必要的， uniqueInstance = new Singleton(); 这段代码其实是分为三步执行： 为 uniqueInstance 分配内存空间 初始化 uniqueInstance 将 uniqueInstance 指向分配的内存地址 使用 volatile 可以禁止 JVM 的指令重排，保证在多线程环境下也能正常运行。 说一下 synchronized 底层实现原理？ synchronized是Java中的一个关键字，在使用的过程中并没有看到显示的加锁和解锁过程。因此有必要通过javap命令，查看相应的字节码文件。 synchronized 同步语句块的情况 public class SynchronizedDemo { public void method() { synchronized (this) { System.out.println(&quot;synchronized 代码块&quot;); } } } 通过JDK 反汇编指令 javap -c -v SynchronizedDemo 可以看出在执行同步代码块之前之后都有一个monitor字样，其中前面的是monitorenter，后面的是离开monitorexit，不难想象一个线程也执行同步代码块，首先要获取锁，而获取锁的过程就是monitorenter ，在执行完代码块之后，要释放锁，释放锁就是执行monitorexit指令。 为什么会有两个monitorexit呢？ 这个主要是防止在同步代码块中线程因异常退出，而锁没有得到释放，这必然会造成死锁（等待的线程永远获取不到锁）。因此最后一个monitorexit是保证在异常情况下，锁也可以得到释放，避免死锁。 仅有ACC_SYNCHRONIZED这么一个标志，该标记表明线程进入该方法时，需要monitorenter，退出该方法时需要monitorexit。 synchronized可重入的原理 重入锁是指一个线程获取到该锁之后，该线程可以继续获得该锁。底层原理维护一个计数器，当线程获取该锁时，计数器加一，再次获得该锁时继续加一，释放锁时，计数器减一，当计数器值为0时，表明该锁未被任何线程所持有，其它线程可以竞争获取锁。 什么是自旋 很多 synchronized 里面的代码只是一些很简单的代码，执行时间非常快，此时等待的线程都加锁可能是一种不太值得的操作，因为线程阻塞涉及到用户态和内核态切换的问题。既然 synchronized 里面的代码执行得非常快，不妨让等待锁的线程不要被阻塞，而是在 synchronized 的边界做忙循环，这就是自旋。如果做了多次循环发现还没有获得锁，再阻塞，这样可能是一种更好的策略。 锁状态 锁的状态总共有四种：无锁状态、偏向锁、轻量级锁和重量级锁。随着锁的竞争，锁可以从偏向锁升级到轻量级锁，再升级的重量级锁（但是锁的升级是单向的，也就是说只能从低到高升级，不会出现锁的降级）。JDK 1.6中默认是开启偏向锁和轻量级锁的，我们也可以通过-XX:-UseBiasedLocking来禁用偏向锁。锁的状态保存在对象的头文件中，以32位的JDK为例： “轻量级”是相对于使用操作系统互斥量来实现的传统锁而言的。但是，首先需要强调一点的是，轻量级锁并不是用来代替重量级锁的，它的本意是在没有多线程竞争的前提下，减少传统的重量级锁使用产生的性能消耗。在解释轻量级锁的执行过程之前，先明白一点，轻量级锁所适应的场景是线程交替执行同步块的情况，如果存在同一时间访问同一锁的情况，就会导致轻量级锁膨胀为重量级锁。 多线程中 synchronized 锁升级的原理是什么？ synchronized 锁升级原理：在锁对象的对象头里面有一个 threadid 字段，在第一次访问的时候 threadid 为空，jvm 让其持有偏向锁，并将 threadid 设置为其线程 id，再次进入的时候会先判断 threadid 是否与其线程 id 一致，如果一致则可以直接使用此对象，如果不一致，则升级偏向锁为轻量级锁，通过自旋循环一定次数来获取锁，执行一定次数之后，如果还没有正常获取到要使用的对象，此时就会把锁从轻量级升级为重量级锁，此过程就构成了 synchronized 锁的升级。 锁的升级的目的：锁升级是为了减低了锁带来的性能消耗。在 Java 6 之后优化 synchronized 的实现方式，使用了偏向锁升级为轻量级锁再升级到重量级锁的方式，从而减低了锁带来的性能消耗。 重量级锁、轻量级锁和偏向锁之间转换图 线程 B 怎么知道线程 A 修改了变量 （1）volatile 修饰变量 （2）synchronized 修饰修改变量的方法 （3）wait/notify （4）while 轮询 当一个线程进入一个对象的 synchronized 方法 A 之后，其它线程是否可进入此对象的 synchronized 方法 B？ 不能。其它线程只能访问该对象的非同步方法，同步方法则不能进入。因为非静态方法上的 synchronized 修饰符要求执行方法时要获得对象的锁，如果已经进入A 方法说明对象锁已经被取走，那么试图进入 B 方法的线程就只能在等锁池（注意不是等待池哦）中等待对象的锁。 synchronized、volatile、CAS 比较 （1）synchronized 是悲观锁，属于抢占式，会引起其他线程阻塞。 （2）volatile 提供多线程共享变量可见性和禁止指令重排序优化。 （3）CAS 是基于冲突检测的乐观锁（非阻塞） synchronized 和 Lock 有什么区别？ 首先synchronized是Java内置关键字，在JVM层面，Lock是个Java类； synchronized 可以给类、方法、代码块加锁；而 lock 只能给代码块加锁。 synchronized 不需要手动获取锁和释放锁，使用简单，发生异常会自动释放锁，不会造成死锁；而 lock 需要自己加锁和释放锁，如果使用不当没有 unLock()去释放锁就会造成死锁。 通过 Lock 可以知道有没有成功获取锁，而 synchronized 却无法办到。 synchronized 和 ReentrantLock 区别是什么？ synchronized 是和 if、else、for、while 一样的关键字，ReentrantLock 是类，这是二者的本质区别。既然 ReentrantLock 是类，那么它就提供了比synchronized 更多更灵活的特性，可以被继承、可以有方法、可以有各种各样的类变量 synchronized 早期的实现比较低效，对比 ReentrantLock，大多数场景性能都相差较大，但是在 Java 6 中对 synchronized 进行了非常多的改进。 相同点：两者都是可重入锁 两者都是可重入锁。“可重入锁”概念是：自己可以再次获取自己的内部锁。比如一个线程获得了某个对象的锁，此时这个对象锁还没有释放，当其再次想要获取这个对象的锁的时候还是可以获取的，如果不可锁重入的话，就会造成死锁。同一个线程每次获取锁，锁的计数器都自增1，所以要等到锁的计数器下降为0时才能释放锁。 主要区别如下： ReentrantLock 使用起来比较灵活，但是必须有释放锁的配合动作； ReentrantLock 必须手动获取与释放锁，而 synchronized 不需要手动释放和开启锁； ReentrantLock 只适用于代码块锁，而 synchronized 可以修饰类、方法、变量等。 二者的锁机制其实也是不一样的。ReentrantLock 底层调用的是 Unsafe 的park 方法加锁，synchronized 操作的应该是对象头中 mark word Java中每一个对象都可以作为锁，这是synchronized实现同步的基础： 普通同步方法，锁是当前实例对象 静态同步方法，锁是当前类的class对象 同步方法块，锁是括号里面的对象 尽可能去使用synchronized而不要去使用LOCK 什么概念呢？我和大家打个比方：你叫jdk，你生了一个孩子叫synchronized，后来呢，你领养了一个孩子叫LOCK。起初，LOCK刚来到新家的时候，它很乖，很懂事，各个方面都表现的比synchronized好。你很开心，但是你内心深处又有一点淡淡的忧伤，你不希望你自己亲生的孩子竟然还不如一个领养的孩子乖巧。这个时候，你对亲生的孩子教育更加深刻了，你想证明，你的亲生孩子synchronized并不会比领养的孩子LOCK差。 那如何教育呢？ 在jdk1.6~jdk1.7的时候，你作为爸爸，你给他优化了，具体优化在哪里呢： 线程自旋和适应性自旋 我们知道，java’线程其实是映射在内核之上的，线程的挂起和恢复会极大的影响开销。并且jdk官方人员发现，很多线程在等待锁的时候，在很短的一段时间就获得了锁，所以它们在线程等待的时候，并不需要把线程挂起，而是让他无目的的循环，一般设置10次。这样就避免了线程切换的开销，极大的提升了性能。 而适应性自旋，是赋予了自旋一种学习能力，它并不固定自旋10次一下。他可以根据它前面线程的自旋情况，从而调整它的自旋，甚至是不经过自旋而直接挂起。 锁消除 什么叫锁消除呢？就是把不必要的同步在编译阶段进行移除。 那么有的小伙伴又迷糊了，我自己写的代码我会不知道这里要不要加锁？我加了锁就是表示这边会有同步呀？ 并不是这样，这里所说的锁消除并不一定指代是你写的代码的锁消除，我打一个比方： 在jdk1.5以前，我们的String字符串拼接操作其实底层是StringBuffer来实现的（写一个简单的demo，然后查看class文件中的字节码指令就清楚了），而在jdk1.5之后，那么是用StringBuilder来拼接的。我们考虑前面的情况，比如如下代码： String str1=&quot;qwe&quot;; String str2=&quot;asd&quot;; String str3=str1+str2; 底层实现会变成这样： StringBuffer sb = new StringBuffer(); sb.append(&quot;qwe&quot;); sb.append(&quot;asd&quot;); 我们知道，StringBuffer是一个线程安全的类，也就是说两个append方法都会同步，通过指针逃逸分析（就是变量不会外泄），我们发现在这段代码并不存在线程安全问题，这个时候就会把这个同步锁消除。 锁粗化 在用synchronized的时候，我们都讲究为了避免大开销，尽量同步代码块要小。那么为什么还要加粗呢？ 我们继续以上面的字符串拼接为例，我们知道在这一段代码中，每一个append都需要同步一次，那么我可以把锁粗化到第一个append和最后一个append（这里不要去纠结前面的锁消除，我只是打个比方） 轻量级锁 轻量级锁的加锁过程 （1）在代码进入同步块的时候，如果同步对象锁状态为无锁状态（锁标志位为“01”状态，是否为偏向锁为“0”），虚拟机首先将在当前线程的栈帧中建立一个名为锁记录（Lock Record）的空间，用于存储锁对象目前的Mark Word的拷贝，官方称之为 Displaced Mark Word。这时候线程堆栈与对象头的状态如图2.1所示。 （2）拷贝对象头中的Mark Word复制到锁记录中。 （3）拷贝成功后，虚拟机将使用CAS操作尝试将对象的Mark Word更新为指向Lock Record的指针，并将Lock record里的owner指针指向object mark word。如果更新成功，则执行步骤（3），否则执行步骤（4）。 （4）如果这个更新动作成功了，那么这个线程就拥有了该对象的锁，并且对象Mark Word的锁标志位设置为“00”，即表示此对象处于轻量级锁定状态，这时候线程堆栈与对象头的状态如图2.2所示。 （5）如果这个更新操作失败了，虚拟机首先会检查对象的Mark Word是否指向当前线程的栈帧，如果是就说明当前线程已经拥有了这个对象的锁，那就可以直接进入同步块继续执行。否则说明多个线程竞争锁，轻量级锁就要膨胀为重量级锁，锁标志的状态值变为“10”，Mark Word中存储的就是指向重量级锁（互斥量）的指针，后面等待锁的线程也要进入阻塞状态。 而当前线程便尝试使用自旋来获取锁，自旋就是为了不让线程阻塞，而采用循环去获取锁的过程。 ​ 图2.1 轻量级锁CAS操作之前堆栈与对象的状态 ​ 图2.2 轻量级锁CAS操作之后堆栈与对象的状态 轻量级锁的解锁过程： （1）通过CAS操作尝试把线程中复制的Displaced Mark Word对象替换当前的Mark Word。 （2）如果替换成功，整个同步过程就完成了。 （3）如果替换失败，说明有其他线程尝试过获取该锁（此时锁已膨胀），那就要在释放锁的同时，唤醒被挂起的线程。 偏向锁 引入偏向锁是为了在无多线程竞争的情况下尽量减少不必要的轻量级锁执行路径，因为轻量级锁的获取及释放依赖多次CAS原子指令，而偏向锁只需要在置换ThreadID的时候依赖一次CAS原子指令（由于一旦出现多线程竞争的情况就必须撤销偏向锁，所以偏向锁的撤销操作的性能损耗必须小于节省下来的CAS原子指令的性能消耗）。上面说过，轻量级锁是为了在线程交替执行同步块时提高性能，而偏向锁则是在只有一个线程执行同步块时进一步提高性能。 1、偏向锁获取过程： （1）访问Mark Word中偏向锁的标识是否设置成1，锁标志位是否为01——确认为可偏向状态。 （2）如果为可偏向状态，则测试线程ID是否指向当前线程，如果是，进入步骤（5），否则进入步骤（3）。 （3）如果线程ID并未指向当前线程，则通过CAS操作竞争锁。如果竞争成功，则将Mark Word中线程ID设置为当前线程ID，然后执行（5）；如果竞争失败，执行（4）。 （4）如果CAS获取偏向锁失败，则表示有竞争。当到达全局安全点（safepoint）时获得偏向锁的线程被挂起，偏向锁升级为轻量级锁，然后被阻塞在安全点的线程继续往下执行同步代码。 （5）执行同步代码。 2、偏向锁的释放： 偏向锁的撤销在上述第四步骤中有提到**。**偏向锁只有遇到其他线程尝试竞争偏向锁时，持有偏向锁的线程才会释放锁，线程不会主动去释放偏向锁。偏向锁的撤销，需要等待全局安全点（在这个时间点上没有字节码正在执行），它会首先暂停拥有偏向锁的线程，判断锁对象是否处于被锁定状态，撤销偏向锁后恢复到未锁定（标志位为“01”）或轻量级锁（标志位为“00”）的状态。 volatile 对于可见性，Java 提供了 volatile 关键字来保证可见性和禁止指令重排。 volatile 提供 happens-before 的保证，确保一个线程的修改能对其他线程是可见的。当一个共享变量被 volatile 修饰时，它会保证修改的值会立即被更新到主存，当有其他线程需要读取时，它会去内存中读取新值。 从实践角度而言，volatile 的一个重要作用就是和 CAS 结合，保证了原子性，详细的可以参见 java.util.concurrent.atomic 包下的类，比如 AtomicInteger。 volatile 常用于多线程环境下的单次操作(单次读或者单次写)。 Java 中能创建 volatile 数组吗？ 能，Java 中可以创建 volatile 类型数组，不过只是一个指向数组的引用，而不是整个数组。意思是，如果改变引用指向的数组，将会受到 volatile 的保护，但是如果多个线程同时改变数组的元素，volatile 标示符就不能起到之前的保护作用了。 volatile 变量和 atomic 变量有什么不同？ volatile 变量可以确保先行关系，即写操作会发生在后续的读操作之前, 但它并不能保证原子性。例如用 volatile 修饰 count 变量，那么 count++ 操作就不是原子性的。 而 AtomicInteger 类提供的 atomic 方法可以让这种操作具有原子性如getAndIncrement()方法会原子性的进行增量操作把当前值加一，其它数据类型和引用变量也可以进行相似操作。 volatile 能使得一个非原子操作变成原子操作吗？ 关键字volatile的主要作用是使变量在多个线程间可见，但无法保证原子性，对于多个线程访问同一个实例变量需要加锁进行同步。 虽然volatile只能保证可见性不能保证原子性，但用volatile修饰long和double可以保证其操作原子性。 所以从Oracle Java Spec里面可以看到： 对于64位的long和double，如果没有被volatile修饰，那么对其操作可以不是原子的。在操作的时候，可以分成两步，每次对32位操作。 如果使用volatile修饰long和double，那么其读写都是原子操作 对于64位的引用地址的读写，都是原子操作 在实现JVM时，可以自由选择是否把读写long和double作为原子操作 推荐JVM实现为原子操作 volatile 修饰符的有过什么实践？ 单例模式 是否 Lazy 初始化：是 是否多线程安全：是 实现难度：较复杂 描述：对于Double-Check这种可能出现的问题（当然这种概率已经非常小了，但毕竟还是有的嘛~），解决方案是：只需要给instance的声明加上volatile关键字即可volatile关键字的一个作用是禁止指令重排，把instance声明为volatile之后，对它的写操作就会有一个内存屏障（什么是内存屏障？），这样，在它的赋值完成之前，就不用会调用读操作。注意：volatile阻止的不是singleton = newSingleton()这句话内部[1-2-3]的指令重排，而是保证了在一个写操作（[1-2-3]）完成之前，不会调用读操作（if (instance == null)）。 public class Singleton7 { private static volatile Singleton7 instance = null; private Singleton7() {} public static Singleton7 getInstance() { if (instance == null) { synchronized (Singleton7.class) { if (instance == null) { instance = new Singleton7(); } } } return instance; } } synchronized 和 volatile 的区别是什么？ synchronized 表示只有一个线程可以获取作用对象的锁，执行代码，阻塞其他线程。 volatile 表示变量在 CPU 的寄存器中是不确定的，必须从主存中读取。保证多线程环境下变量的可见性；禁止指令重排序。 区别 volatile 是变量修饰符；synchronized 可以修饰类、方法、变量。 volatile 仅能实现变量的修改可见性，不能保证原子性；而 synchronized 则可以保证变量的修改可见性和原子性。 volatile 不会造成线程的阻塞；synchronized 可能会造成线程的阻塞。 volatile标记的变量不会被编译器优化；synchronized标记的变量可以被编译器优化。 volatile关键字是线程同步的轻量级实现，所以volatile性能肯定比synchronized关键字要好。但是volatile关键字只能用于变量而synchronized关键字可以修饰方法以及代码块。synchronized关键字在JavaSE1.6之后进行了主要包括为了减少获得锁和释放锁带来的性能消耗而引入的偏向锁和轻量级锁以及其它各种优化之后执行效率有了显著提升，实际开发中使用 synchronized 关键字的场景还是更多一些。 类别 synchronized Lock 存在层次 Java的关键字，在jvm层面上 是一个类 锁的释放 1、已获取锁的线程执行完同步代码，释放锁 2、线程执行发生异常，jvm会让线程释放锁 在finally中必须释放锁，不然容易造成线程死锁 锁的获取 假设A线程获得锁，B线程等待。如果A线程阻塞，B线程会一直等待 分情况而定，Lock有多个锁获取的方式，具体下面会说道，大致就是可以尝试获得锁，线程可以不用一直等待 锁状态 无法判断 可以判断 锁类型 可重入 不可中断 非公平 可重入 可判断 可公平（两者皆可） 性能 少量同步 大量同步 final 不可变对象(Immutable Objects)即对象一旦被创建它的状态（对象的数据，也即对象属性值）就不能改变，反之即为可变对象(Mutable Objects)。 不可变对象的类即为不可变类(Immutable Class)。Java 平台类库中包含许多不可变类，如 String、基本类型的包装类、BigInteger 和 BigDecimal 等。 只有满足如下状态，一个对象才是不可变的； 它的状态不能在创建后再被修改； 所有域都是 final 类型；并且，它被正确创建（创建期间没有发生 this 引用的逸出）。 不可变对象保证了对象的内存可见性，对不可变对象的读取不需要进行额外的同步手段，提升了代码执行效率。 ","link":"https://tianxiawuhao.github.io/c-lkdzfwM/"},{"title":"多线程基础概念二","content":"线程的状态和基本操作 线程的生命周期及五种基本状态 新建(new)：新创建了一个线程对象。 可运行(runnable)：线程对象创建后，当调用线程对象的 start()方法，该线程处于就绪状态，等待被线程调度选中，获取cpu的使用权。 运行(running)：可运行状态(runnable)的线程获得了cpu时间片（timeslice），执行程序代码。注：就绪状态是进入到运行状态的唯一入口，也就是说，线程要想进入运行状态执行，首先必须处于就绪状态中； 阻塞(block)：处于运行状态中的线程由于某种原因，暂时放弃对 CPU的使用权，停止执行，此时进入阻塞状态，直到其进入到就绪状态，才 有机会再次被 CPU 调用以进入到运行状态。 阻塞的情况分三种： (一). 等待阻塞：运行状态中的线程执行 wait()方法，JVM会把该线程放入等待队列(waitting queue)中，使本线程进入到等待阻塞状态； (二). 同步阻塞：线程在获取 synchronized 同步锁失败(因为锁被其它线程所占用)，，则JVM会把该线程放入锁池(lock pool)中，线程会进入同步阻塞状态； (三). 其他阻塞: 通过调用线程的 sleep()或 join()或发出了 I/O 请求时，线程会进入到阻塞状态。当 sleep()状态超时、join()等待线程终止或者超时、或者 I/O 处理完毕时，线程重新转入就绪状态。 死亡(dead)：线程run()、main()方法执行结束，或者因异常退出了run()方法，则该线程结束生命周期。死亡的线程不可再次复生。 Java 中用到的线程调度算法是什么？ 计算机通常只有一个 CPU，在任意时刻只能执行一条机器指令，每个线程只有获得CPU 的使用权才能执行指令。所谓多线程的并发运行，其实是指从宏观上看，各个线程轮流获得 CPU 的使用权，分别执行各自的任务。在运行池中，会有多个处于就绪状态的线程在等待 CPU，JAVA 虚拟机的一项任务就是负责线程的调度，线程调度是指按照特定机制为多个线程分配 CPU 的使用权。 有两种调度模型：分时调度模型和抢占式调度模型。 分时调度模型是指让所有的线程轮流获得 cpu 的使用权，并且平均分配每个线程占用的 CPU 的时间片这个也比较好理解。 Java虚拟机采用抢占式调度模型，是指优先让可运行池中优先级高的线程占用CPU，如果可运行池中的线程优先级相同，那么就随机选择一个线程，使其占用CPU。处于运行状态的线程会一直运行，直至它不得不放弃 CPU。 线程的调度策略 线程调度器选择优先级最高的线程运行，但是，如果发生以下情况，就会终止线程的运行： （1）线程体中调用了 yield 方法让出了对 cpu 的占用权利 （2）线程体中调用了 sleep 方法使线程进入睡眠状态 （3）线程由于 IO 操作受到阻塞 （4）另外一个更高优先级线程出现 （5）在支持时间片的系统中，该线程的时间片用完 什么是线程调度器(Thread Scheduler)和时间分片(Time Slicing )？ 线程调度器是一个操作系统服务，它负责为 Runnable 状态的线程分配 CPU 时间。一旦我们创建一个线程并启动它，它的执行便依赖于线程调度器的实现。 时间分片是指将可用的 CPU 时间分配给可用的 Runnable 线程的过程。分配 CPU 时间可以基于线程优先级或者线程等待的时间。 线程调度并不受到 Java 虚拟机控制，所以由应用程序来控制它是更好的选择（也就是说不要让你的程序依赖于线程的优先级）。 请说出与线程同步以及线程调度相关的方法。 （1） wait()：使一个线程处于等待（阻塞）状态，并且释放所持有的对象的锁； （2）sleep()：使一个正在运行的线程处于睡眠状态，是一个静态方法，调用此方法要处理 InterruptedException 异常； （3）notify()：唤醒一个处于等待状态的线程，当然在调用此方法的时候，并不能确切的唤醒某一个等待状态的线程，而是由 JVM 确定唤醒哪个线程，而且与优先级无关； （4）notityAll()：唤醒所有处于等待状态的线程，该方法并不是将对象的锁给所有线程，而是让它们竞争，只有获得锁的线程才能进入就绪状态； sleep() 和 wait() 有什么区别？ 两者都可以暂停线程的执行 类的不同：sleep() 是 Thread线程类的静态方法，wait() 是 Object类的方法。 是否释放锁：sleep() 不释放锁；wait() 释放锁。 用途不同：Wait 通常被用于线程间交互/通信，sleep 通常被用于暂停执行。 用法不同：wait() 方法被调用后，线程不会自动苏醒，需要别的线程调用同一个对象上的 notify() 或者 notifyAll() 方法。sleep() 方法执行完成后，线程会自动苏醒。或者可以使用wait(long timeout)超时后线程会自动苏醒。 你是如何调用 wait() 方法的？使用 if 块还是循环？为什么？ 处于等待状态的线程可能会收到错误警报和伪唤醒，如果不在循环中检查等待条件，程序就会在没有满足结束条件的情况下退出。 wait() 方法应该在循环调用，因为当线程获取到 CPU 开始执行的时候，其他条件可能还没有满足，所以在处理前，循环检测条件是否满足会更好。下面是一段标准的使用 wait 和 notify 方法的代码： synchronized (monitor) { // 判断条件谓词是否得到满足 while(!locked) { // 等待唤醒 monitor.wait(); } // 处理其他的业务逻辑 } 为什么线程通信的方法 wait(), notify()和 notifyAll()被定义在 Object 类里？ Java中，任何对象都可以作为锁，并且 wait()，notify()等方法用于等待对象的锁或者唤醒线程，在 Java 的线程中并没有可供任何对象使用的锁，所以任意对象调用方法一定要定义在Object类中。 wait(), notify()和 notifyAll()这些方法在同步代码块中调用 有的人会说，既然是线程放弃对象锁，那也可以把wait()定义在Thread类里面啊，新定义的线程继承于Thread类，也不需要重新定义wait()方法的实现。然而，这样做有一个非常大的问题，一个线程完全可以持有很多锁，你一个线程放弃锁的时候，到底要放弃哪个锁？当然了，这种设计并不是不能实现，只是管理起来更加复杂。 综上所述，wait()、notify()和notifyAll()方法要定义在Object类中。 为什么 wait(), notify()和 notifyAll()必须在同步方法或者同步块中被调用？ 当一个线程需要调用对象的 wait()方法的时候，这个线程必须拥有该对象的锁，接着它就会释放这个对象锁并进入等待状态直到其他线程调用这个对象上的 notify()方法。同样的，当一个线程需要调用对象的 notify()方法时，它会释放这个对象的锁，以便其他在等待的线程就可以得到这个对象锁。由于所有的这些方法都需要线程持有对象的锁，这样就只能通过同步来实现，所以他们只能在同步方法或者同步块中被调用。 Thread 类中的 yield 方法有什么作用？ 使当前线程从执行状态（运行状态）变为可执行态（就绪状态）。 当前线程到了就绪状态，那么接下来哪个线程会从就绪状态变成执行状态呢？可能是当前线程，也可能是其他线程，看系统的分配了。 为什么 Thread 类的 sleep()和 yield ()方法是静态的？ Thread 类的 sleep()和 yield()方法将在当前正在执行的线程上运行。所以在其他处于等待状态的线程上调用这些方法是没有意义的。这就是为什么这些方法是静态的。它们可以在当前正在执行的线程中工作，并避免程序员错误的认为可以在其他非运行线程调用这些方法。 线程的 sleep()方法和 yield()方法有什么区别？ （1） sleep()方法给其他线程运行机会时不考虑线程的优先级，因此会给低优先级的线程以运行的机会；yield()方法只会给相同优先级或更高优先级的线程以运行的机会； （2） 线程执行 sleep()方法后转入阻塞（blocked）状态，而执行 yield()方法后转入就绪（ready）状态； （3）sleep()方法声明抛出 InterruptedException，而 yield()方法没有声明任何异常； （4）sleep()方法比 yield()方法（跟操作系统 CPU 调度相关）具有更好的可移植性，通常不建议使用yield()方法来控制并发线程的执行。 如何停止一个正在运行的线程？ 在java中有以下3种方法可以终止正在运行的线程： 使用退出标志，使线程正常退出，也就是当run方法完成后线程终止。 使用stop方法强行终止，但是不推荐这个方法，因为stop和suspend及resume一样都是过期作废的方法。 使用interrupt方法中断线程。 Java 中 interrupted 和 isInterrupted 方法的区别？ interrupt：用于中断线程。调用该方法的线程的状态为将被置为”中断”状态。 注意：线程中断仅仅是置线程的中断状态位，不会停止线程。需要用户自己去监视线程的状态为并做处理。支持线程中断的方法（也就是线程中断后会抛出interruptedException 的方法）就是在监视线程的中断状态，一旦线程的中断状态被置为“中断状态”，就会抛出中断异常。 interrupted：是静态方法，查看当前中断信号是true还是false并且清除中断信号。如果一个线程被中断了，第一次调用 interrupted 则返回 true，第二次和后面的就返回 false 了。 isInterrupted：查看当前中断信号是true还是false 什么是阻塞式方法？ 阻塞式方法是指程序会一直等待该方法完成期间不做其他事情，ServerSocket 的accept()方法就是一直等待客户端连接。这里的阻塞是指调用结果返回之前，当前线程会被挂起，直到得到结果之后才会返回。此外，还有异步和非阻塞式方法在任务完成前就返回。 Java 中你怎样唤醒一个阻塞的线程？ 首先 ，wait()、notify() 方法是针对对象的，调用任意对象的 wait()方法都将导致线程阻塞，阻塞的同时也将释放该对象的锁，相应地，调用任意对象的 notify()方法则将随机解除该对象阻塞的线程，但它需要重新获取该对象的锁，直到获取成功才能往下执行； 其次，wait、notify 方法必须在 synchronized 块或方法中被调用，并且要保证同步块或方法的锁对象与调用 wait、notify 方法的对象是同一个，如此一来在调用 wait 之前当前线程就已经成功获取某对象的锁，执行 wait 阻塞后当前线程就将之前获取的对象锁释放。 notify() 和 notifyAll() 有什么区别？ 如果线程调用了对象的 wait()方法，那么线程便会处于该对象的等待池中，等待池中的线程不会去竞争该对象的锁。 notifyAll() 会唤醒所有的线程，notify() 只会唤醒一个线程。 notifyAll() 调用后，会将全部线程由等待池移到锁池，然后参与锁的竞争，竞争成功则继续执行，如果不成功则留在锁池等待锁被释放后再次参与竞争。而 notify()只会唤醒一个线程，具体唤醒哪一个线程由虚拟机控制。 如何在两个线程间共享数据？ 在两个线程间共享变量即可实现共享。 一般来说，共享变量要求变量本身是线程安全的，然后在线程内使用的时候，如果有对共享变量的复合操作，那么也得保证复合操作的线程安全性。 可以通过中断 和 共享变量的方式实现线程间的通讯和协作 比如说最经典的生产者-消费者模型：当队列满时，生产者需要等待队列有空间才能继续往里面放入商品，而在等待的期间内，生产者必须释放对临界资源（即队列）的占用权。因为生产者如果不释放对临界资源的占用权，那么消费者就无法消费队列中的商品，就不会让队列有空间，那么生产者就会一直无限等待下去。因此，一般情况下，当队列满时，会让生产者交出对临界资源的占用权，并进入挂起状态。然后等待消费者消费了商品，然后消费者通知生产者队列有空间了。同样地，当队列空时，消费者也必须等待，等待生产者通知它队列中有商品了。这种互相通信的过程就是线程间的协作。 Java中线程通信协作的最常见的两种方式： syncrhoized加锁的线程的Object类的wait()/notify()/notifyAll() ReentrantLock类加锁的线程的Condition类的await()/signal()/signalAll() 线程间直接的数据交换： 通过管道进行线程间通信：1）字节流；2）字符流 同步方法和同步块，哪个是更好的选择？ Lock lock = new ReentrantLock(); lock. lock(); try { System. out. println(&quot;获得锁&quot;); } catch (Exception e) { // TODO: handle exception } finally { System. out. println(&quot;释放锁&quot;); lock. unlock(); } 同步块更要符合开放调用的原则，只在需要锁住的代码块锁住相应的对象，这样从侧面来说也可以避免死锁。 请知道一条原则：同步的范围越小越好。 什么是线程同步和线程互斥，有哪几种实现方式？ 当一个线程对共享的数据进行操作时，应使之成为一个&quot;原子操作&quot;，即在没有完成相关操作之前，不允许其他线程打断它，否则，就会破坏数据的完整性，必然会得到错误的处理结果，这就是线程的同步。 在多线程应用中，考虑不同线程之间的数据同步和防止死锁。当两个或多个线程之间同时等待对方释放资源的时候就会形成线程之间的死锁。为了防止死锁的发生，需要通过同步来实现线程安全。 线程互斥是指对于共享的进程系统资源，在各单个线程访问时的排它性。当有若干个线程都要使用某一共享资源时，任何时刻最多只允许一个线程去使用，其它要使用该资源的线程必须等待，直到占用资源者释放该资源。线程互斥可以看成是一种特殊的线程同步。 线程间的同步方法大体可分为两类：用户模式和内核模式。顾名思义，内核模式就是指利用系统内核对象的单一性来进行同步，使用时需要切换内核态与用户态，而用户模式就是不需要切换到内核态，只在用户态完成操作。 用户模式下的方法有：原子操作（例如一个单一的全局变量），临界区。 内核模式下的方法有：事件，信号量，互斥量。 实现线程同步的方法 同步代码方法：sychronized 关键字修饰的方法 同步代码块：sychronized 关键字修饰的代码块 使用特殊变量域volatile实现线程同步：volatile关键字为域变量的访问提供了一种免锁机制 使用重入锁实现线程同步：reentrantlock类是可重入、互斥、实现了lock接口的锁，它与sychronized方法具有相同的基本行为和语义 在监视器(Monitor)内部，是如何做线程同步的？程序应该做哪种级别的同步？ 在 java 虚拟机中，每个对象( Object 和 class )通过某种逻辑关联监视器,每个监视器和一个对象引用相关联，为了实现监视器的互斥功能，每个对象都关联着一把锁。 一旦方法或者代码块被 synchronized 修饰，那么这个部分就放入了监视器的监视区域，确保一次只能有一个线程执行该部分的代码，线程在获取锁之前不允许执行该部分的代码 另外 java 还提供了显式监视器( Lock )和隐式监视器( synchronized )两种锁方案 如果你提交任务时，线程池队列已满，这时会发生什么 这里区分一下： （1）如果使用的是无界队列 LinkedBlockingQueue，也就是无界队列的话，没关系，继续添加任务到阻塞队列中等待执行，因为 LinkedBlockingQueue 可以近乎认为是一个无穷大的队列，可以无限存放任务 （2）如果使用的是有界队列比如 ArrayBlockingQueue，任务首先会被添加到ArrayBlockingQueue 中，ArrayBlockingQueue 满了，会根据maximumPoolSize 的值增加线程数量，如果增加了线程数量还是处理不过来，ArrayBlockingQueue 继续满，那么则会使用拒绝策略RejectedExecutionHandler 处理满了的任务，默认是 AbortPolicy 什么叫线程安全？servlet 是线程安全吗? 线程安全是编程中的术语，指某个方法在多线程环境中被调用时，能够正确地处理多个线程之间的共享变量，使程序功能正确完成。 Servlet 不是线程安全的，servlet 是单实例多线程的，当多个线程同时访问同一个方法，是不能保证共享变量的线程安全性的。 Struts2 的 action 是多实例多线程的，是线程安全的，每个请求过来都会 new 一个新的 action 分配给这个请求，请求完成后销毁。 SpringMVC 的 Controller 是线程安全的吗？不是的，和 Servlet 类似的处理流程。 Struts2 好处是不用考虑线程安全问题；Servlet 和 SpringMVC 需要考虑线程安全问题，但是性能可以提升不用处理太多的 gc，可以使用 ThreadLocal 来处理多线程的问题。 在 Java 程序中怎么保证多线程的运行安全？ 方法一：使用安全类，比如 java.util.concurrent 下的类，使用原子类AtomicInteger 方法二：使用自动锁 synchronized。 方法三：使用手动锁 Lock。 手动锁 Java 示例代码如下： Lock lock = new ReentrantLock(); lock. lock(); try { System. out. println(&quot;获得锁&quot;); } catch (Exception e) { // TODO: handle exception } finally { System. out. println(&quot;释放锁&quot;); lock. unlock(); } 你对线程优先级的理解是什么？ 每一个线程都是有优先级的，一般来说，高优先级的线程在运行时会具有优先权，但这依赖于线程调度的实现，这个实现是和操作系统相关的(OS dependent)。我们可以定义线程的优先级，但是这并不能保证高优先级的线程会在低优先级的线程前执行。线程优先级是一个 int 变量(从 1-10)，1 代表最低优先级，10 代表最高优先级。 Java 的线程优先级调度会委托给操作系统去处理，所以与具体的操作系统优先级有关，如非特别需要，一般无需设置线程优先级。 线程类的构造方法、静态块是被哪个线程调用的 这是一个非常刁钻和狡猾的问题。请记住：线程类的构造方法、静态块是被 new这个线程类所在的线程所调用的，而 run 方法里面的代码才是被线程自身所调用的。 如果说上面的说法让你感到困惑，那么我举个例子，假设 Thread2 中 new 了Thread1，main 函数中 new 了 Thread2，那么： （1）Thread2 的构造方法、静态块是 main 线程调用的，Thread2 的 run()方法是Thread2 自己调用的 （2）Thread1 的构造方法、静态块是 Thread2 调用的，Thread1 的 run()方法是Thread1 自己调用的 一个线程运行时发生异常会怎样？ 如果异常没有被捕获该线程将会停止执行。Thread.UncaughtExceptionHandler是用于处理未捕获异常造成线程突然中断情况的一个内嵌接口。当一个未捕获异常将造成线程中断的时候，JVM 会使用 Thread.getUncaughtExceptionHandler()来查询线程的 UncaughtExceptionHandler 并将线程和异常作为参数传递给 handler 的 uncaughtException()方法进行处理。 Java 线程数过多会造成什么异常？ 线程的生命周期开销非常高 消耗过多的 CPU 资源如果可运行的线程数量多于可用处理器的数量，那么有线程将会被闲置。大量空闲的线程会占用许多内存，给垃圾回收器带来压力，而且大量的线程在竞争 CPU资源时还将产生其他性能的开销。 降低JVM稳定性 在可创建线程的数量上存在一个限制，这个限制值将随着平台的不同而不同，并且承受着多个因素制约，包括 JVM 的启动参数、Thread 构造函数中请求栈的大小，以及底层操作系统对线程的限制等。如果破坏了这些限制，那么可能抛出OutOfMemoryError 异常。 ","link":"https://tianxiawuhao.github.io/h9DSVN_A0/"},{"title":" 多线程基础概念","content":"什么是线程 是操作系统能够进行运算调度的最小单位。它被包含在进程之中，是进程中的实际运作单位。 线程安全的问题 并发编程三要素是什么？在 Java 程序中怎么保证多线程的运行安全？ 并发编程三要素（线程的安全性问题体现在）： 原子性：原子，即一个不可再被分割的颗粒。原子性指的是一个或多个操作要么全部执行成功要么全部执行失败。(synchronized,lock,unlock) 可见性：一个线程对共享变量的修改,另一个线程能够立刻看到。（synchronized,volatile,final） 有序性：程序执行的顺序按照代码的先后顺序执行。（处理器可能会对指令进行重排序）（synchronized,volatile） 出现线程安全问题的原因： 线程切换带来的原子性问题 缓存导致的可见性问题 编译优化带来的有序性问题 解决办法： JDK Atomic开头的原子类、synchronized、LOCK，可以解决原子性问题 synchronized、volatile、LOCK，可以解决可见性问题 Happens-Before 规则可以解决有序性问题 并发与并行 并发：多个任务在同一个 CPU 核上，按细分的时间片轮流(交替)执行，从逻辑上来看那些任务是同时执行。 并行：单位时间内，多个处理器或多核处理器同时处理多个任务，是真正意义上的“同时进行”。 串行：有n个任务，由一个线程按顺序执行。由于任务、方法都在一个线程执行所以不存在线程不安全情况，也就不存在临界区的问题。 并发编程（多线程）的优点 早期的CPU是单核的，为了提升计算能力，将多个计算单元整合到一起。形成了多核CPU。多线程就是为了将多核CPU发挥到极致，一边提高性能。 方便进行业务拆分，提升系统并发能力和性能：在特殊的业务场景下，先天的就适合于并发编程。现在的系统动不动就要求百万级甚至千万级的并发量，而多线程并发编程正是开发高并发系统的基础，利用好多线程机制可以大大提高系统整体的并发能力以及性能。面对复杂业务模型，并行程序会比串行程序更适应业务需求，而并发编程更能吻合这种业务拆分 并发编程（多线程）的缺点 上面说了多线程的优点是：为了提高计算性能。那么一定会提高？答案是不一定的。有时候多线程不一定比单线程计算快 多线程会带来额外的开销和复杂的问题 上下文切换 死锁 内存泄漏 上下文切换 时间片是CPU分配给各个线程的时间，因为时间非常短，所以CPU不断通过切换线程，让我们觉得多个线程是同时执行的，时间片一般是几十毫秒。而每次切换时，需要保存当前的状态起来，以便能够进行恢复先前状态，而这个切换时非常损耗性能，过于频繁反而无法发挥出多线程编程的优势。 减少上下文切换可以采用无锁并发编程，CAS算法，使用最少的线程和使用协程。 无锁并发编程：可以参照concurrentHashMap锁分段的思想，不同的线程处理不同段的数据，这样在多线程竞争的条件下，可以减少上下文切换的时间。 CAS算法，利用Atomic下使用CAS算法来更新数据，使用了乐观锁，可以有效的减少一部分不必要的锁竞争带来的上下文切换 使用最少线程：避免创建不需要的线程，比如任务很少，但是创建了很多的线程，这样会造成大量的线程都处于等待状态 协程：在单线程里实现多任务的调度，并在单线程里维持多个任务间的切换 线程死锁 死锁是指两个或两个以上的进程（线程）在执行过程中，由于竞争资源或者由于彼此通信而造成的一种阻塞的现象，若无外力作用，它们都将无法推进下去。此时称系统处于死锁状态或系统产生了死锁，这些永远在互相等待的进程（线程）称为死锁进程（线程）。 多个线程同时被阻塞，它们中的一个或者全部都在等待某个资源被释放。由于线程被无限期地阻塞，因此程序不可能正常终止。 如下图所示，线程 A 持有资源 2，线程 B 持有资源 1，他们同时都想申请对方的资源，所以这两个线程就会互相等待而进入死锁状态。 下面通过一个例子来说明线程死锁： public class DeadLockDemo { private static Object resource1 = new Object();//资源 1 private static Object resource2 = new Object();//资源 2 public static void main(String[] args) { new Thread(() -&gt; { synchronized (resource1) { System.out.println(Thread.currentThread() + &quot;get resource1&quot;); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread() + &quot;waiting get resource2&quot;); synchronized (resource2) { System.out.println(Thread.currentThread() + &quot;get resource2&quot;); } } }, &quot;线程 1&quot;).start(); new Thread(() -&gt; { synchronized (resource2) { System.out.println(Thread.currentThread() + &quot;get resource2&quot;); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread() + &quot;waiting get resource1&quot;); synchronized (resource1) { System.out.println(Thread.currentThread() + &quot;get resource1&quot;); } } }, &quot;线程 2&quot;).start(); } } 输出结果 Thread[线程 1,5,main]get resource1 Thread[线程 2,5,main]get resource2 Thread[线程 1,5,main]waiting get resource2 Thread[线程 2,5,main]waiting get resource1 线程 A 通过 synchronized (resource1) 获得 resource1 的监视器锁，然后通过Thread.sleep(1000)；让线程 A 休眠 1s 为的是让线程 B 得到CPU执行权，然后获取到 resource2 的监视器锁。线程 A 和线程 B 休眠结束了都开始企图请求获取对方的资源，然后这两个线程就会陷入互相等待的状态，这也就产生了死锁。上面的例子符合产生死锁的四个必要条件。 形成死锁的四个必要条件是什么 互斥条件：线程(进程)对于所分配到的资源具有排它性，即一个资源只能被一个线程(进程)占用，直到被该线程(进程)释放 请求与保持条件：一个线程(进程)因请求被占用资源而发生阻塞时，对已获得的资源保持不放。 不剥夺条件：线程(进程)已获得的资源在末使用完之前不能被其他线程强行剥夺，只有自己使用完毕后才释放资源。 循环等待条件：当发生死锁时，所等待的线程(进程)必定会形成一个环路（类似于死循环），造成永久阻塞 如何避免线程死锁 我们只要破坏产生死锁的四个条件中的其中一个就可以了。 破坏互斥条件 这个条件我们没有办法破坏，因为我们用锁本来就是想让他们互斥的（临界资源需要互斥访问）。 破坏请求与保持条件 一次性申请所有的资源。 破坏不剥夺条件 占用部分资源的线程进一步申请其他资源时，如果申请不到，可以主动释放它占有的资源。 破坏循环等待条件 靠按序申请资源来预防。按某一顺序申请资源，释放资源则反序释放。破坏循环等待条件。 创建线程的四种方式 创建线程有四种方式： 继承 Thread 类； 实现 Runnable 接口； 实现 Callable 接口； 使用 Executors 工具类创建线程池 继承 Thread 类 步骤 定义Thread 类的子类，并重写该类的run() 方法，该run() 方法的方法体就代表类线程需要完成的任务。因此把run() 方法称为线程执行体。 创建 Thread 子类的实例，即创建线程对象 调用子类实例的star()方法来启动线程 /** * 继承 thread 的内部类，以买票例子 */ public class FirstThread extends Thread{ private int i; private int ticket = 10; @Override public void run() { for (;i&lt;20;i++) { //当继承thread 时，直接使用this 可以获取当前的线程,getName()获取当前线程的名字 if(this.ticket&gt;0){ Log.e(TAG, getName() + &quot;, 卖票：ticket=&quot; + ticket--); } } } } private void starTicketThread(){ Log.d(TAG,&quot;starTicketThread, &quot;+Thread.currentThread().getName()); FirstThread thread1 = new FirstThread(); FirstThread thread2 = new FirstThread(); FirstThread thread3 = new FirstThread(); thread1.start(); thread2.start(); thread3.start(); //开启3个线程进行买票，每个线程都卖了10张，总共就30张票 } 运行结果 注意 ：可以看到 3 个线程输入的 票数变量不连续，注意：ticket 是 FirstThread 的实例属性，而不是局部变量，但是因为程序每次创建线程对象都需要创建一个FirstThread 的对象，所有多个线程不共享该实例的属性。 //使用Lambda表达式，实现多线程 new Thread(() -&gt; { System.out.println(Thread.currentThread().getName() + &quot;新线程创建了！&quot;); } ).start(); 实现 Runnable 接口 步骤 定义Runnable接口实现类SecondThread，并重写run()方法 创建SecondThread实例secondThread，以secondThread作为target创建Thead对象，该Thread对象才是真正的线程对象 调用线程对象的start()方法 /** * 实现 runnable 接口，创建线程类 */ public class SecondThread implements Runnable{ private int i; private int ticket = 100; @Override public void run() { for (;i&lt;20;i++) { //如果线程类实现 runnable 接口 //获取当前的线程，只能用 Thread.currentThread() 获取当前的线程名 Log.d(TAG,Thread.currentThread().getName()+&quot; &quot;+i); if(this.ticket&gt;0){ Log.e(TAG, Thread.currentThread().getName() + &quot;, 卖票：ticket=&quot; + ticket--); } } } } private void starTicketThread2(){ Log.d(TAG,&quot;starTicketThread2, &quot;+Thread.currentThread().getName()); SecondThread secondThread = new SecondThread(); //通过new Thread(target,name)创建新的线程 new Thread(secondThread,&quot;买票人1&quot;).start(); new Thread(secondThread,&quot;买票人2&quot;).start(); new Thread(secondThread,&quot;买票人3&quot;).start(); //虽然是开启了3个线程，但是一共只买了100张票 } 执行结果 注意：可以看到 3 个线程输入的 票数变量是连续的，采用 Runnable 接口的方式创建多个线程可以共享线程类的实例的属性。这是因为在这种方式下，程序所创建的Runnable 对象只是线程的 target ,而多个线程可以共享同一个 target,所以多个线程可以共享同一个线程类（实际上应该是该线程的target 类）的实例属性。 //使用匿名内部类的方式，实现多线程 new Thread(new Runnable() { @Override public void run() { System.out.println(Thread.currentThread().getName() + &quot;新线程创建了！&quot;); } }).start(); 实现 Callable 接口 步骤 创建callable接口的实现类，并实现call() 方法，该call() 方法将作为线程的执行体，且该call() 方法是有返回值的。 创建 callable实现类的实例，使用 FutureTask 类来包装Callable对象，该FutureTask 对象封装 call() 方法的返回值。 使用FutureTask 对象作为Thread对象的target创建并启动新线程。 调用FutureTask对象的get()方法来获取子线程执行结束后的返回值。 /** * 使用callable 来实现线程类 */ public class ThirdThread implements Callable&lt;Integer&gt;{ private int ticket = 20; @Override public Integer call(){ for ( int i = 0;i&lt;10;i++) { //获取当前的线程，只能用 Thread.currentThread() 获取当前的线程名 // Log.d(TAG,Thread.currentThread().getName()+&quot; &quot;+i); if(this.ticket&gt;0){ Log.e(TAG, Thread.currentThread().getName() + &quot;, 卖票：ticket=&quot; + ticket--); } } return ticket; } } private void starCallableThread(){ ThirdThread thirdThread = new ThirdThread(); FutureTask&lt;Integer&gt; task = new FutureTask&lt;Integer&gt;(thirdThread); new Thread(task,&quot;有返回值的线程&quot;).start(); try { Integer integer = task.get(); Log.d(TAG,&quot;starCallableThread, 子线程的返回值=&quot;+integer); } catch (InterruptedException e) { e.printStackTrace(); } catch (ExecutionException e) { e.printStackTrace(); } } 执行结果 注意：注意：Callable的call() 方法允许声明抛出异常，并且允许带有返回值。 程序最后调用FutureTask 对象的get()方法来返回Call(）方法的返回值，导致主线程被阻塞，直到call()方法结束并返回为止。 使用 Executors 工具类创建线程池 Executors提供了一系列工厂方法用于创先线程池，返回的线程池都实现了ExecutorService接口。 主要有newFixedThreadPool，newCachedThreadPool，newSingleThreadExecutor，newScheduledThreadPool，后续详细介绍这四种线程池 public class MyRunnable implements Runnable { @Override public void run() { System.out.println(Thread.currentThread().getName() + &quot; run()方法执行中...&quot;); } } public class SingleThreadExecutorTest { public static void main(String[] args) { ExecutorService executorService = Executors.newSingleThreadExecutor(); MyRunnable runnableTest = new MyRunnable(); for (int i = 0; i &lt; 5; i++) { executorService.execute(runnableTest); } System.out.println(&quot;线程任务开始执行&quot;); executorService.shutdown(); } } 执行结果 说一下 runnable 和 callable 有什么区别？ 相同点 都是接口 都可以编写多线程程序 都采用Thread.start()启动线程 主要区别 Runnable 接口 run 方法无返回值；Callable 接口 call 方法有返回值，是个泛型，和Future、FutureTask配合可以用来获取异步执行的结果 Runnable 接口 run 方法只能抛出运行时异常，且无法捕获处理；Callable 接口 call 方法允许抛出异常，可以获取异常信息 注：Callalbe接口支持返回执行结果，需要调用FutureTask.get()得到，此方法会阻塞主进程的继续往下执行，如果不调用不会阻塞。 线程的 run()和 start()有什么区别？ 每个线程都是通过某个特定Thread对象所对应的方法run()来完成其操作的，run()方法称为线程体。通过调用Thread类的start()方法来启动一个线程。 start() 方法用于启动线程，run() 方法用于执行线程的运行时代码。run() 可以重复调用，而 start() 只能调用一次。 为什么我们调用 start() 方法时会执行 run() 方法，为什么我们不能直接调用 run() 方法？ new 一个 Thread，线程进入了新建状态。调用 start() 方法，会启动一个线程并使线程进入了就绪状态，当分配到时间片后就可以开始运行了。 start() 会执行线程的相应准备工作，然后自动执行 run() 方法的内容，这是真正的多线程工作。 而直接执行 run() 方法，会把 run 方法当成一个 main 线程下的普通方法去执行，并不会在某个线程中执行它，所以这并不是多线程工作。 总结： 调用 start 方法方可启动线程并使线程进入就绪状态，而 run 方法只是 thread 的一个普通方法调用，还是在主线程里执行。 什么是 Callable 和 Future? Callable 接口类似于 Runnable，从名字就可以看出来了，但是 Runnable 不会返回结果，并且无法抛出返回结果的异常，而 Callable 功能更强大一些，被线程执行后，可以返回值，这个返回值可以被 Future 拿到，也就是说，Future 可以拿到异步执行任务的返回值。 Future 接口表示异步任务，是一个可能还没有完成的异步任务的结果。所以说 Callable用于产生结果，Future 用于获取结果。 什么是 FutureTask FutureTask 表示一个异步运算的任务。FutureTask 里面可以传入一个 Callable 的具体实现类，可以对这个异步运算的任务的结果进行等待获取、判断是否已经完成、取消任务等操作。只有当运算完成的时候结果才能取回，如果运算尚未完成 get 方法将会阻塞。一个 FutureTask 对象可以对调用了 Callable 和 Runnable 的对象进行包装，由于 FutureTask 也是Runnable 接口的实现类，所以 FutureTask 也可以放入线程池中。 ","link":"https://tianxiawuhao.github.io/ddmZREDcW/"},{"title":"java基础之一文件操作","content":" 一、抽象类： 字节流： InputStream（读入流） OutputStream(写出流） 字符流： Reader（字符 读入流） Writer （字符写出流） 字节流读取字符数据的问题 : 汉字等字符,往往有多个字节组成,那么到底哪几个字节应该组成一个汉字呢? 字节流不知道.如果我们自己用字节流读取字节数据,然后手动自己转字符,就有可能对规则不了解.从而造成错误. Java中的字符流恰好就可以帮助我们解决这个问题. Java提供的字符流的底层，它自己会根据读取到的字节数据的特点，然后自动的帮助我们把字节转成字符，最后我们就会自然的得到想要的字符数据。 字符流怎么解决这个问题呢? ​ 字符流 = 字节流 + 编码表 编码表其实里面定义了字节与字符的转换规则. 字符流在读取数据的时候,底层还是字节流在读取数据,字符流会自动根据编码规则把字节数据转为字符数据,就不会造成错误! 编码表的介绍 : 说明 : 计算机中保存的数据都是二进制（1010100101001），我们要把生活中的数据保存到计算机中，需要把生活中的数据转成二进制数据，然后才能让计算机来识别生活中的数据，进而对这些数据进行处理。 &quot;补充 :&quot; 要把生活中的数据转成计算机能够识别的数据，就需要定义一个固定的转换关系： 老美他们为了把自己的生活中的数据保存到计算机中，他们发明了一张表，然后在这张表中规定了生活中所有的字符和二进制之间的对应关系：老美的文字和计算机中的二进制的对应关系表：ASCII。 ASCII码表： 它采用的是一个字节表示所有的文字（标点符号，英文字母，其他的特殊符号，数字等）。 生活中的字符 十进制 二进制 A 65 01000001 B 66 01000010 欧洲也定义一张拉丁码表：ISO-8859-1 这个表码表兼容ASCII码表。也采用的一个字节表示字符数据。 ASCII： 一个字节表示一个字符： 0xxx xxxx 规定二进制的最高位是0，其他的7位表示某个字符的编码值。 ISO-8859-1： 一个字节表示一个字符：把整个字节表示字符，xxxx xxxx 全部用来表示字符数据 中国的编码表： GBK编码表采用2个字节表示一个汉字。 GB2312 ： 它识别六七千的文字。兼容ASCII编码表。 GBK： 识别两万多字符。目前主流的编码表. GB18030： 识别更多。 世界计算机协会也制定了一个张国际通用的编码表： ​ Unicode编码表： 它也采用2个字节表示一个字符。 Unicode编码表升级：UTF-8。 ​ UTF-8： 它对字符的编码规律可以使用一个字节表示的字符就使用一个字节。 可以使用两个字节表示的字符就使用两个字节。 可以使用三个字节表示的字符就使用三个字节（基本汉字都使用三个字节）。 可以识别汉字的编码表： GB2312、GBK、Unicode、UTF-8 二、文件操作流 字节流： FileInputStream ，FileOutputStream 字符流： FileReader, FileWriter File file = new File(&quot;D:/test/testIO.java&quot;); InputStream in = new FileInputStream(file); InputStream in = new FileInputStream(&quot;D:/test/testIO.java&quot;); OutputStream out = new FileOutputStream(file); OutputStream out = new FileOutputStream(&quot;D:/test/testIO.java&quot;); Reader reader = new FileReader(&quot;demo01.txt&quot;); FileWriter writer = new FileWriter(&quot;demo02.txt&quot;); Writer writer = new OutputStreamWriter(new FileOutputStream(&quot;demo03_utf8.txt&quot;), &quot;UTF-8&quot;); Reader reader = new InputStreamReader(new FileInputStream(&quot;demo03_utf8.txt&quot;), &quot;UTF-8&quot;); //1.指定要读 的文件目录及名称 File file =new File(&quot;文件路径&quot;); //2.创建文件读入流对象 FileInputStream fis =new FileInputStream(file); //3.定义结束标志,可用字节数组读取 int i =0 ; while((i = fis.read())!=-1){ //i 就是从文件中读取的字节，读完后返回-1 } //4.关闭流 fis.close(); //5.处理异常 //1.指定要写到的文件目录及名称 File file =new File(&quot;文件路径&quot;); //2.创建文件读入流对象 FileOutputStream fos =new FileOutputStream(file); //3.定义结束标志 fos.write(要写出的字节或者字节数组); //4.刷新和关闭流 fos.flush(); fos.close(); //5.处理异常 三、缓冲流： ​ 字节缓冲流： BufferedInputStream,BufferedOutputStream ​ 字符缓冲流：BufferedReader ,BufferedWriter ​ 缓冲流是对流的操作的功能的加强，提高了数据的读写效率。既然缓冲流是对流的功能和读写效率的加强和提高，所以在创建缓冲流的对象时应该要传入要加强的流对象。 //保存其参数，即输入流 in，以便将来使用。创建一个内部缓冲区数组并将 //其存储在 buf 中,该buf的大小默认为8192。 public BufferedInputStream(InputStream in); //创建具有指定缓冲区大小的 BufferedInputStream 并保存其参数， //即输入流 in，以便将来使用。创建一个长度为 size 的内部缓冲区数组并 //将其存储在 buf 中。 public BufferedInputStream(InputStream in,int size); //创建一个新的缓冲输出流，以将数据写入指定的底层输出流。 public BufferedOutputStream(OutputStream out); //创建一个新的缓冲输出流，以将具有指定缓冲区大小的数据写入指定的底层输出流。 public BufferedOutputStream(OutputStream out,int size); BufferedWriter bw = new BufferedWriter(new OutputStreamWriter(new FileOutputStream(&quot;demo08_utf8.txt&quot;), &quot;UTF-8&quot;)); BufferedReader br = new BufferedReader(new InputStreamReader(new FileInputStream(&quot;demo08_utf8.txt&quot;), &quot;UTF-8&quot;)); //1.指定要读 的文件目录及名称 File file =new File(&quot;文件路径&quot;); //2.创建文件读入流对象 FileInputStream fis =new FileInputStream(file); //3.创建缓冲流对象加强fis功能 BufferedInputStream bis =new BufferedInputStream(fis); //4.定义结束标志,可用字节数组读取 int i =0 ; while((i = bis.read())!=-1){ //i 就是从文件中读取的字节，读完后返回-1 } //5.关闭流 bis.close(); //6.处理异常 //1.指定要写到的文件目录及名称 File file =new File(&quot;文件路径&quot;); //2.创建文件读入流对象 FileOutputStream fos =new FileOutputStream(file); //3.创建缓冲流对象加强fos功能 BufferedOutputStream bos=new BufferedOutputStream(fos); //4.向流中写入数据 bos.write(要写出的字节或者字节数组); //5.刷新和关闭流 bos.flush(); bos.close(); //6.处理异常 由以上看出流的操作基本相同，此流与文件流操作是几乎一样的只是将文件流作为参数传入缓冲流的构造方法中堆文件流读写文件的功能进行加强 注1：在字符读入缓冲流BufferedReader 中还提供了读一行的方法 readLine() 可以读取一行文本 在字符写出缓冲流BufferedWriter 中还提供了写入一个行分隔符的方法writeLine(),用于写出时换行 注2：此处用到的是设计模式中的装饰模式 //装饰的对象只要是抽象类的子类即可 BufferedInputStream bis =new BufferedInputStream(new FileInputStream(new File(&quot;文件路径&quot;))); 四、转换流： 这类流是用于将字符转换为字节输入输出，用于操作字符文件，属于字符流的子类，所以后缀为reader，writer；前缀inputstream，outputstream； 注 ：要传入字节流作为参赛 InputStreamReader: 字符转换输出流 OutputStreamWriter：字符转换输入流 需求：读取键盘输入的一行文本,再将输入的写到本地磁盘上 //1.获取键盘输入的字节流对象in InputStream in =Stream.in; /*2.用转换流将字节流对象转换为字符流对象，方便调用字符缓冲流的readeLine()方法*/ InputStreamReader isr =new InputStreamReader(in); /*5.创建字符转换输出流对象osw，方便把输入的字符流转换为字节输出到本地文件。*/ OutputStreamWriter osw =new OutputStreamWriter(new FileOutputStream(new File(&quot;文件名&quot;))); /*3.现在isr是字符流，可以作为参数传入字符缓冲流中*/ BufferedReader br =new BufferedReader(isr); /*4.可以调用字符缓冲流br的readLine()方法度一行输入文本*/ String line =null; while((line =br.readLine()){ osw.write(line);//osw是字符流对象，可以直接操作字符串 } //刷新此缓冲的输出流，保证数据全部都能写出 osw.flush(); br.close(); osw.close(); 读取数据 //读取方式一 : 效率低! int content = -1; while ((content = reader.read()) != -1) { System.out.print((char)content); } //读取方式二 : 数组之后的无用数据都会被读取! int len = -1; char[] cbuf = new char[1024]; while ((len = reader.read(cbuf)) != -1) { System.out.println(cbuf); } // 读取方式三 : 将读取的数据内容转换为 `字符串` int len = -1; char[] cbuf = new char[1024]; while ((len = reader.read(cbuf)) != -1) { String str = new String(cbuf, 0, len); System.out.println(str); } &quot;转换流 和 操作流 之间的关系 :&quot; &quot;转换流 :&quot; 1. OutputStreamWriter = OutputStream + 任意编码表 2. InputStreamReader = InputStream + 任意编码表 &quot;操作流 :&quot; 1. FileWriter = OutputStreamWriter + 默认编码表 (GBK) 2. FileReader = InputStreamReader + 默认编码表(GBK) &quot;总结 : 一般我们都使用操作流,当需要指定编码表的时候,才会使用转换流&quot; 拆分文件及还原 public class Test { public static void main(String[] args) throws IOException { int count = judge(); System.out.println(count); BufferedInputStream bis = new BufferedInputStream(new FileInputStream( &quot;somethings\\\\test.flv&quot;)); incision(count, bis, 6); BufferedOutputStream bos = new BufferedOutputStream( new FileOutputStream(&quot;somethings\\\\jzc.flv&quot;)); joint(bos,6); bos.close(); bis.close(); } public static void joint( BufferedOutputStream bos,int n) throws FileNotFoundException, IOException { for (int i = 1; i &lt;= n; i++) { BufferedInputStream bis = new BufferedInputStream( new FileInputStream(&quot;somethings\\\\jzc&quot; + i + &quot;.flv&quot;)); byte[] bys = new byte[1024]; int len = -1; while ((len = bis.read(bys)) != -1) { bos.write(bys, 0, len); } bis.close(); } bos.close(); System.out.println(&quot;拼接完了&quot;); } public static void incision(int count, BufferedInputStream bis, int n) throws FileNotFoundException, IOException { byte[] bys = new byte[1024]; int len = -1; for (int i = 1; i &lt;= n; i++) { int count2 = 0; BufferedOutputStream bos = new BufferedOutputStream( new FileOutputStream(&quot;somethings\\\\jzc&quot; + i + &quot;.flv&quot;)); while ((len = bis.read(bys)) != -1) { count2 += 1024; System.out.println(count2); if (count2 &lt; (count / n)+1024&amp;&amp;count2 &gt; (count / n)-1024) { break; } else { bos.write(bys, 0, len); } } bos.close(); } System.out.println(&quot;切割完了&quot;); } public static int judge() throws FileNotFoundException, IOException { BufferedInputStream bis = new BufferedInputStream(new FileInputStream( &quot;somethings\\\\test.flv&quot;)); byte[] bys = new byte[1024]; int len = -1; int count = 0; while ((len = bis.read(bys)) != -1) { count += 1024; } bis.close(); return count; } } ","link":"https://tianxiawuhao.github.io/FNm9kw7nn/"},{"title":"java基础之一lambda","content":"一、引言 java8最大的特性就是引入Lambda表达式，即函数式编程，可以将行为进行传递。总结就是：使用不可变值与函数，函数对不可变值进行处理，映射成另一个值。 二、java重要的函数式接口 1、什么是函数式接口 函数接口是只有一个抽象方法的接口，用作 Lambda 表达式的类型。使用@FunctionalInterface注解修饰的类，编译器会检测该类是否只有一个抽象方法或接口，否则，会报错。可以有多个默认方法，静态方法。 1.1 java8自带的常用函数式接口。 函数接口 抽象方法 功能 参数 返回类型 示例 Predicate test(T t) 判断真假 T boolean 金刚的身高大于185cm吗？ Consumer accept(T t) 消费消息 T void 输出一个值 Function R apply(T t) 将T映射为R（转换功能） T R 获得student对象的名字 Supplier T get() 生产消息 None T 工厂方法 UnaryOperator T apply(T t) 一元操作 T T 逻辑非（!） BinaryOperator apply(T t, U u) 二元操作 (T，T) (T) 求两个数的乘积（*） public class Test { public static void main(String[] args) { Predicate&lt;Integer&gt; predicate = x -&gt; x &gt; 185; Student student = new Student(&quot;金刚&quot;, 23, 175); System.out.println(&quot;金刚的身高高于185吗？：&quot; + predicate.test(student.getStature())); Consumer&lt;String&gt; consumer = System.out::println; consumer.accept(&quot;消费消息，输出一个值&quot;); Function&lt;Student, String&gt; function = Student::getName; String name = function.apply(student); System.out.println(name); Supplier&lt;Integer&gt; supplier = () -&gt; Integer.valueOf(BigDecimal.TEN.toString()); System.out.println(supplier.get()); UnaryOperator&lt;Boolean&gt; unaryOperator = uglily -&gt; !uglily; Boolean apply2 = unaryOperator.apply(true); System.out.println(apply2); BinaryOperator&lt;Integer&gt; operator = (x, y) -&gt; x * y; Integer integer = operator.apply(2, 3); System.out.println(integer); test(() -&gt; &quot;我是一个演示的函数式接口&quot;); } /** * 演示自定义函数式接口使用 * * @param worker */ public static void test(Worker worker) { String work = worker.work(); System.out.println(work); } public interface Worker { String work(); } } //金刚的身高高于185吗？：false //消费消息，输出一个值 //金刚 //10 //false //6 //我是一个演示的函数式接口 以上演示了lambda接口的使用及自定义一个函数式接口并使用。下面，我们看看java8将函数式接口封装到流中如何高效的帮助我们处理集合。 注意：Student::getName例子中这种编写lambda表达式的方式称为方法引用。格式为ClassName::methodName。 示例：本篇所有示例都基于以下三个类。OutstandingClass：班级；Student：学生；SpecialityEnum：特长。 1.2 惰性求值与及早求值 惰性求值：只描述Stream，操作的结果也是Stream，这样的操作称为惰性求值。 惰性求值可以像建造者模式一样链式使用，最后再使用及早求值得到最终结果。 及早求值：得到最终的结果而不是Stream，这样的操作称为及早求值。 2、常用的流 2.1 collect(Collectors.toList()) 将流转换为list。还有toSet()，toMap()等。及早求值。 public class TestCase { public static void main(String[] args) { List&lt;Student&gt; studentList = Stream.of(new Student(&quot;路飞&quot;, 22, 175), new Student(&quot;红发&quot;, 40, 180), new Student(&quot;白胡子&quot;, 50, 185)).collect(Collectors.toList()); System.out.println(studentList); } } //输出结果 //[Student{name='路飞', age=22, stature=175, specialities=null}, //Student{name='红发', age=40, stature=180, specialities=null}, //Student{name='白胡子', age=50, stature=185, specialities=null}] //转成map public Map&lt;Long, String&gt; getIdNameMap(List&lt;Account&gt; accounts) { return accounts.stream().collect(Collectors.toMap(Account::getId, Account::getUsername)); } //收集成实体本身map public Map&lt;Long, Account&gt; getIdAccountMap(List&lt;Account&gt; accounts) { return accounts.stream().collect(Collectors.toMap(Account::getId, account -&gt; account)); } //account -&gt; account是一个返回本身的lambda表达式，其实还可以使用Function接口中的一个默认方法代替，使整个方法更简洁优雅： public Map&lt;Long, Account&gt; getIdAccountMap(List&lt;Account&gt; accounts) { return accounts.stream().collect(Collectors.toMap(Account::getId, Function.identity())); } //重复key的情况，下面name可能重复 public Map&lt;String, Account&gt; getNameAccountMap(List&lt;Account&gt; accounts) { return accounts.stream().collect(Collectors.toMap(Account::getUsername, Function.identity())); } //toMap有个重载方法，可以传入一个合并的函数来解决key冲突问题 public Map&lt;String, Account&gt; getNameAccountMap(List&lt;Account&gt; accounts) { return accounts.stream().collect(Collectors.toMap(Account::getUsername, Function.identity(), (key1, key2) -&gt; key2)); } //按id分组 Map&lt;Long, List&lt;Account&gt;&gt; map = accounts.stream().collect(Collectors.groupingBy(Account::getId)); //指定具体收集的map public Map&lt;String, Account&gt; getNameAccountMap(List&lt;Account&gt; accounts) { return accounts.stream().collect(Collectors.toMap(Account::getUsername, Function.identity(), (key1, key2) -&gt; key2, LinkedHashMap::new)); } 2.2 filter 顾名思义，起过滤筛选的作用。内部就是Predicate接口。惰性求值。 比如我们筛选出出身高小于180的同学。 public class TestCase { public static void main(String[] args) { List&lt;Student&gt; students = new ArrayList&lt;&gt;(3); students.add(new Student(&quot;路飞&quot;, 22, 175)); students.add(new Student(&quot;红发&quot;, 40, 180)); students.add(new Student(&quot;白胡子&quot;, 50, 185)); List&lt;Student&gt; list = students.stream() .filter(stu -&gt; stu.getStature() &lt; 180) .collect(Collectors.toList()); System.out.println(list); } } //输出结果 //[Student{name='路飞', age=22, stature=175, specialities=null}] 2.3 map 转换功能，内部就是Function接口。惰性求值 public class TestCase { public static void main(String[] args) { List&lt;Student&gt; students = new ArrayList&lt;&gt;(3); students.add(new Student(&quot;路飞&quot;, 22, 175)); students.add(new Student(&quot;红发&quot;, 40, 180)); students.add(new Student(&quot;白胡子&quot;, 50, 185)); List&lt;String&gt; names = students.stream().map(student -&gt; student.getName()) .collect(Collectors.toList()); System.out.println(names); } } //输出结果 //[路飞, 红发, 白胡子] 例子中将student对象转换为String对象，获取student的名字。 2.4 flatMap 将多个Stream合并为一个Stream。惰性求值 public class TestCase { public static void main(String[] args) { List&lt;Student&gt; students = new ArrayList&lt;&gt;(3); students.add(new Student(&quot;路飞&quot;, 22, 175)); students.add(new Student(&quot;红发&quot;, 40, 180)); students.add(new Student(&quot;白胡子&quot;, 50, 185)); List&lt;Student&gt; studentList = Stream.of(students, asList(new Student(&quot;艾斯&quot;, 25, 183), new Student(&quot;雷利&quot;, 48, 176))) .flatMap(students1 -&gt; students1.stream()).collect(Collectors.toList()); System.out.println(studentList); } } //输出结果 //[Student{name='路飞', age=22, stature=175, specialities=null}, //Student{name='红发', age=40, stature=180, specialities=null}, //Student{name='白胡子', age=50, stature=185, specialities=null}, //Student{name='艾斯', age=25, stature=183, specialities=null}, //Student{name='雷利', age=48, stature=176, specialities=null}] 调用Stream.of的静态方法将两个list转换为Stream，再通过flatMap将两个流合并为一个。 2.5 max和min 我们经常会在集合中求最大或最小值，使用流就很方便。及早求值。 public class TestCase { public static void main(String[] args) { List&lt;Student&gt; students = new ArrayList&lt;&gt;(3); students.add(new Student(&quot;路飞&quot;, 22, 175)); students.add(new Student(&quot;红发&quot;, 40, 180)); students.add(new Student(&quot;白胡子&quot;, 50, 185)); Optional&lt;Student&gt; max = students.stream() .max(Comparator.comparing(stu -&gt; stu.getAge())); Optional&lt;Student&gt; min = students.stream() .min(Comparator.comparing(stu -&gt; stu.getAge())); //判断是否有值 if (max.isPresent()) { System.out.println(max.get()); } if (min.isPresent()) { System.out.println(min.get()); } } } //输出结果 //Student{name='白胡子', age=50, stature=185, specialities=null} //Student{name='路飞', age=22, stature=175, specialities=null} max、min接收一个Comparator（例子中使用java8自带的静态函数，只需要传进需要比较值即可。）并且返回一个Optional对象，该对象是java8新增的类，专门为了防止null引发的空指针异常。可以使用max.isPresent()判断是否有值；可以使用max.orElse(new Student())，当值为null时就使用给定值；也可以使用max.orElseGet(() -&gt; new Student());这需要传入一个Supplier的lambda表达式。 2.6 count 统计功能，一般都是结合filter使用，因为先筛选出我们需要的再统计即可。及早求值 public class TestCase { public static void main(String[] args) { List&lt;Student&gt; students = new ArrayList&lt;&gt;(3); students.add(new Student(&quot;路飞&quot;, 22, 175)); students.add(new Student(&quot;红发&quot;, 40, 180)); students.add(new Student(&quot;白胡子&quot;, 50, 185)); long count = students.stream().filter(s1 -&gt; s1.getAge() &lt; 45).count(); System.out.println(&quot;年龄小于45岁的人数是：&quot; + count); } } //输出结果 //年龄小于45岁的人数是：2 2.7 reduce reduce 操作可以实现从一组值中生成一个值。在上述例子中用到的 count 、 min 和 max 方 法，因为常用而被纳入标准库中。事实上，这些方法都是 reduce 操作。及早求值。 public class TestCase { public static void main(String[] args) { Integer reduce = Stream.of(1, 2, 3, 4).reduce(0, (acc, x) -&gt; acc+ x); System.out.println(reduce); } } //输出结果 //10 我们看得reduce接收了一个初始值为0的累加器，依次取出值与累加器相加，最后累加器的值就是最终的结果。 三、高级集合类及收集器 3.1 转换成值 **收集器，一种通用的、从流生成复杂值的结构。**只要将它传给 collect 方法，所有 的流就都可以使用它了。标准类库已经提供了一些有用的收集器，以下示例代码中的收集器都是从 java.util.stream.Collectors 类中静态导入的。 public class CollectorsTest { public static void main(String[] args) { List&lt;Student&gt; students1 = new ArrayList&lt;&gt;(3); students1.add(new Student(&quot;路飞&quot;, 23, 175)); students1.add(new Student(&quot;红发&quot;, 40, 180)); students1.add(new Student(&quot;白胡子&quot;, 50, 185)); OutstandingClass ostClass1 = new OutstandingClass(&quot;一班&quot;, students1); //复制students1，并移除一个学生 List&lt;Student&gt; students2 = new ArrayList&lt;&gt;(students1); students2.remove(1); OutstandingClass ostClass2 = new OutstandingClass(&quot;二班&quot;, students2); //将ostClass1、ostClass2转换为Stream Stream&lt;OutstandingClass&gt; classStream = Stream.of(ostClass1, ostClass2); OutstandingClass outstandingClass = biggestGroup(classStream); System.out.println(&quot;人数最多的班级是：&quot; + outstandingClass.getName()); System.out.println(&quot;一班平均年龄是：&quot; + averageNumberOfStudent(students1)); } /** * 获取人数最多的班级 */ private static OutstandingClass biggestGroup(Stream&lt;OutstandingClass&gt; outstandingClasses) { return outstandingClasses.collect( maxBy(comparing(ostClass -&gt; ostClass.getStudents().size()))) .orElseGet(OutstandingClass::new); } /** * 计算平均年龄 */ private static double averageNumberOfStudent(List&lt;Student&gt; students) { return students.stream().collect(averagingInt(Student::getAge)); } } //输出结果 //人数最多的班级是：一班 //一班平均年龄是：37.666666666666664 maxBy或者minBy就是求最大值与最小值。 3.2 转换成块 常用的流操作是将其分解成两个集合，Collectors.partitioningBy帮我们实现了，接收一个Predicate函数式接口。 将示例学生分为会唱歌与不会唱歌的两个集合。 public class PartitioningByTest { public static void main(String[] args) { //省略List&lt;student&gt; students的初始化 Map&lt;Boolean, List&lt;Student&gt;&gt; listMap = students.stream().collect( Collectors.partitioningBy(student -&gt; student.getSpecialities(). contains(SpecialityEnum.SING))); } } 3.3 数据分组 数据分组是一种更自然的分割数据操作，与将数据分成 ture 和 false 两部分不同，可以使 用任意值对数据分组。Collectors.groupingBy接收一个Function做转换。 如图，我们使用groupingBy将根据进行分组为圆形一组，三角形一组，正方形一组。 例子：根据学生第一个特长进行分组 public class GroupingByTest { public static void main(String[] args) { //省略List&lt;student&gt; students的初始化 Map&lt;SpecialityEnum, List&lt;Student&gt;&gt; listMap = students.stream().collect( Collectors.groupingBy(student -&gt; student.getSpecialities().get(0))); } } Collectors.groupingBy与SQL 中的 group by 操作是一样的。 3.4 字符串拼接 如果将所有学生的名字拼接起来，怎么做呢？通常只能创建一个StringBuilder，循环拼接。使用Stream，使用Collectors.joining()简单容易。 public class JoiningTest { public static void main(String[] args) { List&lt;Student&gt; students = new ArrayList&lt;&gt;(3); students.add(new Student(&quot;路飞&quot;, 22, 175)); students.add(new Student(&quot;红发&quot;, 40, 180)); students.add(new Student(&quot;白胡子&quot;, 50, 185)); String names = students.stream() .map(Student::getName).collect(Collectors.joining(&quot;,&quot;,&quot;[&quot;,&quot;]&quot;)); System.out.println(names); } } //输出结果 //[路飞,红发,白胡子] joining接收三个参数，第一个是分界符，第二个是前缀符，第三个是结束符。也可以不传入参数Collectors.joining()，这样就是直接拼接。 原文地址：https://juejin.cn/post/6844903849753329678#hjava ","link":"https://tianxiawuhao.github.io/7BfPpY68W/"},{"title":"Java基础之—反射","content":"反射是框架设计的灵魂（使用的前提条件：必须先得到代表的字节码的Class，Class类用于表示.class文件（字节码）） 反射的概述 JAVA反射机制是在运行状态中，对于任意一个类，都能够知道这个类的所有属性和方法；对于任意一个对象，都能够调用它的任意一个方法和属性；这种动态获取的信息以及动态调用对象的方法的功能称为java语言的反射机制。 要想解剖一个类,必须先要获取到该类的字节码文件对象。而解剖使用的就是Class类中的方法.所以先要获取到每一个字节码文件对应的Class类型的对象. 反射就是把java类中的各种成分映射成一个个的Java对象 例如：一个类有：成员变量、方法、构造方法、包等等信息，利用反射技术可以对一个类进行解剖，把个个组成部分映射成一个个对象。（其实：一个类中这些成员方法、构造方法、在加入类中都有一个类来描述） 如图是类的正常加载过程：Class对象的由来是将class文件读入内存，并为之创建一个Class对象。 其中这个Class对象很特殊。我们先了解一下这个Class类 查看Class类在java中的api详解 Class 类的实例表示正在运行的 Java 应用程序中的类和接口。也就是jvm中有N多的实例每个类都有该Class对象。（包括基本数据类型） Class 没有公共构造方法。Class 对象是在加载类时由 Java 虚拟机以及通过调用类加载器中的defineClass 方法自动构造的。也就是这不需要我们自己去处理创建，JVM已经帮我们创建好了。 没有公共的构造方法，方法共有64个。 反射的使用 获取Class对象的三种方式 1.1 Object ——&gt; getClass(); 1.2 任何数据类型（包括基本数据类型）都有一个“静态”的class属性 1.3 通过Class类的静态方法：forName（String className）(常用) 其中1.1是因为Object类中的getClass方法、因为所有类都继承Object类。从而调用Object类来获取 package fanshe; /** * 获取Class对象的三种方式 * 1 Object ——&gt; getClass(); * 2 任何数据类型（包括基本数据类型）都有一个“静态”的class属性 * 3 通过Class类的静态方法：forName（String className）(常用) * */ public class Fanshe { public static void main(String[] args) { //第一种方式获取Class对象 Student stu1 = new Student();//这一new 产生一个Student对象，一个Class对象。 Class stuClass = stu1.getClass();//获取Class对象 System.out.println(stuClass.getName()); //第二种方式获取Class对象 Class stuClass2 = Student.class; System.out.println(stuClass == stuClass2);//判断第一种方式获取的Class对象和第二种方式获取的是否是同一个 //第三种方式获取Class对象 try { Class stuClass3 = Class.forName(&quot;fanshe.Student&quot;);//注意此字符串必须是真实路径，就是带包名的类路径，包名.类名 System.out.println(stuClass3 == stuClass2);//判断三种方式是否获取的是同一个Class对象 } catch (ClassNotFoundException e) { e.printStackTrace(); } } } 注意：在运行期间，一个类，只有一个Class对象产生。 三种方式常用第三种，第一种对象都有了还要反射干什么。第二种需要导入类的包，依赖太强，不导包就抛编译错误。一般都第三种，一个字符串可以传入也可写在配置文件中等多种方法。 通过反射获取构造方法并使用： package fanshe; public class Student { //---------------构造方法------------------- //（默认的构造方法） Student(String str){ System.out.println(&quot;(默认)的构造方法 s = &quot; + str); } //无参构造方法 public Student(){ System.out.println(&quot;调用了公有、无参构造方法执行了。。。&quot;); } //有一个参数的构造方法 public Student(char name){ System.out.println(&quot;姓名：&quot; + name); } //有多个参数的构造方法 public Student(String name ,int age){ System.out.println(&quot;姓名：&quot;+name+&quot;年龄：&quot;+ age);//这的执行效率有问题，以后解决。 } //受保护的构造方法 protected Student(boolean n){ System.out.println(&quot;受保护的构造方法 n = &quot; + n); } //私有构造方法 private Student(int age){ System.out.println(&quot;私有的构造方法 年龄：&quot;+ age); } } 共有6个构造方法； /* * 通过Class对象可以获取某个类中的：构造方法、成员变量、成员方法；并访问成员； * * 1.获取构造方法： * 1).批量的方法： * public Constructor[] getConstructors()：所有&quot;公有的&quot;构造方法 * public Constructor[] getDeclaredConstructors()：获取所有的构造方法(包括私有、受保护、默认、公有) * * 2).获取单个的方法，并调用： * public Constructor getConstructor(Class... parameterTypes):获取单个的&quot;公有的&quot;构造方法： * public Constructor getDeclaredConstructor(Class... parameterTypes):获取&quot;某个构造方法&quot;可以是私有的，或受保护、默认、公有； * *调用构造方法： * Constructor--&gt;newInstance(Object... initargs) * 使用此 Constructor 对象表示的构造方法来创建该构造方法的声明类的新实例，并用指定的初始化参数初始化该实例。 * 它的返回值是T类型，所以newInstance是创建了一个构造方法的声明类的新实例对象。并为之调用 */ package fanshe; import java.lang.reflect.Constructor; public class Constructors { public static void main(String[] args) throws Exception { //1.加载Class对象 Class clazz = Class.forName(&quot;fanshe.Student&quot;); //2.获取所有公有构造方法 System.out.println(&quot;**********************所有公有构造方法*********************************&quot;); Constructor[] conArray = clazz.getConstructors(); for(Constructor c : conArray){ System.out.println(c); } System.out.println(&quot;************所有的构造方法(包括：私有、受保护、默认、公有)***************&quot;); conArray = clazz.getDeclaredConstructors(); for(Constructor c : conArray){ System.out.println(c); } System.out.println(&quot;*****************获取公有、无参的构造方法*******************************&quot;); Constructor con = clazz.getConstructor(null); //1&gt;、因为是无参的构造方法所以类型是一个null,不写也可以：这里需要的是一个参数的类型，切记是类型 //2&gt;、返回的是描述这个无参构造函数的类对象。 System.out.println(&quot;con = &quot; + con); //调用构造方法 Object obj = con.newInstance(); // System.out.println(&quot;obj = &quot; + obj); // Student stu = (Student)obj; System.out.println(&quot;******************获取私有构造方法，并调用*******************************&quot;); con = clazz.getDeclaredConstructor(char.class); System.out.println(con); //调用构造方法 con.setAccessible(true);//暴力访问(忽略掉访问修饰符) obj = con.newInstance('男'); } } 后台输出： **********************所有公有构造方法********************************* public fanshe.Student(java.lang.String,int) public fanshe.Student(char) public fanshe.Student() ************所有的构造方法(包括：私有、受保护、默认、公有)*************** private fanshe.Student(int) protected fanshe.Student(boolean) public fanshe.Student(java.lang.String,int) public fanshe.Student(char) public fanshe.Student() fanshe.Student(java.lang.String) *****************获取公有、无参的构造方法******************************* con = public fanshe.Student() 调用了公有、无参构造方法执行了。。。 ******************获取私有构造方法，并调用******************************* public fanshe.Student(char) 姓名：男 获取成员变量并调用： package fanshe.field; public class Student { public Student(){ } //**********字段*************// public String name; protected int age; char sex; private String phoneNum; @Override public String toString() { return &quot;Student [name=&quot; + name + &quot;, age=&quot; + age + &quot;, sex=&quot; + sex + &quot;, phoneNum=&quot; + phoneNum + &quot;]&quot;; } } 测试类： /* * 获取成员变量并调用： * * 1.批量的 * 1).Field[] getFields():获取所有的&quot;公有字段&quot; * 2).Field[] getDeclaredFields():获取所有字段，包括：私有、受保护、默认、公有； * 2.获取单个的： * 1).public Field getField(String fieldName):获取某个&quot;公有的&quot;字段； * 2).public Field getDeclaredField(String fieldName):获取某个字段(可以是私有的) * * 设置字段的值： * Field --&gt; public void set(Object obj,Object value): * 参数说明： * 1.obj:要设置的字段所在的对象； * 2.value:要为字段设置的值； * */ package fanshe.field; import java.lang.reflect.Field; public class Fields { public static void main(String[] args) throws Exception { //1.获取Class对象 Class stuClass = Class.forName(&quot;fanshe.field.Student&quot;); //2.获取字段 System.out.println(&quot;************获取所有公有的字段********************&quot;); Field[] fieldArray = stuClass.getFields(); for(Field f : fieldArray){ System.out.println(f); } System.out.println(&quot;************获取所有的字段(包括私有、受保护、默认的)********************&quot;); fieldArray = stuClass.getDeclaredFields(); for(Field f : fieldArray){ System.out.println(f); } System.out.println(&quot;*************获取公有字段**并调用***********************************&quot;); Field f = stuClass.getField(&quot;name&quot;); System.out.println(f); //获取一个对象 Object obj = stuClass.getConstructor().newInstance();//产生Student对象--》Student stu = new Student(); //为字段设置值 f.set(obj, &quot;刘德华&quot;);//为Student对象中的name属性赋值--》stu.name = &quot;刘德华&quot; //验证 Student stu = (Student)obj; System.out.println(&quot;验证姓名：&quot; + stu.name); System.out.println(&quot;**************获取私有字段****并调用********************************&quot;); f = stuClass.getDeclaredField(&quot;phoneNum&quot;); System.out.println(f); f.setAccessible(true);//暴力反射，解除私有限定 f.set(obj, &quot;18888889999&quot;); System.out.println(&quot;验证电话：&quot; + stu); } } 后台输出： ************获取所有公有的字段******************** public java.lang.String fanshe.field.Student.name ************获取所有的字段(包括私有、受保护、默认的)******************** public java.lang.String fanshe.field.Student.name protected int fanshe.field.Student.age char fanshe.field.Student.sex private java.lang.String fanshe.field.Student.phoneNum *************获取公有字段**并调用*********************************** public java.lang.String fanshe.field.Student.name 验证姓名：刘德华 **************获取私有字段****并调用******************************** private java.lang.String fanshe.field.Student.phoneNum 验证电话：Student [name=刘德华, age=0, sex=，phoneNum=18888889999] 获取成员方法并调用 package fanshe.method; public class Student { //**************成员方法***************// public void show1(String s){ System.out.println(&quot;调用了：公有的，String参数的show1(): s = &quot; + s); } protected void show2(){ System.out.println(&quot;调用了：受保护的，无参的show2()&quot;); } void show3(){ System.out.println(&quot;调用了：默认的，无参的show3()&quot;); } private String show4(int age){ System.out.println(&quot;调用了，私有的，并且有返回值的，int参数的show4(): age = &quot; + age); return &quot;abcd&quot;; } } 测试类： /* * 获取成员方法并调用： * * 1.批量的： * public Method[] getMethods():获取所有&quot;公有方法&quot;；（包含了父类的方法也包含Object类） * public Method[] getDeclaredMethods():获取所有的成员方法，包括私有的(不包括继承的) * 2.获取单个的： * public Method getMethod(String name,Class&lt;?&gt;... parameterTypes): * 参数： * name : 方法名； * Class ... : 形参的Class类型对象 * public Method getDeclaredMethod(String name,Class&lt;?&gt;... parameterTypes) * * 调用方法： * Method --&gt; public Object invoke(Object obj,Object... args): * 参数说明： * obj : 要调用方法的对象； * args:调用方式时所传递的实参； ): */ package fanshe.method; import java.lang.reflect.Method; public class MethodClass { public static void main(String[] args) throws Exception { //1.获取Class对象 Class stuClass = Class.forName(&quot;fanshe.method.Student&quot;); //2.获取所有公有方法 System.out.println(&quot;***************获取所有的”公有“方法*******************&quot;); stuClass.getMethods(); Method[] methodArray = stuClass.getMethods(); for(Method m : methodArray){ System.out.println(m); } System.out.println(&quot;***************获取所有的方法，包括私有的*******************&quot;); methodArray = stuClass.getDeclaredMethods(); for(Method m : methodArray){ System.out.println(m); } System.out.println(&quot;***************获取公有的show1()方法*******************&quot;); Method m = stuClass.getMethod(&quot;show1&quot;, String.class); System.out.println(m); //实例化一个Student对象 Object obj = stuClass.getConstructor().newInstance(); m.invoke(obj, &quot;刘德华&quot;); System.out.println(&quot;***************获取私有的show4()方法******************&quot;); m = stuClass.getDeclaredMethod(&quot;show4&quot;, int.class); System.out.println(m); m.setAccessible(true);//解除私有限定 Object result = m.invoke(obj, 20);//需要两个参数，一个是要调用的对象（获取有反射），一个是实参 System.out.println(&quot;返回值：&quot; + result); } } 控制台输出： ***************获取所有的”公有“方法******************* public void fanshe.method.Student.show1(java.lang.String) public final void java.lang.Object.wait(long,int) throws java.lang.InterruptedException public final native void java.lang.Object.wait(long) throws java.lang.InterruptedException public final void java.lang.Object.wait() throws java.lang.InterruptedException public boolean java.lang.Object.equals(java.lang.Object) public java.lang.String java.lang.Object.toString() public native int java.lang.Object.hashCode() public final native java.lang.Class java.lang.Object.getClass() public final native void java.lang.Object.notify() public final native void java.lang.Object.notifyAll() ***************获取所有的方法，包括私有的******************* public void fanshe.method.Student.show1(java.lang.String) private java.lang.String fanshe.method.Student.show4(int) protected void fanshe.method.Student.show2() void fanshe.method.Student.show3() ***************获取公有的show1()方法******************* public void fanshe.method.Student.show1(java.lang.String) 调用了：公有的，String参数的show1(): s = 刘德华 ***************获取私有的show4()方法****************** private java.lang.String fanshe.method.Student.show4(int) 调用了，私有的，并且有返回值的，int参数的show4(): age = 20 返回值：abcd 反射main方法 package fanshe.main; public class Student { public static void main(String[] args) { System.out.println(&quot;main方法执行了。。。&quot;); } } 测试类： package fanshe.main; import java.lang.reflect.Method; /** * 获取Student类的main方法、不要与当前的main方法搞混了 */ public class Main { public static void main(String[] args) { try { //1、获取Student对象的字节码 Class clazz = Class.forName(&quot;fanshe.main.Student&quot;); //2、获取main方法 Method methodMain = clazz.getMethod(&quot;main&quot;, String[].class);//第一个参数：方法名称，第二个参数：方法形参的类型， //3、调用main方法 // methodMain.invoke(null, new String[]{&quot;a&quot;,&quot;b&quot;,&quot;c&quot;}); //第一个参数，对象类型，因为方法是static静态的，所以为null可以，第二个参数是String数组，这里要注意在jdk1.4时是数组，jdk1.5之后是可变参数 //这里拆的时候将 new String[]{&quot;a&quot;,&quot;b&quot;,&quot;c&quot;} 拆成3个对象。。。所以需要将它强转。 methodMain.invoke(null, (Object)new String[]{&quot;a&quot;,&quot;b&quot;,&quot;c&quot;});//方式一 // methodMain.invoke(null, new Object[]{new String[]{&quot;a&quot;,&quot;b&quot;,&quot;c&quot;}});//方式二 } catch (Exception e) { e.printStackTrace(); } } } 控制台输出： main方法执行了。。。 反射方法的其它使用之---通过反射运行配置文件内容 public class Student { public void show(){ System.out.println(&quot;is show()&quot;); } } 配置文件以txt文件为例子（pro.txt）： className = cn.fanshe.Student methodName = show 测试类： import java.io.FileNotFoundException; import java.io.FileReader; import java.io.IOException; import java.lang.reflect.Method; import java.util.Properties; /* * 我们利用反射和配置文件，可以使：应用程序更新时，对源码无需进行任何修改 * 我们只需要将新类发送给客户端，并修改配置文件即可 */ public class Demo { public static void main(String[] args) throws Exception { //通过反射获取Class对象 Class stuClass = Class.forName(getValue(&quot;className&quot;));//&quot;cn.fanshe.Student&quot; //2获取show()方法 Method m = stuClass.getMethod(getValue(&quot;methodName&quot;));//show //3.调用show()方法 m.invoke(stuClass.getConstructor().newInstance()); } //此方法接收一个key，在配置文件中获取相应的value public static String getValue(String key) throws IOException{ Properties pro = new Properties();//获取配置文件的对象 FileReader in = new FileReader(&quot;pro.txt&quot;);//获取输入流 pro.load(in);//将流加载到配置文件对象中 in.close(); return pro.getProperty(key);//返回根据key获取的value值 } } 控制台输出： is show() 需求： 当我们升级这个系统时，不要Student类，而需要新写一个Student2的类时，这时只需要更改pro.txt的文件内容就可以了。代码就一点不用改动 要替换的student2类： public class Student2 { public void show2(){ System.out.println(&quot;is show2()&quot;); } } 配置文件更改为： className = cn.fanshe.Student2 methodName = show2 控制台输出： is show2(); 反射方法的其它使用之---通过反射越过泛型检查 泛型用在编译期，编译过后泛型擦除（消失掉）。所以是可以通过反射越过泛型检查的 import java.lang.reflect.Method; import java.util.ArrayList; /* * 通过反射越过泛型检查 * * 例如：有一个String泛型的集合，怎样能向这个集合中添加一个Integer类型的值？ */ public class Demo { public static void main(String[] args) throws Exception{ ArrayList&lt;String&gt; strList = new ArrayList&lt;&gt;(); strList.add(&quot;aaa&quot;); strList.add(&quot;bbb&quot;); // strList.add(100); //获取ArrayList的Class对象，反向的调用add()方法，添加数据 Class listClass = strList.getClass(); //得到 strList 对象的字节码 对象 //获取add()方法 Method m = listClass.getMethod(&quot;add&quot;, Object.class); //调用add()方法 m.invoke(listClass , 100); //遍历集合 for(Object obj : strList){ System.out.println(obj); } } } 控制台输出： aaa bbb 100 ","link":"https://tianxiawuhao.github.io/NjXzyw7sR/"},{"title":"java基础之一集合","content":"基础数据结构说明 ​ 数组：采用一段连续的存储单元来存储数据。对于指定下标的查找，时间复杂度为O(1)；通过给定值进行查找，需要遍历数组，逐一比对给定关键字和数组元素，时间复杂度为O(n)，当然，对于有序数组，则可采用二分查找，插值查找，斐波那契查找等方式，可将查找复杂度提高为O(logn)；对于一般的插入删除操作，涉及到数组元素的移动，其平均复杂度也为O(n) 线性链表：对于链表的新增，删除等操作（在找到指定操作位置后），仅需处理结点间的引用即可，时间复杂度为O(1)，而查找操作需要遍历链表逐一进行比对，复杂度为O(n) 二叉树：对一棵相对平衡的有序二叉树，对其进行插入，查找，删除等操作，平均复杂度均为O(logn)。 哈希表：相比上述几种数据结构，在哈希表中进行添加，删除，查找等操作，性能十分之高，不考虑哈希冲突的情况下，仅需一次定位即可完成，时间复杂度为O(1)，接下来我们就来看看哈希表是如何实现达到惊艳的常数阶O(1)的。 哈希表具体说明 我们知道，数据结构的物理存储结构只有两种：顺序存储结构和链式存储结构（像栈，队列，树，图等是从逻辑结构去抽象的，映射到内存中，也是这两种物理组织形式），而在上面我们提到过，在数组中根据下标查找某个元素，一次定位就可以达到，哈希表利用了这种特性，哈希表的主干就是数组。 比如我们要新增或查找某个元素，我们通过把当前元素的关键字 通过某个函数映射到数组中的某个位置，通过数组下标一次定位就可完成操作。 存储位置 = f(关键字) 其中，这个函数f一般称为哈希函数，这个函数的设计好坏会直接影响到哈希表的优劣。举个例子，比如我们要在哈希表中执行插入操作： 查找操作同理，先通过哈希函数计算出实际存储地址，然后从数组中对应地址取出即可。 哈希冲突 然而万事无完美，如果两个不同的元素，通过哈希函数得出的实际存储地址相同怎么办？也就是说，当我们对某个元素进行哈希运算，得到一个存储地址，然后要进行插入的时候，发现已经被其他元素占用了，其实这就是所谓的哈希冲突，也叫哈希碰撞。前面我们提到过，哈希函数的设计至关重要，好的哈希函数会尽可能地保证 计算简单和散列地址分布均匀但是，我们需要清楚的是，数组是一块连续的固定长度的内存空间，再好的哈希函数也不能保证得到的存储地址绝对不发生冲突。那么哈希冲突如何解决呢？哈希冲突的解决方案有多种:开放定址法（线性探测）（发生冲突，继续寻找下一块未被占用的存储地址），再散列函数法，链地址法。 R-B Tree简介 R-B Tree，全称是Red-Black Tree，又称为“红黑树”，它一种特殊的二叉查找树。红黑树的每个节点上都有存储位表示节点的颜色，可以是红(Red)或黑(Black)。 红黑树的特性: （1）每个节点或者是黑色，或者是红色。 （2）根节点是黑色。 （3）每个叶子节点（NIL）是黑色。 [注意：这里叶子节点，是指为空(NIL或NULL)的叶子节点！] （4）如果一个节点是红色的，则它的子节点必须是黑色的。 （5）从一个节点到该节点的子孙节点的所有路径上包含相同数目的黑节点。 注意： (01) 特性(3)中的叶子节点，是只为空(NIL或null)的节点。 (02) 特性(5)，确保没有一条路径会比其他路径长出俩倍。因而，红黑树是相对是接近平衡的二叉树。 红黑树示意图如下： 性质 红黑树是每个节点都带有颜色属性的二叉查找树，颜色或红色或黑色。在二叉查找树强制一般要求以外，对于任何有效的红黑树我们增加了如下的额外要求： 【1】性质1. 节点是红色或黑色。 【2】性质2. 根节点是黑色。 【3】性质3 每个叶节点是黑色的。 【4】性质4 每个红色节点的两个子节点都是黑色。(从每个叶子到根的所有路径上不能有两个连续的红色节点) 【5】性质5. 从任一节点到其每个叶子的所有路径都包含相同数目的黑色节点。 用途和好处 红黑树和AVL树一样都对插入时间、删除时间和查找时间提供了最好可能的最坏情况担保。这不只是使它们在时间敏感的应用如即时应用(real time application)中有价值，而且使它们有在提供最坏情况担保的其他数据结构中作为建造板块的价值；例如，在计算几何中使用的很多数据结构都可以基于红黑树。 ​ 红黑树在函数式编程中也特别有用，在这里它们是最常用的持久数据结构之一，它们用来构造关联数组和集合，在突变之后它们能保持为以前的版本。除了O(log n)的时间之外，红黑树的持久版本对每次插入或删除需要O(log n)的空间。 ​ 红黑树是 2-3-4树的一种等同。换句话说，对于每个 2-3-4 树，都存在至少一个数据元素是同样次序的红黑树。在 2-3-4 树上的插入和删除操作也等同于在红黑树中颜色翻转和旋转。这使得 2-3-4 树成为理解红黑树背后的逻辑的重要工具，这也是很多介绍算法的教科书在红黑树之前介绍 2-3-4 树的原因，尽管 2-3-4 树在实践中不经常使用。 红黑树的数据结构 public class RBTree&lt;T extends Comparable&lt;T&gt;&gt; { private RBTNode&lt;T&gt; mRoot; // 根结点 private static final boolean RED = false; private static final boolean BLACK = true; public class RBTNode&lt;T extends Comparable&lt;T&gt;&gt; { boolean color; // 颜色 T key; // 关键字(键值) RBTNode&lt;T&gt; left; // 左孩子 RBTNode&lt;T&gt; right; // 右孩子 RBTNode&lt;T&gt; parent; // 父结点 public RBTNode(T key, boolean color, RBTNode&lt;T&gt; parent, RBTNode&lt;T&gt; left, RBTNode&lt;T&gt; right) { this.key = key; this.color = color; this.parent = parent; this.left = left; this.right = right; } } ... } 集合说明 ArrayList实现原理要点概括 ArrayList是List接口的可变数组非同步实现，并允许包括null在内的所有元素。 ArrayList的数据结构如下： 说明：底层的数据结构就是数组，数组元素类型为Object类型，即可以存放所有类型数据。我们对ArrayList类的实例的所有的操作底层都是基于数组的。 该集合是可变长度数组，数组扩容时，会将老数组中的元素重新拷贝一份到新的数组中，每次数组容量增长大约是其容量的1.5倍，这种操作的代价很高。 newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1) 采用了Fail-Fast机制，面对并发的修改时，迭代器很快就会完全失败，而不是冒着在将来某个不确定时间发生任意不确定行为的风险 remove方法会让下标到数组末尾的元素向前移动一个单位，并把最后一位的值置空，方便GC LinkedList实现原理要点概括 LinkedList是List接口的双向链表非同步实现，并允许包括null在内的所有元素。从JDK1.7开始，LinkedList 由双向循环链表改为双向链表 LinkedList数据结构如下 说明：如上图所示，LinkedList底层使用的双向链表结构，有一个头结点和一个尾结点，双向链表意味着我们可以从头开始正向遍历，或者是从尾开始逆向遍历，并且可以针对头部和尾部进行相应的操作。 双向链表节点对应的类Node的实例，Node中包含成员变量：prev，next，item。其中，prev是该节点的上一个节点，next是该节点的下一个节点，item是该节点所包含的值。 public class LinkedList&lt;E&gt; extends AbstractSequentialList&lt;E&gt; implements List&lt;E&gt;, Deque&lt;E&gt;, Cloneable, java.io.Serializable { transient int size = 0; /** * Pointer to first node. * Invariant: (first == null &amp;&amp; last == null) || * (first.prev == null &amp;&amp; first.item != null) */ transient Node&lt;E&gt; first; /** * Pointer to last node. * Invariant: (first == null &amp;&amp; last == null) || * (last.next == null &amp;&amp; last.item != null) */ transient Node&lt;E&gt; last; /** * 节点结构 */ private static class Node&lt;E&gt; { E item; Node&lt;E&gt; next; Node&lt;E&gt; prev; Node(Node&lt;E&gt; prev, E element, Node&lt;E&gt; next) { this.item = element; this.next = next; this.prev = prev; } } } 说明：LinkedList的属性非常简单，一个头结点、一个尾结点、一个表示链表中实际元素个数的变量。注意，头结点、尾结点都有transient关键字修饰，这也意味着在序列化时该域是不会序列化的。 它的查找是分两半查找，先判断index是在链表的哪一半，然后再去对应区域查找，这样最多只要遍历链表的一半节点即可找到 HashMap实现原理要点概括 HashMap是基于哈希表的Map接口的非同步实现，允许使用null值和null键，但不保证映射的顺序。 底层使用数组实现，数组中每一项是个单向链表，即数组和链表的结合体；当链表长度大于8时，链表转换为红黑树，这样减少链表查询时间。 HashMap在底层将key-value当成一个整体进行处理，这个整体就是一个Node对象。HashMap底层采用一个Node[]数组来保存所有的key-value对，当需要存储一个Node对象时，会根据key的hash算法来决定其在数组中的存储位置，再根据equals方法决定其在该数组位置上的链表中的存储位置；当需要取出一个Node时，也会根据key的hash算法找到其在数组中的存储位置，再根据equals方法从该位置上的链表中取出该Node。 HashMap解决hash冲突是采用了链地址法，也就是数组+链表的方式， HashMap进行数组扩容需要重新计算扩容后每个元素在数组中的位置，很耗性能 采用了Fail-Fast机制，通过一个modCount值记录修改次数，对HashMap内容的修改都将增加这个值。迭代器初始化过程中会将这个值赋给迭代器的expectedModCount，在迭代过程中，判断modCount跟expectedModCount是否相等，如果不相等就表示已经有其他线程修改了Map，马上抛出异常 重写equals方法需同时重写hashCode方法 public static void main(String []args){ HashMap&lt;Person,String&gt; map = new HashMap&lt;Person, String&gt;(); Person person = new Person(1234,&quot;乔峰&quot;); //put到hashmap中去 map.put(person,&quot;天龙八部&quot;); //get取出，从逻辑上讲应该能输出“天龙八部” System.out.println(&quot;结果:&quot;+map.get(new Person(1234,&quot;乔峰&quot;))); } ​ 如果我们已经对HashMap的原理有了一定了解，这个结果就不难理解了。尽管我们在进行get和put操作的时候，使用的key从逻辑上讲是等值的（通过equals比较是相等的），但由于没有重写hashCode方法，所以put操作时，key(hashcode1)--&gt;hash--&gt;indexFor--&gt;最终索引位置 ，而通过key取出value的时候 key(hashcode2)--&gt;hash--&gt;indexFor--&gt;最终索引位置，由于hashcode1不等于hashcode2，导致没有定位到一个数组位置而返回逻辑上错误的值null（也有可能碰巧定位到一个数组位置，但是也会判断其entry的hash值是否相等。） 所以，在重写equals的方法的时候，必须注意重写hashCode方法，同时还要保证通过equals判断相等的两个对象，调用hashCode方法要返回同样的整数值。而如果equals判断不相等的两个对象，其hashCode可以相同（只不过会发生哈希冲突，应尽量避免）。 Hashtable实现原理要点概括 Hashtable是基于哈希表的Map接口的同步实现，不允许使用null值和null键，Hashtable中的映射不是有序的 底层使用数组实现，数组中每一项是个单链表，即数组和链表的结合体 Hashtable在底层将key-value当成一个整体进行处理，这个整体就是一个Entry对象。Hashtable底层采用一个Entry[]数组来保存所有的key-value对，当需要存储一个Entry对象时，会根据key的hash算法来决定其在数组中的存储位置，在根据equals方法决定其在该数组位置上的链表中的存储位置；当需要取出一个Entry时，也会根据key的hash算法找到其在数组中的存储位置，再根据equals方法从该位置上的链表中取出该Entry。 synchronized是针对整张Hash表的，即每次锁住整张表让线程独占 ConcurrentHashMap实现原理要点概括 ConcurrentHashMap允许多个修改操作并发进行，其关键在于使用了锁分离技术。 它使用了多个锁来控制对hash表的不同段进行的修改，每个段其实就是一个小的hashtable，它们有自己的锁。只要多个并发发生在不同的段上，它们就可以并发进行。 ConcurrentHashMap在底层将key-value当成一个整体进行处理，这个整体就是一个Entry对象。Hashtable底层采用一个Entry[]数组来保存所有的key-value对，当需要存储一个Entry对象时，会根据key的hash算法来决定其在数组中的存储位置，在根据equals方法决定其在该数组位置上的链表中的存储位置；当需要取出一个Entry时，也会根据key的hash算法找到其在数组中的存储位置，再根据equals方法从该位置上的链表中取出该Entry。 与HashMap不同的是，ConcurrentHashMap使用多个子Hash表，也就是段(Segment) ConcurrentHashMap完全允许多个读操作并发进行，读操作并不需要加锁。如果使用传统的技术，如HashMap中的实现，如果允许可以在hash链的中间添加或删除元素，读操作不加锁将得到不一致的数据。 ConcurrentHashMap 1.8为什么要使用CAS+Synchronized取代Segment+ReentrantLock ​ 大家应该都知道ConcurrentHashMap在1.8的时候有了很大的改动,当然,我这里要说的改动不是指链表长度大于8就转为红黑树这种常识,我要说的是ConcurrentHashMap在1.8为什么用CAS+Synchronized取代Segment+ReentrantLock了 ​ 首先,我假设你对CAS,Synchronized,ReentrantLock这些知识很了解,并且知道AQS,自旋锁,偏向锁,轻量级锁,重量级锁这些知识,也知道Synchronized和ReentrantLock在唤醒被挂起线程竞争的时候有什么区别 ​ 首先我们说下1.8以前的ConcurrentHashMap是怎么保证线程并发的,首先在初始化ConcurrentHashMap的时候,会初始化一个Segment数组,容量为16,而每个Segment呢,都继承了ReentrantLock类,也就是说每个Segment类本身就是一个锁,之后Segment内部又有一个table数组,而每个table数组里的索引数据呢,又对应着一个Node链表. ​ 那么这样的好处是什么呢?我先从老版本的添加流程说起吧,由于电脑里没有JDK1.7及以下的版本我没法给你看代码,所以使用文字描述的方式,首先,当我们使用put方法的时候,是对我们的key进行hash拿到一个整型,然后将整型对16取模,拿到对应的Segment,之后调用Segment的put方法,然后上锁,请注意,这里lock()的时候其实是this.lock(),也就是说,每个Segment的锁是分开的 ​ 其中一个上锁不会影响另一个,此时也就代表了我可以有十六个线程进来,而ReentrantLock上锁的时候如果只有一个线程进来,是不会有线程挂起的操作的,也就是说只需要在AQS里使用CAS改变一个state的值为1,此时就能对代码进行操作,这样一来,我们等于将并发量/16了. 好,说完了老版本的ConcurrentHashMap,我们再说说新版本的,请看下面的图: ​ 请注意Synchronized上锁的对象,请记住,Synchronized是靠对象的对象头和此对象对应的monitor来保证上锁的,也就是对象头里的重量级锁标志指向了monitor,而monitor呢,内部则保存了一个当前线程,也就是抢到了锁的线程. ​ 那么这里的这个f是什么呢?它是Node链表里的每一个Node,也就是说,Synchronized是将每一个Node对象作为了一个锁,这样做的好处是什么呢?将锁细化了,也就是说,除非两个线程同时操作一个Node,注意,是一个Node而不是一个Node链表哦,那么才会争抢同一把锁. ​ 如果使用ReentrantLock其实也可以将锁细化成这样的,只要让Node类继承ReentrantLock就行了,这样的话调用f.lock()就能做到和Synchronized(f)同样的效果,但为什么不这样做呢? ​ 请大家试想一下,锁已经被细化到这种程度了,那么出现并发争抢的可能性还高吗?还有就是,哪怕出现争抢了,只要线程可以在30到50次自旋里拿到锁,那么Synchronized就不会升级为重量级锁,而等待的线程也就不用被挂起,我们也就少了挂起和唤醒这个上下文切换的过程开销. ​ 但如果是ReentrantLock呢?它则只有在线程没有抢到锁,然后新建Node节点后再尝试一次而已,不会自旋,而是直接被挂起,这样一来,我们就很容易会多出线程上下文开销的代价.当然,你也可以使用tryLock(),但是这样又出现了一个问题,你怎么知道tryLock的时间呢?在时间范围里还好,假如超过了呢? ​ 所以,在锁被细化到如此程度上,使用Synchronized是最好的选择了.这里再补充一句,Synchronized和ReentrantLock他们的开销差距是在释放锁时唤醒线程的数量,Synchronized是唤醒锁池里所有的线程+刚好来访问的线程,而ReentrantLock则是当前线程后进来的第一个线程+刚好来访问的线程. ​ 如果是线程并发量不大的情况下,那么Synchronized因为自旋锁,偏向锁,轻量级锁的原因,不用将等待线程挂起,偏向锁甚至不用自旋,所以在这种情况下要比ReentrantLock高效 1.8弃用分段锁的原因由以下几点： 1. 加入多个分段锁浪费内存空间。 2. 生产环境中， map 在放入时竞争同一个锁的概率非常小，分段锁反而会造成更新等操作的长时间等待。 3. 为了提高 GC 的效率 HashSet实现原理要点概括 HashSet由哈希表(实际上是一个HashMap实例)支持，不保证set的迭代顺序，并允许使用null元素。 对于HashSet而言，它是基于HashMap实现的，HashSet底层使用HashMap来保存所有元素，因此HashSet 的实现比较简单，相关HashSet的操作，基本上都是直接调用底层HashMap的相关方法来完成，API也是对HashMap的行为进行了封装，可参考HashMap LinkedHashMap实现原理要点概括 LinkedHashMap继承于HashMap，底层使用哈希表和双向链表来保存所有元素，并且它是非同步，允许使用null值和null键。 基本操作与父类HashMap相似，通过重写HashMap相关方法，重新定义了数组中保存的元素Entry，来实现自己的链接列表特性。该Entry除了保存当前对象的引用外，还保存了其上一个元素before和下一个元素after的引用，从而构成了双向链接列表。 LinkedHashSet实现原理要点概括 对于LinkedHashSet而言，它继承与HashSet、又基于LinkedHashMap来实现的。LinkedHashSet底层使用LinkedHashMap来保存所有元素，它继承与HashSet，其所有的方法操作上又与HashSet相同。 java集合遍历的几种方式总结及比较 集合类的通用遍历方式, 用迭代器迭代: Iterator it = list.iterator(); while(it.hasNext()) { Object obj = it.next(); } Map遍历方式： 1、通过获取所有的key按照key来遍历 //Set&lt;Integer&gt; set = map.keySet(); //得到所有key的集合 for (Integer in : map.keySet()) { String str = map.get(in);//得到每个key多对用value的值 } 2、通过Map.entrySet使用iterator遍历key和value Iterator&lt;Map.Entry&lt;Integer, String&gt;&gt; it = map.entrySet().iterator(); while (it.hasNext()) { Map.Entry&lt;Integer, String&gt; entry = it.next(); System.out.println(&quot;key= &quot; + entry.getKey() + &quot; and value= &quot; + entry.getValue()); } 3、通过Map.entrySet遍历key和value，推荐，尤其是容量大时 for (Map.Entry&lt;Integer, String&gt; entry : map.entrySet()) { //Map.entry&lt;Integer,String&gt; 映射项（键-值对） 有几个方法：用上面的名字entry //entry.getKey() ;entry.getValue(); entry.setValue(); //map.entrySet() 返回此映射中包含的映射关系的 Set视图。 System.out.println(&quot;key= &quot; + entry.getKey() + &quot; and value= &quot; + entry.getValue()); } 4、通过Map.values()遍历所有的value，但不能遍历key for (String v : map.values()) { System.out.println(&quot;value= &quot; + v); } List遍历方式： 第一种： Iterator iterator = list.iterator(); while(iterator.hasNext()){ int i = (Integer) iterator.next(); System.out.println(i); } 第二种： for (Object object : list) { System.out.println(object); } 第三种： for(int i = 0 ;i&lt;list.size();i++) { int j= (Integer) list.get(i); System.out.println(j); } 每个遍历方法的实现原理是什么？ 传统的for循环遍历，基于计数器的： ​ 遍历者自己在集合外部维护一个计数器，然后依次读取每一个位置的元素，当读取到最后一个元素后，停止。主要就是需要按元素的位置来读取元素。 迭代器遍历，Iterator： ​ 每一个具体实现的数据集合，一般都需要提供相应的Iterator。相比于传统for循环，Iterator取缔了显式的遍历计数器。所以基于顺序存储集合的Iterator可以直接按位置访问数据。而基于链式存储集合的Iterator，正常的实现，都是需要保存当前遍历的位置。然后根据当前位置来向前或者向后移动指针。 foreach循环遍历： ​ 根据反编译的字节码可以发现，foreach内部也是采用了Iterator的方式实现，只不过Java编译器帮我们生成了这些代码。 各遍历方式对于不同的存储方式，性能如何？ 1、传统的for循环遍历，基于计数器的： ​ 因为是基于元素的位置，按位置读取。所以我们可以知道，对于顺序存储，因为读取特定位置元素的平均时间复杂度是O(1)，所以遍历整个集合的平均时间复杂度为O(n)。而对于链式存储，因为读取特定位置元素的平均时间复杂度是O(n)，所以遍历整个集合的平均时间复杂度为O(n2)（n的平方）。 ArrayList按位置读取的代码：直接按元素位置读取。 transient Object[] elementData; public E get(int index) { rangeCheck(index); return elementData(index); } E elementData(int index) { return (E) elementData[index]; } LinkedList按位置读取的代码：每次都需要从第0个元素开始向后读取。其实它内部也做了小小的优化。 transient int size = 0; transient Node&lt;E&gt; first; transient Node&lt;E&gt; last; public E get(int index) { checkElementIndex(index); return node(index).item; } Node&lt;E&gt; node(int index) { if (index &lt; (size &gt;&gt; 1)) { //查询位置在链表前半部分，从链表头开始查找 Node&lt;E&gt; x = first; for (int i = 0; i &lt; index; i++) x = x.next; return x; } else { //查询位置在链表后半部分，从链表尾开始查找 Node&lt;E&gt; x = last; for (int i = size - 1; i &gt; index; i--) x = x.prev; return x; } } 2、迭代器遍历，Iterator： ​ 那么对于RandomAccess类型的集合来说，没有太多意义，反而因为一些额外的操作，还会增加额外的运行时间。但是对于Sequential Access的集合来说，就有很重大的意义了，因为Iterator内部维护了当前遍历的位置，所以每次遍历，读取下一个位置并不需要从集合的第一个元素开始查找，只要把指针向后移一位就行了，这样一来，遍历整个集合的时间复杂度就降低为O(n)； （这里只用LinkedList做例子）LinkedList的迭代器，内部实现，就是维护当前遍历的位置，然后操作指针移动就可以了： public E next() { checkForComodification(); if (!hasNext()) throw new NoSuchElementException(); lastReturned = next; next = next.next; nextIndex++; return lastReturned.item; } public E previous() { checkForComodification(); if (!hasPrevious()) throw new NoSuchElementException(); lastReturned = next = (next == null) ? last : next.prev; nextIndex--; return lastReturned.item; } 3、foreach循环遍历： ​ 分析Java字节码可知，foreach内部实现原理，也是通过Iterator实现的，只不过这个Iterator是Java编译器帮我们生成的，所以我们不需要再手动去编写。但是因为每次都要做类型转换检查，所以花费的时间比Iterator略长。时间复杂度和Iterator一样。 Iterator和foreach字节码如下： //使用Iterator的字节码： Code: 0: new #16 // class java/util/ArrayList 3: dup 4: invokespecial #18 // Method java/util/ArrayList.&quot;&lt;init&gt;&quot;:()V 7: astore_1 8: aload_1 9: invokeinterface #19, 1 // InterfaceMethod java/util/List.iterator:()Ljava/util/Iterator; 14: astore_2 15: goto 25 18: aload_2 19: invokeinterface #25, 1 // InterfaceMethod java/util/Iterator.next:()Ljava/lang/Object; 24: pop 25: aload_2 26: invokeinterface #31, 1 // InterfaceMethod java/util/Iterator.hasNext:()Z 31: ifne 18 34: return //使用foreach的字节码： Code: 0: new #16 // class java/util/ArrayList 3: dup 4: invokespecial #18 // Method java/util/ArrayList.&quot;&lt;init&gt;&quot;:()V 7: astore_1 8: aload_1 9: invokeinterface #19, 1 // InterfaceMethod java/util/List.iterator:()Ljava/util/Iterator; 14: astore_3 15: goto 28 18: aload_3 19: invokeinterface #25, 1 // InterfaceMethod java/util/Iterator.next:()Ljava/lang/Object; 24: checkcast #31 // class loop/Model 27: astore_2 28: aload_3 29: invokeinterface #33, 1 // InterfaceMethod java/util/Iterator.hasNext:()Z 34: ifne 18 37: return 各遍历方式的适用于什么场合？ 传统的for循环遍历，基于计数器的： ​ 顺序存储：读取性能比较高。适用于遍历顺序存储集合。 ​ 链式存储：时间复杂度太大，不适用于遍历链式存储的集合。 迭代器遍历，Iterator： ​ 顺序存储：如果不是太在意时间，推荐选择此方式，毕竟代码更加简洁，也防止了Off-By-One的问题。 ​ 链式存储：意义就重大了，平均时间复杂度降为O(n)，还是挺诱人的，所以推荐此种遍历方式。 foreach循环遍历： ​ foreach只是让代码更加简洁了，但是他有一些缺点，就是遍历过程中不能操作数据集合（删除等），所以有些场合不使用。而且它本身就是基于Iterator实现的，但是由于类型转换的问题，所以会比直接使用Iterator慢一点，但是还好，时间复杂度都是一样的。所以怎么选择，参考上面两种方式，做一个折中的选择。 RandomAccess接口标记 Java数据集合框架中，提供了一个RandomAccess接口，该接口没有方法，只是一个标记。通常被List接口的实现使用，用来标记该List的实现是否支持Random Access。 一个数据集合实现了该接口，就意味着它支持Random Access，按位置读取元素的平均时间复杂度为O(1)。比如ArrayList。 而没有实现该接口的，就表示不支持Random Access。比如LinkedList。 所以看来JDK开发者也是注意到这个问题的，那么推荐的做法就是，如果想要遍历一个List，那么先判断是否支持Random Access，也就是 list instanceof RandomAccess。 比如： if (list instanceof RandomAccess) { //使用传统的for循环遍历。 } else { //使用Iterator或者foreach。 } Array、List、Set互转 Array、List互转 Array 转List String[] s = new String[]{&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;,&quot;E&quot;}; List&lt;String&gt; list = Arrays.asList(s); # 注意这里list里面的元素直接是s里面的元素( list backed by the specified array)，换句话就是说：对s的修改，直接影响list。 s[0] =&quot;AA&quot;; System.out.println(&quot;list: &quot; + list); # 输出结果 list: [AA, B, C, D, E] List转Array String[] dest = list.toArray(new String[0]);//new String[0]是指定返回数组的类型 System.out.println(&quot;dest: &quot; + Arrays.toString(dest)); # 输出结果 dest: [AA, B, C, D, E] # 注意这里的dest里面的元素不是list里面的元素，换句话就是说：对list中关于元素的修改，不会影响dest。 list.set(0, &quot;Z&quot;); System.out.println(&quot;modified list: &quot; + list); System.out.println(&quot;dest: &quot; + Arrays.toString(dest)); # 输出结果 modified list: [Z, B, C, D, E] dest: [AA, B, C, D, E] # 可以看到list虽然被修改了，但是dest数组没有没修改。 List、Set互转 因为List和Set都实现了Collection接口，且addAll(Collection&lt;? extends E&gt; c);方法，因此可以采用addAll()方法将List和Set互相转换；另外，List和Set也提供了Collection&lt;? extends E&gt; c作为参数的构造函数，因此通常采用构造函数的形式完成互相转化。 //List转Set Set&lt;String&gt; set = new HashSet&lt;&gt;(list); System.out.println(&quot;set: &quot; + set); //Set转List List&lt;String&gt; list_1 = new ArrayList&lt;&gt;(set); System.out.println(&quot;list_1: &quot; + list_1); # 和toArray()一样，被转换的List(Set)的修改不会对被转化后的Set（List）造成影响。 Array、Set互转 //array转set s = new String[]{&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;,&quot;E&quot;}; set = new HashSet&lt;&gt;(Arrays.asList(s)); System.out.println(&quot;set: &quot; + set); //set转array dest = set.toArray(new String[0]); System.out.println(&quot;dest: &quot; + Arrays.toString(dest)); Java 中初始化 List 集合的 6 种方式! 常规方式 # 后面缺失的泛型类型在 JDK 7 之后就可以不用写具体的类型了，改进后会自动推断类型 List&lt;String&gt; list = new ArrayList&lt;&gt;(); list.add(&quot;1&quot;); list.add(&quot;2&quot;); list.add(&quot;3&quot;); Arrays 工具类 import static java.util.Arrays.asList; List&lt;String&gt; list = asList(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;); # 注意，上面的 asList 是 Arrays 的静态方法，这里使用了静态导入。这种方式添加的是不可变的 List, 即不能添加、删除等操作，需要警惕。。 # 如果要可变，那就使用 ArrayList 再包装一下，如下面所示。 List&lt;String&gt; numbers = new ArrayList&lt;&gt;(Arrays.asList(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;)); numbers.add(&quot;4&quot;); Collections 工具类 List&lt;String&gt; list = Collections.nCopies(3, &quot;list&quot;); # 这种方式添加的是不可变的、复制某个元素N遍的工具类，以上程序输出： # [list, list, list] # 老规则，如果要可变，使用 ArrayList 包装一遍。 List&lt;String&gt; list = new ArrayList&lt;&gt;(Collections.nCopies(3, &quot;list&quot;)); list.add(&quot;list&quot;); # 还有初始化单个对象的 List 工具类，这种方式也是不可变的，集合内只能有一个元素，这种也用得很少啊。 List&lt;String&gt; list = Collections.singletonList(&quot;list&quot;); # 还有一个创建空 List 的工具类，没有默认容量，节省空间，但不知道实际工作中没有用。 List&lt;String&gt; list = Collections.emptyList(&quot;list&quot;); 匿名内部类 List&lt;String&gt; list = new ArrayList&lt;&gt;() {{ add(&quot;1&quot;); add(&quot;2&quot;); add(&quot;3&quot;); }}; 这种其实有效率和内存泄漏的问题，参考： https://blog.csdn.net/xukun5137/article/details/78275201 https://www.cnblogs.com/wenbronk/p/7000643.html JDK8 Stream import static java.util.stream.Collectors.toList; List&lt;String&gt; list = Stream.of(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;).collect(toList()); JDK 9 List.of List&lt;String&gt; list = List.of(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;); 这是 JDK 9 里面新增的 List 接口里面的静态方法，同样也是不可变的。 ","link":"https://tianxiawuhao.github.io/FRRzAbqMV/"},{"title":"Java线程同步五种方法","content":"线程同步 即当有一个线程在对内存进行操作时，其他线程都不可以对这个内存地址进行操作，直到该线程完成操作， 其他线程才能对该内存地址进行操作，而其他线程又处于等待状态，实现线程同步的方法有很多，下面介绍java线程同步的五种方法。 同步方法 使用synchronized关键字修饰的方法。由于java的每个对象都有一个内置锁，当用关键字修饰此方法时，内置锁会保护整个方法。在调用该方法前，需要获得内置锁，否则该线程就处于阻塞状态。 public synchronized void method(){} # synchronized关键字也可以修饰静态方法，此时如果调用该静态方法，将会锁住整个类。 public static synchronized void method(){} 同步代码块 即有synchronized关键字修饰的语句块。被关键字修饰的的语句块会自动加上内置锁，从而实现同步。 synchronized(object){ # 代码 } 注：同步是一种高开销的操作，因此应该尽量减少同步的内容。通常没有必要去同步整个方法，使用关键字synchronized修饰关键代码即可。 使用特殊域变量修饰符volatile实现线程同步 （1）volatile关键字是非锁，比synchronized稍弱的同步机制 （2）保证此变量对所有的线程的可见性，当一个线程修改了这个变量的值，volatile 保证了新值能立即同步到主内存，以及每次使用前立即从主内存刷新。 （3）volatile不会提供原子操作，并不能保证线程安全，也不能用来修饰final类型的变量。 （4）禁止指令重排序优化。有volatile修饰的变量，赋值后多执行了一个添加内存屏障操作（指令重排序时不能把后面的指令重排序到内存屏障之前的位置） public class Test extends Thread { volatile int x = 0; //此处可以将volatile去除 或者 替换为 static，经过对比可看出volatile的作用 private void write() { x = 5; } private void read() { while (x != 5) {} if(x == 5){ System.out.println(&quot;------stoped&quot;); } } public static void main(String[] args) throws Exception { Test example = new Test(); Thread writeThread = new Thread(new Runnable() { public void run() { example.write(); } }); Thread readThread = new Thread(new Runnable() { public void run() { example.read(); } }); readThread.start(); TimeUnit.SECONDS.sleep(5); //记住此处一定要暂停5秒，以保证writeThread一定会在readThread中执行 System.out.println(&quot;------&quot;); writeThread.start(); } } 注：多线程中的非同步问题主要出现在对域的读写上，如果域自身避免这个问题，那么就不需要修改操作该域的方法。用final域，有锁保护的域可以避免非同步的问题。 使用重入锁ReentrantLock实现线程同步 jdk1.5中java.util.concurrent包下ReentrantLock类是可重入、互斥、实现了Lock接口的锁。它与synchronized修饰的方法具有相同的基本行为与语义，并且扩展了其能力。 ReentrantLock类的常用方法有： （1）ReentrantLock()：创建一个ReentrantLock实例。 （2）lock():获得锁 （3）unlock()：释放锁 上述代码可以修改为： public class Test extends Thread { private int x = 0; /** * 使用可重入锁 */ private Lock lock=new ReentrantLock(); private void write() { lock.lock(); try { x = 5; } catch (Exception e) { // TODO Auto-generated catch block e.printStackTrace(); }finally { lock.unlock();//释放锁 } } private void read() { lock.lock(); try { while (x != 5) {} if(x == 5){ System.out.println(&quot;------stoped&quot;); } } catch (Exception e) { // TODO Auto-generated catch block e.printStackTrace(); }finally { lock.unlock();//释放锁 } } public static void main(String[] args) throws Exception { Test example = new Test(); Thread writeThread = new Thread(new Runnable() { public void run() { example.write(); } }); Thread readThread = new Thread(new Runnable() { public void run() { example.read(); } }); readThread.start(); TimeUnit.SECONDS.sleep(5); //记住此处一定要暂停5秒，以保证writeThread一定会在readThread中执行 System.out.println(&quot;------&quot;); writeThread.start(); } } 注： ReentrantLock()还可以通过public ReentrantLock(boolean fair)构造方法创建公平锁，即，优先运行等待时间最长的线程，这样大幅度降低程序运行效率。 关于Lock对象和synchronized关键字的选择： （1）最好两个都不用，使用一种java.util.concurrent包提供的机制，能够帮助用户处理所有与锁相关的代码。 （2）如果synchronized关键字能够满足用户的需求，就用synchronized，他能简化代码。 （3）如果需要使用更高级的功能，就用ReentrantLock类，此时要注意及时释放锁，否则会出现死锁，通常在finally中释放锁。 使用ThreadLocal管理局部变量实现线程同步 ThreadLocal管理变量，则每一个使用该变量的线程都获得一个该变量的副本，副本之间相互独立，这样每一个线程都可以随意修改自己的副本，而不会对其他线程产生影响。 ThreadLocal类常用的方法： get()：返回该线程局部变量的当前线程副本中的值。 initialValue()：返回此线程局部变量的当前线程的”初始值“。 remove()：移除此线程局部变量当前线程的值。 set(T value)：将此线程局部变量的当前线程副本中的值设置为指定值value。 上述代码修改为： public class Test extends Thread { private static ThreadLocal&lt;Integer&gt; count=new ThreadLocal&lt;Integer&gt;(){ @Override protected Integer initialValue() { // TODO Auto-generated method stub return 0; } }; private void write() { count.set(count.get()+5); } private void read() { while (count.get() != 5) {} if(count.get() == 5){ System.out.println(&quot;------stoped&quot;); } } public static void main(String[] args) throws Exception { Test example = new Test(); Thread writeThread = new Thread(new Runnable() { public void run() { example.write(); } }); Thread readThread = new Thread(new Runnable() { public void run() { example.read(); } }); readThread.start(); TimeUnit.SECONDS.sleep(5); //记住此处一定要暂停5秒，以保证writeThread一定会在readThread中执行 System.out.println(&quot;------&quot;); writeThread.start(); } } ","link":"https://tianxiawuhao.github.io/fE1AHZqDl/"},{"title":"ThreadLocal是什么","content":" ThreadLocal是一个本地线程副本变量工具类。主要用于将私有线程和该线程存放的副本对象做一个映射，各个线程之间的变量互不干扰，在高并发场景下，可以实现无状态的调用，特别适用于各个线程依赖不通的变量值完成操作的场景。 数据结构 下图为ThreadLocal的内部结构图 ThreadLocal结构内部核心机制: 每个Thread线程内部都有一个Map。 Map里面存储线程本地对象（key）和线程的变量副本（value） Thread内部的Map是由ThreadLocal维护的，由ThreadLocal负责向map获取和设置线程的变量值。 所以对于不同的线程，每次获取副本值时，别的线程并不能获取到当前线程的副本值，形成了副本的隔离，互不干扰。 Thread线程内部的Map在类中描述如下： public class Thread implements Runnable { /* ThreadLocal values pertaining to this thread. This map is maintained * by the ThreadLocal class. */ ThreadLocal.ThreadLocalMap threadLocals = null; } 深入解析ThreadLocal ThreadLocal类提供如下几个核心方法： public T get() public void set(T value) public void remove() # get()方法用于获取当前线程的副本变量值。 # set()方法用于保存当前线程的副本变量值。 # initialValue()为当前线程初始副本变量值。 # remove()方法移除当前前程的副本变量值。 get()方法 /** * Returns the value in the current thread's copy of this * thread-local variable. If the variable has no value for the * current thread, it is first initialized to the value returned * by an invocation of the {@link #initialValue} method. * * @return the current thread's value of this thread-local */ public T get() { Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) { ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) return (T)e.value; } return setInitialValue(); } ThreadLocalMap getMap(Thread t) { return t.threadLocals; } private T setInitialValue() { T value = initialValue(); Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); return value; } protected T initialValue() { return null; } 步骤： 获取当前线程的ThreadLocalMap对象threadLocals 从map中获取线程存储的K-V Entry节点。 从Entry节点获取存储的Value副本值返回。 map为空的话返回初始值null，即线程变量副本为null，在使用时需要注意判断NullPointerException。 set()方法 /** * Sets the current thread's copy of this thread-local variable * to the specified value. Most subclasses will have no need to * override this method, relying solely on the {@link #initialValue} * method to set the values of thread-locals. * * @param value the value to be stored in the current thread's copy of * this thread-local. */ public void set(T value) { Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); } ThreadLocalMap getMap(Thread t) { return t.threadLocals; } void createMap(Thread t, T firstValue) { t.threadLocals = new ThreadLocalMap(this, firstValue); } 步骤： 获取当前线程的成员变量map map非空，则重新将ThreadLocal和新的value副本放入到map中。 map空，则对线程的成员变量ThreadLocalMap进行初始化创建，并将ThreadLocal和value副本放入map中。 remove()方法 remove方法比较简单，不做赘述。 ThreadLocalMap ThreadLocalMap是ThreadLocal的内部类，没有实现Map接口，用独立的方式实现了Map的功能，其内部的Entry也独立实现。 ThreadLocalMap类图 在ThreadLocalMap中，也是用Entry来保存K-V结构数据的。但是Entry中key只能是ThreadLocal对象，这点被Entry的构造方法已经限定死了。 static class Entry extends WeakReference&lt;ThreadLocal&gt; { /** The value associated with this ThreadLocal. */ Object value; Entry(ThreadLocal k, Object v) { super(k); value = v; } } Entry继承自WeakReference（弱引用，生命周期只能存活到下次GC前），但只有Key是弱引用类型的，Value并非弱引用。 ThreadLocalMap的成员变量： static class ThreadLocalMap { /** * The initial capacity -- MUST be a power of two. */ private static final int INITIAL_CAPACITY = 16; /** * The table, resized as necessary. * table.length MUST always be a power of two. */ private Entry[] table; /** * The number of entries in the table. */ private int size = 0; /** * The next size value at which to resize. */ private int threshold; // Default to 0 } Hash冲突怎么解决 和HashMap的最大的不同在于，ThreadLocalMap结构非常简单，没有next引用，也就是说ThreadLocalMap中解决Hash冲突的方式并非链表的方式，而是采用线性探测的方式，所谓线性探测，就是根据初始key的hashcode值确定元素在table数组中的位置，如果发现这个位置上已经有其他key值的元素被占用，则利用固定的算法寻找一定步长的下个位置，依次判断，直至找到能够存放的位置。 ThreadLocalMap解决Hash冲突的方式就是简单的步长加1或减1，寻找下一个相邻的位置。 /** * Increment i modulo len. */ private static int nextIndex(int i, int len) { return ((i + 1 &lt; len) ? i + 1 : 0); } /** * Decrement i modulo len. */ private static int prevIndex(int i, int len) { return ((i - 1 &gt;= 0) ? i - 1 : len - 1); } 显然ThreadLocalMap采用线性探测的方式解决Hash冲突的效率很低，如果有大量不同的ThreadLocal对象放入map中时发送冲突，或者发生二次冲突，则效率很低。 所以这里引出的良好建议是：每个线程只存一个变量，这样的话所有的线程存放到map中的Key都是相同的ThreadLocal，如果一个线程要保存多个变量，就需要创建多个ThreadLocal，多个ThreadLocal放入Map中时会极大的增加Hash冲突的可能。 ThreadLocalMap的问题 由于ThreadLocalMap的key是弱引用，而Value是强引用。这就导致了一个问题，ThreadLocal在没有外部对象强引用时，发生GC时弱引用Key会被回收，而Value不会回收，如果创建ThreadLocal的线程一直持续运行，那么这个Entry对象中的value就有可能一直得不到回收，发生内存泄露。 如何避免泄漏 既然Key是弱引用，那么我们要做的事，就是在调用ThreadLocal的get()、set()方法时完成后再调用remove方法，将Entry节点和Map的引用关系移除，这样整个Entry对象在GC Roots分析后就变成不可达了，下次GC的时候就可以被回收。 如果使用ThreadLocal的set方法之后，没有显示的调用remove方法，就有可能发生内存泄露，所以养成良好的编程习惯十分重要，使用完ThreadLocal之后，记得调用remove方法。 ThreadLocal&lt;Session&gt; threadLocal = new ThreadLocal&lt;Session&gt;(); try { threadLocal.set(new Session(1, &quot;Misout的博客&quot;)); // 其它业务逻辑 } finally { threadLocal.remove(); } 应用场景 还记得Hibernate的session获取场景吗？ private static final ThreadLocal&lt;Session&gt; threadLocal = new ThreadLocal&lt;Session&gt;(); //获取Session public static Session getCurrentSession(){ Session session = threadLocal.get(); //判断Session是否为空，如果为空，将创建一个session，并设置到本地线程变量中 try { if(session ==null&amp;&amp;!session.isOpen()){ if(sessionFactory==null){ rbuildSessionFactory();// 创建Hibernate的SessionFactory }else{ session = sessionFactory.openSession(); } } threadLocal.set(session); } catch (Exception e) { // TODO: handle exception } return session; } 为什么？每个线程访问数据库都应当是一个独立的Session会话，如果多个线程共享同一个Session会话，有可能其他线程关闭连接了，当前线程再执行提交时就会出现会话已关闭的异常，导致系统异常。此方式能避免线程争抢Session，提高并发下的安全性。 使用ThreadLocal的典型场景正如上面的数据库连接管理，线程会话管理等场景，只适用于独立变量副本的情况，如果变量为全局共享的，则不适用在高并发下使用。 总结 每个ThreadLocal只能保存一个变量副本，如果想要上线一个线程能够保存多个副本以上，就需要创建多个ThreadLocal。 ThreadLocal内部的ThreadLocalMap键为弱引用，会有内存泄漏的风险。 适用于无状态，副本变量独立后不影响业务逻辑的高并发场景。如果如果业务逻辑强依赖于副本变量，则不适合用ThreadLocal解决，需要另寻解决方案。 ","link":"https://tianxiawuhao.github.io/cQAJ3QFSZ/"},{"title":"HashMap负载因子为什么是0.75，链表转红黑树阈值为什么是8","content":"1.HashMap负载因子为什么是0.75 这是时间与空间成本上的折中。 1.1时间成本 假设负载因子是1时，虽然空间利用率高了，但是随之提高的是哈希碰撞的概率。 而Hashmap中哈希碰撞的解决方法采用的拉链法，哈希冲突高了会导致链表越来越长（虽然后面会转换成红黑树），我们知道链表的查询效率是比较低的，所以负载因子太高会导致时间成本上升。 1.2空间成本 那么为了减少哈希冲突，提高查询效率，负载因子是不是越低越好呢？答案显然是否定的。 假设负载因子为0.5时，那么空间利用率只有50%。例如大小为64时，至少有32没有被利用，大小为1024时，就有512没有被利用。扩容后的空间越大，空出的空间也就越大。 所以Java开发人员经过权衡，负载因子不能太大也不能太小，折中选择为0.75。 2.链表转红黑树阈值为什么是8 当负载因子是0.75的情况下，哈希碰撞的概率遵循参数约为0.5的泊松分布 这是概率论的范畴，不知道的同学只需要记住当负载因子是0.75的情况下，我们能够计算出哈希碰撞的概率 Because TreeNodes are about twice the size of regular nodes, we use them only when bins contain enough nodes to warrant use (see TREEIFY_THRESHOLD). And when they become too small (due to removal or resizing) they are converted back to plain bins. In usages with well-distributed user hashCodes, tree bins are rarely used. Ideally, under random hashCodes, the frequency of nodes in bins follows a Poisson distribution (http://en.wikipedia.org/wiki/Poisson_distribution) with a parameter of about 0.5 on average for the default resizing threshold of 0.75, although with a large variance because of resizing granularity. Ignoring variance, the expected occurrences of list size k are (exp(-0.5)*pow(0.5, k)/factorial(k)). The first values are: 0: 0.60653066 1: 0.30326533 2: 0.07581633 3: 0.01263606 4: 0.00157952 5: 0.00015795 6: 0.00001316 7: 0.00000094 8: 0.00000006 more: less than 1 in ten million 在HashMap源码中，给出了计算出的哈希碰撞的概率。 我们看到碰撞8次(链表长度达到8)的概率为0.00000006,几乎是一个不可能事件。 所以Java开发人员将链表转红黑树阈值默认设为8，是为了避免链表转红黑树这种耗时操作的事件发生。 ","link":"https://tianxiawuhao.github.io/mdZO-HjSu/"},{"title":"java中静态代码块，非静态代码块，构造函数之间的执行顺序","content":"它们之间的执行顺序为：静态代码块—&gt;非静态代码块—&gt;构造方法。 静态代码块只在第一次加载类的时候执行一次，之后不再执行；而非静态代码块和构造函数都是在每new一次就执行一次，只不过非静态代码块在构造函数之前执行而已。 如果存在子类，则加载顺序为先父类后子类。 看如下的代码： package com.ykp.test; class ClassA { public ClassA() { System.out.println(&quot;父类构造函数&quot;); } { System.out.println(&quot;父类非静态代码块1&quot;); } { System.out.println(&quot;父类非静态代码块2&quot;); } static { System.out.println(&quot;父类静态代码块 1&quot;); } static { System.out.println(&quot;父类静态代码块 2&quot;); } } public class ClassB extends ClassA { public ClassB() { System.out.println(&quot;子类构造函数&quot;); } { System.out.println(&quot;子类非静态代码块2&quot;); } { System.out.println(&quot;子类非静态代码块1&quot;); } static { System.out.println(&quot;子类静态代码块 2&quot;); } static { System.out.println(&quot;子类静态代码块 1&quot;); } public static void main(String[] args) { System.out.println(&quot;....主方法开始....&quot;); new ClassB(); System.out.println(&quot;************&quot;); new ClassB(); System.out.println(&quot;....主方法结束....&quot;); } } 执行结果： 父类静态代码块 1 父类静态代码块 2 子类静态代码块 2 子类静态代码块 1 ....主方法开始.... 父类非静态代码块1 父类非静态代码块2 父类构造函数 子类非静态代码块2 子类非静态代码块1 子类构造函数 ************ 父类非静态代码块1 父类非静态代码块2 父类构造函数 子类非静态代码块2 子类非静态代码块1 子类构造函数 ....主方法结束.... 从结果可以看出，首先加载类，加载的时候先加载父类，然后子类，类加载的时候就执行静态代码快，也就是说静态代码块是在类加载的时候就加载的，而且只加载一次。如果存在多个执行顺序按照代码的先后来。 对于非静态代码块，是在new的时候加载的，只是在构造函数之前加载而已。如果存在多个执行顺序按照代码的先后来。 ","link":"https://tianxiawuhao.github.io/8H8sHTUTf/"},{"title":"指令重排序","content":"数据依赖性 如果两个操作访问同一个变量，且这两个操作中有一个为写操作，此时这两个操作之间就存在数据依赖性。数据依赖分下列三种类型： 名称 代码示例 说明 写后读 a = 1;b = a; 写一个变量之后，再读这个位置。 写后写 a = 1;a = 2; 写一个变量之后，再写这个变量。 读后写 a = b;b = 1; 读一个变量之后，再写这个变量。 上面三种情况，只要重排序两个操作的执行顺序，程序的执行结果将会被改变。 前面提到过，编译器和处理器可能会对操作做重排序。编译器和处理器在重排序时，会遵守数据依赖性，编译器和处理器不会改变存在数据依赖关系的两个操作的执行顺序。 注意，这里所说的数据依赖性仅针对单个处理器中执行的指令序列和单个线程中执行的操作，不同处理器之间和不同线程之间的数据依赖性不被编译器和处理器考虑。 as-if-serial 语义 as-if-serial 语义的意思指：不管怎么重排序（编译器和处理器为了提高并行度），（单线程）程序的执行结果不能被改变。编译器，runtime 和处理器都必须遵守 as-if-serial 语义。 为了遵守 as-if-serial 语义，编译器和处理器不会对存在数据依赖关系的操作做重排序，因为这种重排序会改变执行结果。但是，如果操作之间不存在数据依赖关系，这些操作可能被编译器和处理器重排序。为了具体说明，请看下面计算圆面积的代码示例： double pi = 3.14; //A double r = 1.0; //B double area = pi * r * r; //C 上面三个操作的数据依赖关系如下图所示： 如上图所示，A 和 C 之间存在数据依赖关系，同时 B 和 C 之间也存在数据依赖关系。因此在最终执行的指令序列中，C 不能被重排序到 A 和 B 的前面（C 排到 A 和 B 的前面，程序的结果将会被改变）。但 A 和 B 之间没有数据依赖关系，编译器和处理器可以重排序 A 和 B 之间的执行顺序。下图是该程序的两种执行顺序： as-if-serial 语义把单线程程序保护了起来，遵守 as-if-serial 语义的编译器，runtime 和处理器共同为编写单线程程序的程序员创建了一个幻觉：单线程程序是按程序的顺序来执行的。as-if-serial 语义使单线程程序员无需担心重排序会干扰他们，也无需担心内存可见性问题。 程序顺序规则 根据 happens- before 的程序顺序规则，上面计算圆的面积的示例代码存在三个 happens- before 关系： A happens- before B； B happens- before C； A happens- before C； 这里的第3个 happens- before 关系，是根据 happens- before 的传递性推导出来的。 这里 A happens- before B，但实际执行时 B 却可以排在 A 之前执行（看上面的重排序后的执行顺序）。在第一章提到过，如果 A happens- before B，JMM 并不要求 A 一定要在 B 之前执行。JMM 仅仅要求前一个操作（执行的结果）对后一个操作可见，且前一个操作按顺序排在第二个操作之前。这里操作 A 的执行结果不需要对操作 B 可见；而且重排序操作 A 和操作 B 后的执行结果，与操作 A 和操作 B 按 happens- before 顺序执行的结果一致。在这种情况下，JMM 会认为这种重排序并不非法（not illegal），JMM 允许这种重排序。 在计算机中，软件技术和硬件技术有一个共同的目标：在不改变程序执行结果的前提下，尽可能的开发并行度。编译器和处理器遵从这一目标，从 happens- before 的定义我们可以看出，JMM 同样遵从这一目标。 重排序对多线程的影响 现在让我们来看看，重排序是否会改变多线程程序的执行结果。请看下面的示例代码： class ReorderExample { int a = 0; boolean flag = false; public void writer() { a = 1; //1 flag = true; //2 } Public void reader() { if (flag) { //3 int i = a * a; //4 …… } } } flag 变量是个标记，用来标识变量 a 是否已被写入。这里假设有两个线程 A 和 B，A 首先执行writer() 方法，随后 B 线程接着执行 reader() 方法。线程B在执行操作4时，能否看到线程 A 在操作1对共享变量 a 的写入？ 答案是：不一定能看到。 由于操作1和操作2没有数据依赖关系，编译器和处理器可以对这两个操作重排序；同样，操作3和操作4没有数据依赖关系，编译器和处理器也可以对这两个操作重排序。让我们先来看看，当操作1和操作2重排序时，可能会产生什么效果？请看下面的程序执行时序图： 如上图所示，操作1和操作2做了重排序。程序执行时，线程A首先写标记变量 flag，随后线程 B 读这个变量。由于条件判断为真，线程 B 将读取变量a。此时，变量 a 还根本没有被线程 A 写入，在这里多线程程序的语义被重排序破坏了！ ※注：本文统一用红色的虚箭线表示错误的读操作，用绿色的虚箭线表示正确的读操作。 下面再让我们看看，当操作3和操作4重排序时会产生什么效果（借助这个重排序，可以顺便说明控制依赖性）。下面是操作3和操作4重排序后，程序的执行时序图： 在程序中，操作3和操作4存在控制依赖关系。当代码中存在控制依赖性时，会影响指令序列执行的并行度。为此，编译器和处理器会采用猜测（Speculation）执行来克服控制相关性对并行度的影响。以处理器的猜测执行为例，执行线程 B 的处理器可以提前读取并计算 a*a，然后把计算结果临时保存到一个名为重排序缓冲（reorder buffer ROB）的硬件缓存中。当接下来操作3的条件判断为真时，就把该计算结果写入变量i中。 从图中我们可以看出，猜测执行实质上对操作3和4做了重排序。重排序在这里破坏了多线程程序的语义！ 在单线程程序中，对存在控制依赖的操作重排序，不会改变执行结果（这也是 as-if-serial 语义允许对存在控制依赖的操作做重排序的原因）；但在多线程程序中，对存在控制依赖的操作重排序，可能会改变程序的执行结果。 编译器指令重排 下面我们简单看一个编译器重排的例子： 线程 1 线程 2 1：x2 = a ; 3: x1 = b ; 2: b = 1; 4: a = 2 ; 两个线程同时执行，分别有1、2、3、4四段执行代码，其中1、2属于线程1 ， 3、4属于线程2 ，从程序的执行顺序上看，似乎不太可能出现x1 = 1 和x2 = 2 的情况，但实际上这种情况是有可能发现的，因为如果编译器对这段程序代码执行重排优化后，可能出现下列情况 线程 1 线程 2 2: b = 1; 4: a = 2 ; 1：x2 = a ; 3: x1 = b ; 这种执行顺序下就有可能出现x1 = 1 和x2 = 2 的情况，这也就说明在多线程环境下，由于编译器优化重排的存在，两个线程中使用的变量能否保证一致性是无法确定的。 源代码和Runtime时执行的代码很可能不一样，这是因为编译器、处理器常常会为了追求性能对改变执行顺序。然而改变顺序执行很危险，很有可能使得运行结果和预想的不一样，特别是当重排序共享变量时。 从源代码到Runtime需要经过三步的重排序： 编译器重排序 为了提高性能，在不改变单线程的执行结果下，可以改变语句执行顺序。 比如尽可能的减少寄存器的读写次数，充分利用局部性。像下面这段代码这样，交替的读x、y，会导致寄存器频繁的交替存储x和y，最糟的情况下寄存器要存储3次x和3次y。如果能让x的一系列操作一块做完，y的一块做完，理想情况下寄存器只需要存储1次x和1次y。 //优化前 int x = 1; int y = 2; int a1 = x * 1; int b1 = y * 1; int a2 = x * 2; int b2 = y * 2; int a3 = x * 3; int b3 = y * 3; //优化后 int x = 1; int y = 2; int a1 = x * 1; int a2 = x * 2; int a3 = x * 3; int b1 = y * 1; int b2 = y * 2; int b3 = y * 3; 指令重排序 指令重排序是处理器层面做的优化。处理器在执行时往往会因为一些限制而等待，如访存的地址不在cache中发生miss，这时就需要到内存甚至外存去取，然而内存和外区的读取速度比CPU执行速度慢得多。 早期处理器是顺序执行(in-order execution)的，在内存、外存读取数据这段时间，处理器就一直处于等待状态。现在处理器一般都是乱序执行(out-of-order execution)，处理器会在等待数据的时候去执行其他已准备好的操作，不会让处理器一直等待。 满足乱序执行的条件： 该缓存的操作数缓存好 有空闲的执行单元 对于下面这段汇编代码，操作1如果发生cache miss，则需要等待读取内存外存。看看有没有能优先执行的指令，操作2依赖于操作1，不能被优先执行，操作3不依赖1和2，所以能优先执行操作3。 所以实际执行顺序是3&gt;1&gt;2 LDR R1, [R0];//操作1 ADD R2, R1, R1;//操作2 ADD R3, R4, R4;//操作3 内存系统重排序 由于处理器有读、写缓存区，写缓存区没有及时刷新到内存，造成其他处理器读到的值不是最新的，使得处理器执行的读写操作与内存上反应出的顺序不一致。 如下面这个例子，可能造成处理器A读到的b=0，处理器B读到的a=0。A1写a=1先写到处理器A的写缓存区中，此时内存中a=0。如果这时处理器B从内存中读a，读到的将是0。 以处理器A来说，处理器A执行的顺序是A1&gt;A2&gt;A3，但是由于写缓存区没有及时刷新到内存，所以实际顺序为A2&gt;A1&gt;A3。 初始化： a = 0; b = 0; 处理器A执行 a = 1; //A1 read(b); //A2 处理器B执行 b = 2; //B1 read(a); //B2 阻止重排序 不论哪种重排序都可能造成共享变量中线程间不可见，这会改变程序运行结果。所以需要禁止对那些要求可见的共享变量重排序。 阻止编译重排序：禁止编译器在某些时候重排序。 阻止指令重排序和内存系统重排序：使用内存屏障或Lock前缀指令 ","link":"https://tianxiawuhao.github.io/J4bdQ7uDb/"},{"title":"happens-before原则和volatile","content":"基本概念 先补充一下概念：Java 内存模型中的可见性、原子性和有序性。 可见性： 可见性是一种复杂的属性，因为可见性中的错误总是会违背我们的直觉。通常，我们无法确保执行读操作的线程能适时地看到其他线程写入的值，有时甚至是根本不可能的事情。为了确保多个线程之间对内存写入操作的可见性，必须使用同步机制。 可见性，是指线程之间的可见性，一个线程修改的状态对另一个线程是可见的也就是一个线程修改的结果。另一个线程马上就能看到。比如：用volatile修饰的变量，就会具有可见性。volatile修饰的变量不允许线程内部缓存和重排序，即直接修改内存。所以对其他线程是可见的。但是这里需要注意一个问题，volatile只能让被他修饰内容具有可见性，但不能保证它具有原子性。比如 volatile int a = 0；之后有一个操作 a++；这个变量a具有可见性，但是a++ 依然是一个非原子操作，也就是这个操作同样存在线程安全问题。 在 Java 中 volatile、synchronized 和 final 实现可见性。 原子性： 原子是世界上的最小单位，具有不可分割性，比如 a=0；（a非long和double类型） 这个操作是不可分割的，那么我们说这个操作是原子操作。再比如：a++； 这个操作实际是a = a + 1；是可分割的，所以他不是一个原子操作。非原子操作都会存在线程安全问题，需要我们使用同步技术（sychronized）来让它变成一个原子操作。一个操作是原子操作，那么我们称它具有原子性。java的concurrent包下提供了一些原子类，我们可以通过阅读API来了解这些原子类的用法。比如：AtomicInteger、AtomicLong、AtomicReference等。 在 Java 中 synchronized 和在 lock、unlock 中操作保证原子性。 有序性： Java 语言提供了 volatile 和 synchronized 两个关键字来保证线程之间操作的有序性，volatile 是因为其本身包含“禁止指令重排序”的语义，synchronized 是由“一个变量在同一个时刻只允许一条线程对其进行 lock 操作”这条规则获得的，此规则决定了持有同一个对象锁的两个同步块只能串行执行。 happens-before 在学习Java内存模型(JMM, Java Memory Model)时，关于线程、主存(main memory)、工作内存(working memory)，这些概念都有着实体的相互对应，线程可能对应着一个内核线程，主存对应着内存，而工作内存则涵盖了写缓冲区、缓存(cache)、寄存器等一系列为了提高数据存取效率的暂存区域。但是，一提到happens-before原则，就让人有点“丈二和尚摸不着头脑”。这个涵盖了整个JMM中可见性原则的规则，究竟如何理解？ 两个操作间具有happens-before关系，并不意味着前一个操作必须要在后一个操作之前执行。happens-before仅仅要求前一个操作对后一个操作可见。happens-before原则和一般意义上的时间先后是不同的 有哪些happens-before规则 程序次序规则：在一个线程内一段代码的执行结果是有序的。就是还会指令重排，但是随便它怎么排，结果是按照我们代码的顺序生成的不会变。 管程锁定规则：就是无论是在单线程环境还是多线程环境，对于同一个锁来说，一个线程对这个锁解锁之后，另一个线程获取了这个锁都能看到前一个线程的操作结果！(管程是一种通用的同步原语，synchronized就是管程的实现） volatile变量规则：就是如果一个线程先去写一个volatile变量，然后一个线程去读这个变量，那么这个写操作的结果一定对读的这个线程可见。 线程启动规则：在主线程A执行过程中，启动子线程B，那么线程A在启动子线程B之前对共享变量的修改结果对线程B可见。 线程终止规则：在主线程A执行过程中，子线程B终止，那么线程B在终止之前对共享变量的修改结果在线程A中可见。也称线程join()规则。 线程中断规则：对线程interrupt()方法的调用先行发生于被中断线程代码检测到中断事件的发生，可以通过Thread.interrupted()检测到是否发生中断。 传递性规则：这个简单的，就是happens-before原则具有传递性，即hb(A, B) ， hb(B, C)，那么hb(A, C)。 对象终结规则：这个也简单的，就是一个对象的初始化的完成，也就是构造函数执行的结束一定 happens-before它的finalize()方法。 顺序一致性内存模型 顺序一致性内存模型是一个被计算机科学家理想化了的理论参考模型，它为程序员提供了极强的内存可见性保证。顺序一致性内存模型有两大特性： 一个线程中的所有操作必须按照程序的顺序来执行。 （不管程序是否同步）所有线程都只能看到一个单一的操作执行顺序。在顺序一致性内存模型中，每个操作都必须原子执行且立刻对所有线程可见。 在概念上，顺序一致性模型有一个单一的全局内存，这个内存通过一个开关可以连接到任意一个线程。同时，每一个线程必须按程序的顺序来执行内存读/写操作。在任意时间点最多只能有一个线程可以连接到内存。当多个线程并发执行时，开关装置能把所有线程的所有内存读/写操作串行化。 为了更好的理解，下面我们来对顺序一致性模型的特性做进一步的说明。 假设有两个线程A和B并发执行。其中A线程有三个操作，它们在程序中的顺序是：A1-&gt;A2-&gt;A3。B线程也有三个操作，它们在程序中的顺序是：B1-&gt;B2-&gt;B3。 假设这两个线程使用监视器来正确同步：A线程的三个操作执行后释放监视器，随后B线程获取同一个监视器。 现在我们再假设这两个线程没有做同步，未同步程序在顺序一致性模型中虽然整体执行顺序是无序的，但所有线程都只能看到一个一致的整体执行顺序。假设，线程A和B看到的执行顺序都是：B1-&gt;A1-&gt;A2-&gt;B2-&gt;A3-&gt;B3。之所以能得到这个保证是因为顺序一致性内存模型中的每个操作必须立即对任意线程可见。 但是，在JMM中就没有这个保证。未同步程序在JMM中不但整体的执行顺序是无序的，而且所有线程看到的操作执行顺序也可能不一致。比如，在当前线程把写过的数据缓存在本地内存中，且还没有刷新到主内存之前，这个写操作仅对当前线程可见；从其他线程的角度来观察，会认为这个写操作根本还没有被当前线程执行。只有当前线程把本地内存中写过的数据刷新到主内存之后，这个写操作才能对其他线程可见。在这种情况下，当前线程和其它线程看到的操作执行顺序将不一致。 Java内存模型 关于Java内存模型的书籍文章，汗牛充栋，想必大家也都有自己的理解。那就仅仅由上面的顺序一致性模型来引出JMM，看看具体区别在哪。 可以看出，本地内存是一个明显区别于顺序一致性内存模型的地方。事实上，造成可见性问题的根源之一，就在于这个本地内存（强调一下，包括缓存、写缓冲和寄存器等等）。本地内存使得每个线程都有了自己的私有存储，大部分时间对数据的存取工作都在这个区域完成。但是我们写一个数据，是直到数据写到主存中才算真正完成。实际上每个线程维护了一个副本，所有线程都在自己的本地内存中不断地读/写一个共享内存中的数据的副本。单线程情况下，这个副本不会造成任何问题；但一旦到多线程，有一个线程将变量写到主存，其他线程却不知道，其他线程的副本就都过期。比如，由于本地内存的存在，程序员写的一段代码，写一个普通的共享变量，其可能先被写到缓冲区，那指令完成的时间就被推迟了，实际表现也就是我们常说的“指令重排序”（这实际上是内存模型层面的重排序，重排序还可能是编译器、机器指令层级上的乱序）。 因此，在Java内存模型中，每个线程不再像顺序一致性模型中那样有确定的指令执行视图，一个指令可能被重排了。从一个线程的角度看，其他线程（甚至是这个线程本身）执行的指令顺序有多种可能性，也就是说，一个线程的执行结果对其他线程的可见性无法保证。 总结一下导致可见性问题的原因： 数据的写无法及时通知到别的线程，如写缓冲区的引入 线程不能及时读到其他线程对共享变量的修改，如缓存的使用 各种层级上对指令的重排序，导致指令执行的顺序无法确定 所以要解决可见性问题，本质是要让线程对共享变量的修改，及时同步到其他线程。我们所使用的硬件架构下，不具备顺序一致性内存模型的全局一致的指令执行顺序，讨论指令执行的时间先后并不存在意义或者说根本没办法确定时间上的先后。可以看看下面程序，每个线程中的flag副本会在多久后被更新呢？答案是：无法确定，看线程何时刷新自己的本地内存。 public class testVisibility { public static boolean flag = false; public static void main(String[] args) { List&lt;Thread&gt; thdList = new ArrayList&lt;Thread&gt;(); for(int i = 0; i &lt; 10; i++) { Thread t = new Thread(new Runnable(){ public void run() { while (true) { if (flag) { // 多运行几次，可能并不会打印出来也可能会打印出来 // 如果不打印，则表示Thread看到的仍然是本地内存中的flag // 可以尝试将flag变成volatile再运行几次看看 System.out.println(Thread.currentThread().getId() + &quot; is true now&quot;); } } } }); t.start(); thdList.add(t); } flag = true; System.out.println(&quot;set flag true&quot;); // 等待线程执行完毕 try { for (Thread t : thdList) { t.join(); } } catch (Exception e) { } } } 那么既然我们无法讨论指令执行的先后，也不需要讨论，我们实际只想知道某线程的操作对另一个线程是否可见，于是就规定了happens-before这个可见性原则，程序员可以基于这个原则进行可见性的判断。 Volatile原理 Java语言提供了一种稍弱的同步机制，即volatile变量，用来确保将变量的更新操作通知到其他线程。当把变量声明为volatile类型后，编译器与运行时都会注意到这个变量是共享的，因此不会将该变量上的操作与其他内存操作一起重排序。volatile变量不会被缓存在寄存器或者对其他处理器不可见的地方，因此在读取volatile类型的变量时总会返回最新写入的值。 在访问volatile变量时不会执行加锁操作，因此也就不会使执行线程阻塞，因此volatile变量是一种比sychronized关键字更轻量级的同步机制。 当对非 volatile 变量进行读写的时候，每个线程先从内存拷贝变量到CPU缓存中。如果计算机有多个CPU，每个线程可能在不同的CPU上被处理，这意味着每个线程可以拷贝到不同的 CPU cache 中。 而声明变量是 volatile 的，JVM 保证了每次读变量都从内存中读，跳过 CPU cache 这一步。 当一个变量定义为 volatile 之后，将具备两种特性： 1. 保证此变量对所有的线程的可见性，这里的“可见性”，如本文开头所述，当一个线程修改了这个变量的值，volatile 保证了新值能立即同步到主内存，以及每次使用前立即从主内存刷新。但普通变量做不到这点，普通变量的值在线程间传递均需要通过主内存来完成。 2. 禁止指令重排序优化。有volatile修饰的变量，赋值后多执行了一个添加内存屏障操作（指令重排序时不能把后面的指令重排序到内存屏障之前的位置），只有一个CPU访问内存时，并不需要内存屏障；（什么是指令重排序：是指CPU采用了允许将多条指令不按程序规定的顺序分开发送给各相应电路单元处理）。 volatile 性能： volatile 的读性能消耗与普通变量几乎相同，但是写操作稍慢，因为它需要在本地代码中插入许多内存屏障指令来保证处理器不发生乱序执行。 volatile变量 volatile就是一个践行happens-before的关键字。看以下对volatile的描述，就不难知道，happens-before指的是线程接收其他线程修改共享变量的消息与该线程读取共享变量的先后关系。大家可以再细想一下，如果没有happens-before原则，岂不是相当于一个线程读取自己的共享变量副本时，其他线程修改这个变量的消息还没有同步过来？这就是可见性问题。 volatile变量规则：对一个volatile的写，happens-before于任意后续对这个volatile变量的读。 线程A写一个volatile变量，实质上是线程A向接下来要获取这个锁的某个线程发出了（线程A对共享变量修改的）消息。 线程B读一个volatile变量，实质上是线程B接收了之前某个线程发出的（对共享变量所做修改的）消息。 线程A写一个volatile变量，随后线程B读这个变量，这个过程实质上是线程A通过主内存向线程B发送消息。 其实仔细看看volatile的实现方式，实际上就是限制了重排序的范围——加入内存屏障(Memory Barrier or Memory Fence)。也即是说，允许指令执行的时间先后顺序在一定范围内发生变化，而这个范围就是根据happens-before原则来规定。内存屏障概括起来有两个功能： 使写缓冲区的内容刷新到内存，保证对其他线程/CPU可见 禁止读写操作的越过内存屏障进行重排序 每个volatile写操作的前面插入一个StoreStore屏障 每个volatile写操作的后面插入一个StoreLoad屏障 每个volatile读操作的后面插入一个LoadLoad屏障 每个volatile读操作的后面插入一个LoadStore屏障 关于内存屏障的种类，这里不是研究的重点。一直困扰我的是，在多处理器系统下，这个屏障如何能跨越处理器来阻止操作执行的顺序呢？比如下面的读写操作： public static volatile int race = 0; // Thread A public static void save(int src) { race = src; } // Thread B public static int load() { return race; } 这就要提到从操作系统到硬件层面的观念转换，可以参看总线事务（Bus transaction）的概念。当CPU要与内存进行数据交换的时候，实际上总线会同步数据交换操作，同一时刻只能有一个CPU进行读/写内存，所以我们所看到的多处理器并行，并行的是CPU的计算资源。在总线看来，对于存储的读写操作就是串行的，是按照一定顺序的。这也就是为什么一个内存屏障能够跨越处理器去限制读写、去完成通信。 ","link":"https://tianxiawuhao.github.io/67CnuNlVA/"},{"title":"Java内存模型JMM","content":"为什么要有内存模型 在介绍Java内存模型之前，先来看一下到底什么是计算机内存模型，然后再来看Java内存模型在计算机内存模型的基础上做了哪些事情。要说计算机的内存模型，就要说一下一段古老的历史，看一下为什么要有内存模型。 内存模型，英文名Memory Model，他是一个很老的老古董了。他是与计算机硬件有关的一个概念。那么我先给你介绍下他和硬件到底有啥关系。 CPU和缓存一致性 我们应该都知道，计算机在执行程序的时候，每条指令都是在CPU中执行的，而执行的时候，又免不了要和数据打交道。而计算机上面的数据，是存放在主存当中的，也就是计算机的物理内存啦。 刚开始，还相安无事的，但是随着CPU技术的发展，CPU的执行速度越来越快。而由于内存的技术并没有太大的变化，所以从内存中读取和写入数据的过程和CPU的执行速度比起来差距就会越来越大,这就导致CPU每次操作内存都要耗费很多等待时间。 这就像一家创业公司，刚开始，创始人和员工之间工作关系其乐融融，但是随着创始人的能力和野心越来越大，逐渐和员工之间出现了差距，普通员工原来越跟不上CEO的脚步。老板的每一个命令，传到到基层员工之后，由于基层员工的理解能力、执行能力的欠缺，就会耗费很多时间。这也就无形中拖慢了整家公司的工作效率。 可是，不能因为内存的读写速度慢，就不发展CPU技术了吧，总不能让内存成为计算机处理的瓶颈吧。 所以，人们想出来了一个好的办法，就是在CPU和内存之间增加高速缓存。缓存的概念大家都知道，就是保存一份数据拷贝。他的特点是速度快，内存小，并且昂贵。 那么，程序的执行过程就变成了： 当程序在运行过程中，会将运算需要的数据从主存复制一份到CPU的高速缓存当中，那么CPU进行计算时就可以直接从它的高速缓存读取数据和向其中写入数据，当运算结束之后，再将高速缓存中的数据刷新到主存当中。 之后，这家公司开始设立中层管理人员，管理人员直接归CEO领导，领导有什么指示，直接告诉管理人员，然后就可以去做自己的事情了。管理人员负责去协调底层员工的工作。因为管理人员是了解手下的人员以及自己负责的事情的。所以，大多数时候，公司的各种决策，通知等，CEO只要和管理人员之间沟通就够了。 而随着CPU能力的不断提升，一层缓存就慢慢的无法满足要求了，就逐渐的衍生出多级缓存。 按照数据读取顺序和与CPU结合的紧密程度，CPU缓存可以分为一级缓存（L1），二级缓存（L2），部分高端CPU还具有三级缓存（L3），每一级缓存中所储存的全部数据都是下一级缓存的一部分。 这三种缓存的技术难度和制造成本是相对递减的，所以其容量也是相对递增的。 那么，在有了多级缓存之后，程序的执行就变成了： 当CPU要读取一个数据时，首先从一级缓存中查找，如果没有找到再从二级缓存中查找，如果还是没有就从三级缓存或内存中查找。 随着公司越来越大，老板要管的事情越来越多，公司的管理部门开始改革，开始出现高层，中层，底层等管理者。一级一级之间逐层管理。 单核CPU只含有一套L1，L2，L3缓存； 如果CPU含有多个核心，即多核CPU，则每个核心都含有一套L1（甚至和L2）缓存，而共享L3（或者和L2）缓存。 公司也分很多种，有些公司只有一个大Boss，他一个人说了算。但是有些公司有比如联席总经理、合伙人等机制。 单核CPU就像一家公司只有一个老板，所有命令都来自于他，那么就只需要一套管理班底就够了。 多核CPU就像一家公司是由多个合伙人共同创办的，那么，就需要给每个合伙人都设立一套供自己直接领导的高层管理人员，多个合伙人共享使用的是公司的底层员工。 还有的公司，不断壮大，开始差分出各个子公司。各个子公司就是多个CPU了，互相之前没有共用的资源。互不影响。 下图为一个单CPU双核的缓存结构。 随着计算机能力不断提升，开始支持多线程。那么问题就来了。我们分别来分析下单线程、多线程在单核CPU、多核CPU中的影响。 单线程。cpu核心的缓存只被一个线程访问。缓存独占，不会出现访问冲突等问题。 单核CPU，多线程进程中的多个线程会同时访问进程中的共享数据，CPU将某块内存加载到缓存后，不同线程在访问相同的物理地址的时候，都会映射到相同的缓存位置，这样即使发生线程的切换，缓存仍然不会失效。但由于任何时刻只能有一个线程在执行，因此不会出现缓存访问冲突。 多核CPU，多线程每个核都至少有一个L1 缓存。多个线程访问进程中的某个共享内存，且这多个线程分别在不同的核心上执行，则每个核心都会在各自的caehe中保留一份共享内存的缓冲。由于多核是可以并行的，可能会出现多个线程同时写各自的缓存的情况，而各自的cache之间的数据就有可能不同。 在CPU和主存之间增加缓存，在多线程场景下就可能存在缓存一致性问题，也就是说，在多核CPU中，每个核的自己的缓存中，关于同一个数据的缓存内容可能不一致。 如果这家公司的命令都是串行下发的话，那么就没有任何问题。 如果这家公司的命令都是并行下发的话，并且这些命令都是由同一个CEO下发的，这种机制是也没有什么问题。因为他的命令执行者只有一套管理体系。 如果这家公司的命令都是并行下发的话，并且这些命令是由多个合伙人下发的，这就有问题了。因为每个合伙人只会把命令下达给自己直属的管理人员，而多个管理人员管理的底层员工可能是公用的。 比如，合伙人1要辞退员工a，合伙人2要给员工a升职，升职后的话他再被辞退需要多个合伙人开会决议。两个合伙人分别把命令下发给了自己的管理人员。合伙人1命令下达后，管理人员a在辞退了员工后，他就知道这个员工被开除了。而合伙人2的管理人员2这时候在没得到消息之前，还认为员工a是在职的，他就欣然的接收了合伙人给他的升职a的命令。 处理器优化和指令重排 上面提到在在CPU和主存之间增加缓存，在多线程场景下会存在缓存一致性问题。除了这种情况，还有一种硬件问题也比较重要。那就是为了使处理器内部的运算单元能够尽量的被充分利用，处理器可能会对输入代码进行乱序执行处理。这就是处理器优化。 除了现在很多流行的处理器会对代码进行优化乱序处理，很多编程语言的编译器也会有类似的优化，比如Java虚拟机的即时编译器（JIT）也会做指令重排。 可想而知，如果任由处理器优化和编译器对指令重排的话，就可能导致各种各样的问题。 关于员工组织调整的情况，如果允许人事部在接到多个命令后进行随意拆分乱序执行或者重排的话，那么对于这个员工以及这家公司的影响是非常大的。 并发编程的问题 前面说的和硬件有关的概念你可能听得有点蒙，还不知道他到底和软件有啥关系。但是关于并发编程的问题你应该有所了解，比如原子性问题，可见性问题和有序性问题。 其实，原子性问题，可见性问题和有序性问题。是人们抽象定义出来的。而这个抽象的底层问题就是前面提到的缓存一致性问题、处理器优化问题和指令重排问题等。 这里简单回顾下这三个问题，并不准备深入展开，感兴趣的读者可以自行学习。我们说，并发编程，为了保证数据的安全，需要满足以下三个特性： 原子性是指在一个操作中就是cpu不可以在中途暂停然后再调度，既不被中断操作，要不执行完成，要不就不执行。 可见性是指当多个线程访问同一个变量时，一个线程修改了这个变量的值，其他线程能够立即看得到修改的值。 有序性即程序执行的顺序按照代码的先后顺序执行。 有没有发现，缓存一致性问题其实就是可见性问题。而处理器优化是可以导致原子性问题的。指令重排即会导致有序性问题。所以，后文将不再提起硬件层面的那些概念，而是直接使用大家熟悉的原子性、可见性和有序性。 什么是内存模型 前面提到的，缓存一致性问题、处理器器优化的指令重排问题是硬件的不断升级导致的。那么，有没有什么机制可以很好的解决上面的这些问题呢？ 最简单直接的做法就是废除处理器和处理器的优化技术、废除CPU缓存，让CPU直接和主存交互。但是，这么做虽然可以保证多线程下的并发问题。但是，这就有点因噎废食了。 所以，为了保证并发编程中可以满足原子性、可见性及有序性。有一个重要的概念，那就是——内存模型。 为了保证共享内存的正确性（可见性、有序性、原子性），内存模型定义了共享内存系统中多线程程序读写操作行为的规范通过这些规则来规范对内存的读写操作，从而保证指令执行的正确性。它与处理器有关、与缓存有关、与并发有关、与编译器也有关。他解决了CPU多级缓存、处理器优化、指令重排等导致的内存访问问题，保证了并发场景下的一致性、原子性和有序性。 内存模型解决并发问题主要采用两种方式：限制处理器优化和使用内存屏障。本文就不深入底层原理来展开介绍了，感兴趣的朋友可以自行学习。 什么是Java内存模型 前面介绍过了计算机内存模型，这是解决多线程场景下并发问题的一个重要规范。那么具体的实现是如何的呢，不同的编程语言，在实现上可能有所不同。 我们知道，Java程序是需要运行在Java虚拟机上面的，Java内存模型（Java Memory Model ,JMM）就是一种符合内存模型规范的，屏蔽了各种硬件和操作系统的访问差异的，保证了Java程序在各种平台下对内存的访问都能保证效果一致的机制及规范。 提到Java内存模型，一般指的是JDK 5 开始使用的新的内存模型，主要由JSR-133: JavaTM Memory Model and Thread Specification 描述。感兴趣的可以参看下这份PDF文档（http://www.cs.umd.edu/~pugh/java/memoryModel/jsr133.pdf） Java内存模型规定了所有的变量都存储在主内存中，每条线程还有自己的工作内存，线程的工作内存中保存了该线程中是用到的变量的主内存副本拷贝，线程对变量的所有操作都必须在工作内存中进行，而不能直接读写主内存。不同的线程之间也无法直接访问对方工作内存中的变量，线程间变量的传递均需要自己的工作内存和主存之间进行数据同步进行。 而JMM就作用于工作内存和主存之间数据同步过程。他规定了如何做数据同步以及什么时候做数据同步。 这里面提到的主内存和工作内存，读者可以简单的类比成计算机内存模型中的主存和缓存的概念。特别需要注意的是，主内存和工作内存与JVM内存结构中的Java堆、栈、方法区等并不是同一个层次的内存划分，无法直接类比。《深入理解Java虚拟机》中认为，如果一定要勉强对应起来的话，从变量、主内存、工作内存的定义来看，主内存主要对应于Java堆中的对象实例数据部分。工作内存则对应于虚拟机栈中的部分区域。 所以，再来总结下，JMM是一种规范，目的是解决由于多线程通过共享内存进行通信时，存在的本地内存数据不一致、编译器会对代码指令重排序、处理器会对代码乱序执行等带来的问题。目的是保证并发编程场景中的原子性、可见性和有序性。 Java内存模型的实现 了解Java多线程的朋友都知道，在Java中提供了一系列和并发处理相关的关键字，比如volatile、synchronized、final、concurrent包等。其实这些就是Java内存模型封装了底层的实现后提供给程序员使用的一些关键字。 在开发多线程的代码的时候，我们可以直接使用synchronized等关键字来控制并发，从来就不需要关心底层的编译器优化、缓存一致性等问题。所以，Java内存模型，除了定义了一套规范，还提供了一系列原语，封装了底层实现后，供开发者直接使用。 本文并不准备把所有的关键字逐一介绍其用法，因为关于各个关键字的用法，网上有很多资料。读者可以自行学习。本文还有一个重点要介绍的就是，我们前面提到，并发编程要解决原子性、有序性和一致性的问题，我们就再来看下，在Java中，分别使用什么方式来保证。 原子性 在Java中，为了保证原子性，提供了两个高级的字节码指令monitorenter和monitorexit。在synchronized的实现原理文章中，介绍过，这两个字节码，在Java中对应的关键字就是synchronized。 因此，在Java中可以使用synchronized来保证方法和代码块内的操作是原子性的。 可见性 Java内存模型是通过在变量修改后将新值同步回主内存，在变量读取前从主内存刷新变量值的这种依赖主内存作为传递媒介的方式来实现的。 Java中的volatile关键字提供了一个功能，那就是被其修饰的变量在被修改后可以立即同步到主内存，被其修饰的变量在每次是用之前都从主内存刷新。因此，可以使用volatile来保证多线程操作时变量的可见性。 除了volatile，Java中的synchronized和final两个关键字也可以实现可见性。只不过实现方式不同，被synchronized修饰的代码，在开始执行时会加锁，执行完成后会进行解锁，但在一个变量解锁之前，必须先把此变量同步回主存中，这样解锁后，后续其它线程就可以访问到被修改后的值，从而保证可见性。 有序性 在Java中，可以使用synchronized和volatile来保证多线程之间操作的有序性。实现方式有所区别： volatile关键字会禁止指令重排。synchronized关键字保证同一时刻只允许一条线程操作。 好了，这里简单的介绍完了Java并发编程中解决原子性、可见性以及有序性可以使用的关键字。读者可能发现了，好像synchronized关键字是万能的，他可以同时满足以上三种特性，这其实也是很多人滥用synchronized的原因。 但是synchronized是比较影响性能的，虽然编译器提供了很多锁优化技术，但是也不建议过度使用。 总结 在读完本文之后，相信你应该了解了什么是Java内存模型、Java内存模型的作用以及Java中内存模型做了什么事情等。 关于Java中这些和内存模型有关的关键字，希望读者还可以继续深入学习，并且自己写几个例子亲自体会一下。可以参考《深入理解Java虚拟机》和《Java并发编程的艺术》两本书。 面试如何回答 前面我介绍完了一些和Java内存模型有关的基础知识，只是基础，并不是全部，因为随便一个知识点还是都可以展开的，如volatile是如何实现可见性的？synchronized是如何实现有序性的？ 但是，当面试官问你：能简单介绍下你理解的内存模型吗？ 首先，先和面试官确认一下：您说的内存模型指的是JMM，也就是和并发编程有关的那一个吧？ 在得到肯定答复后，再开始介绍（如果不是，那可能就要回答堆、栈、方法区哪些了….囧…）： Java内存模型，其实是保证了Java程序在各种平台下对内存的访问都能够得到一致效果的机制及规范。目的是解决由于多线程通过共享内存进行通信时，存在的原子性、可见性（缓存一致性）以及有序性问题。 除此之外，Java内存模型还提供了一系列原语，封装了底层实现后，供开发者直接使用。如我们常用的一些关键字：synchronized、volatile以及并发包等。 回答到这里就可以了，然后面试官可能会继续追问，然后根据他的追问再继续往下回答即可。 所以，当有人再问你Java内存模型的时候，不要一张嘴就直接回答堆栈、方法区甚至GC了，那样显得很不专业！ 作者： Hollis 原文地址：Java内存模型 ","link":"https://tianxiawuhao.github.io/rNVfAh0Yb/"},{"title":" JVM 的核心知识梳理","content":"前言 运行时数据区域 java虚拟机运行时有哪些数据区域，他们都有什么用途？ 有程序计数器、java虚拟机栈、本地方法栈、堆和方法区五大模块。请看下图： 程序计数器 程序计数器是一块较小的内存空间，他可以看做是当期线程所执行的字节码的行号指令器。字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令。在任何一个确定的时刻，一个处理器都只会执行一条线程中的指令。因此，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各个线程之间互不影响，独立存储，所以程序计数器是“线程私有的”。另外，程序计数器是唯一一个在java虚拟机规范中没有规定OOM的区域。 Java虚拟机栈 Java虚拟机栈也是线程私有的，它的生命周期与线程相同，虚拟机栈描述的是Java方法执行的内存模型，每个方法在执行的同时都会创建一个栈帧用于存储局部变量表、操作数栈、动态链接、方法出口等信息。每个方法从调用直至执行完成的过程，对应着一个栈帧在虚拟机栈中入栈到出栈的过程。（程序员经常会把Java内存划分为堆内存和栈内存，这种说法比较粗糙，其中的栈内存就是指虚拟机栈，或者说是虚拟机栈中的局部变量表的部分） 在Java虚拟机规范中，对这个区域规定了两种异常：如果线程请求的栈深度大于虚拟机所允许的深度，将抛出StackOverFlowError异常。如果虚拟机栈可以动态扩展，如果扩展时无法申请到足够的内存，就会抛出OOM异常。 本地方法栈 本地方法栈与虚拟机栈作用类似，他们之间的区别不过是虚拟机栈是为虚拟机执行Java方法（也就是字节码）服务，而本地方法栈则为虚拟机使用到的Native方法服务。与虚拟机栈一样，本地方法栈也会抛出StackOverFlowError异常和OOM异常。 堆 Java堆是Java虚拟机管理的最大的一块内存，是所有线程共享的区域，在虚拟机启动时就创建。堆用来存放对象实例，几乎所有的对象实例都在这里分配内存（注意是几乎所有）。这一点在Java虚拟机规范中描述为：所有的对象实例以及数组都要在堆上分配，但随着JIT编译器的发展和逃逸分析技术的成熟，栈上分配、标量替换技术将会导致一些微妙的变化发生，所有对象都分配在堆上也不是那么绝对了。 如果在堆中没有内存完成实例分配，并且堆也无法再扩展时，将会抛出OOM异常。 方法区 方法区与Java堆一样，是各个线程共享的区域，它用来存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。根据Java虚拟机规范，当方法区无法满足内存分配的 需求时，将抛出OOM异常。 运行时常量池是方法区的一部分，用于存放编译器生成的各种字面量和符号引用。运行时常量池相对于Class文件常量池的另外一个重要特征就是具备动态性。也就是说运行期间也可能将新的常量 放入池中，这种特性被利用的比较多的就是String类的intern()方法。 直接内存 直接内存并不是运行时数据区的一部分，也不是Java虚拟机定义的内存区域。本机直接内存的分配不受Java堆大小的限制，但是受本机总内存大小以及处理器寻址空间的限制。 内存溢出 堆内存溢出 Java堆用于存储对象实例，只要不断创建对象，并且保证GC Roots到对象之间有可达路径来避免垃圾回收机制清除这些对象，那么在对象数量达到最大堆容量限制后就会OOM。（轻易不要运行） public class HeapOOMTest { static class OOMObject { } public static void main(String [] args) { List&lt;OOMObject&gt; list = new ArrayList&lt;&gt;(); while (true) { list.add(new OOMObject()); } } } Java堆内存的OOM异常是实际应用中最常见的内存溢出，当出现了咋办？一般的手段是先通过内存映像分析工具对Dump出来的堆转储快照进行分析，重点是确认内存中的对象是否是必要的，也就是确认是内存泄露还是内存溢出。 如果是内存泄露，可进一步通过工具查看泄露对象到GCRoots的引用链，找到为什么垃圾收集器无法回收它们。如果不存在泄露，就是内存中的对象必须都存活，那就要检查虚拟机的堆内存是否可以调大，从代码上检查是否某些对象生命周期过长，减少内存消耗，优化代码。 虚拟机栈溢出 关于虚拟机栈，在java虚拟机规范中描述了两种异常： （1）如果线程请求的栈深度大于虚拟机所允许的最大深度，将抛出StackOverFlowError异常。 （2）如果虚拟机在扩展栈时无法申请到足够的内存空间，则抛出OutOdMemoryError异常。 这里描述的两种情况实际上有些重叠：当栈空间无法继续分配的时候，到底是内存太小导致的，还是已使用的栈空间太大，本质上是一样的。 public class StackSOFTest { private int stackLength = -1; public void stackLeak() { stackLength++; stackLeak(); } public static void main(String [] args) throws Throwable { StackSOFTest oom = new StackSOFTest(); try { oom.stackLeak(); } catch (Throwable e) { System.out.println(&quot;stack length:&quot;+oom.stackLength); throw e; } } } 运行结果： tack length:13980 Exception in thread &quot;main&quot; java.lang.StackOverflowError at oom.StackSOFTest.stackLeak(StackSOFTest.java:14) at oom.StackSOFTest.stackLeak(StackSOFTest.java:14) at oom.StackSOFTest.stackLeak(StackSOFTest.java:14) ...后续省略 实验结果表明：在单个线程下，无论是由于栈帧太大还是虚拟机栈容量太小，当内存无法分配时，虚拟机抛出的都是StackOverflowError异常。 哪些内存需要回收？ 猿们都知道JVM的内存结构包括五大区域：程序计数器、虚拟机栈、本地方法栈、堆区、方法区。其中程序计数器、虚拟机栈、本地方法栈3个区域随线程而生、随线程而灭，因此这几个区域的内存分配和回收都具备确定性，就不需要过多考虑回收的问题，因为方法结束或者线程结束时，内存自然就跟随着回收了。而Java堆区和方法区则不一样、不一样!(怎么不一样说的朗朗上口)，这部分内存的分配和回收是动态的，正是垃圾收集器所需关注的部分。 垃圾收集器在对堆区和方法区进行回收前，首先要确定这些区域的对象哪些可以被回收，哪些暂时还不能回收，这就要用到判断对象是否存活的算法！（面试官肯定没少问你吧） 2.1 引用计数算法 2.1.1 算法分析 引用计数是垃圾收集器中的早期策略。在这种方法中，堆中每个对象实例都有一个引用计数。当一个对象被创建时，就将该对象实例分配给一个变量，该变量计数设置为1。当任何其它变量被赋值为这个对象的引用时，计数加1（a = b,则b引用的对象实例的计数器+1），但当一个对象实例的某个引用超过了生命周期或者被设置为一个新值时，对象实例的引用计数器减1。任何引用计数器为0的对象实例可以被当作垃圾收集。当一个对象实例被垃圾收集时，它引用的任何对象实例的引用计数器减1。 2.1.2 优缺点 优点：引用计数收集器可以很快的执行，交织在程序运行中。对程序需要不被长时间打断的实时环境比较有利。 缺点：无法检测出循环引用。如父对象有一个对子对象的引用，子对象反过来引用父对象。这样，他们的引用计数永远不可能为0。 2.1.3 是不是很无趣，来段代码压压惊 public class ReferenceFindTest { public static void main(String[] args) { MyObject object1 = new MyObject(); MyObject object2 = new MyObject(); object1.object = object2; object2.object = object1; object1 = null; object2 = null; } } 这段代码是用来验证引用计数算法不能检测出循环引用。最后面两句将object1和object2赋值为null，也就是说object1和object2指向的对象已经不可能再被访问，但是由于它们互相引用对方，导致它们的引用计数器都不为0，那么垃圾收集器就永远不会回收它们。 2.2 可达性分析算法 可达性分析算法是从离散数学中的图论引入的，程序把所有的引用关系看作一张图，从一个节点GC ROOT开始，寻找对应的引用节点，找到这个节点以后，继续寻找这个节点的引用节点，当所有的引用节点寻找完毕之后，剩余的节点则被认为是没有被引用到的节点，即无用的节点，无用的节点将会被判定为是可回收的对象。 在Java语言中，可作为GC Roots的对象包括下面几种： a) 虚拟机栈中引用的对象（栈帧中的本地变量表）； b) 方法区中类静态属性引用的对象； c) 方法区中常量引用的对象； d) 本地方法栈中JNI（Native方法）引用的对象。 对象是“生”是“死 对象的四种引用 引用分为强引用，软引用，弱引用和虚引用四种，这四种引用强度依次逐渐减弱。 强引用 强引用就是指在程序代码中普遍存在的，是指创建一个对象并把这个对象赋给一个引用变量，类似Object obj = new Object()这类的引用，只要强引用还存在，垃圾收集器就永远不会回收被引用的对象。如果想中断强引用和某个对象之间的关联，可以显示的将引用赋值为null，这样jvm在合适的时间就会回收该对象。 软引用 软引用是用来描述一些还有用但并非必需的对象。对于软引用关联着的对象，在系统将会发生内存溢出异常之前，将会把这些对象列进回收范围之中进行第二次回收。SoftReference的特点是它的一个实例保存对一个Java对象的软引用，该软引用的存在不妨碍垃圾收集器线程对该Java对象的回收。 弱引用 弱引用也是用来描述非必需对象的。当JVM进行垃圾回收时，无论内存是否充足，都会回收被弱引用关联的对象。在java中，用java.lang.ref.WeakReference类来表示。 虚引用 虚引用和前面的软引用和弱引用不同，它并不影响对象的生命周期。在java中使用PhantomReference类来表示。如果一个对象与虚引用关联，跟没有引用与之关联一样，任何时候都可能被回收。要注意的是，虚引用必须和引用队列关联使用。当垃圾收集器准备回收一个对象时，如果发现它还有虚引用，就会把这个虚引用加入到与之关联的引用队列中。为对象设置虚引用的唯一目的就是能在这个对象被垃圾收集器回收时收到一个系统通知。 引用计数法的缺陷 引用计数法就是给对象添加一个引用计数器，每当有一个地方引用它时，计数器值就加1.当引用失效时，计数器的值就减一。任何时刻计数器值为0的对象就是不可能再被使用的。 优缺点：实现简单，判断效率高，大部分情况下是个很不错的算法。但是致命问题是没办法解决对象之间相互循环引用的问题。 public class ReferenceCountingGC { private Object instance = null; private static final int _1MB = 1024*1024; private byte[] bigSize = new byte[2*_1MB]; public static void main(String [] args) { ReferenceCountingGC objA = new ReferenceCountingGC(); ReferenceCountingGC objB = new ReferenceCountingGC(); objA.instance = objB; objB.instance = objA; objA = null; objB = null; //假设在这行发生GC，ObjA和ObjB是否能被回收 System.gc(); } } 观察GC日志可以看出GC发生了内存回收，意味着虚拟机并没有因为这两个对象相互引用就不回收它们，这也从侧面说明虚拟机并没有采用引用计数法来判断对象是否存活。 可达性分析 这个算法的基本思想是通过一系列被称为“GC Roots”的对象作为起始点，从这些节点开始向下搜索，搜索走过的路径叫做引用链，当一个对象到GC Roots没有任何引用链相连 （用图论的话来说就是从GC Roots到这个对象不可达），则证明此对象是不可用的。 在 Java语言中，可作为GC Roots的对象包括以下几种 （1）虚拟机栈（栈帧中的本地变量表）中引用的对象。 （2）方法区中类静态属性引用的对象。 （3）方法区中常量引用的对象。 （4）本地方法栈中JNI（即一般说的Native方法）引用的对象。 对象是生存还是死亡？ 即使在可达性分析法中不可达的对象，也并非“非死不可”，他们还有拯救自己的机会。要宣告一个对象死亡，至少要经历两次标记过程：如果对象在进行可达性分析后没有与GC Roots的引用链，那么它将会被第一次标记，并且此时需要判断是否有必要执行finalize()方法。没有必要的话，那么这个对象就宣告死亡，可以回收了。 如果有必要执行，那么这个对象会被放置在一个叫做F-Queue的队列中，并在稍后由虚拟机自动建立的低优先级的Finalizer线程去执行它。finalize()是对象拯救自己的最后一次机会-只要重新与引用链上的 任何一个对象建立关联即可（譬如把自己赋值给某个类变量或者对象的成员变量），那么在第二次标记时它将被移除“可回收”的集合，如果对象还没有逃脱，基本上就真的被回收了。 具体的过程见下图： 垃圾收集算法 标记清除算法 标记-清除算法是最基础的算法，分为“标记”和“清除”两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收。 它的主要不足有两个：一是效率问题，标记和清除两个过程的效率都不高；另一个是空间问题，标记清楚之后会产生大量不连续的内存碎片。 标记-清除算法的执行过程见下图： 复制算法 为了解决效率问题，“复制”算法出现了。它将内存空间划分为大小相等的两块，每次只使用其中的一块，当这一块的内存用完了，就将还存活的对象复制到另外一块上，然后再把已使用的的内存空间一次性清理掉。 这样每次都是对整个半区进行内存回收，内存分配时也就不用考虑内存碎片等复杂情况，只要移动堆顶指针，按顺序分配内存即可，实现简单，运行高效。但是这种算法的代码是将内存空间缩小为原来的一半。 复制算法的执行过程见下图： 标记-整理算法 标记过程仍然与标记-清除算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉边界以外的内存。 标记-整理算法的执行过程见下图： 分代收集算法 分代收集算法是目前大部分JVM的垃圾收集器采用的算法。它的核心思想是根据对象存活的生命周期将内存划分为若干个不同的区域。一般情况下将堆区划分为老年代（Tenured Generation）和新生代（Young Generation），在堆区之外还有一个代就是永久代（Permanet Generation）。老年代的特点是每次垃圾收集时只有少量对象需要被回收，而新生代的特点是每次垃圾回收时都有大量的对象需要被回收，那么就可以根据不同代的特点采取最适合的收集算法。 3.4.1 年轻代（Young Generation）的回收算法 a) 所有新生成的对象首先都是放在年轻代的。年轻代的目标就是尽可能快速的收集掉那些生命周期短的对象。 b) 新生代内存按照8:1:1的比例分为一个eden区和两个survivor(survivor0,survivor1)区。一个Eden区，两个 Survivor区(一般而言)。大部分对象在Eden区中生成。回收时先将eden区存活对象复制到一个survivor0区，然后清空eden区，当这个survivor0区也存放满了时，则将eden区和survivor0区存活对象复制到另一个survivor1区，然后清空eden和这个survivor0区，此时survivor0区是空的，然后将survivor0区和survivor1区交换，即保持survivor1区为空， 如此往复。 c) 当survivor1区不足以存放 eden和survivor0的存活对象时，就将存活对象直接存放到老年代。若是老年代也满了就会触发一次Full GC，也就是新生代、老年代都进行回收。 d) 新生代发生的GC也叫做Minor GC，MinorGC发生频率比较高(不一定等Eden区满了才触发)。 3.4.2 年老代（Old Generation）的回收算法 a) 在年轻代中经历了N次垃圾回收后仍然存活的对象，就会被放到年老代中。因此，可以认为年老代中存放的都是一些生命周期较长的对象。 b) 内存比新生代也大很多(大概比例是1:2)，当老年代内存满时触发Major GC即Full GC，Full GC发生频率比较低，老年代对象存活时间比较长，存活率标记高。 3.4.3 持久代（Permanent Generation）的回收算法 用于存放静态文件，如Java类、方法等。持久代对垃圾回收没有显著影响，但是有些应用可能动态生成或者调用一些class，例如Hibernate 等，在这种时候需要设置一个比较大的持久代空间来存放这些运行过程中新增的类。持久代也称方法区， 垃圾收集器 堆内存是垃圾收集器主要回收垃圾对象的地方，堆内存可以根据对象生命周期的不同分为新生代和老年代，分代收集，新生代使用复制算法，老年代使用标记清除或者标记整理算法。 HotSpot虚拟机提供了7种垃圾收集器，其中新生代三种：Serial/ParNew/Parallel Scavenge收集器，老年代三种：Serial Old/Parallel Old/CMS，都适用的是G1收集器。所有垃圾收集器组合 情况如下图： Serial收集器 最基本也是发展历史最长的垃圾收集器，在进行垃圾收集时，必须Stop The World(暂停其他工作线程)，直到收集结束。只使用一条线程完成垃圾收集，但是效率高，因为没有线程交互的开销，拥有更高的单线程收集效率。发生在新生代区域，使用复制算法。 ParNew收集器 Serial收集器的多线程版本。在进行垃圾收集时同样需要Stop The World（暂停其他工作线程），直到收集结束。使用多条线程进行垃圾收集（由于存在线程交互的开销，所以在单CPU的环境下，性能差于Serial收集器）。目前，只有Parnew收集器能与CMS收集器配合工作。发生在新生代区域，使用复制算法。 Parallel Scavenge收集器 ParNew收集器的升级版，具备ParNew收集器并发多线程收集的特点，以达到可控制吞吐量为目标。（吞吐量：CPU用于运行用户代码的时间与CPU总消耗时间（运行用户代码时间+垃圾收集时间）的比值）。该垃圾收集器能根据当前系统运行情况，动态调整自身参数，从而达到最大吞吐量的目标。（该特性成为GC自适应的调节策略）。发生在新生代，使用复制算法。 Serial Old收集器 Serial 收集器应用在老年代的版本。并发、单线程、效率高。使用标记整理算法。 Parallel Old收集器 是Parallel Scavenge应用在老年代的版本，以达到可控制吞吐量、自适应调节和多线程收集为目标，使用标记整理算法。 CMS收集器 CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。使用这类收集器的应用重视服务的响应速度，希望系统停顿时间最短，以带来更好的用户体验。 使用标记清除算法，一共四个步骤：初始标记、并发标记、重新标记和并发清除。详情见下表： 下面说下CMS的优缺点： 优点： （1）并行：用户线程和垃圾收集线程同时进行。 （2）单线程收集：只使用一条线程完成垃圾收集。 （3）垃圾收集停顿时间短：获取最短的回收停顿时间，即希望系统停顿的时间最短，提高响应速度。 缺点： （1）总吞吐量会降低：因为该收集器对CPU资源非常敏感，在并发阶段不会导致停顿用户线程，但会因为占用部分线程（CPU资源）导致应用程序变慢，总吞吐量会降低。 （2）无法处理浮动垃圾：由于并发清理时用户线程还在运行，所以会有新的垃圾不断产生，只能等到下一次GC时再清理。（因为这一部分垃圾出现在标记过程之后，所以CMS 无法在当次GC中处理他们，因此CMS无法等到老年代填满再进行Full GC，CMS需要预留一部分空间）。 （3）垃圾收集后会产生大量的内存碎片：因为CMS收集器是使用标记-清除算法的。 下面一张图了解下CMS的工作过程： G1收集器 G1收集器是最新最前沿的垃圾收集器。特点如下： （1）并行：用户线程和垃圾收集线程同时进行。 （2）多线程：即使用多条垃圾收集线程进行垃圾回收。（并发和并行充分利用多CPU和多核环境的硬件优势来缩短垃圾收集的停顿时间） （3）垃圾收集效率高：G1收集器是针对性对Java堆内存区域进行垃圾收集，而非每次都对整个区域进行收集。即G1除了将Java堆内存分为新生代和老年代之外，还会细分为许多个 大小相等的独立区域（Region），然后G1收集器会跟踪每个Region里的垃圾代价值大小，并在后台维护一个列表。每次回收时，会根据允许的垃圾收集时间优先回收价值最大的 Region，从而避免了对整个Java堆内存区域的回收，提高了效率。因为上述机制，G1收集器还能建立可预测的时间模型：即让使用者明确执行一个长度为M毫秒的时间片段，消耗在 垃圾收集上的时间不得超出N毫秒。即具备实时性。 （4）不会产生内存碎片。从整理上看，G1收集器是基于标记-整理算法的，从局部看是基于复制算法的。在新生代使用复制算法，在老年代使用标记-整理算法。 下面了解下工作流程，跟CMS有点像。 下面是G1的工作过程： 总结 本文讲解了运行时数据区域，内存溢出，如何判断对象是否存活，垃圾回收算法和垃圾收集器几个方面，涵盖jvm的核心考点，希望你有所收获。 ","link":"https://tianxiawuhao.github.io/8iu5aa5DN/"},{"title":"Java内存结构JVM","content":"JVM内存模型 内存空间(Runtime Data Area)中可以按照是否线程共享分为两块，线程共享的是方法区(Method Area)和堆(Heap)，线程独享的是Java虚拟机栈(Java Stack)，本地方法栈(Native Method Stack)和PC寄存器(Program Counter Register)。具体参见下图： 虚拟机栈： 虚拟机栈是Java方法执行的内存模型，栈中存放着栈帧，每个栈帧分别对应一个被调用的方法，方法的调用过程对应栈帧在虚拟机中入栈到出栈的过程。 栈是线程私有的，也就是线程之间的栈是隔离的；当程序中某个线程开始执行一个方法时就会相应的创建一个栈帧并且入栈（位于栈顶），在方法结束后，栈帧出栈。 下图表示了一个Java栈的模型以及栈帧的组成： 栈帧:是用于支持虚拟机进行方法调用和方法执行的数据结构，它是虚拟机运行时数据区中的虚拟机栈的栈元素。 每个栈帧中包括： 局部变量表:用来存储方法中的局部变量（非静态变量、函数形参）。当变量为基本数据类型时，直接存储值，当变量为引用类型时，存储的是指向具体对象的引用。 操作数栈:Java虚拟机的解释执行引擎被称为&quot;基于栈的执行引擎&quot;，其中所指的栈就是指操作数栈。 指向运行时常量池的引用:存储程序执行时可能用到常量的引用。 方法返回地址:存储方法执行完成后的返回地址。 每个线程有一个私有的栈，随着线程的创建而创建。栈里面存放着一种叫做“栈帧”的东西，每个方法会创建一个栈帧，栈帧中存放了局部变量表(基本数据类型和对象引用)、操作数栈、方法出口等信息。栈的大小可以固定也可以动态扩展。当栈调用深度大于JVM所允许的范围，会抛出StackOverflowError的错误，不过这个深度范围不是一个恒定的值，我们通过下面这段程序可以测试一下这个结果： // 栈溢出测试源码 ackage com.paddx.test.memory; /** * Created by root on 2/28/17. */ public class StackErrorMock { private static int index = 1; public void call() { index++; call(); } public static void main(String[] args) { StackErrorMock mock = new StackErrorMock(); try { mock.call(); } catch(Throwable e) { System.out.println(&quot;Stack deep: &quot; + index); e.printStackTrace(); } } } 运行三次，可以看出每次栈的深度都是不一样的，输出结果如下： 查看三张结果图，可以看出每次的Stack deep值都有所不同。究其原因，就需要深入到JVM的源码中才能探讨，这里不作赘述。 虚拟机栈除了上述错误外，还有另一种错误，那就是当申请不到空间时，会抛出OutOfMemoryError。这里有一个小细节需要注意，catch捕获的是Throwable，而不是Exception，这是因为StackOverflowError和OutOfMemoryError都不属于Exception的子类。 本地方法栈： 这部分主要与虚拟机用到的Native方法相关，一般情况下，Java应用程序员并不需要关系这部分内容。 PC寄存器： PC寄存器，也叫程序计数器。JVM支持多个线程同时运行，每个线程都有自己的程序计数器。倘若当前执行的是JVM方法，则该寄存器中保存当前执行指令的地址；倘若执行的是native方法，则PC寄存器为空。 堆 ： 堆内存是JVM所有线程共享的部分，在虚拟机启动的时候就已经创建。所有的对象和数组都在堆上进行分配。这部分空间可通过GC进行回收。当申请不到空间时，会抛出OutOfMemoryError。下面我们简单的模拟一个堆内存溢出的情况： package com.paddx.test.memory; import java.util.ArrayList; import java.util.List; /** * Created by root on 2/28/17. */ public class HeapOomMock { public static void main(String[] args) { List&lt;byte[]&gt; list = new ArrayList&lt;byte[]&gt;(); int i = 0; boolean flag = true; while(flag) { try { i++; list.add(new byte[1024 * 1024]); // 每次增加1M大小的数组对象 }catch(Throwable e) { e.printStackTrace(); flag = false; System.out.println(&quot;Count = &quot; + i); // 记录运行的次数 } } } } 首先配置运行时虚拟机的启动参数： 然后运行代码，输出结果如下： 注意，这里我们指定了堆内存的大小为16M，所以这个地方显示的Count=13(这个数字不是固定的)，至于为什么会是13或其他数字，需要根据GC日志来判断。 方法区： 方法区是一块所有线程共享的内存逻辑区域，在JVM中只有一个方法区，用来存储一些线程可共享的内容，它是线程安全的，多个线程同时访问方法区中同一个内容时，只能有一个线程装载该数据，其它线程只能等待。 方法区可存储的内容有：类的全路径名、类的直接超类的权全限定名、类的访问修饰符、类的类型（类或接口）、类的直接接口全限定名的有序列表、常量池（字段，方法信息，静态变量，类型引用（class））等。 关于方法区的内存溢出问题会在下文中详细讨论。 方法区和元空间 Java1.7及之前的虚拟机在运行中会把它所管理的内存分为如上图的若干数据区域。 方法区用于存储已被虚拟机加载的类信息、常量、静态变量、动态生成的类等数据。实际上在Java虚拟机的规范中方法区是堆中的一个逻辑部分，但是它却拥有一个叫做非堆(Non-Heap)的别名。 对于方法区的实现，不同虚拟机中策略也不同。以我们常用的HotSpot虚拟机为例，其设计团队使用永久带来实现方法区，并把GC的分代收集扩展至永久带。这样设计的好处就是能够省去专门为方法区编写内存管理的代码。但是在实际的场景中，这样的实现并不是一个好的方式，因为永久带有MAX上限，所以这样做会更容易遇到内存溢出问题。 关于方法区的GC回收，Java虚拟机规范并没有严格的限制。虚拟机在实现中可以自由选择是否实现垃圾回收。主要原因还是方法区内存回收的效果比较难以令人满意，方法区的内存回收主要是针对常量池(1.7已经将常量池逐步移除方法区)以及类型的卸载，但是类型卸载在实际场景中的条件相当苛刻。 另外还需要注意的是在HotSpot虚拟机中永久带和堆虽然相互隔离，但是他们的物理内存是连续的。而且老年代和永久带的垃圾收集器进行了捆绑，因此无论谁满了都会触发永久带和老年的GC。 因此在Java1.8中，HotSpot虚拟机已经将方法区(永久带)移除，取而代之的就是元空间。 Java1.8的运行时数据区域如图所示。方法区已经不见了踪影，多出来的是叫做元数据区的区域。 元空间在1.8中不在与堆是连续的物理内存，而是改为使用本地内存(Native memory)。元空间使用本地内存也就意味着只要本地内存足够，就不会出现OOM的错误。 PermGen(永久代) 绝大部分Java程序员应该都见过“java.lang.OutOfMemoryError: PremGen space”异常。这里的“PermGen space”其实指的就是方法区。不过方法区和“PermGen space”又有着本质的区别。前者是JVM的规范，而后者则是JVM规范的一种实现，并且只有HotSpot才有“PermGen space”，而对于其他类型的虚拟机，如JRockit(Oracle)、J9(IBM)并没有“PermGen space”。由于方法区主要存储类的相关信息，所以对于动态生成类的情况比较容易出现永久代的内存溢出。最典型的场景就是，在JSP页面比较多的情况，容易出现永久代内存溢出。我们现在通过动态生成类来模拟“PermGen space”的内存溢出： package com.paddx.test.memory; import java.io.File; import java.net.URL; import java.net.URLClassLoader; import java.util.ArrayList; import java.util.List; public class PermGenOomMock { public static void main(String[] args) { URL url = null; List&lt;ClassLoader&gt; classLoaderList = new ArrayList&lt;ClassLoader&gt;(); try { url = new File(&quot;/tmp&quot;).toURI().toURL(); URL[] urls = {url}; while(true) { ClassLoader loader = new URLClassLoader(urls); classLoaderList.add(loader); loader.loadClass(&quot;com.paddx.test.memory.Test&quot;); } }catch(Exception e) { e.printStackTrace(); } } } package com.paddx.test.memory; public class Test {} 运行结果如下： 本例中使用的JDK版本是1.7，指定的PermGen区的大小为8M。通过每次生成不同URLClassLoader对象加载Test类，从而生成不同的类对象，这样就能看到我们熟悉的“java.lang.OutOfMemoryError: PermGen space”异常了。这里之所以采用JDK 1.7，是因为在JDK 1.8中，HotSpot已经没有“PermGen space”这个区间了，取而代之是一个叫做Metaspace(元空间)的东西。下面我们就来看看Metaspace与PermGen space的区别。 Metaspace(元空间) 其实，移除永久代的工作从JDK 1.7就开始了。JDK 1.7中，存储在永久代的部分数据就已经转移到Java Heap或者Native Heap。但永久代仍存在于JDK 1.7中，并没有完全移除，譬如符号引用(Symbols)转移到了native heap；字面量(interned strings)转移到了Java heap；类的静态变量(class statics)转移到了Java heap。我们可以通过一段程序来比较JDK 1.6、JDK 1.7与JDK 1.8的区别，以字符串常量为例： package com.paddx.test.memory; import java.util.ArrayList; import java.util.List; public class StringOomMock { static String base = &quot;string&quot;; public static void main(String[] args) { List&lt;String&gt; list = new ArrayList&lt;String&gt;(); for (int i = 0; i &lt; Integer.MAX_VALUE; i++) { String str = base + base; base = str; list.add(str.intern()); } } } 这段程序以2的指数级不断的生成新的字符串，这样可以比较快速的消耗内存。我们通过JDK 1.6、JDK 1.7和JDK 1.8分别运行： JDK 1.6的运行结果： JDK 1.7的运行结果： JDK 1.8的运行结果： 从上述结果可以看出，JDK 1.6下，会出现“PermGen space”的内存溢出，而在JDK 1.7和JDK 1.8中，会出现堆内存溢出，并且JDK 1.8中参数PermSize和MaxPermSize已经失效。因此，可以大致验证JDK 1.7和JDK 1.8中将字符串常量由永久代转移到堆中，并且JDK 1.8中已经不存在永久代的结论。现在我们来看一看元空间到底是一个什么东西？ JDK1.8对JVM架构的改造将类元数据放到本地内存中，另外，将常量池和静态变量放到Java堆里。HotSpot VM将会为类的元数据明确分配和释放本地内存。在这种架构下，类元信息就突破了原来-XX:MaxPermSize的限制，现在可以使用更多的本地内存。这样就从一定程度上解决了原来在运行时生成大量类造成经常Full GC问题，如运行时使用反射、代理等。所以升级以后Java堆空间可能会增加。 元空间的本质和永久代类似，都是对JVM规范中方法区的实现。不过元空间与永久代之间的最大区别在于：元空间并不在虚拟机中，而是使用本地内存。因此，默认情况下，元空间的大小仅受本地内存限制，但可以通过以下参数指定元空间的大小： -XX:MetaspaceSize，初始空间大小，达到该值就会触发垃圾收集进行类型卸载，同时GC会对改值进行调整：如果释放了大量的空间，就适当降低该值；如果释放了很少的空间，那么在不超过MaxMetaspaceSize时，适当提高该值。 -XX:MaxMetaspaceSize，最大空间，默认是没有限制的。 除了上面的两个指定大小的选项外，还有两个与GC相关的属性： -XX:MinMetaspaceFreeRatio，在GC之后，最小的Metaspace剩余空间容量的百分比，减少为分配空间所导致的垃圾收集。 -XX:MaxMetaspaceFreeRatio，在GC之后，最大的Metaspace剩余空间容量的百分比，减少为释放空间所导致的垃圾收集。 现在我们在JDK 1.8重新运行一下上面第二部分(PermGen(永久代))的代码，不过这次不再指定PermSize和MaxPermSize。而是制定MetaspaceSize和MaxMetaspaceSize的大小。输出结果如下： 从输出结果，我们可以看出，这次不再出现永久代溢出，而是出现元空间的溢出。 四、总结 通过上面的分析，大家应该大致了解了JVM的内存划分，也清楚了JDK 1.8中永久代向元空间的转换。不过大家应该有一个疑问，就是为什么要做这个转换？以下为大家总结几点原因： 字符串在永久代中，容易出现性能问题和内存溢出。 类及方法的信息等比较难确定大小，因此对于永久代的大小指定比较困难，太小容易出现永久代溢出，太大则容易出现老年代溢出。 永久代会为GC带来不必要的复杂度，并且回收效率偏低。 Oracle可能会将HotSpot与JRockit合二为一。 ","link":"https://tianxiawuhao.github.io/nxmi3zjCk/"},{"title":" 漫画：什么是B+树？","content":"原创 [程序员小灰(公众号，强烈推荐)] 在上一篇漫画中，我们介绍了B-树的原理和应用，没看过的小伙伴们可以点击下面的链接： 漫画：什么是B-树？ 这一次我们来介绍B+树。 ————————————————— 一个m阶的B树具有如下几个特征： 1.根结点至少有两个子女。 2.每个中间节点都包含k-1个元素和k个孩子，其中 m/2 &lt;= k &lt;= m 3.每一个叶子节点都包含k-1个元素，其中 m/2 &lt;= k &lt;= m 4.所有的叶子结点都位于同一层。 5.每个节点中的元素从小到大排列，节点当中k-1个元素正好是k个孩子包含的元素的值域分划。 一个m阶的B+树具有如下几个特征： 1.有k个子树的中间节点包含有k个元素（B树中是k-1个元素），每个元素不保存数据，只用来索引，所有数据都保存在叶子节点。 2.所有的叶子结点中包含了全部元素的信息，及指向含这些元素记录的指针，且叶子结点本身依关键字的大小自小而大顺序链接。 3.所有的中间节点元素都同时存在于子节点，在子节点元素中是最大（或最小）元素。 B-树中的卫星数据（Satellite Information）： B+树中的卫星数据（Satellite Information）： 需要补充的是，在数据库的聚集索引（Clustered Index）中，叶子节点直接包含卫星数据。在非聚集索引（NonClustered Index）中，叶子节点带有指向卫星数据的指针。 第一次磁盘IO： 第二次磁盘IO： 第三次磁盘IO： B-树的范围查找过程 ** ** 自顶向下，查找到范围的下限（3）： 中序遍历到元素6： 中序遍历到元素8： 中序遍历到元素9： 中序遍历到元素11，遍历结束： B+树的范围查找过程 ** ** 自顶向下，查找到范围的下限（3）： 通过链表指针，遍历到元素6, 8： 通过链表指针，遍历到元素9, 11，遍历结束： B+树的特征： 1.有k个子树的中间节点包含有k个元素（B树中是k-1个元素），每个元素不保存数据，只用来索引，所有数据都保存在叶子节点。 2.所有的叶子结点中包含了全部元素的信息，及指向含这些元素记录的指针，且叶子结点本身依关键字的大小自小而大顺序链接。 3.所有的中间节点元素都同时存在于子节点，在子节点元素中是最大（或最小）元素。 B+树的优势： 1.单一节点存储更多的元素，使得查询的IO次数更少。 2.所有查询都要查找到叶子节点，查询性能稳定。 3.所有叶子节点形成有序链表，便于范围查询。 —————END————— ","link":"https://tianxiawuhao.github.io/jVTlR3ljT/"},{"title":"漫画：什么是B-树？","content":"原创 [程序员小灰(公众号，强烈推荐)] ———————————— ———————————— 二叉查找树的结构： 第1次磁盘IO： 第2次磁盘IO： 第3次磁盘IO： 第4次磁盘IO： 下面来具体介绍一下B-树（Balance Tree），一个m阶的B树具有如下几个特征： 1.根结点至少有两个子女。 2.每个中间节点都包含k-1个元素和k个孩子，其中 m/2 &lt;= k &lt;= m 3.每一个叶子节点都包含k-1个元素，其中 m/2 &lt;= k &lt;= m 4.所有的叶子结点都位于同一层。 5.每个节点中的元素从小到大排列，节点当中k-1个元素正好是k个孩子包含的元素的值域分划。 第1次磁盘IO： 在内存中定位（和9比较）： 第2次磁盘IO： 在内存中定位（和2，6比较）： 第3次磁盘IO： 在内存中定位（和3，5比较）： 自顶向下查找4的节点位置，发现4应当插入到节点元素3，5之间。 节点3，5已经是两元素节点，无法再增加。父亲节点 2， 6 也是两元素节点，也无法再增加。根节点9是单元素节点，可以升级为两元素节点。于是拆分节点3，5与节点2，6，让根节点9升级为两元素节点4，9。节点6独立为根节点的第二个孩子。 自顶向下查找元素11的节点位置。 删除11后，节点12只有一个孩子，不符合B树规范。因此找出12,13,15三个节点的中位数13，取代节点12，而节点12自身下移成为第一个孩子。（这个过程称为左旋） —————END————— ","link":"https://tianxiawuhao.github.io/LhRxiTagh/"},{"title":"数据结构常见的八大排序算法","content":"八大排序，三大查找是《数据结构》当中非常基础的知识点，在这里为了复习顺带总结了一下常见的八种排序算法。 常见的八大排序算法，他们之间关系如下： 他们的性能比较： 下面，利用java分别将他们进行实现。 直接插入排序 算法思想： 直接插入排序的核心思想就是：将数组中的所有元素依次跟前面已经排好的元素相比较，如果选择的元素比已排序的元素小，则交换，直到全部元素都比较过。 因此，从上面的描述中我们可以发现，直接插入排序可以用两个循环完成： 第一层循环：遍历待比较的所有数组元素 第二层循环：将本轮选择的元素(selected)与已经排好序的元素(ordered)相比较。 如果：selected &lt; ordered，那么将二者交换 代码实现 package sort; import java.util.Arrays; /** * @author wuhao * @desc 直接插入排序 * @date 2020-12-02 15:04:57 * 直接插入排序的核心思想就是：将数组中的所有元素依次跟前面已经排好的元素相比较，如果选择的元素比已排序的元素小，则交换，直到全部元素都比较过。 * 因此，从上面的描述中我们可以发现，直接插入排序可以用两个循环完成： * 第一层循环：遍历待比较的所有数组元素 * 第二层循环：将本轮选择的元素(selected)与已经排好序的元素(ordered)相比较。 * 如果：selected &lt; ordered，那么将二者交换 */ public class DirectInsertionSort { public static void main(String[] args) { int[] arr = {12, 32, 22, 7, 48}; insertSort(arr); } private static void insertSort(int[] arr) { for (int i = 1; i &lt; arr.length; i++) { int temp = arr[i]; int j; for (j = i - 1; j &gt;= 0; j--) { if (temp &lt; arr[j]) { arr[j + 1] = arr[j]; } else { break; } } arr[j + 1] = temp; } System.out.println(Arrays.toString(arr)); } } 希尔排序 算法思想： 希尔排序的算法思想：将待排序数组按照步长gap进行分组，然后将每组的元素利用直接插入排序的方法进行排序；每次将gap折半减小，循环上述操作；当gap=1时，利用直接插入，完成排序。 同样的：从上面的描述中我们可以发现：希尔排序的总体实现应该由三个循环完成： 第一层循环：将gap依次折半，对序列进行分组，直到gap=1 第二、三层循环：也即直接插入排序所需要的两次循环。具体描述见上。 代码实现 package sort; import java.util.Arrays; /** * @author wuhao * @desc 希尔排序 * @date 2020-12-02 15:20:43 * 希尔排序的算法思想：将待排序数组按照步长gap进行分组，然后将每组的元素利用直接插入排序的方法进行排序；每次将gap折半减小，循环上述操作；当gap=1时，利用直接插入，完成排序。 * 同样的：从上面的描述中我们可以发现：希尔排序的总体实现应该由三个循环完成： * 第一层循环：将gap依次折半，对序列进行分组，直到gap=1 * 第二、三层循环：也即直接插入排序所需要的两次循环。具体描述见上。 */ public class HillSort { public static void main(String[] args) { int[] arr = {12, 32, 22, 7, 48, 3, 5, 6, 8, 24}; shellSort(arr); } private static void shellSort(int[] arr) { for (int step = arr.length / 2; step &gt; 0; step /= 2) { // for (int i = step; i &lt; arr.length; i++) { // int temp = arr[i]; // int j; // for (j = i - step; j &gt;= 0 &amp;&amp; arr[j] &gt; temp; j -= step) { // arr[j + step] = arr[j]; // } // arr[j + step] = temp; // } for (int i = step; i &lt; arr.length; i++) { int k=i; for (int j = i-step; j &gt;=0 &amp;&amp; arr[j] &gt; arr[k]; j -= step,k -=step) { int temp = arr[j]; arr[j]= arr[k]; arr[k]=temp; } } } System.out.println(Arrays.toString(arr)); } } 简单选择排序 算法思想 简单选择排序的基本思想：比较+交换。 从待排序序列中，找到关键字最小的元素； 如果最小元素不是待排序序列的第一个元素，将其和第一个元素互换； 从余下的 N - 1 个元素中，找出关键字最小的元素，重复(1)、(2)步，直到排序结束。 因此我们可以发现，简单选择排序也是通过两层循环实现。 第一层循环：依次遍历序列当中的每一个元素 第二层循环：将遍历得到的当前元素依次与余下的元素进行比较，符合最小元素的条件，则交换。 代码实现 package sort; import java.util.Arrays; /** * @author wuhao * @desc 简单选择排序 * @date 2020-12-02 16:21:47 * 简单选择排序的基本思想：比较+交换。 * 从待排序序列中，找到关键字最小的元素； * 如果最小元素不是待排序序列的第一个元素，将其和第一个元素互换； * 从余下的 N - 1 个元素中，找出关键字最小的元素，重复(1)、(2)步，直到排序结束。 * 因此我们可以发现，简单选择排序也是通过两层循环实现。 * 第一层循环：依次遍历序列当中的每一个元素 * 第二层循环：将遍历得到的当前元素依次与余下的元素进行比较，符合最小元素的条件，则交换。 */ public class SimpleSelectionSort { public static void main(String[] args) { int[] arr = {12, 32, 22, 7, 48, 3, 5, 6, 8, 24}; selectionSort(arr); } private static void selectionSort(int[] arr) { for (int i = 0; i &lt; arr.length; i++) { int k = i; for (int j = i + 1; j &lt; arr.length; j++) { if (arr[j] &lt; arr[k]) { k = j; } } if (k != i) { int temp = arr[i]; arr[i] = arr[k]; arr[k] = temp; } } System.out.println(Arrays.toString(arr)); } } 堆排序 堆的概念 堆：本质是一种数组对象。特别重要的一点性质：任意的叶子节点小于（或大于）它所有的父节点。对此，又分为大顶堆和小顶堆，大顶堆要求节点的元素都要大于其孩子，小顶堆要求节点元素都小于其左右孩子，两者对左右孩子的大小关系不做任何要求。 利用堆排序，就是基于大顶堆或者小顶堆的一种排序方法。下面，我们通过大顶堆来实现。 基本思想： 堆排序可以按照以下步骤来完成： 构建大顶堆.png Paste_Image.png 构建初始堆，将待排序列构成一个大顶堆(或者小顶堆)，升序大顶堆，降序小顶堆； 将堆顶元素与堆尾元素交换，并断开(从待排序列中移除)堆尾元素。 重新构建堆。 重复2~3，直到待排序列中只剩下一个元素(堆顶元素)。 代码实现： package sort; import java.util.Arrays; /** * @author wuhao * @desc 堆排序 * @date 2020-12-04 09:13:58 * 堆排序可以按照以下步骤来完成： * 首先将序列构建称为大顶堆； * （这样满足了大顶堆那条性质：位于根节点的元素一定是当前序列的最大值） * 取出当前大顶堆的根节点，将其与序列末尾元素进行交换； * （此时：序列末尾的元素为已排序的最大值；由于交换了元素，当前位于根节点的堆并不一定满足大顶堆的性质） * 对交换后的n-1个序列元素进行调整，使其满足大顶堆的性质； * 重复2.3步骤，直至堆中只有1个元素为止 */ public class HeapSort { public static void main(String[] args) { int[] arr = {11, 44, 23, 67, 88, 65, 34, 48, 9, 12}; heapSort(arr); System.out.println(Arrays.toString(arr)); } private static void heapSort(int[] a) { // 首先需要创建根堆 for (int i = a.length / 2; i &gt;= 0; i--) { // 从最后一个非终端结点开始，然后一次-- HeapAdjust(a, i, a.length); } for (int i = a.length - 1; i &gt; 0; --i) {// 这个循环是把最大值a[0]放到末尾 ， int temp = a[0]; a[0] = a[i]; // 此时i代表最后一个元素 a[i] = temp; HeapAdjust(a, 0, i ); } } // 调整堆 private static void HeapAdjust(int[] a, int parent, int m) {// parent代表当前 m代表最后 int temp = a[parent]; // 先把a[parent]的值赋给temp保存起来 for (int j = 2 * parent; j &lt; m; j *= 2) { if (j+1 &lt; m &amp;&amp; a[j] &lt; a[j + 1]) { // 判断是a[parent]大还是a[j + 1]大，如果a[j + 1]大 就++j，把j换成当前最大 j++; } if (temp &gt;= a[j]) { // 如果temp中比最大值还大，代表本身就是一个根堆，break break;// 如果大于，就代表当前为大跟对，退出 } a[parent] = a[j];// 否则就把最大给[parent] parent = j;// 然后把最大下标给parent，继续循环,检查是否因为调整根堆而破坏了子树 } a[parent] = temp; } /** * 创建堆， * @param arr 待排序列 */ // private static void heapSort(int[] arr) { // //创建堆 // for (int i = (arr.length - 1) / 2; i &gt;= 0; i--) { // //从第一个非叶子结点从下至上，从右至左调整结构 // adjustHeap(arr, i, arr.length); // } // // //调整堆结构+交换堆顶元素与末尾元素 // for (int i = arr.length - 1; i &gt; 0; i--) { // //将堆顶元素与末尾元素进行交换 // int temp = arr[i]; // arr[i] = arr[0]; // arr[0] = temp; // // //重新对堆进行调整 // adjustHeap(arr, 0, i); // } // } /** * 调整堆 * @param arr 待排序列 * @param parent 父节点 * @param length 待排序列尾元素索引 */ // private static void adjustHeap(int[] arr, int parent, int length) { // //将temp作为父节点 // int temp = arr[parent]; // //左孩子 // int lChild = 2 * parent + 1; // // while (lChild &lt; length) { // //右孩子 // int rChild = lChild + 1; // // 如果有右孩子结点，并且右孩子结点的值大于左孩子结点，则选取右孩子结点 // if (rChild &lt; length &amp;&amp; arr[lChild] &lt; arr[rChild]) { // lChild++; // } // // // 如果父结点的值已经大于孩子结点的值，则直接结束 // if (temp &gt;= arr[lChild]) { // break; // } // // // 把孩子结点的值赋给父结点 // arr[parent] = arr[lChild]; // // //选取孩子结点的左孩子结点,继续向下筛选 // parent = lChild; // lChild = 2 * lChild + 1; // } // arr[parent] = temp; // } } 冒泡排序 基本思想 冒泡排序思路比较简单： 将序列当中的左右元素，依次比较，保证右边的元素始终大于左边的元素； （ 第一轮结束后，序列最后一个元素一定是当前序列的最大值；） 对序列当中剩下的n-1个元素再次执行步骤1。 对于长度为n的序列，一共需要执行n-1轮比较 （利用while循环可以减少执行次数） 代码实现 package sort; import java.util.Arrays; /** * @author wuhao * @desc 冒泡排序 * @date 2020-12-02 16:34:57 * 冒泡排序思路比较简单： * 将序列当中的左右元素，依次比较，保证右边的元素始终大于左边的元素； * （ 第一轮结束后，序列最后一个元素一定是当前序列的最大值；） * 对序列当中剩下的n-1个元素再次执行步骤1。 * 对于长度为n的序列，一共需要执行n-1轮比较 * （利用while循环可以减少执行次数） */ public class BubbleSort { public static void main(String[] args) { int[] arr = {12, 32, 22, 7, 48}; bubbleSort(arr); } private static void bubbleSort(int[] arr) { for (int i = 0; i &lt; arr.length; i++) { for (int j = 0; j &lt; i; j++) { if (arr[i] &lt; arr[j]) { int temp=arr[j]; arr[j]=arr[i]; arr[i]=temp; } } } System.out.println(Arrays.toString(arr)); } } 快速排序 算法思想： 快速排序的基本思想：挖坑填数+分治法 从序列当中选择一个基准数(pivot) 在这里我们选择序列当中第一个数为基准数 将序列当中的所有数依次遍历，比基准数大的位于其右侧，比基准数小的位于其左侧 重复步骤a.b，直到所有子集当中只有一个元素为止。 用伪代码描述如下： 1．i =L; j = R; 将基准数挖出形成第一个坑a[i]。 2．j--由后向前找比它小的数，找到后挖出此数填前一个坑a[i]中。 3．i++由前向后找比它大的数，找到后也挖出此数填到前一个坑a[j]中。 4．再重复执行2，3二步，直到i==j，将基准数填入a[i]中 代码实现： package sort; import java.util.Arrays; import java.util.Stack; /** * @author wuhao * @desc 快速排序 * @date 2020-12-02 17:08:57 * 快速排序的基本思想：挖坑填数+分治法 * 从序列当中选择一个基准数(pivot) * 在这里我们选择序列当中第一个数为基准数 * 将序列当中的所有数依次遍历，比基准数大的位于其右侧，比基准数小的位于其左侧 * 重复步骤a.b，直到所有子集当中只有一个元素为止。 * 用伪代码描述如下： * 1．i =L; j = R; 将基准数挖出形成第一个坑a[i]。 * 2．j--由后向前找比它小的数，找到后挖出此数填前一个坑a[i]中。 * 3．i++由前向后找比它大的数，找到后也挖出此数填到前一个坑a[j]中。 * 4．再重复执行2，3二步，直到i==j，将基准数填入a[i]中 */ public class QuickSort { public static void main(String[] args) { int[] arr = {12, 32, 22, 7, 48, 3, 35, 6, 8, 42}; // quickSort(arr, 0, arr.length - 1); System.out.println(Arrays.toString(arr)); /*-----------非递归实现----------*/ sort(arr, 0, arr.length - 1); System.out.println(Arrays.toString(arr)); } private static void quickSort(int[] arr, int left, int right) { if (left &gt; right) { return; } // base中存放基准数 int base = arr[left]; int i = left, j = right; while (i != j) { // 顺序很重要，先从右边开始往左找，直到找到比base值小的数 while (arr[j] &gt;= base &amp;&amp; i &lt; j) { j--; } // 再从左往右边找，直到找到比base值大的数 while (arr[i] &lt;= base &amp;&amp; i &lt; j) { i++; } // 上面的循环结束表示找到了位置或者(i&gt;=j)了，交换两个数在数组中的位置 if (i &lt; j) { int tmp = arr[i]; arr[i] = arr[j]; arr[j] = tmp; } } // 将基准数放到中间的位置（基准数归位） arr[left] = arr[i]; arr[i] = base; // 递归，继续向基准的左右两边执行和上面同样的操作 // i的索引处为上面已确定好的基准值的位置，无需再处理 quickSort(arr, left, i - 1); quickSort(arr, i + 1, right); } /*-----------------------------非递归实现------------------------------*/ public static void sort(int[] arr, int left, int right) { int privot, top, last; Stack&lt;Integer&gt; s = new Stack&lt;&gt;(); privot = QuickSort(arr, left, right); if (privot &gt; left + 1) { s.push(left); s.push(privot - 1); } if (privot &lt; right - 1) { s.push(privot + 1); s.push(right); } while (!s.empty()) { top = s.pop(); last = s.pop(); privot = QuickSort(arr, last, top); if (privot &gt; last + 1) { s.push(last); s.push(privot - 1); } if (privot &lt; top - 1) { //System.out.println(top); s.push(privot + 1); s.push(top); } } } public static int QuickSort(int[] arr, int left, int right) { int privot = left; while (left &lt; right) { while ((arr[privot] &lt; arr[right]) &amp; left &lt; right) { right--; } int temp = arr[privot]; arr[privot] = arr[right]; arr[right] = temp; privot = right; while ((arr[privot] &gt; arr[left]) &amp; left &lt; right) { left++; } temp = arr[privot]; arr[privot] = arr[left]; arr[left] = temp; privot = left; } return privot; } } 归并排序 算法思想： 归并排序是建立在归并操作上的一种有效的排序算法，该算法是采用0的一个典型的应用。它的基本操作是：将已有的子序列合并，达到完全有序的序列；即先使每个子序列有序，再使子序列段间有序。 归并排序其实要做两件事： 分解----将序列每次折半拆分 合并----将划分后的序列段两两排序合并 因此，归并排序实际上就是两个操作，拆分+合并 如何合并？ L[first...mid]为第一段，L[mid+1...last]为第二段，并且两端已经有序，现在我们要将两端合成达到L[first...last]并且也有序。 首先依次从第一段与第二段中取出元素比较，将较小的元素赋值给temp[] 重复执行上一步，当某一段赋值结束，则将另一段剩下的元素赋值给temp[] 此时将temp[]中的元素复制给L[]，则得到的L[first...last]有序 如何分解？ 在这里，我们采用递归的方法，首先将待排序列分成A,B两组；然后重复对A、B序列 分组；直到分组后组内只有一个元素，此时我们认为组内所有元素有序，则分组结束。 代码实现 package sort; import java.util.Arrays; /** * @author wuhao * @desc 归并排序 * @date 2020-12-03 09:35:00 * 归并排序是建立在归并操作上的一种有效的排序算法，该算法是采用0的一个典型的应用。它的基本操作是：将已有的子序列合并，达到完全有序的序列；即先使每个子序列有序，再使子序列段间有序。 * 归并排序其实要做两件事： * 分解----将序列每次折半拆分 * 合并----将划分后的序列段两两排序合并 * 因此，归并排序实际上就是两个操作，拆分+合并 * 如何合并？ * L[first...mid]为第一段，L[mid+1...last]为第二段，并且两端已经有序，现在我们要将两端合成达到L[first...last]并且也有序。 * 首先依次从第一段与第二段中取出元素比较，将较小的元素赋值给temp[] * 重复执行上一步，当某一段赋值结束，则将另一段剩下的元素赋值给temp[] * 此时将temp[]中的元素复制给L[]，则得到的L[first...last]有序 * 如何分解？ * 在这里，我们采用递归的方法，首先将待排序列分成A,B两组；然后重复对A、B序列 * 分组；直到分组后组内只有一个元素，此时我们认为组内所有元素有序，则分组结束。 */ public class MergeSort { public static void main(String[] args) { int[] arr = {11, 44, 23, 67, 88, 65, 34, 48, 9, 12}; int[] tmp = new int[arr.length]; //新建一个临时数组存放 mergeSort(arr, 0, arr.length - 1, tmp); System.out.println(Arrays.toString(arr)); } private static void mergeSort(int[] arr, int low, int high, int[] tmp) { if (low &lt; high) { int mid = (low + high) / 2; mergeSort(arr, low, mid, tmp); mergeSort(arr, mid + 1, high, tmp); merge(arr, low, mid, high, tmp); } } private static void merge(int[] arr, int low, int mid, int high, int[] tmp) { int i = 0; int j = low, k = mid + 1; while (j &lt;= mid &amp;&amp; k &lt;= high) { if (arr[j] &lt; arr[k]) { tmp[i++] = arr[j++]; } else { tmp[i++] = arr[k++]; } } while (j &lt;= mid) { tmp[i++] = arr[j++]; } while (k &lt;= high) { tmp[i++] = arr[k++]; } for (int l = 0; l &lt; i; l++) { arr[low+l] = tmp[l]; } } } 基数排序 算法思想 基数排序：通过序列中各个元素的值，对排序的N个元素进行若干趟的“分配”与“收集”来实现排序。 分配：我们将L[i]中的元素取出，首先确定其个位上的数字，根据该数字分配到与之序号相同的桶中 收集：当序列中所有的元素都分配到对应的桶中，再按照顺序依次将桶中的元素收集形成新的一个待排序列L[ ] 对新形成的序列L[]重复执行分配和收集元素中的十位、百位...直到分配完该序列中的最高位，则排序结束 代码实现 package sort; import java.util.Arrays; /** * @author wuhao * @desc 基数排序 * @date 2020-12-03 09:58:32 * 基数排序：通过序列中各个元素的值，对排序的N个元素进行若干趟的“分配”与“收集”来实现排序。 * 分配：我们将L[i]中的元素取出，首先确定其个位上的数字，根据该数字分配到与之序号相同的桶中 * 收集：当序列中所有的元素都分配到对应的桶中，再按照顺序依次将桶中的元素收集形成新的一个待排序列L[ ] * 对新形成的序列L[]重复执行分配和收集元素中的十位、百位...直到分配完该序列中的最高位，则排序结束 * 根据上述“基数排序”的展示，我们可以清楚的看到整个实现的过程 */ public class BaseSort { public static void main(String[] args) { int[] arr = {63, 157, 189, 51, 101, 47, 141, 121, 157, 156, 194, 117, 98, 139, 67, 133, 181, 12, 28, 0, 109}; radixSort(arr); System.out.println(Arrays.toString(arr)); } private static void radixSort(int[] arr) { //待排序列最大值 int max = arr[0]; int exp;//指数 //计算最大值 for (int anArr : arr) { if (anArr &gt; max) { max = anArr; } } //从个位开始，对数组进行排序 for (exp = 1; max / exp &gt; 0; exp *= 10) { //存储待排元素的临时数组 int[] temp = new int[arr.length]; //分桶个数 int[] buckets = new int[10]; //将数据出现的次数存储在buckets中 for (int value : arr) { //(value / exp) % 10 :value的最底位(个位) buckets[(value / exp) % 10]++; } //更改buckets[i]， for (int i = 1; i &lt; 10; i++) { buckets[i] += buckets[i - 1]; } //将数据存储到临时数组temp中 for (int i = arr.length - 1; i &gt;= 0; i--) { temp[buckets[(arr[i] / exp) % 10] - 1] = arr[i]; buckets[(arr[i] / exp) % 10]--; } //将有序元素temp赋给arr System.arraycopy(temp, 0, arr, 0, arr.length); } } } 后记 写完之后运行了一下时间比较： 从运行结果上来看，堆排序、归并排序、基数排序是真的快。 对于快速排序迭代深度超过的问题，可以将考虑将快排通过非递归的方式进行实现。 ","link":"https://tianxiawuhao.github.io/0BBhLzjWl/"},{"title":"Linux文件操作高频使用命令","content":"新建操作： mkdir abc #新建一个文件夹 touch abc.sh #新建一个文件 查看操作 查看目录： ll #显示目录文件详细信息 du -h 文件/目录 #查看大小 pwd #显示路径 查看文件内容： cat|head|tail命令 #查看abc的内容 cat abc.txt #查看abc前5行内容。默认是10行 head -5 abc.txt tail [选项] 文件名 各选项的含义如下： +num：从第num行以后开始显示 -num：从距文件尾num行处开始显示。如果省略num参数，系统默认值为10. -f: 循环读取,例如查看服务器日志时，可以实时观察 #filename 文件里的最尾部的内容显示在屏幕上，并且不断刷新。 tail -f filename #查看最后20行 tail -f filename more命令： more命令一次显示一屏信息，若信息未显示完屏幕底部将出现“-More-（xx%）”。 此时按Space键，可显示下一屏内容； 按“回车”键，显示下一行内容； 按B键，显示上一屏； 按Q键，可退出more命令。 less命令： 和more命令类似，但是比more命令更强大。在很多时候，必须使用less,比如管道。例如： ll /etc | less stat 命令： 查看文件的详细信息，比如创建修改时间，大小等 [root@localhost zx]# stat index.html 文件：&quot;index.html&quot; 大小：29006 块：64 IO 块：4096 普通文件 设备：fd00h/64768d Inode：17589607 硬链接：1 权限：(0644/-rw-r--r--) Uid：( 0/ root) Gid：( 0/ root) 环境：unconfined_u:object_r:home_root_t:s0 最近访问：2021-04-25 21:47:41.824053666 +0800 最近更改：2021-04-25 21:44:33.588587500 +0800 最近改动：2021-04-25 21:44:33.588587500 +0800 创建时间：- du 命令： 选项：-h 以合适的单位显示（会根据文件的大小自动选择kb或M等单位） [root@localhost zx]# du -h index.html 32K index.html 复制操作 同一机器的复制： cp:复制文件或目录 语法： cp [options] source dest -a：此选项通常在复制目录时使用，它保留链接、文件属性，并复制目录下的所有内容。其作用等于dpR参数组合。 -d：复制时保留链接。这里所说的链接相当于Windows系统中的快捷方式。 -f：覆盖已经存在的目标文件而不给出提示。 -i：与-f选项相反，在覆盖目标文件之前给出提示，要求用户确认是否覆盖，回答&quot;y&quot;时目标文件将被覆盖。 -p：除复制文件的内容外，还把修改时间和访问权限也复制到新文件中。 -r：若给出的源文件是一个目录文件，此时将复制该目录下所有的子目录和文件。 -l：不复制文件，只是生成链接文件。 举例： #将../html/index.html 复制到当前目录 cp ../html/index.html . #将../html/ 目录下的文件及子目录复制到当前的tt目录下，如果tt不存在，会自动创建 cp -r ../html/ tt/ #将文件file复制到目录/usr/men/tmp下，并改名为file1 cp file /usr/men/tmp/file1 #如果dir2目录已存在，则需要使用 cp -r dir1/. dir2 #如果这时使用cp -r dir1 dir2,则也会将dir1目录复制到dir2中，明显不符合要求。 ps:dir1、dir2改成对应的目录路径即可 远程复制 #将当前目录下的test.txt复制到远程111.12机器的/zx目录下 scp test.txt root@192.168.111.12:/zx #将test.txt复制到远程用户的根目录，并命名为textA.txt scp test.txt root@192.168.111.12:testA.txt #也可以不指定用户，在后续提示中再输入，如下： scp test.txt 192.168.111.12:/zx #从远程复制到本地： -r用于递归整个目录 scp -r remote_user@remote_ip:remote_folder local_path 移动操作: 移动操作可以理解成复制文件后，删除原文件。 #复制/zx/soft目录中的所有文件到当前目录 mv /zx/soft/* . #复制当前目录a.txt到当前的test目录下。 mv a.txt ./test/a.txt #复制文件夹到/tmp/下，必须保证tmp是存在的文件夹 mv /zx/soft/ /tmp/soft 重命名操作 重命名还是用的移动操作命令，比如： #将目录(文件)A重命名为B mv A B #将/a目录(文件)移动到/b下，并重命名为c。要保证b目录存在。 mv /a /b/c #将当前test1目录移动到当前的test目录并命名为b mv ./test1 ./test/b 解压压缩操作 tar -c: 建立压缩档案 -x：解压 -t：查看内容 -r：向压缩归档文件末尾追加文件 -u：更新原压缩包中的文件 这五个是独立的命令，压缩解压都要用到其中一个，可以和别的命令连用但只能用其中一个。下面的参数是根据需要在压缩或解压档案时可选的。 -z：有gzip属性的 -j：有bz2属性的 -Z：有compress属性的 -v：显示所有过程 -O：将文件解开到标准输出 下面的参数-f是必须的 -f: 使用档案名字，切记，这个参数是最后一个参数，后面只能接档案名。 举例说明 tar -cf all.tar *.jpg 这条命令是将所有.jpg的文件打成一个名为all.tar的包。-c是表示产生新的包，-f指定包的文件名。 tar -tf all.tar 这条命令是列出all.tar包中所有文件，-t是列出文件的意思 tar -xf all.tar 这条命令是解出all.tar包中所有文件，-x是解开的意思 压缩 tar –cvf jpg.tar *.jpg //将目录里所有jpg文件打包成jpg.tar eg2: tar -xzf nginx-1.14.0.tar.gz //解压到当前目录 tar -zxf nginx-1.14.0.tar.gz -C /usr/local/nginx #解压到对应目录 eg3: tar -zxvf nginx...tar.gz #解压并显示过程 注意：有些压缩程序提示命令找不到，需要进行安装，例如： yum install unzip 或在ubuntu上： apt-get install unzip 总结 1、*.tar 用 tar –xvf 解压 2、*.gz 用 gzip -d或者gunzip 解压 3、*.tar.gz和*.tgz 用 tar –xzf 解压 4、*.bz2 用 bzip2 -d或者用bunzip2 解压 5、*.tar.bz2用tar –xjf 解压 6、*.Z 用 uncompress 解压 7、*.tar.Z 用tar –xZf 解压 8、*.rar 用 unrar e解压 9、*.zip 用 unzip 解压 解压的时候，有时候不想覆盖已经存在的文件，那么可以加上-n参数 unzip -n test.zip unzip -n -d /temp test.zip 只看一下zip压缩包中包含哪些文件，不进行解压缩 unzip -l test.zip 查看显示的文件列表还包含压缩比率 unzip -v test.zip 检查zip文件是否损坏 unzip -t test.zip 如果已有相同的文件存在，要求unzip命令覆盖原先的文件 unzip -o test.zip -d /tmp/ 示例： eg1: unzip mydata.zip -d mydatabak #解压到mydatabak目录 1. xz 这是两层压缩，外面是xz压缩方式，里层是tar压缩,所以可以分两步实现解压 $ xz -d node-v6.10.1-linux-x64.tar.xz $ tar -xvf node-v6.10.1-linux-x64.tar 上传文件工具 从本地windows上传一些文件到远程Linux服务器可以通过xshell的xftp也可以通过下面这个小工具lrzsz，使用更加方便。 #安装工具 yum install lrzsz 常用命令： #下载文件dist.zip到本地 sz dist.zip #会打开窗口，上传文件到远程服务器 rz ln、file和touch命令 ln命令 用于创建链接文件，包括硬链接(Hard Link)和符号链接（Symbolic Link) 。我们常用的是符号链接，也称软连接。软连接就类似windows里的快捷方式。 示例： #在当前目录创建一个软连接，指向/etc/fastab，名称也是fastab ln -s /etc/fastab #在当前目录创建一个指向/boot/grub的软连接，命名为gb ln -s /boot/grub gb 注意：删除软连接 正确方式是： rm -rf ./gb 错误方式: rm -rf ./gb/ 这样会删除了原有grub下的内容。特别是针对系统文件的软连接，删除一定要慎重。 file命令 用于识别文件的类型 Linux中文件后缀只是方便使用者识别，没有实质的约束作用。file命令可以查看文件的实质类型： file [-bcLz] 文件|目录 选项说明： 文件|目录：需要识别的文件或目录 -b: 显示识别结果时，不显示文件名 -c: 显示执行过程 -L: 直接显示符号链接文件指向的文件类型 -z: 尝试去解读压缩文件的内容 示例： 可以看出，index.mp4本质是一个HTML而非一个mp4文件 [root@VM_0_13_centos soft]# file index.mp4 index.mp4: HTML document, UTF-8 Unicode text, with very long lines**touch命令：** 用于改变文件或目录的访问时间和修改时间。 touch命令： 用于改变文件或目录的访问时间和修改时间。 touch [-am] [-t&lt;日期时间&gt;] [目录|文件] 如果指定目录文件不存在，则会直接创建一个空文件，所以touch也常用来创建一个空白文件 #创建一个新文件aa.txt touch aa.txt 选项说明： -a: 只修改访问时间 -m : 只修改 修改时间 -t : 使用指定日期时间，而非系统时间 。例如要修改为2019年10月20日16：38分13秒。参数就是：‘20191020163813’ 示例： 修改之前可以先查看文件的时间戳: 用stat 命令查看 [root@VM_0_13_centos soft]# stat index.html File: ‘index.html’ Size: 17215 Blocks: 40 IO Block: 4096 regular file Device: fd01h/64769d Inode: 529352 Links: 1 Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root) Access: 2021-04-25 15:15:37.280616254 +0800 Modify: 2021-04-25 15:15:37.280616254 +0800 Change: 2021-04-25 15:15:37.290616257 +0800 Birth: - 开始修改：将index.html文件的访问和修改时间修改成当前系统的时间。 touch index.html 查找操作命令： 对于要用到的文件，目录等，经常有忘记的时候，所以查找命令就显得极为必要： find: 查找文件或目录 (常用) 语法如下： find [目录…] [-amin &lt;分钟&gt;] [-atime &lt;24小时数&gt;] [-cmin &lt;分钟&gt;] [-ctime&lt;24小时数&gt;][-empty][-exec&lt;执行命令&gt;][-fls&lt;列表文件&gt;][-follow] [-fstype &lt;系统文件类型&gt;] [-gid &lt;组编号&gt;] [-group &lt;组名称&gt;] [-nogroup] [-mmin &lt;分钟&gt;] [-mtime &lt;24小时数&gt;] [-name &lt;查找内容&gt;] [-nogroup] [-nouser] [-perm &lt;权限数值&gt;] [-size &lt;文件大小&gt;] [-uid &lt;用户编号&gt;] [-user &lt;用户名称&gt;] [-nouser] 几个常用选项说明： -size &lt;文件大小&gt;：查找符合指定大小的文件。文件大小单位可以是“c”表示Byte；“k”表示KB。如配置为“100k”，find命令会查找文件大小正好100KB的文件；配置为“+100k”，find命令会查找文件大小大于100KB的文件；配置为“-100k”，find命令会查找文件大小小于100KB的文件。 -user&lt;用户名称&gt;：查找所有者是指定用户的文件或目录，也能以用户编号指定 -name &lt;查找内容&gt;：查找指定的内容，在查找内容中使用“*” 表示任意个字符；使用“?”表示任何一个字符 -mtime &lt;24小时数&gt;：查找在指定时间**曾更改过内容**的文件或目录，单位以24小时计算。如配置为2，find命令会查找刚好在48小时之前更改过内容的文件；配置为+2，find命令会查找超过在48小时之前更改过内容的文件；配置为-2，find命令会查找在48小时之内更改过内容的文件。 -mmin &lt;分钟&gt;：查找在指定时间曾被**更改过内容**的文件或目录，单位以分钟计算。 -cmin &lt;分钟&gt;：查找在指定时间曾被**更改过权限**属性的文件或目录，单位以分钟计算。-ctime对应小时。 -amin &lt;分钟&gt;：查找的是指定时间**访问过**的文件或目录。-atim对应小时。 -perm &lt;权限数值&gt;：查找符合指定权限数值（有关权限数值见第6章）的文件或目录。如配置为“0700”，find命令会查找权限数值正好是“0700”的文件或目录；配置为“+0700”，find命令会查找权限数值大于 “0700”的文件或目录；配置为“-0700”，find 选项大概有以下几类： 1.按时间范围查找 2.按文件大小查找 3.按文件名称查找 4.按其他：比如权限、用户组、类型等 示例： #从根目开始，查找名称以nginx开头的目录和文件 find / -name nginx* #查找文件大小超过100M的文件 find / -size +100M #查找/home/zx目录下，10分钟内被修改过的文件和目录 find /home/zx/ -mmin -10 locate： 查找文件或目录(不常用) locate 查找内容 例如：locate nginx 会将所有包含nginx的目录和文件都列出来。可以用* 或？等匹配符。 locate的查找速度非常快，因为该命令查找的是数据库，所以有些刚修改的文件和目录，可能无法找到。可以采用：updatedb 命令更新数据库。 which: 查找文件(不常用) which [文件] which命令只会在PATH环境变量定义的路径及命令别名中查找，所以范围有限。 whereis : 查找文件(不常用) whichis [-bu] [-B&lt;目录&gt;] [-M&lt;目录&gt;] [-S&lt;目录&gt;] [文件] 常用选项： 文件：要查找的命令 -b: 只查找二进制文件 -u: 查找不包含指定类型的文件 -B&lt;目录&gt;： 只在指定目录下查找二进制文件 -M&lt;目录&gt;：只在指定目录查找帮助文件 -S&lt;目录&gt;：只在指定目录查找源码目录 例如： 默认只会在指定目录查找（/bin ,/etc ,/usr) [root@VM_0_13_centos soft]# whereis nginx nginx: /usr/local/nginx /usr/local/nginx/sbin/nginx /usr/local/nginx/sbin/nginx.bak ","link":"https://tianxiawuhao.github.io/yzriNkrRn/"},{"title":"JVM调优","content":"堆大小设置 ​ JVM 中最大堆大小有三方面限制：相关操作系统的数据模型（32-bt还是64-bit）限制；系统的可用虚拟内存限制；系统的可用物理内存限制。32位系统下，一般限制在1.5G~2G；64为操作系统对内存无限制。我在Windows Server 2003 系统，3.5G物理内存，JDK5.0下测试，最大可设置为1478m。 典型设置： java -Xmx3550m -Xms3550m -Xmn2g -Xss128k // -Xmx3550m：设置JVM最大堆可用内存为3550M。 // -Xms3550m：设置JVM初始堆内存为3550m。此值可以设置与-Xmx相同，以避免每次垃圾回收完成后JVM重新分配内存。 // -Xmn2g：设置年轻代大小为2G。整个JVM内存大小=年轻代大小 + 年老代大小 + 持久代大小。持久代一般固定大小为64m，所以增大年轻代后，将会减小年老代大小。此值对系统性能影响较大，Sun官方推荐配置为整个堆的3/8。 // -Xss128k：设置每个线程的堆栈大小。JDK5.0以后每个线程堆栈大小为1M，以前每个线程堆栈大小为256K。根据应用的线程所需内存大小进行调整。在相同物理内存下，减小这个值能生成更多的线程。但是操作系统对一个进程内的线程数还是有限制的，不能无限生成，经验值在3000~5000左右。 java -Xmx3550m -Xms3550m -Xss128k -XX:NewRatio=4 -XX:SurvivorRatio=4 -XX:MaxPermSize=16m -XX:MaxTenuringThreshold=0 // -XX:NewRatio=4:设置年轻代（包括Eden和两个Survivor区）与年老代的比值（除去持久代）。设置为4，则年轻代与年老代所占比值为1：4，年轻代占整个堆栈的1/5 // -XX:SurvivorRatio=4：设置年轻代中Eden区与Survivor区的大小比值。设置为4，则两个Survivor区与一个Eden区的比值为2:4，一个Survivor区占整个年轻代的1/6 // -XX:MaxPermSize=16m:设置持久代大小为16m。 // -XX:MaxTenuringThreshold=0：设置垃圾最大年龄。如果设置为0的话，则年轻代对象不经过Survivor区，直接进入年老代。对于年老代比较多的应用，可以提高效率。如果将此值设置为一个较大值，则年轻代对象会在Survivor区进行多次复制，这样可以增加对象再年轻代的存活时间，增加在年轻代即被回收的概论。 回收器选择 ​ JVM给了三种选择：串行收集器、并行收集器、并发收集器，但是串行收集器只适用于小数据量的情况，所以这里的选择主要针对并行收集器和并发收集器。默认情况下，JDK5.0以前都是使用串行收集器，如果想使用其他收集器需要在启动时加入相应参数。JDK5.0以后，JVM会根据当前系统配置进行判断。 吞吐量优先的并行收集器 如上文所述，并行收集器主要以到达一定的吞吐量为目标，适用于科学技术和后台处理等。 典型配置： java -Xmx3800m -Xms3800m -Xmn2g -Xss128k -XX:+UseParallelGC -XX:ParallelGCThreads=20 // -XX:+UseParallelGC：选择垃圾收集器为并行收集器。此配置仅对年轻代有效。即上述配置下，年轻代使用并发收集，而年老代仍旧使用串行收集。 // -XX:ParallelGCThreads=20：配置并行收集器的线程数，即：同时多少个线程一起进行垃圾回收。此值最好配置与处理器数目相等。 java -Xmx3550m -Xms3550m -Xmn2g -Xss128k -XX:+UseParallelGC -XX:ParallelGCThreads=20 -XX:+UseParallelOldGC // -XX:+UseParallelOldGC：配置年老代垃圾收集方式为并行收集。JDK6.0支持对年老代并行收集。 java -Xmx3550m -Xms3550m -Xmn2g -Xss128k -XX:+UseParallelGC -XX:MaxGCPauseMillis=100 // -XX:MaxGCPauseMillis=100:设置每次年轻代垃圾回收的最长时间，如果无法满足此时间，JVM会自动调整年轻代大小，以满足此值。 java -Xmx3550m -Xms3550m -Xmn2g -Xss128k -XX:+UseParallelGC -XX:MaxGCPauseMillis=100 -XX:+UseAdaptiveSizePolicy // -XX:+UseAdaptiveSizePolicy：设置此选项后，并行收集器会自动选择年轻代区大小和相应的Survivor区比例，以达到目标系统规定的最低相应时间或者收集频率等，此值建议使用并行收集器时，一直打开。 响应时间优先的并发收集器 如上文所述，并发收集器主要是保证系统的响应时间，减少垃圾收集时的停顿时间。适用于应用服务器、电信领域等。 典型配置： java -Xmx3550m -Xms3550m -Xmn2g -Xss128k -XX:ParallelGCThreads=20 -XX:+UseConcMarkSweepGC -XX:+UseParNewGC // -XX:+UseConcMarkSweepGC：设置年老代为并发收集。测试中配置这个以后，-XX:NewRatio=4的配置失效了，原因不明。所以，此时年轻代大小最好用-Xmn设置。 // -XX:+UseParNewGC:设置年轻代为并行收集。可与CMS收集同时使用。JDK5.0以上，JVM会根据系统配置自行设置，所以无需再设置此值。 java -Xmx3550m -Xms3550m -Xmn2g -Xss128k -XX:+UseConcMarkSweepGC -XX:CMSFullGCsBeforeCompaction=5 -XX:+UseCMSCompactAtFullCollection // -XX:CMSFullGCsBeforeCompaction：由于并发收集器不对内存空间进行压缩、整理，所以运行一段时间以后会产生“碎片”，使得运行效率降低。此值设置运行多少次GC以后对内存空间进行压缩、整理。 // -XX:+UseCMSCompactAtFullCollection：打开对年老代的压缩。可能会影响性能，但是可以消除碎片 辅助信息 JVM提供了大量命令行参数，打印信息，供调试使用。主要有以下一些： -XX:+PrintGC 输出形式：[GC 118250K-&gt;113543K(130112K), 0.0094143 secs] [Full GC 121376K-&gt;10414K(130112K), 0.0650971 secs] -XX:+PrintGCDetails 输出形式：[GC [DefNew: 8614K-&gt;781K(9088K), 0.0123035 secs] 118250K-&gt;113543K(130112K), 0.0124633 secs] [GC [DefNew: 8614K-&gt;8614K(9088K), 0.0000665 secs][Tenured: 112761K-&gt;10414K(121024K), 0.0433488 secs] 121376K-&gt;10414K(130112K), 0.0436268 secs] -XX:+PrintGCTimeStamps -XX:+PrintGC：PrintGCTimeStamps可与上面两个混合使用 输出形式：11.851: [GC 98328K-&gt;93620K(130112K), 0.0082960 secs] -XX:+PrintGCApplicationConcurrentTime:打印每次垃圾回收前，程序未中断的执行时间。可与上面混合使用 输出形式：Application time: 0.5291524 seconds -XX:+PrintGCApplicationStoppedTime：打印垃圾回收期间程序暂停的时间。可与上面混合使用 输出形式：Total time for which application threads were stopped: 0.0468229 seconds -XX:PrintHeapAtGC:打印GC前后的详细堆栈信息 -Xloggc:filename:与上面几个配合使用，把相关日志信息记录到文件以便分析。 常见配置汇总 堆设置 -Xms:等价于(-XX:InitialHeapSize)初始堆大小 -Xmx:等价于(-XX:MaxHeapSize)最大堆大小 -Xmn:堆中新生代初始及最大大小，如果需要进一步细化，初始化大小用-XX:NewSize，最大大小用-XX:MaxNewSize -Xss:等价于(-XX:ThreadStackSize)每个线程堆栈的大小 -XX:NewSize=n:设置年轻代大小 -XX:NewRatio=n:设置年轻代和年老代的比值。如:为3，表示年轻代与年老代比值为1：3，年轻代占整个年轻代年老代和的1/4 -XX:SurvivorRatio=n:年轻代中Eden区与两个Survivor区的比值。注意Survivor区有两个。如：3，表示Eden：Survivor=3：2，一个Survivor区占整个年轻代的1/5 -XX:MaxPermSize=n:设置持久代大小 -XX:MaxTenuringThreshold=n:设置垃圾最大年龄,年轻代复制次数 收集器设置 -XX:+UseSerialGC:设置串行收集器 -XX:+UseParallelGC:设置并行收集器 -XX:+UseParalledlOldGC:设置并行年老代收集器 -XX:+UseConcMarkSweepGC:设置并发收集器 垃圾回收统计信息 -XX:+PrintGC :打印GC信息 -XX:+PrintGCDetails :打印GC时的内存 -XX:+PrintGCTimeStamps ：选择打印GC的方式后，再添加此参数。比如：-XX:+PrintGC -XX:+PrintGCTimeStamps每次GC时会打印程序启动后至GC发生的时间戳。 -Xloggc:filename：将GC日志输出到指定位置 并行收集器设置 -XX:ParallelGCThreads=n:设置并行收集器收集时使用的CPU数。并行收集线程数。 -XX:MaxGCPauseMillis=n:设置并行收集最大暂停时间 -XX:GCTimeRatio=n:设置垃圾回收时间占程序运行时间的百分比。公式为1/(1+n) 并发收集器设置 -XX:+CMSIncrementalMode:设置为增量模式。适用于单CPU情况。 -XX:ParallelGCThreads=n:设置并发收集器年轻代收集方式为并行收集时，使用的CPU数。并行收集线程数。 调优总结 年轻代大小选择 响应时间优先的应用：尽可能设大，直到接近系统的最低响应时间限制（根据实际情况选择）。在此种情况下，年轻代收集发生的频率也是最小的。同时，减少到达年老代的对象。 吞吐量优先的应用：尽可能的设置大，可能到达Gbit的程度。因为对响应时间没有要求，垃圾收集可以并行进行，一般适合8CPU以上的应用。 年老代大小选择 响应时间优先的应用：年老代使用并发收集器，所以其大小需要小心设置，一般要考虑并发会话率和会话持续时间等一些参数。如果堆设置小了，可以会造成内存碎片、高回收频率以及应用暂停而使用传统的标记清除方式；如果堆大了，则需要较长的收集时间。最优化的方案，一般需要参考以下数据获得： 并发垃圾收集信息 持久代并发收集次数 传统GC信息 花在年轻代和年老代回收上的时间比例 减少年轻代和年老代花费的时间，一般会提高应用的效率 吞吐量优先的应用：一般吞吐量优先的应用都有一个很大的年轻代和一个较小的年老代。原因是，这样可以尽可能回收掉大部分短期对象，减少中期的对象，而年老代尽存放长期存活对象。 较小堆引起的碎片问题 因为年老代的并发收集器使用标记、清除算法，所以不会对堆进行压缩。当收集器回收时，他会把相邻的空间进行合并，这样可以分配给较大的对象。但是，当堆空间较小时，运行一段时间以后，就会出现“碎片”，如果并发收集器找不到足够的空间，那么并发收集器将会停止，然后使用传统的标记、清除方式进行回收。如果出现“碎片”，可能需要进行如下配置： -XX:+UseCMSCompactAtFullCollection：使用并发收集器时，开启对年老代的压缩。 -XX:CMSFullGCsBeforeCompaction=0：上面配置开启的情况下，这里设置多少次Full GC后，对年老代进行压缩 实际应用命令 jps 查看所有的jvm进程，包括进程ID，进程启动的路径等等。 我自己也用PS，即：ps -ef | grep java jstack 观察jvm中当前所有线程的运行情况和线程当前状态。 系统崩溃了？如果java程序崩溃生成core文件，jstack工具可以用来获得core文件的java stack和native stack的信息，从而可以轻松地知道java程序是如何崩溃和在程序何处发生问题。 系统hung住了？jstack工具还可以附属到正在运行的java程序中，看到当时运行的java程序的java stack和native stack的信息, 如果现在运行的java程序呈现hung的状态，jstack是非常有用的。 jstat jstat利用JVM内建的指令对Java应用程序的资源和性能进行实时的命令行的监控，包括了对进程的classloader，compiler，gc情况； 特别的，一个极强的监视内存的工具，可以用来监视VM内存内的各种堆和非堆的大小及其内存使用量，以及加载类的数量。 jmap 监视进程运行中的jvm物理内存的占用情况，该进程内存内，所有对象的情况，例如产生了哪些对象，对象数量； 系统崩溃了？jmap 可以从core文件或进程中获得内存的具体匹配情况，包括Heap size, Perm size等等 jinfo 观察进程运行环境参数，包括Java System属性和JVM命令行参数 系统崩溃了？jinfo可以从core文件里面知道崩溃的Java应用程序的配置信息。 java -XX:+PrintFlagsInitial -version:查看所有初始化默认配置 java -XX:+PrintFlagsFinal -version:查看所有及修改更新配置 java -XX:+printCommandLineFlags -version:打印命令参数配置 :=:被jvm或人为修改过的参数 -XX:+UseParallelGC:采用并行垃圾回收器 -XX:+UseSerialGC:采用串行垃圾回收器 ","link":"https://tianxiawuhao.github.io/HEMJYLqMW/"},{"title":"Java的值传递","content":"形参与实参 我们先来重温一组语法： 形参：方法被调用时需要传递进来的参数，如：func(int a)中的a，它只有在func被调用期间a才有意义，也就是会被分配内存空间，在方法func执行完成后，a就会被销毁释放空间，也就是不存在了 实参：方法被调用时传入的实际值，它在方法被调用前就已经被初始化并且在方法被调用时传入。 举个栗子： public static void func(int a){ a=20; System.out.println(a); } public static void main(String[] args) { int a=10; func(a); } 例子中 int a=10;中的a在被调用之前就已经创建并初始化，在调用func方法时，他被当做参数传入，所以这个a是实参。 而func(int a)中的a只有在func被调用时它的生命周期才开始，而在func调用结束之后，它也随之被JVM释放掉，所以这个a是形参。 Java的数据类型 所谓数据类型，是编程语言中对内存的一种抽象表达方式，我们知道程序是由代码文件和静态资源组成，在程序被运行前，这些代码存在在硬盘里，程序开始运行，这些代码会被转成计算机能识别的内容放到内存中被执行。 因此 数据类型实质上是用来定义编程语言中相同类型的数据的存储形式，也就是决定了如何将代表这些值的位存储到计算机的内存中。 所以，数据在内存中的存储，是根据数据类型来划定存储形式和存储位置的。 那么 Java的数据类型有哪些？ 基本类型：编程语言中内置的最小粒度的数据类型。它包括四大类八种类型： 4种整数类型：byte、short、int、long 2种浮点数类型：float、double 1种字符类型：char 1种布尔类型：boolean 引用类型：引用也叫句柄，引用类型，是编程语言中定义的在句柄中存放着实际内容所在地址的地址值的一种数据形式。它主要包括： 类 接口 数组 有了数据类型，JVM对程序数据的管理就规范化了，不同的数据类型，它的存储形式和位置是不一样的，要想知道JVM是怎么存储各种类型的数据，就得先了解JVM的内存划分以及每部分的职能。 JVM内存的划分及职能 ​ Java语言本身是不能操作内存的，它的一切都是交给JVM来管理和控制的，因此Java内存区域的划分也就是JVM的区域划分，在说JVM的内存划分之前，我们先来看一下Java程序的执行过程，如下图： 有图可以看出：Java代码被编译器编译成字节码之后，JVM开辟一片内存空间（也叫运行时数据区），通过类加载器加到到运行时数据区来存储程序执行期间需要用到的数据和相关信息，在这个数据区中，它由以下几部分组成： 虚拟机栈 堆 程序计数器 方法区 本地方法栈 我们接着来了解一下每部分的原理以及具体用来存储程序执行过程中的哪些数据。 虚拟机栈 虚拟机栈是Java方法执行的内存模型，栈中存放着栈帧，每个栈帧分别对应一个被调用的方法，方法的调用过程对应栈帧在虚拟机中入栈到出栈的过程。 栈是线程私有的，也就是线程之间的栈是隔离的；当程序中某个线程开始执行一个方法时就会相应的创建一个栈帧并且入栈（位于栈顶），在方法结束后，栈帧出栈。 下图表示了一个Java栈的模型以及栈帧的组成： 栈帧:是用于支持虚拟机进行方法调用和方法执行的数据结构，它是虚拟机运行时数据区中的虚拟机栈的栈元素。 每个栈帧中包括： 局部变量表:用来存储方法中的局部变量（非静态变量、函数形参）。当变量为基本数据类型时，直接存储值，当变量为引用类型时，存储的是指向具体对象的引用。 操作数栈:Java虚拟机的解释执行引擎被称为&quot;基于栈的执行引擎&quot;，其中所指的栈就是指操作数栈。 指向运行时常量池的引用:存储程序执行时可能用到常量的引用。 方法返回地址:存储方法执行完成后的返回地址。 堆 堆是用来存储对象本身和数组的，在JVM中只有一个堆，因此，堆是被所有线程共享的。 方法区： ​ 方法区是一块所有线程共享的内存逻辑区域，在JVM中只有一个方法区，用来存储一些线程可共享的内容，它是线程安全的，多个线程同时访问方法区中同一个内容时，只能有一个线程装载该数据，其它线程只能等待。 ​ 方法区可存储的内容有：类的全路径名、类的直接超类的权全限定名、类的访问修饰符、类的类型（类或接口）、类的直接接口全限定名的有序列表、常量池（字段，方法信息，静态变量，类型引用（class））等。 本地方法栈： ​ 本地方法栈的功能和虚拟机栈是基本一致的，并且也是线程私有的，它们的区别在于虚拟机栈是为执行Java方法服务的，而本地方法栈是为执行本地方法服务的。 有人会疑惑：什么是本地方法？为什么Java还要调用本地方法？ 程序计数器： ​ 线程私有的。记录着当前线程所执行的字节码的行号指示器，在程序运行过程中，字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、异常处理、线程恢复等基础功能都需要依赖计数器完成。 数据如何在内存中存储？ 从上面程序运行图我们可以看到，JVM在程序运行时的内存分配有三个地方： 堆 栈 静态方法区 常量区 相应地，每个存储区域都有自己的内存分配策略： 堆式： 栈式 静态 我们已经知道：Java中的数据类型有基本数据类型和引用数据类型，那么这些数据的存储都使用哪一种策略呢？ 这里要分以下的情况进行探究： 基本数据类型的存储： A. 基本数据类型的局部变量 B. 基本数据类型的成员变量 C. 基本数据类型的静态变量 引用数据类型的存储 基本数据类型的存储 我们分别来研究一下： A.基本数据类型的局部变量 定义基本数据类型的局部变量以及数据都是直接存储在内存中的栈上，也就是前面说到的“虚拟机栈”，数据本身的值就是存储在栈空间里面。 如上图，在方法内定义的变量直接存储在栈中，如 int age=50; int weight=50; int grade=6; 当我们写“int age=50；”，其实是分为两步的： int age;//定义变量 age=50;//赋值 ​ 首先JVM创建一个名为age的变量，存于局部变量表中，然后去栈中查找是否存在有字面量值为50的内容，如果有就直接把age指向这个地址，如果没有，JVM会在栈中开辟一块空间来存储“50”这个内容，并且把age指向这个地址。因此我们可以知道： ​ 我们声明并初始化基本数据类型的局部变量时，变量名以及字面量值都是存储在栈中，而且是真实的内容。 ​ 我们再来看“int weight=50；”，按照刚才的思路：字面量为50的内容在栈中已经存在，因此weight是直接指向这个地址的。由此可见：栈中的数据在当前线程下是共享的。 ​ 那么如果再执行下面的代码呢？ ​ weight=40； ​ 当代码中重新给weight变量进行赋值时，JVM会去栈中寻找字面量为40的内容，发现没有，就会开辟一块内存空间存储40这个内容，并且把weight指向这个地址。由此可知： ​ 基本数据类型的数据本身是不会改变的，当局部变量重新赋值时，并不是在内存中改变字面量内容，而是重新在栈中寻找已存在的相同的数据，若栈中不存在，则重新开辟内存存新数据，并且把要重新赋值的局部变量的引用指向新数据所在地址。 B. 基本数据类型的成员变量 成员变量：顾名思义，就是在类体中定义的变量。 看下图： 我们看per的地址指向的是堆内存中的一块区域，我们来还原一下代码： public class Person{ private int age; private String name; private int grade; //省略setter getter方法 static void run(){ System.out.println(&quot;run....&quot;); }; } //调用 Person per=new Person(); ​ 同样是局部变量的age、name、grade却被存储到了堆中为per对象开辟的一块空间中。因此可知：基本数据类型的成员变量名和值都存储于堆中，其生命周期和对象的是一致的。 C. 基本数据类型的静态变量 ​ 前面提到方法区用来存储一些共享数据，因此基本数据类型的静态变量名以及值存储于方法区的运行时常量池中，静态变量随类加载而加载，随类消失而消失 引用数据类型的存储: 上面提到：堆是用来存储对象本身和数组，而引用（句柄）存放的是实际内容的地址值，因此通过上面的程序运行图，也可以看出，当我们定义一个对象时 Person per=new Person(); 实际上，它也是有两个过程： Person per;//定义变量 per=new Person();//赋值 ​ 在执行Person per;时，JVM先在虚拟机栈中的变量表中开辟一块内存存放per变量，在执行per=new Person()时，JVM会创建一个Person类的实例对象并在堆中开辟一块内存存储这个实例，同时把实例的地址值赋值给per变量。因此可见： 对于引用数据类型的对象/数组，变量名存在栈中，变量值存储的是对象的地址，并不是对象的实际内容。 值传递和引用传递 ​ 前面已经介绍过形参和实参，也介绍了数据类型以及数据在内存中的存储形式，接下来，就是文章的主题：值传递和引用的传递。 值传递： 在方法被调用时，实参通过形参把它的内容副本传入方法内部，此时形参接收到的内容是实参值的一个拷贝，因此在方法内对形参的任何操作，都仅仅是对这个副本的操作，不影响原始值的内容。 来看个例子： public static void valueCrossTest(int age,float weight){ System.out.println(&quot;传入的age：&quot;+age); System.out.println(&quot;传入的weight：&quot;+weight); age=33; weight=89.5f; System.out.println(&quot;方法内重新赋值后的age：&quot;+age); System.out.println(&quot;方法内重新赋值后的weight：&quot;+weight); } //测试 public static void main(String[] args) { int a=25; float w=77.5f; valueCrossTest(a,w); System.out.println(&quot;方法执行后的age：&quot;+a); System.out.println(&quot;方法执行后的weight：&quot;+w); } 输出结果： 传入的age：25 传入的weight：77.5 方法内重新赋值后的age：33 方法内重新赋值后的weight：89.5 方法执行后的age：25 方法执行后的weight：77.5 从上面的打印结果可以看到： a和w作为实参传入valueCrossTest之后，无论在方法内做了什么操作，最终a和w都没变化。 这是什么造型呢？！！ 下面我们根据上面学到的知识点，进行详细的分析： 首先程序运行时，调用mian()方法，此时JVM为main()方法往虚拟机栈中压入一个栈帧，即为当前栈帧，用来存放main()中的局部变量表(包括参数)、操作栈、方法出口等信息，如a和w都是mian()方法中的局部变量，因此可以断定，a和w是躺着mian方法所在的栈帧中 如图： 而当执行到valueCrossTest()方法时，JVM也为其往虚拟机栈中压入一个栈，即为当前栈帧，用来存放valueCrossTest()中的局部变量等信息，因此age和weight是躺着valueCrossTest方法所在的栈帧中，而他们的值是从a和w的值copy了一份副本而得，如图： 因而可以a和age、w和weight对应的内容是不一致的，所以当在方法内重新赋值时，实际流程如图： 也就是说，age和weight的改动，只是改变了当前栈帧（valueCrossTest方法所在栈帧）里的内容，当方法执行结束之后，这些局部变量都会被销毁，mian方法所在栈帧重新回到栈顶，成为当前栈帧，再次输出a和w时，依然是初始化时的内容。 因此： 值传递传递的是真实内容的一个副本，对副本的操作不影响原内容，也就是形参怎么变化，不会影响实参对应的内容。 引用传递： ​ ”引用”也就是指向真实内容的地址值，在方法调用时，实参的地址通过方法调用被传递给相应的形参，在方法体内，形参和实参指向同一块内存地址，对形参的操作会影响的真实内容。 举个栗子： 先定义一个对象： public class Person { private String name; private int age; public String getName() { return name; } public void setName(String name) { this.name = name; } public int getAge() { return age; } public void setAge(int age) { this.age = age; } } 我们写个函数测试一下： public static void PersonCrossTest(Person person){ System.out.println(&quot;传入的person的name：&quot;+person.getName()); person.setName(&quot;我是张三&quot;); System.out.println(&quot;方法内重新赋值后的name：&quot;+person.getName()); } //测试 public static void main(String[] args) { Person p=new Person(); p.setName(&quot;我是李四&quot;); p.setAge(45); PersonCrossTest(p); System.out.println(&quot;方法执行后的name：&quot;+p.getName()); } 输出结果： 传入的person的name：我是李四 方法内重新赋值后的name：我是张三 方法执行后的name：我是张三 可以看出，person经过personCrossTest()方法的执行之后，内容发生了改变，这印证了上面所说的“引用传递”，对形参的操作，改变了实际对象的内容。 那么，到这里就结题了吗？ 不是的，没那么简单， 能看得到想要的效果 是因为刚好选对了例子而已！！！ 下面我们对上面的例子稍作修改，加上一行代码， public static void PersonCrossTest(Person person){ System.out.println(&quot;传入的person的name：&quot;+person.getName()); person=new Person();//加多此行代码 person.setName(&quot;我是张三&quot;); System.out.println(&quot;方法内重新赋值后的name：&quot;+person.getName()); } 输出结果： 传入的person的name：我是李四 方法内重新赋值后的name：我是张三 方法执行后的name：我是李四 为什么这次的输出和上次的不一样了呢？ 看出什么问题了吗？ 按照上面讲到JVM内存模型可以知道，对象和数组是存储在Java堆区的，而且堆区是共享的，因此程序执行到main（）方法中的下列代码时 Person p=new Person(); p.setName(&quot;我是李四&quot;); p.setAge(45); PersonCrossTest(p); JVM会在堆内开辟一块内存，用来存储p对象的所有内容，同时在main（）方法所在线程的栈区中创建一个引用p存储堆区中p对象的真实地址，如图： 当执行到PersonCrossTest()方法时，因为方法内有这么一行代码： person=new Person(); JVM需要在堆内另外开辟一块内存来存储new Person()，假如地址为“xo3333”，那此时形参person指向了这个地址，假如真的是引用传递，那么由上面讲到：引用传递中形参实参指向同一个对象，形参的操作会改变实参对象的改变。 可以推出：实参也应该指向了新创建的person对象的地址，所以在执行PersonCrossTest()结束之后，最终输出的应该是后面创建的对象内容。 然而实际上，最终的输出结果却跟我们推测的不一样，最终输出的仍然是一开始创建的对象的内容。 由此可见：引用传递，在Java中并不存在。 但是有人会疑问：为什么第一个例子中，在方法内修改了形参的内容，会导致原始对象的内容发生改变呢？ 这是因为：无论是基本类型和是引用类型，在实参传入形参时，都是值传递，也就是说传递的都是一个副本，而不是内容本身。 有图可以看出，方法内的形参person和实参p并无实质关联，它只是由p处copy了一份指向对象的地址，此时： p和person都是指向同一个对象。 因此在第一个例子中，对形参p的操作，会影响到实参对应的对象内容。而在第二个例子中，当执行到new Person()之后，JVM在堆内开辟一块空间存储新对象，并且把person改成指向新对象的地址，此时： p依旧是指向旧的对象，person指向新对象的地址。 所以此时对person的操作，实际上是对新对象的操作，于实参p中对应的对象毫无关系。 结语 因此可见：在Java中所有的参数传递，不管基本类型还是引用类型，都是值传递，或者说是副本传递。 只是在传递过程中： 如果是对基本数据类型的数据进行操作，由于原始内容和副本都是存储实际值，并且是在不同的栈区，因此形参的操作，不影响原始内容。 如果是对引用类型的数据进行操作，分两种情况，一种是形参和实参保持指向同一个对象地址，则形参的操作，会影响实参指向的对象的内容。一种是形参被改动指向新的对象地址（如重新赋值引用），则形参的操作，不会影响实参指向的对象的内容。 ","link":"https://tianxiawuhao.github.io/nRZ1sLPT7/"},{"title":"附录 kafka常见面试问题汇总","content":"基础题目 1、Apache Kafka 是什么? Apach Kafka 是一款分布式流处理框架，用于实时构建流处理应用。它有一个核心 的功能广为人知，即作为企业级的消息引擎被广泛使用。 你一定要先明确它的流处理框架地位，这样能给面试官留 下一个很专业的印象。 2、什么是消费者组? 消费者组是 Kafka 独有的概念，如果面试官问这 个，就说明他对此是有一定了解的。我先给出标准答案： 1、定义：即消费者组是 Kafka 提供的可扩展且具有容错性的消费者机制。 2、原理：在 Kafka 中，消费者组是一个由多个消费者实例 构成的组。多个实例共同订阅若干个主题，实现共同消费。同一个组下的每个实例都配置有 相同的组 ID，被分配不同的订阅分区。当某个实例挂掉的时候，其他实例会自动地承担起 它负责消费的分区。 此时，又有一个小技巧给到你:消费者组的题目，能够帮你在某种程度上掌控下面的面试方向。 2.1消费者组的位移提交机制 broker维护消费者的消费位移信息，老的版本存储在zk上，新版本存储在内部的topic里。本质上，位移信息消费者自己维护也可以，但是如果消费者挂了或者重启，对于某一个分区的消费位移不就丢失了吗？所以，还是需要提交到broker端做持久化的。 2.2消费者组与 Broker 之间的交互 Kafka中消费者的消费方式 consumer采用pull(拉)模式从broker中读取数据。 拉取模式也有不足，如果kafka没有数据，消费者可能会陷入循环中，一直返回空数据。针对这一点，kafka消费者在消费数据时会传入一个时长参数timeout，如果当前没有数据可供消费，consumer会等待一段时间后再返回，这段时长即为timeout Kafka的分区分配策略 一个消费者组中有多个消费者，一个broker有多个分区，所有必然会涉及到分区分配问题，即确定哪一个分区由哪一个consumer来消费。kafka有两种分区分配策略：RoundRobin和Range 1） RoundRobin 按照消费者组划分，将消费者组作为一个整体，要求整个消费者组内的消费者订阅相同的主题，否则会导致错误的分配的问题。 2） Range （默认） 按照单个主题划分，可能导致消费者消费分区个数不对等的问题； offset的维护 由于kafka可能出现故障，故障之后要恢复到上一次消费的位置，往下继续进行消费。因此consumer需要实时记录自己消费到了哪一个offset，从0.9版本开始，consumer默认将offset保存在kafka的内置topic中，该topic为_consumer_offsets（0.9版本前放在zookeeper中） 3、在 Kafka 中，ZooKeeper 的作用是什么? 目前，Kafka 使用 ZooKeeper 存放集群元数据、成员管理、Controller 选举，以及其他一些管理类任务。之后，等 KIP-500 提案完成后，Kafka 将完全不再依赖 于 ZooKeeper。 记住，一定要突出“目前”，以彰显你非常了解社区的演进计划。“存放元数据”是指主题 分区的所有数据都保存在 ZooKeeper 中，且以它保存的数据为权威，其他“人”都要与它 保持对齐。“成员管理”是指 Broker 节点的注册、注销以及属性变更，等 等。“Controller 选举”是指选举集群 Controller，而其他管理类任务包括但不限于主题 删除、参数配置等。 不过，抛出 KIP-500 也可能是个双刃剑。碰到非常资深的面试官，他可能会进一步追问你 KIP-500 是怎么做的。一言以蔽之:KIP-500 思想，是使用社区自研的基于 Raft 的共识算法， 替代 ZooKeeper，实现 Controller 自选举。 4、解释下 Kafka 中位移(offset)的作用 在 Kafka 中，每个 主题分区下的每条消息都被赋予了一个唯一的 ID 数值，用于标识它在分区中的位置。这个 ID 数值，就被称为位移，或者叫偏移量。一旦消息被写入到分区日志，它的位移值将不能 被修改。 答完这些之后，你还可以把整个面试方向转移到你希望的地方。常见方法有以下 3 种: 如果你深谙 Broker 底层日志写入的逻辑，可以强调下消息在日志中的存放格式; 如果你明白位移值一旦被确定不能修改，可以强调下“Log Cleaner 组件都不能影响位 移值”这件事情; 如果你对消费者的概念还算熟悉，可以再详细说说位移值和消费者位移值之间的区别。 5、Partition的Leader Replica和Follower Replica的区别 这道题表面上是考核你对 Leader 和 Follower 区别的理解，但很容易引申到 Kafka 的同步 机制上。因此，我建议你主动出击，一次性地把隐含的考点也答出来，也许能够暂时把面试 官“唬住”，并体现你的专业性。 你可以这么回答:Kafka 副本当前分为领导者副本和追随者副本。只有 Leader 副本才能 对外提供读写服务，响应 Clients 端的请求。Follower 副本只是采用拉(PULL)的方 式，被动地同步 Leader 副本中的数据，并且在 Leader 副本所在的 Broker 宕机后，随时 准备应聘 Leader 副本。 通常来说，回答到这个程度，其实才只说了 60%，因此，我建议你再回答两个额外的加分 项。 强调 Follower 副本也能对外提供读服务。自 Kafka 2.4 版本开始，社区通过引入新的 Broker 端参数，允许 Follower 副本有限度地提供读服务。 强调 Leader 和 Follower 的消息序列在实际场景中不一致。很多原因都可能造成 Leader 和 Follower 保存的消息序列不一致，比如程序 Bug、网络问题等。这是很严重 的错误，必须要完全规避。你可以补充下，之前确保一致性的主要手段是高水位机制， 但高水位值无法保证 Leader 连续变更场景下的数据一致性，因此，社区引入了 Leader Epoch 机制，来修复高水位值的弊端。关于“Leader Epoch 机制”，国内的资料不是 很多，它的普及度远不如高水位，不妨大胆地把这个概念秀出来，力求惊艳一把。 炫技式题目 6、LEO、LSO、AR、ISR、HW 都表示什么含义? LEO:Log End Offset。日志末端位移值或末端偏移量，表示日志下一条待插入消息的 位移值。举个例子，如果日志有 10 条消息，位移值从 0 开始，那么，第 10 条消息的位 移值就是 9。此时，LEO = 10。 LSO:Log Stable Offset。这是 Kafka 事务的概念。如果你没有使用到事务，那么这个 值不存在(其实也不是不存在，只是设置成一个无意义的值)。该值控制了事务型消费 者能够看到的消息范围。它经常与 Log Start Offset，即日志起始位移值相混淆，因为 有些人将后者缩写成 LSO，这是不对的。在 Kafka 中，LSO 就是指代 Log Stable Offset。 AR:Assigned Replicas。AR 是主题被创建后，分区创建时被分配的副本集合，副本个 数由副本因子决定。 ISR:In-Sync Replicas。Kafka 中特别重要的概念，指代的是 AR 中那些与 Leader 保 持同步的副本集合。在 AR 中的副本可能不在 ISR 中，但 Leader 副本天然就包含在 ISR 中。关于 ISR，还有一个常见的面试题目是如何判断副本是否应该属于 ISR。目前的判断 依据是:Follower 副本的 LEO 落后 Leader LEO 的时间，是否超过了 Broker 端参数 replica.lag.time.max.ms 值。如果超过了，副本就会被从 ISR 中移除。 HW:高水位值(High watermark)。这是控制消费者可读取消息范围的重要字段。一 个普通消费者只能“看到”Leader 副本上介于 Log Start Offset 和 HW(不含)之间的 所有消息。水位以上的消息是对消费者不可见的。关于 HW，问法有很多，我能想到的 最高级的问法，就是让你完整地梳理下 Follower 副本拉取 Leader 副本、执行同步机制 的详细步骤。这就是我们的第 20 道题的题目，一会儿我会给出答案和解析。 7，Kafka 的 ISR 机制是什么? 这个机制简单来说，就是会自动给每个 Partition 维护一个 ISR 列表，这个列表里一定会有 Leader，然后还会包含跟 Leader 保持同步的 Follower。 也就是说，只要 Leader 的某个 Follower 一直跟他保持数据同步，那么就会存在于 ISR 列表里。 但是如果 Follower 因为自身发生一些问题，导致不能及时的从 Leader 同步数据过去，那么这个 Follower 就会被认为是“out-of-sync”，被从 ISR 列表里踢出去。 所以大家先得明白这个 ISR 是什么，说白了，就是 Kafka 自动维护和监控哪些 Follower 及时的跟上了 Leader 的数据同步。 8，Kafka 写入的数据如何保证不丢失? 所以如果要让写入 Kafka 的数据不丢失，你需要保证如下几点： 每个 Partition 都至少得有 1 个 Follower 在 ISR 列表里，跟上了 Leader 的数据同步。 每次写入数据的时候，都要求至少写入 Partition Leader 成功，同时还有至少一个 ISR 里的 Follower 也写入成功，才算这个写入是成功了。 如果不满足上述两个条件，那就一直写入失败，让生产系统不停的尝试重试，直到满足上述两个条件，然后才能认为写入成功。 按照上述思路去配置相应的参数，才能保证写入 Kafka 的数据不会丢失。 如上图所示，假如现在 Leader 没有 Follower 了，或者是刚写入 Leader，Leader 立马就宕机，还没来得及同步给 Follower。 在这种情况下，写入就会失败，然后你就让生产者不停的重试，直到 Kafka 恢复正常满足上述条件，才能继续写入。这样就可以让写入 Kafka 的数据不丢失。 9. 高水位和Epoch 在Kafka中，高水位的作用主要有两个 定义消息可见性，即用来标识分区下的哪些消息是可以被消费者消费的。 帮助Kafka完成副本同步 下面这张图展示了多个与高水位相关的 Kafka 术语。 假设这是某个分区 Leader 副本的高水位图。首先，请注意图中的“已提交消息”和“未提交消息”。之前在讲到 Kafka 持久性保障的时候，特意对两者进行了区分。现在，再次强调一下。在分区高水位以下的消息被认为是已提交消息，反之就是未提交消息。 消费者只能消费已提交消息，即图中位移小于 8 的所有消息。注意，这里我们不讨论 Kafka 事务，因为事务机制会影响消费者所能看到的消息的范围，它不只是简单依赖高水位来判断。它依靠一个名为 LSO（Log Stable Offset）的位移值来判断事务型消费者的可见性。 另外，位移值等于高水位的消息也属于未提交消息。也就是说，高水位上的消息是不能被消费者消费的。 Log End Offset（LEO） Log End Offset（LEO）表示副本写入下一条消息的位移值。注意，数字 15 所在的方框是虚线，这就说明，这个副本当前只有 15 条消息，位移值是从 0 到 14，下一条新消息的位移是 15。显然，介于高水位和 LEO 之间的消息就属于未提交消息。这也从侧面告诉了我们一个重要的事实，那就是：同一个副本对象，其高水位值不会大于 LEO 值。 高水位和 LEO 是副本对象的两个重要属性。Kafka 所有副本都有对应的高水位和 LEO 值，而不仅仅是 Leader 副本。只不过 Leader 副本比较特殊，Kafka 使用 Leader 副本的高水位来定义所在分区的高水位。换句话说，分区的高水位就是其 Leader 副本的高水位。 高水位和LEO更新机制 现在，我们知道了每个副本对象都保存了一组高水位值和 LEO 值，但实际上，在 Leader 副本所在的 Broker 上，还保存了其他 Follower 副本的HW和LEO 值。 如上图所示，Broker 0 上保存了某分区的 Leader 副本和所有 Follower 副本的 LEO 值，而 Broker 1 上仅仅保存了该分区的某个 Follower 副本。Kafka 把 Broker 0 上保存的这些 Follower 副本又称为远程副本（Remote Replica）。Kafka 副本机制在运行过程中，会更新 Broker 1 上 Follower 副本的高水位和 LEO 值，同时也会更新 Broker 0 上 Leader 副本的高水位和 LEO 以及所有远程副本的 LEO，但它不会更新远程副本的高水位值，也就是我在图中标记为灰色的部分。 保存远程副本的作用主要是帮助 Leader 副本确定其高水位，也就是分区高水位。下图是副本同步机制： 10,Leader副本保持同步 与Leader副本保持同步的判断条件有两个： 该远程Follower副本在ISR中。 该远程Follower副本LEO值落后于Leader副本LEO值的时间，不超过Broker端参数replica.lag.time.max.ms的值（默认值10秒） 副本同步全流程 当生产者发送一条消息时，Leader 和 Follower 副本对应的高水位是怎么被更新的呢？ 首先是初始状态。下面这张图中的 remote LEO 就是刚才的远程副本的 LEO 值。在初始状态时，所有值都是 0。 当生产者给主题分区发送一条消息后，状态变更为： 此时，Leader 副本成功将消息写入了本地磁盘，故 LEO 值被更新为 1。 Follower 再次尝试从 Leader 拉取消息。和之前不同的是，这次有消息可以拉取了，因此状态进一步变更为： 这时，Follower 副本也成功地更新 LEO 为 1。此时，Leader 和 Follower 副本的 LEO 都是 1，但各自的高水位依然是 0，还没有被更新。它们需要在下一轮的拉取中被更新，如下图所示： 在新一轮的拉取请求中，由于位移值是 0 的消息已经拉取成功，因此 Follower 副本这次请求拉取的是位移值 =1 的消息。Leader 副本接收到此请求后，更新远程副本 LEO 为 1，然后更新 Leader 高水位为 1。做完这些之后，它会将当前已更新过的高水位值 1 发送给 Follower 副本。Follower 副本接收到以后，也将自己的高水位值更新成 1。至此，一次完整的消息同步周期就结束了。事实上，Kafka 就是利用这样的机制，实现了 Leader 和 Follower 副本之间的同步。 Leader Epoch 从刚才的分析中，我们知道，Follower 副本的高水位更新需要一轮额外的拉取请求才能实现。如果把上面那个例子扩展到多个 Follower 副本，情况可能更糟，也许需要多轮拉取请求。也就是说，Leader 副本高水位更新和 Follower 副本高水位更新在时间上是存在错配的。这种错配是很多“数据丢失”或“数据不一致”问题的根源。基于此，社区在 0.11 版本正式引入了 Leader Epoch 概念，来规避因高水位更新错配导致的各种不一致问题。 所谓 Leader Epoch，我们大致可以认为是 Leader 版本。它由两部分数据组成。 Epoch。一个单调增加的版本号。每当副本领导权发生变更时，都会增加该版本号。小版本号的 Leader 被认为是过期 Leader，不能再行使 Leader 权力。 起始位移（Start Offset）。Leader 副本在该 Epoch 值上写入的首条消息的位移。 举个例子来说明一下 Leader Epoch。假设现在有两个 Leader Epoch&lt;0, 0&gt; 和 &lt;1, 120&gt;，那么，第一个 Leader Epoch 表示版本号是 0，这个版本的 Leader 从位移 0 开始保存消息，一共保存了 120 条消息。之后，Leader 发生了变更，版本号增加到 1，新版本的起始位移是 120。 Kafka Broker 会在内存中为每个分区都缓存 Leader Epoch 数据，同时它还会定期地将这些信息持久化到一个 checkpoint 文件中。当 Leader 副本写入消息到磁盘时，Broker 会尝试更新这部分缓存。如果该 Leader 是首次写入消息，那么 Broker 会向缓存中增加一个 Leader Epoch 条目，否则就不做更新。这样，每次有 Leader 变更时，新的 Leader 副本会查询这部分缓存，取出对应的 Leader Epoch 的起始位移，以避免数据丢失和不一致的情况。 接下来看一个实际的例子，它展示的是 Leader Epoch 是如何防止数据丢失的。 引用 Leader Epoch 机制后，Follower 副本 B 重启回来后，需要向 A 发送一个特殊的请求去获取 Leader 的 LEO 值。在这个例子中，该值为 2。当获知到 Leader LEO=2 后，B 发现该 LEO 值不比它自己的 LEO 值小，而且缓存中也没有保存任何起始位移值 &gt; 2 的 Epoch 条目，因此 B 无需执行任何日志截断操作。这是对高水位机制的一个明显改进，即副本是否执行日志截断不再依赖于高水位进行判断。 现在，副本 A 宕机了，B 成为 Leader。同样地，当 A 重启回来后，执行与 B 相同的逻辑判断，发现也不用执行日志截断，至此位移值为 1 的那条消息在两个副本中均得到保留。后面当生产者程序向 B 写入新消息时，副本 B 所在的 Broker 缓存中，会生成新的 Leader Epoch 条目：[Epoch=1, Offset=2]。之后，副本 B 会使用这个条目帮助判断后续是否执行日志截断操作。这样，通过 Leader Epoch 机制，Kafka 完美地规避了这种数据丢失场景。 深度思考题 11、Kafka 为什么不支持读写分离? 这道题目考察的是你对 Leader/Follower 模型的思考。 Leader/Follower 模型并没有规定 Follower 副本不可以对外提供读服务。很多框架都是允 许这么做的，只是 Kafka 最初为了避免不一致性的问题，而采用了让 Leader 统一提供服 务的方式。 不过，在开始回答这道题时，你可以率先亮出观点:自 Kafka 2.4 之后，Kafka 提供了有限度的读写分离，也就是说，Follower 副本能够对外提供读服务。 说完这些之后，你可以再给出之前的版本不支持读写分离的理由。 场景不适用。读写分离适用于那种读负载很大，而写操作相对不频繁的场景，可 Kafka 不属于这样的场景。 同步机制。Kafka 采用 PULL 方式实现 Follower 的同步，因此，Follower 与 Leader 存 在不一致性窗口。如果允许读 Follower 副本，就势必要处理消息滞后(Lagging)的问题。 12、如何调优 Kafka? 回答任何调优问题的第一步，就是 确定优化目标，并且定量给出目标! 这点特别重要。对于 Kafka 而言，常见的优化目标是吞吐量、延时、持久性和可用性。每一个方向的优化思路都 是不同的，甚至是相反的。 确定了目标之后，还要明确优化的维度。有些调优属于通用的优化思路，比如对操作系统、 JVM 等的优化;有些则是有针对性的，比如要优化 Kafka 的 TPS。我们需要从 3 个方向去考虑 Producer 端:增加 batch.size、linger.ms，启用压缩，关闭重试等。 Broker 端:增加 num.replica.fetchers，提升 Follower 同步 TPS，避免 Broker Full GC 等。 Consumer:增加 fetch.min.bytes 等 13、Controller 发生网络分区(Network Partitioning)时，Kafka 会怎 么样? 这道题目能够诱发我们对分布式系统设计、CAP 理论、一致性等多方面的思考。不过，针 对故障定位和分析的这类问题，我建议你首先言明“实用至上”的观点，即不论怎么进行理论分析，永远都要以实际结果为准。一旦发生 Controller 网络分区，那么，第一要务就是 查看集群是否出现“脑裂”，即同时出现两个甚至是多个 Controller 组件。这可以根据 Broker 端监控指标 ActiveControllerCount 来判断。 现在，我们分析下，一旦出现这种情况，Kafka 会怎么样。 由于 Controller 会给 Broker 发送 3 类请求，即LeaderAndIsrRequest、 StopReplicaRequest 和 UpdateMetadataRequest，因此，一旦出现网络分区，这些请求将不能顺利到达 Broker 端。这将影响主题的创建、修改、删除操作的信息同步，表现为 集群仿佛僵住了一样，无法感知到后面的所有操作。因此，网络分区通常都是非常严重的问 题，要赶快修复。 14、Java Consumer 为什么采用单线程来获取消息? 在回答之前，如果先把这句话说出来，一定会加分:Java Consumer 是双线程的设计。一 个线程是用户主线程，负责获取消息;另一个线程是心跳线程，负责向 Kafka 汇报消费者 存活情况。将心跳单独放入专属的线程，能够有效地规避因消息处理速度慢而被视为下线 的“假死”情况。 单线程获取消息的设计能够避免阻塞式的消息获取方式。单线程轮询方式容易实现异步非阻塞式，这样便于将消费者扩展成支持实时流处理的操作算子。因为很多实时流处理操作算子都不能是阻塞式的。另外一个可能的好处是，可以简化代码的开发。多线程交互的代码是非常容易出错的。 15、简述 Follower 副本消息同步的完整流程 首先，Follower 发送 FETCH 请求给 Leader。接着，Leader 会读取底层日志文件中的消 息数据，再更新它内存中的 Follower 副本的 LEO 值，更新为 FETCH 请求中的 fetchOffset 值。最后，尝试更新分区高水位值。Follower 接收到 FETCH 响应之后，会把 消息写入到底层日志，接着更新 LEO 和 HW 值。 Leader 和 Follower 的 HW 值更新时机是不同的，Follower 的 HW 更新永远落后于 Leader 的 HW。这种时间上的错配是造成各种不一致的原因。 ","link":"https://tianxiawuhao.github.io/aFuh1RRNc/"},{"title":"第三章 Apache Kafka 与Spark的集成","content":"在本章中，我们将讨论如何将Apache Kafka与Spark Streaming API集成。 关于Spark Spark Streaming API支持实时数据流的可扩展，高吞吐量，容错流处理。 数据可以从诸如Kafka，Flume，Twitter等许多源中提取，并且可以使用复杂的算法来处理，例如地图，缩小，连接和窗口等高级功能。 最后，处理的数据可以推送到文件系统，数据库和活动仪表板。 弹性分布式数据集(RDD)是Spark的基本数据结构。 它是一个不可变的分布式对象集合。 RDD中的每个数据集划分为逻辑分区，可以在集群的不同节点上计算。 与Spark集成 Kafka是Spark流式传输的潜在消息传递和集成平台。 Kafka充当实时数据流的中心枢纽，并使用Spark Streaming中的复杂算法进行处理。 一旦数据被处理，Spark Streaming可以将结果发布到另一个Kafka主题或存储在HDFS，数据库或仪表板中。 下图描述了概念流程。 现在，让我们详细了解Kafka-Spark API。 SparkConf API 它表示Spark应用程序的配置。 用于将各种Spark参数设置为键值对。 SparkConf 类有以下方法 - set(string key，string value) - 设置配置变量。 remove(string key) - 从配置中移除密钥。 setAppName(string name) - 设置应用程序的应用程序名称。 get(string key) - get key StreamingContext API 这是Spark功能的主要入口点。 SparkContext表示到Spark集群的连接，可用于在集群上创建RDD，累加器和广播变量。 签名的定义如下所示。 public StreamingContext(String master, String appName, Duration batchDuration, String sparkHome, scala.collection.Seq&lt;String&gt; jars, scala.collection.Map&lt;String,String&gt; environment) 主 - 要连接的群集网址(例如mesos:// host:port，spark:// host:port，local [4])。 appName - 作业的名称，以显示在集群Web UI上 batchDuration - 流式数据将被分成批次的时间间隔 public StreamingContext(SparkConf conf, Duration batchDuration) 通过提供新的SparkContext所需的配置创建StreamingContext。 conf - Spark参数 batchDuration - 流式数据将被分成批次的时间间隔 KafkaUtils API KafkaUtils API用于将Kafka集群连接到Spark流。 此API具有如下定义的显着方法 createStream 。 public static ReceiverInputDStream&lt;scala.Tuple2&lt;String,String&gt;&gt; createStream( StreamingContext ssc, String zkQuorum, String groupId, scala.collection.immutable.Map&lt;String,Object&gt; topics, StorageLevel storageLevel) 上面显示的方法用于创建从Kafka Brokers提取消息的输入流。 ssc - StreamingContext对象。 zkQuorum - Zookeeper quorum。 groupId - 此消费者的组ID。 主题 - 返回要消费的主题的地图。 storageLevel - 用于存储接收的对象的存储级别。 KafkaUtils API有另一个方法createDirectStream，用于创建一个输入流，直接从Kafka Brokers拉取消息，而不使用任何接收器。 这个流可以保证来自Kafka的每个消息都包含在转换中一次。 示例应用程序在Scala中完成。 要编译应用程序，请下载并安装 sbt ，scala构建工具(类似于maven)。 主要应用程序代码如下所示。 import java.util.HashMap import org.apache.kafka.clients.producer.{KafkaProducer, ProducerConfig, Produc-erRecord} import org.apache.spark.SparkConf import org.apache.spark.streaming._ import org.apache.spark.streaming.kafka._ object KafkaWordCount { def main(args: Array[String]) { if (args.length &lt; 4) { System.err.println(&quot;Usage: KafkaWordCount &lt;zkQuorum&gt;&lt;group&gt; &lt;topics&gt; &lt;numThreads&gt;&quot;) System.exit(1) } val Array(zkQuorum, group, topics, numThreads) = args val sparkConf = new SparkConf().setAppName(&quot;KafkaWordCount&quot;) val ssc = new StreamingContext(sparkConf, Seconds(2)) ssc.checkpoint(&quot;checkpoint&quot;) val topicMap = topics.split(&quot;,&quot;).map((_, numThreads.toInt)).toMap val lines = KafkaUtils.createStream(ssc, zkQuorum, group, topicMap).map(_._2) val words = lines.flatMap(_.split(&quot; &quot;)) val wordCounts = words.map(x =&gt; (x, 1L)) .reduceByKeyAndWindow(_ &amp;plus; _, _ - _, Minutes(10), Seconds(2), 2) wordCounts.print() ssc.start() ssc.awaitTermination() } } 构建脚本 spark-kafka集成取决于Spark，Spark流和Spark与Kafka的集成jar。 创建一个新文件 build.sbt ，并指定应用程序详细信息及其依赖关系。 在编译和打包应用程序时， sbt 将下载所需的jar。 name := &quot;Spark Kafka Project&quot; version := &quot;1.0&quot; scalaVersion := &quot;2.10.5&quot; libraryDependencies += &quot;org.apache.spark&quot; %% &quot;spark-core&quot; % &quot;1.6.0&quot; libraryDependencies += &quot;org.apache.spark&quot; %% &quot;spark-streaming&quot; % &quot;1.6.0&quot; libraryDependencies += &quot;org.apache.spark&quot; %% &quot;spark-streaming-kafka&quot; % &quot;1.6.0&quot; 编译/包装 运行以下命令以编译和打包应用程序的jar文件。 我们需要将jar文件提交到spark控制台以运行应用程序。 sbt package 提交到Spark 启动Kafka Producer CLI(在上一章中解释)，创建一个名为 my-first-topic 的新主题，并提供一些样本消息，如下所示。 Another spark test message 运行以下命令将应用程序提交到spark控制台。 /usr/local/spark/bin/spark-submit --packages org.apache.spark:spark-streaming -kafka_2.10:1.6.0 --class &quot;KafkaWordCount&quot; --master local[4] target/scala-2.10/spark -kafka-project_2.10-1.0.jar localhost:2181 &lt;group name&gt; &lt;topic name&gt; &lt;number of threads&gt; 此应用程序的示例输出如下所示。 spark console messages .. (Test,1) (spark,1) (another,1) (message,1) spark console message .. 原文：https://www.w3cschool.cn/apache_kafka/apache_kafka_integration_spark.html ","link":"https://tianxiawuhao.github.io/2csXIfKdw/"},{"title":"第二章 Apache Kafka 整合 Storm","content":"在本章中，我们将学习如何将Kafka与Apache Storm集成。 关于Storm Storm最初由Nathan Marz和BackType的团队创建。 在短时间内，Apache Storm成为分布式实时处理系统的标准，允许您处理大量数据。 Storm是非常快的，并且一个基准时钟为每个节点每秒处理超过一百万个元组。 Apache Storm持续运行，从配置的源(Spouts)消耗数据，并将数据传递到处理管道(Bolts)。 联合，Spouts和Bolt构成一个拓扑。 与Storm集成 Kafka和Storm自然互补，它们强大的合作能够实现快速移动的大数据的实时流分析。 Kafka和Storm集成是为了使开发人员更容易地从Storm拓扑获取和发布数据流。 概念流 Spouts是流的源。 例如，一个喷头可以从Kafka Topic读取元组并将它们作为流发送。 Bolt消耗输入流，处理并可能发射新的流。 Bolt可以从运行函数，过滤元组，执行流聚合，流连接，与数据库交谈等等做任何事情。 Storm拓扑中的每个节点并行执行。 拓扑无限运行，直到终止它。 Storm将自动重新分配任何失败的任务。 此外，Storm保证没有数据丢失，即使机器停机和消息被丢弃。 让我们详细了解Kafka-Storm集成API。 有三个主要类集成Kafka与Storm。 他们如下 - BrokerHosts - ZkHosts &amp; StaticHosts BrokerHosts是一个接口，ZkHosts和StaticHosts是它的两个主要实现。 ZkHosts用于通过在ZooKeeper中维护细节来动态跟踪Kafka代理，而StaticHosts用于手动/静态设置Kafka代理及其详细信息。 ZkHosts是访问Kafka代理的简单快捷的方式。 ZkHosts的签名如下 - public ZkHosts(String brokerZkStr, String brokerZkPath) public ZkHosts(String brokerZkStr) 其中brokerZkStr是ZooKeeper主机，brokerZkPath是ZooKeeper路径以维护Kafka代理详细信息。 KafkaConfig API 此API用于定义Kafka集群的配置设置。 Kafka Con-fig的签名定义如下 public KafkaConfig(BrokerHosts hosts, string topic) 主机 - BrokerHosts可以是ZkHosts / StaticHosts。 主题 - 主题名称。 SpoutConfig API Spoutconfig是KafkaConfig的扩展，支持额外的ZooKeeper信息。 public SpoutConfig(BrokerHosts hosts, string topic, string zkRoot, string id) 主机 - BrokerHosts可以是BrokerHosts接口的任何实现 主题 - 主题名称。 zkRoot - ZooKeeper根路径。 id - spouts存储在Zookeeper中消耗的偏移量的状态。 ID应该唯一标识您的喷嘴。 SchemeAsMultiScheme SchemeAsMultiScheme是一个接口，用于指示如何将从Kafka中消耗的ByteBuffer转换为风暴元组。 它源自MultiScheme并接受Scheme类的实现。 有很多Scheme类的实现，一个这样的实现是StringScheme，它将字节解析为一个简单的字符串。 它还控制输出字段的命名。 签名定义如下。 public SchemeAsMultiScheme(Scheme scheme) 方案 - 从kafka消耗的字节缓冲区。 KafkaSpout API KafkaSpout是我们的spout实现，它将与Storm集成。 它从kafka主题获取消息，并将其作为元组发送到Storm生态系统。 KafkaSpout从SpoutConfig获取其配置详细信息。 下面是一个创建一个简单的Kafka喷水嘴的示例代码。 // ZooKeeper connection string BrokerHosts hosts = new ZkHosts(zkConnString); //Creating SpoutConfig Object SpoutConfig spoutConfig = new SpoutConfig(hosts, topicName, &quot;/&quot; + topicName UUID.randomUUID().toString()); //convert the ByteBuffer to String. spoutConfig.scheme = new SchemeAsMultiScheme(new StringScheme()); //Assign SpoutConfig to KafkaSpout. KafkaSpout kafkaSpout = new KafkaSpout(spoutConfig); 创建Bolt Bolt是一个使用元组作为输入，处理元组，并产生新的元组作为输出的组件。 Bolt将实现IRichBolt接口。 在此程序中，使用两个Bolt类WordSplitter-Bolt和WordCounterBolt来执行操作。 IRichBolt接口有以下方法 - 准备 - 为Bolt提供要执行的环境。 执行器将运行此方法来初始化喷头。 执行 - 处理单个元组的输入。 清理 - 当Bolt要关闭时调用。 declareOutputFields - 声明元组的输出模式。 让我们创建SplitBolt.java，它实现逻辑分割一个句子到词和CountBolt.java，它实现逻辑分离独特的单词和计数其出现。 SplitBolt.java import java.util.Map; import backtype.storm.tuple.Tuple; import backtype.storm.tuple.Fields; import backtype.storm.tuple.Values; import backtype.storm.task.OutputCollector; import backtype.storm.topology.OutputFieldsDeclarer; import backtype.storm.topology.IRichBolt; import backtype.storm.task.TopologyContext; public class SplitBolt implements IRichBolt { private OutputCollector collector; @Override public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) { this.collector = collector; } @Override public void execute(Tuple input) { String sentence = input.getString(0); String[] words = sentence.split(&quot; &quot;); for(String word: words) { word = word.trim(); if(!word.isEmpty()) { word = word.toLowerCase(); collector.emit(new Values(word)); } } collector.ack(input); } @Override public void declareOutputFields(OutputFieldsDeclarer declarer) { declarer.declare(new Fields(&quot;word&quot;)); } @Override public void cleanup() {} @Override public Map&lt;String, Object&gt; getComponentConfiguration() { return null; } } CountBolt.java import java.util.Map; import java.util.HashMap; import backtype.storm.tuple.Tuple; import backtype.storm.task.OutputCollector; import backtype.storm.topology.OutputFieldsDeclarer; import backtype.storm.topology.IRichBolt; import backtype.storm.task.TopologyContext; public class CountBolt implements IRichBolt{ Map&lt;String, Integer&gt; counters; private OutputCollector collector; @Override public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) { this.counters = new HashMap&lt;String, Integer&gt;(); this.collector = collector; } @Override public void execute(Tuple input) { String str = input.getString(0); if(!counters.containsKey(str)){ counters.put(str, 1); }else { Integer c = counters.get(str) +1; counters.put(str, c); } collector.ack(input); } @Override public void cleanup() { for(Map.Entry&lt;String, Integer&gt; entry:counters.entrySet()){ System.out.println(entry.getKey()&amp;plus;&quot; : &quot; &amp;plus; entry.getValue()); } } @Override public void declareOutputFields(OutputFieldsDeclarer declarer) { } @Override public Map&lt;String, Object&gt; getComponentConfiguration() { return null; } } 提交拓扑 Storm拓扑基本上是一个Thrift结构。 TopologyBuilder类提供了简单而容易的方法来创建复杂的拓扑。 TopologyBuilder类具有设置spout(setSpout)和设置bolt(setBolt)的方法。 最后，TopologyBuilder有createTopology来创建to-pology。 shuffleGrouping和fieldsGrouping方法有助于为喷头和Bolt设置流分组。 本地集群 - 为了开发目的，我们可以使用 LocalCluster 对象创建本地集群，然后使用 LocalCluster的 submitTopology 类。 KafkaStormSample.java import backtype.storm.Config; import backtype.storm.LocalCluster; import backtype.storm.topology.TopologyBuilder; import java.util.ArrayList; import java.util.List; import java.util.UUID; import backtype.storm.spout.SchemeAsMultiScheme; import storm.kafka.trident.GlobalPartitionInformation; import storm.kafka.ZkHosts; import storm.kafka.Broker; import storm.kafka.StaticHosts; import storm.kafka.BrokerHosts; import storm.kafka.SpoutConfig; import storm.kafka.KafkaConfig; import storm.kafka.KafkaSpout; import storm.kafka.StringScheme; public class KafkaStormSample { public static void main(String[] args) throws Exception{ Config config = new Config(); config.setDebug(true); config.put(Config.TOPOLOGY_MAX_SPOUT_PENDING, 1); String zkConnString = &quot;localhost:2181&quot;; String topic = &quot;my-first-topic&quot;; BrokerHosts hosts = new ZkHosts(zkConnString); SpoutConfig kafkaSpoutConfig = new SpoutConfig (hosts, topic, &quot;/&quot; + topic, UUID.randomUUID().toString()); kafkaSpoutConfig.bufferSizeBytes = 1024 * 1024 * 4; kafkaSpoutConfig.fetchSizeBytes = 1024 * 1024 * 4; kafkaSpoutConfig.forceFromStart = true; kafkaSpoutConfig.scheme = new SchemeAsMultiScheme(new StringScheme()); TopologyBuilder builder = new TopologyBuilder(); builder.setSpout(&quot;kafka-spout&quot;, new KafkaSpout(kafkaSpoutCon-fig)); builder.setBolt(&quot;word-spitter&quot;, new SplitBolt()).shuffleGroup-ing(&quot;kafka-spout&quot;); builder.setBolt(&quot;word-counter&quot;, new CountBolt()).shuffleGroup-ing(&quot;word-spitter&quot;); LocalCluster cluster = new LocalCluster(); cluster.submitTopology(&quot;KafkaStormSample&quot;, config, builder.create-Topology()); Thread.sleep(10000); cluster.shutdown(); } } 在移动编译之前，Kakfa-Storm集成需要策展人ZooKeeper客户端java库。 策展人版本2.9.1支持Apache Storm 0.9.5版(我们在本教程中使用)。 下载下面指定的jar文件并将其放在java类路径中。 curator-client-2.9.1.jar curator-framework-2.9.1.jar 在包括依赖文件之后，使用以下命令编译程序， javac -cp &quot;/path/to/Kafka/apache-storm-0.9.5/lib/*&quot; *.java 执行 启动Kafka Producer CLI(在上一章节中解释)，创建一个名为 my-first-topic 的新主题，并提供一些样本消息，如下所示 - hello kafka storm spark test message another test message 现在使用以下命令执行应用程序 - java -cp “/path/to/Kafka/apache-storm-0.9.5/lib/*&quot;:. KafkaStormSample 此应用程序的示例输出如下所示 - storm : 1 test : 2 spark : 1 another : 1 kafka : 1 hello : 1 message : 2 原文：https://www.w3cschool.cn/apache_kafka/apache_kafka_integration_storm.html ","link":"https://tianxiawuhao.github.io/wAqEEPZon/"},{"title":"第一章 Kafka的安装与使用","content":"Kafka 基础知识 对于大数据，我们要考虑的问题有很多，首先海量数据如何收集（如 Flume），然后对于收集到的数据如何存储（典型的分布式文件系统 HDFS、分布式数据库 HBase、NoSQL 数据库 Redis），其次存储的数据不是存起来就没事了，要通过计算从中获取有用的信息，这就涉及到计算模型（典型的离线计算 MapReduce、流式实时计算Storm、Spark），或者要从数据中挖掘信息，还需要相应的机器学习算法。在这些之上，还有一些各种各样的查询分析数据的工具（如 Hive、Pig 等）。除此之外，要构建分布式应用还需要一些工具，比如分布式协调服务 Zookeeper 等等。 这里，我们讲到的是消息系统，Kafka 专为分布式高吞吐量系统而设计，其他消息传递系统相比，Kafka 具有更好的吞吐量，内置分区，复制和固有的容错能力，这使得它非常适合大规模消息处理应用程序。 消息系统 ​ 点对点消息系统：生产者发送一条消息到queue，一个queue可以有很多消费者，但是一个消息只能被一个消费者接受，当没有消费者可用时，这个消息会被保存直到有 一个可用的消费者，所以Queue实现了一个可靠的负载均衡。 ​ 发布订阅消息系统：发布者发送到topic的消息，只有订阅了topic的订阅者才会收到消息。topic实现了发布和订阅，当你发布一个消息，所有订阅这个topic的服务都能得到这个消息，所以从1到N个订阅者都能得到这个消息的拷贝。 kafka术语 Apache Kafka 是一个分布式发布 - 订阅消息系统和一个强大的队列，可以处理大量的数据，并使你能够将消息从一个端点传递到另一个端点。 Kafka 适合离线和在线消息消费。 Kafka 消息保留在磁盘上，并在群集内复制以防止数据丢失。 Kafka 构建在 ZooKeeper 同步服务之上。 它与 Apache Storm 和 Spark 非常好地集成，用于实时流式数据分析。 Kafka 是一个分布式消息队列，具有高性能、持久化、多副本备份、横向扩展能力。生产者往队列里写消息，消费者从队列里取消息进行业务逻辑。一般在架构设计中起到解耦、削峰、异步处理的作用。 ​ 消息由producer产生，消息按照topic归类，并发送到broker中，broker中保存了一个或多个topic的消息，consumer通过订阅一组topic的消息，通过持续的poll操作从broker获取消息，并进行后续的消息处理。 Producer ：消息生产者，就是向broker发指定topic消息的客户端。 Consumer ：消息消费者，通过订阅一组topic的消息，从broker读取消息的客户端。 Broker ：一个kafka集群包含一个或多个服务器，一台kafka服务器就是一个broker，用于保存producer发送的消息。一个broker可以容纳多个topic。 Topic ：每条发送到broker的消息都有一个类别，可以理解为一个队列或者数据库的一张表。 Partition：一个topic的消息由多个partition队列存储的，一个partition队列在kafka上称为一个分区。每个partition是一个有序的队列，多个partition间则是无序的。partition中的每条消息都会被分配一个有序的id（offset）。 Offset：偏移量。kafka为每条在分区的消息保存一个偏移量offset，这也是消费者在分区的位置。kafka的存储文件都是按照offset.kafka来命名，位于2049位置的即为2048.kafka的文件。比如一个偏移量是5的消费者，表示已经消费了从0-4偏移量的消息，下一个要消费的消息的偏移量是5。 Consumer Group （CG）：若干个Consumer组成的集合。这是kafka用来实现一个topic消息的广播（发给所有的consumer）和单播（发给任意一个consumer）的手段。一个topic可以有多个CG。topic的消息会复制（不是真的复制，是概念上的）到所有的CG，但每个CG只会把消息发给该CG中的一个consumer。如果需要实现广播，只要每个consumer有一个独立的CG就可以了。要实现单播只要所有的consumer在同一个CG。用CG还可以将consumer进行自由的分组而不需要多次发送消息到不同的topic。 ​ 假如一个消费者组有两个消费者，订阅了一个具有4个分区的topic的消息，那么这个消费者组的每一个消费者都会消费两个分区的消息。消费者组的成员是动态维护的，如果新增或者减少了消费者组中的消费者，那么每个消费者消费的分区的消息也会动态变化。比如原来一个消费者组有两个消费者，其中一个消费者因为故障而不能继续消费消息了，那么剩下一个消费者将会消费全部4个分区的消息。 Apache Kafka基本原理 1、分布式和分区（distributed、partitioned） 我们说 kafka 是一个分布式消息系统，所谓的分布式，实际上我们已经大致了解。消息保存在 Topic 中，而为了能够实现大数据的存储，一个 topic 划分为多个分区，每个分区对应一个文件，可以分别存储到不同的机器上，以实现分布式的集群存储。另外，每个 partition 可以有一定的副本，备份到多台机器上，以提高可用性。 总结起来就是：一个 topic 对应的多个 partition 分散存储到集群中的多个 broker 上，存储方式是一个 partition 对应一个文件，每个 broker 负责存储在自己机器上的 partition 中的消息读写。 2、副本（replicated ） kafka 还可以配置 partitions 需要备份的个数(replicas),每个 partition 将会被备份到多台机器上,以提高可用性，备份的数量可以通过配置文件指定。 这种冗余备份的方式在分布式系统中是很常见的，那么既然有副本，就涉及到对同一个文件的多个备份如何进行管理和调度。kafka 采取的方案是：每个 partition 选举一个 server 作为“leader”，由 leader 负责所有对该分区的读写，其他 server 作为 follower 只需要简单的与 leader 同步，保持跟进即可。如果原来的 leader 失效，会重新选举由其他的 follower 来成为新的 leader。 至于如何选取 leader，实际上如果我们了解 ZooKeeper，就会发现其实这正是 Zookeeper 所擅长的，Kafka 使用 ZK 在 Broker 中选出一个 Controller，用于 Partition 分配和 Leader 选举。 另外，这里我们可以看到，实际上作为 leader 的 server 承担了该分区所有的读写请求，因此其压力是比较大的，从整体考虑，从多少个 partition 就意味着会有多少个leader，kafka 会将 leader 分散到不同的 broker 上，确保整体的负载均衡。 3、整体数据流程 Kafka 的总体数据流满足下图，该图可以说是概括了整个 kafka 的基本原理。 （1）数据生产过程（Produce） 对于生产者要写入的一条记录，可以指定四个参数：分别是 topic、partition、key 和 value，其中 topic 和 value（要写入的数据）是必须要指定的，而 key 和 partition 是可选的。 对于一条记录，先对其进行序列化，然后按照 Topic 和 Partition，放进对应的发送队列中。如果 Partition 没填，那么情况会是这样的：a、Key 有填。按照 Key 进行哈希，相同 Key 去一个 Partition。b、Key 没填。Round-Robin 来选 Partition。 producer 将会和Topic下所有 partition leader 保持 socket 连接，消息由 producer 直接通过 socket 发送到 broker。其中 partition leader 的位置( host : port )注册在 zookeeper 中，producer 作为 zookeeper client，已经注册了 watch 用来监听 partition leader 的变更事件，因此，可以准确的知道谁是当前的 leader。 producer 端采用异步发送：将多条消息暂且在客户端 buffer 起来，并将他们批量的发送到 broker，小数据 IO 太多，会拖慢整体的网络延迟，批量延迟发送事实上提升了网络效率。 （2）数据消费过程（Consume） 对于消费者，不是以单独的形式存在的，每一个消费者属于一个 consumer group，一个 group 包含多个 consumer。特别需要注意的是：订阅 Topic 是以一个消费组来订阅的，发送到 Topic 的消息，只会被订阅此 Topic 的每个 group 中的一个 consumer 消费。 如果所有的 Consumer 都具有相同的 group，那么就像是一个点对点的消息系统；如果每个 consumer 都具有不同的 group，那么消息会广播给所有的消费者。 具体说来，这实际上是根据 partition 来分的，一个 Partition，只能被消费组里的一个消费者消费，但是可以同时被多个消费组消费，消费组里的每个消费者是关联到一个 partition 的，因此有这样的说法：对于一个 topic,同一个 group 中不能有多于 partitions 个数的 consumer 同时消费,否则将意味着某些 consumer 将无法得到消息。 同一个消费组的两个消费者不会同时消费一个 partition。 在 kafka 中，采用了 pull 方式，即 consumer 在和 broker 建立连接之后，主动去 pull(或者说 fetch )消息，首先 consumer 端可以根据自己的消费能力适时的去 fetch 消息并处理，且可以控制消息消费的进度(offset)。 partition 中的消息只有一个 consumer 在消费，且不存在消息状态的控制，也没有复杂的消息确认机制，可见 kafka broker 端是相当轻量级的。当消息被 consumer 接收之后，需要保存 Offset 记录消费到哪，以前保存在 ZK 中，由于 ZK 的写性能不好，以前的解决方法都是 Consumer 每隔一分钟上报一次，在 0.10 版本后，Kafka 把这个 Offset 的保存，从 ZK 中剥离，保存在一个名叫 consumeroffsets topic 的 Topic 中，由此可见，consumer 客户端也很轻量级。 4、消息传送机制 Kafka 支持 3 种消息投递语义,在业务中，常常都是使用 At least once 的模型。 At most once：最多一次，消息可能会丢失，但不会重复。 At least once：最少一次，消息不会丢失，可能会重复。 Exactly once：只且一次，消息不丢失不重复，只且消费一次。 kafka安装和使用 在Windows安装运行Kafka：https://blog.csdn.net/weixin_38004638/article/details/91893910 kafka运行 一次写入，支持多个应用读取，读取信息是相同的 kafka-study.pom &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka_2.12&lt;/artifactId&gt; &lt;version&gt;2.2.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-nop&lt;/artifactId&gt; &lt;version&gt;1.7.24&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.0&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; Producer生产者 ​ 发送消息的方式，只管发送，不管结果：只调用接口发送消息到 Kafka 服务器，但不管成功写入与否。由于 Kafka 是高可用的，因此大部分情况下消息都会写入，但在异常情况下会丢消息 同步发送：调用 send() 方法返回一个 Future 对象，我们可以使用它的 get() 方法来判断消息发送成功与否 异步发送：调用 send() 时提供一个回调方法，当接收到 broker 结果后回调此方法 public class MyProducer { private static KafkaProducer&lt;String, String&gt; producer; //初始化 static { Properties properties = new Properties(); //kafka启动，生产者建立连接broker的地址 properties.put(&quot;bootstrap.servers&quot;, &quot;127.0.0.1:9092&quot;); //kafka序列化方式 properties.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); properties.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); //自定义分区分配器 properties.put(&quot;partitioner.class&quot;, &quot;com.imooc.kafka.CustomPartitioner&quot;); producer = new KafkaProducer&lt;&gt;(properties); } /** * 创建topic：.\\bin\\windows\\kafka-topics.bat --create --zookeeper localhost:2181 * --replication-factor 1 --partitions 1 --topic kafka-study * 创建消费者：.\\bin\\windows\\kafka-console-consumer.bat --bootstrap-server localhost:9092 * --topic kafka-study --from-beginning */ //发送消息，发送完后不做处理 private static void sendMessageForgetResult() { ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(&quot;kafka-study&quot;, &quot;name&quot;, &quot;ForgetResult&quot;); producer.send(record); producer.close(); } //发送同步消息，获取发送的消息 private static void sendMessageSync() throws Exception { ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(&quot;kafka-study&quot;, &quot;name&quot;, &quot;sync&quot;); RecordMetadata result = producer.send(record).get(); System.out.println(result.topic());//kafka-study System.out.println(result.partition());//分区为name的hash对应分区 System.out.println(result.offset());//已发送一条消息，此时偏移量+1 producer.close(); } /** * 创建topic：.\\bin\\windows\\kafka-topics.bat --create --zookeeper localhost:2181 * --replication-factor 1 --partitions 3 --topic kafka-study-x * 创建消费者：.\\bin\\windows\\kafka-console-consumer.bat --bootstrap-server localhost:9092 * --topic kafka-study-x --from-beginning */ //发送异步消息，获取回调的消息 private static void sendMessageCallback() { ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(&quot;kafka-study-x&quot;, &quot;name&quot;, &quot;callback&quot;); producer.send(record, new MyProducerCallback()); //发送多条消息 record = new ProducerRecord&lt;&gt;(&quot;kafka-study-x&quot;, &quot;name-x&quot;, &quot;callback&quot;); producer.send(record, new MyProducerCallback()); producer.close(); } } //发送异步消息 //场景：每条消息发送有延迟，多条消息发送，无需同步等待，可以执行其他操作，程序会自动异步调用 private static class MyProducerCallback implements Callback { @Override public void onCompletion(RecordMetadata recordMetadata, Exception e) { if (e != null) { e.printStackTrace(); return; } System.out.println(&quot;*** MyProducerCallback ***&quot;); System.out.println(recordMetadata.topic()); System.out.println(recordMetadata.partition()); System.out.println(recordMetadata.offset()); } } public static void main(String[] args) throws Exception { //sendMessageForgetResult(); //sendMessageSync(); sendMessageCallback(); } } 自定义分区分配器：决定消息存放在哪个分区.。默认分配器使用轮询存放，轮到已满分区将会写入失败。 public class CustomPartitioner implements Partitioner { @Override public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) { //获取topic所有分区 List&lt;PartitionInfo&gt; partitionInfos = cluster.partitionsForTopic(topic); int numPartitions = partitionInfos.size(); //消息必须有key if (null == keyBytes || !(key instanceof String)) { throw new InvalidRecordException(&quot;kafka message must have key&quot;); } //如果只有一个分区，即0号分区 if (numPartitions == 1) {return 0;} //如果key为name，发送至最后一个分区 if (key.equals(&quot;name&quot;)) {return numPartitions - 1;} return Math.abs(Utils.murmur2(keyBytes)) % (numPartitions - 1); } @Override public void close() {} @Override public void configure(Map&lt;String, ?&gt; map) {} } 启动生产者发送消息，通过自定义分区分配器分配，查询到topic信息的offset、partitioner topic partition offset kafka-study 0 1 kafka-study 1 2 kafka-study-x 0 1 Kafka消费者（组） public class MyConsumer { private static KafkaConsumer&lt;String, String&gt; consumer; private static Properties properties; //初始化 static { properties = new Properties(); //建立连接broker的地址 properties.put(&quot;bootstrap.servers&quot;, &quot;127.0.0.1:9092&quot;); //kafka反序列化 properties.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); properties.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); //指定消费者组 properties.put(&quot;group.id&quot;, &quot;KafkaStudy&quot;); } //自动提交位移：由consume自动管理提交 private static void generalConsumeMessageAutoCommit() { //配置 properties.put(&quot;enable.auto.commit&quot;, true); consumer = new KafkaConsumer&lt;&gt;(properties); //指定topic consumer.subscribe(Collections.singleton(&quot;kafka-study-x&quot;)); try { while (true) { boolean flag = true; //拉取信息，超时时间100ms ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); //遍历打印消息 for (ConsumerRecord&lt;String, String&gt; record : records) { System.out.println(String.format( &quot;topic = %s, partition = %s, key = %s, value = %s&quot;, record.topic(), record.partition(), record.key(), record.value() )); //消息发送完成 if (record.value().equals(&quot;done&quot;)) { flag = false; } } if (!flag) { break; } } } finally { consumer.close(); } } //手动同步提交当前位移，根据需求提交，但容易发送阻塞，提交失败会进行重试直到抛出异常 private static void generalConsumeMessageSyncCommit() { properties.put(&quot;auto.commit.offset&quot;, false); consumer = new KafkaConsumer&lt;&gt;(properties); consumer.subscribe(Collections.singletonList(&quot;kafka-study-x&quot;)); while (true) { boolean flag = true; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) { System.out.println(String.format( &quot;topic = %s, partition = %s, key = %s, value = %s&quot;, record.topic(), record.partition(), record.key(), record.value() )); if (record.value().equals(&quot;done&quot;)) { flag = false; } } try { //手动同步提交 consumer.commitSync(); } catch (CommitFailedException ex) { System.out.println(&quot;commit failed error: &quot; + ex.getMessage()); } if (!flag) { break; } } } //手动异步提交当前位移，提交速度快，但失败不会记录 private static void generalConsumeMessageAsyncCommit() { properties.put(&quot;auto.commit.offset&quot;, false); consumer = new KafkaConsumer&lt;&gt;(properties); consumer.subscribe(Collections.singletonList(&quot;kafka-study-x&quot;)); while (true) { boolean flag = true; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) { System.out.println(String.format( &quot;topic = %s, partition = %s, key = %s, value = %s&quot;, record.topic(), record.partition(), record.key(), record.value() )); if (record.value().equals(&quot;done&quot;)) { flag = false; } } //手动异步提交 consumer.commitAsync(); if (!flag) { break; } } } //手动异步提交当前位移带回调 private static void generalConsumeMessageAsyncCommitWithCallback() { properties.put(&quot;auto.commit.offset&quot;, false); consumer = new KafkaConsumer&lt;&gt;(properties); consumer.subscribe(Collections.singletonList(&quot;kafka-study-x&quot;)); while (true) { boolean flag = true; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) { System.out.println(String.format( &quot;topic = %s, partition = %s, key = %s, value = %s&quot;, record.topic(), record.partition(), record.key(), record.value() )); if (record.value().equals(&quot;done&quot;)) { flag = false; } } //使用java8函数式编程 consumer.commitAsync((map, e) -&gt; { if (e != null) { System.out.println(&quot;commit failed for offsets: &quot; + e.getMessage()); } }); if (!flag) { break; } } } //混合同步与异步提交位移 @SuppressWarnings(&quot;all&quot;) private static void mixSyncAndAsyncCommit() { properties.put(&quot;auto.commit.offset&quot;, false); consumer = new KafkaConsumer&lt;&gt;(properties); consumer.subscribe(Collections.singletonList(&quot;kafka-study-x&quot;)); try { while (true) { //boolean flag = true; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) { System.out.println(String.format( &quot;topic = %s, partition = %s, key = %s, &quot; + &quot;value = %s&quot;, record.topic(), record.partition(), record.key(), record.value() )); //if (record.value().equals(&quot;done&quot;)) { flag = false; } } //手动异步提交，保证性能 consumer.commitAsync(); //if (!flag) { break; } } } catch (Exception ex) { System.out.println(&quot;commit async error: &quot; + ex.getMessage()); } finally { try { //异步提交失败，再尝试手动同步提交 consumer.commitSync(); } finally { consumer.close(); } } } public static void main(String[] args) { //自动提交位移 generalConsumeMessageAutoCommit(); //手动同步提交当前位移 //generalConsumeMessageSyncCommit(); //手动异步提交当前位移 //generalConsumeMessageAsyncCommit(); //手动异步提交当前位移带回调 //generalConsumeMessageAsyncCommitWithCallback() //混合同步与异步提交位移 //mixSyncAndAsyncCommit(); } } 先启动消费者等待接收消息，再启动生产者发送消息，进行消费消息 topic partition key value kafka-study-x 1 name-x callback kafka-study-x 2 name-x callback kafka-study-x 1 name-x callback kafka-study-x 1 name-x callback kafka-study-x 2 name-x callback ","link":"https://tianxiawuhao.github.io/w7MMk_8-a/"},{"title":"附录 Flink常见面试问题汇总","content":"面试题一：应用架构 问题： 公司怎么提交的实时任务， 有多少 Job Manager、 Task Manager？ 解答： 我们使用 yarn session 模式提交任务； 另一种方式是每次提交都会创建一个新的 Flink 集群， 为每一个 job 提供资源， 任务之间互相独立， 互不影响， 方便管理。 任务执行完成之后创建的集群也会消失。 线上命令脚本如下：bin/yarn-session.sh -n 7 -s 8 -jm 3072 -tm 32768 -qu root.. -nm - -d其中申请 7 个 taskManager， 每个 8 核， 每个 taskmanager 有 32768M 内存。 集群默认只有一个 Job Manager。 但为了防止单点故障， 我们配置了高可用。 对于 standlone 模式， 我们公司一般配置一个主 Job Manager， 两个备用 Job Manager， 然后结合 ZooKeeper 的使用， 来达到高可用； 对于 yarn 模式， yarn 在 Job Mananger 故障会自动进行重启， 所以只需要一个， 我们配置的最大重启次数是 10 次。 面试题二：压测和监控 问题： 怎么做压力测试和监控？ 解答： 我们一般碰到的压力来自以下几个方面： 产生数据流的速度如果过快， 而下游的算子消费不过来的话， 会产生背压。 背压的监控可以使用 Flink Web UI(localhost:8081) 来可视化监控 Metrics， 一旦报警 就能知道。 一般情况下背压问题的产生可能是由于 sink 这个 操作符没有优化好， 做一下优化就可以了。 比如如果是写入 ElasticSearch， 那么可以改成批量写入， 可以调大 ElasticSearch 队列的大小等等策略。 设置 watermark 的最大延迟时间这个参数， 如果设置的过大， 可能会造成 内存的压力。 可以设置最大延迟时间小一些， 然后把迟到元素发送到侧输出流中去。 晚一点更新结果。 或者使用类似于 RocksDB 这样的状态后端， RocksDB 会开辟 堆外存储空间， 但 IO 速度会变慢， 需要权衡。 还有就是滑动窗口的长度如果过长， 而滑动距离很短的话， Flink 的性能 会下降的很厉害。 我们主要通过时间分片的方法， 将每个元素只存入一个“ 重叠窗 口” ， 这样就可以减少窗口处理中状态的写入。 参见链接： https://www.infoq.cn/article/sIhs_qY6HCpMQNblTI9M 状态后端使用 RocksDB， 还没有碰到被撑爆的问题。 面试题三：为什么用 Flink 问题： 为什么使用 Flink 替代 Spark？ 解答： 主要考虑的是 flink 的低延迟、 高吞吐量和对流式数据应用场景更好的支 持； 另外， flink 可以很好地处理乱序数据， 而且可以保证 exactly-once 的状态一致 性。 详见文档第一章， 有 Flink 和 Spark 的详细对比。 面试题四：checkpoint 的存储 问题： Flink 的 checkpoint 存在哪里？ 解答： 可以是内存， 文件系统， 或者 RocksDB。 详见文档 第九章 状态编程和容错机制。 面试题五：exactly-once 的保证 问题： 如果下级存储不支持事务， Flink 怎么保证 exactly-once？ 解答： 端到端的 exactly-once 对 sink 要求比较高， 具体实现主要有幂等写入和 事务性写入两种方式。 幂等写入的场景依赖于业务逻辑， 更常见的是用事务性写入。 而事务性写入又有预写日志（ WAL） 和两阶段提交（ 2PC） 两种方式。 如果外部系统不支持事务， 那么可以用预写日志的方式， 把结果数据先当成状 态保存， 然后在收到 checkpoint 完成的通知时， 一次性写入 sink 系统。 参见文档第九章 状态编程和容错机制 面试题六：状态机制 问题： 说一下 Flink 状态机制？ 解答： Flink 内置的很多算子， 包括源 source， 数据存储 sink 都是有状态的。 在 Flink 中， 状态始终与特定算子相关联。 Flink 会以 checkpoint 的形式对各个任务的 状态进行快照， 用于保证故障恢复时的状态一致性。 Flink 通过状态后端来管理状态 和 checkpoint 的存储， 状态后端可以有不同的配置选择。 详见文档第九章。 面试题七：海量 key 去重 问题： 怎么去重？ 考虑一个实时场景： 双十一场景， 滑动窗口长度为 1 小时， 滑动距离为 10 秒钟， 亿级用户， 怎样计算 UV？ 解答： 使用类似于 scala 的 set 数据结构或者 redis 的 set 显然是不行的， 因为可能有上亿个 Key， 内存放不下。 所以可以考虑使用布隆过滤器（ Bloom Filter） 来去重。 面试题八：checkpoint 与 spark 比较 问题： Flink 的 checkpoint 机制对比 spark 有什么不同和优势？ 解答： spark streaming 的 checkpoint 仅仅是针对 driver 的故障恢复做了数据 和元数据的 checkpoint。 而 flink 的 checkpoint 机制 要复杂了很多， 它采用的是 轻量级的分布式快照， 实现了每个算子的快照， 及流动中的数据的快照。 参见文档 第九章 状态编程和容错机制及文章链接： https://cloud.tencent.com/developer/article/1189624 面试题九：watermark 机制 问题： 请详细解释一下 Flink 的 Watermark 机制。 解答： Watermark 本质是 Flink 中衡量 EventTime 进展的一个机制， 主要用来处 理乱序数据。 详见文档第七章 时间语义与 Wartermark。 面试题十：exactly-once 如何实现 问题： Flink 中 exactly-once 语义是如何实现的， 状态是如何存储的？ 解答： Flink 依靠 checkpoint 机制来实现 exactly-once 语义， 如果要实现端到端 的 exactly-once， 还需要外部 source 和 sink 满足一定的条件。 状态的存储通过状态 后端来管理， Flink 中可以配置不同的状态后端。 详见文档 第九章 状态编程和容错机制。 面试题十一：CEP 问题： Flink CEP 编程中当状态没有到达的时候会将数据保存在哪里？ 解答： 在流式处理中， CEP 当然是要支持 EventTime 的， 那么相对应的也要 支持数据的迟到现象， 也就是 watermark 的处理逻辑。 CEP 对未匹配成功的事件序 列的处理， 和迟到数据是类似的。 在 Flink CEP 的处理逻辑中， 状态没有满足的和 迟到的数据， 都会存储在一个 Map 数据结构中， 也就是说， 如果我们限定判断事件 序列的时长为 5 分钟， 那么内存中就会存储 5 分钟的数据， 这在我看来， 也是对内 存的极大损伤之一。 面试题十二：三种时间语义 问题： Flink 三种时间语义是什么， 分别说出应用场景？ 解答： Event Time： 这是实际应用最常见的时间语义， 具体见文档第七章。 Processing Time： 没有事件时间的情况下， 或者对实时性要求超高的情况下。 Ingestion Time： 存在多个 Source Operator 的情况下， 每个 Source Operator 可以使用自己本地系统时钟指派 Ingestion Time。 后续基于时间相关的各种操作， 都会使用数据记录中的 Ingestion Time。 面试题十三：数据高峰的处理 问题： Flink 程序在面对数据高峰期时如何处理？ 解答： 使用大容量的 Kafka 把数据先放到消息队列里面作为数据源， 再使用 Flink 进行消费， 不过这样会影响到一点实时性 ","link":"https://tianxiawuhao.github.io/EEiV7EREL/"},{"title":"第十章 Table API 与 SQL","content":"Flink Table&amp;SQL flink table &amp; sql时间属性与窗口 flink版本：1.13.1 scala版本：2.12 maven 依赖引用 &lt;!-- 使用table api 引入的依赖，使用桥接器和底层datastream api连接支持--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-table-api-java-bridge_${scala.binary.version}&lt;/artifactId&gt; &lt;version&gt;${flink.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!--如果需要在本地运行table api和sql 还需要引入一下依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-table-planner-blink_${scala.binary.version}&lt;/artifactId&gt; &lt;version&gt;${flink.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-scala_${scala.binary.version}&lt;/artifactId&gt; &lt;version&gt;${flink.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!--如果想实现自定义的数据格式来做序列化，需要引入一下依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-table-common&lt;/artifactId&gt; &lt;version&gt;${flink.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!--连接外部数据格式解析,采用csv方式来解析--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-csv&lt;/artifactId&gt; &lt;version&gt;${flink.version}&lt;/version&gt; &lt;/dependency&gt; 时间属性 1 事件时间 在DDL连接表中创建 CREATE TABLE EventTable( user STRING, url STRING, ts TIMESTAMP(3), // 传入得值是bigint,自动转换 WATERMARK FOR ts AS ts - INTERVAL '5' SECOND ) WITH ( ... ); 说明： 这里我们把 ts 字段定义为事件时间属性，而且基于 ts 设置了 5 秒的水位线延迟。 这里的“5 秒”是以“时间间隔”的形式定义的，格式是 INTERVAL &lt;数值&gt; &lt;时间单位&gt;：INTERVAL ‘5’ SECOND，这里的数值必须用单引号引起来，而单位用 SECOND 和 SECONDS 是等效的。 TIMESTAMP会自动转为国际UTF时间，使用当地时间需要使用TIMESTAMP_LTZ CREATE TABLE events ( user STRING, url STRING, ts BIGINT, ts_ltz AS TO_TIMESTAMP_LTZ(ts, 3), WATERMARK FOR ts_ltz AS ts_ltz - INTERVAL '5' SECOND ) WITH ( ... ); 说明： Flink 中支持的事件时间属性数据类型必须为 TIMESTAMP 或者 TIMESTAMP_LTZ。这里 TIMESTAMP_LTZ 是指带有本地时区信息的时间戳（TIMESTAMP WITH LOCAL TIME ZONE）；一般情况下如果数据中的时间戳是“年-月-日-时-分-秒”的形式，那就是不带时区信 息的，可以将事件时间属性定义为 TIMESTAMP 类型。 而如果原始的时间戳就是一个长整型的毫秒数，这时就需要另外定义一个字段来表示事件 时间属性，类型定义为 TIMESTAMP_LTZ 会更方便。 在数据流转换为表时定义 public class TimeAndWindowTest { public static void main(String[] args) throws Exception{ StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env); //2.在流转换成Table的时候定义时间属性 SingleOutputStreamOperator&lt;Event&gt; clickStream = env.addSource(new ClickSource()) .assignTimestampsAndWatermarks(WatermarkStrategy.&lt;Event&gt;forBoundedOutOfOrderness(Duration.ZERO) .withTimestampAssigner(new SerializableTimestampAssigner&lt;Event&gt;() { @Override public long extractTimestamp(Event element, long recordTimestamp) { return element.timestamp; } })); Table clickTable = tableEnv.fromDataStream(clickStream, $(&quot;user&quot;), $(&quot;url&quot;), $(&quot;timestamp&quot;).as(&quot;ts&quot;), $(&quot;et&quot;).rowtime()); clickTable.printSchema(); } } 说明 设置水位线(assignTimestampsAndWatermarks),(“et”).rowtime()自动获取，et为虚拟表别名(“et”).rowtime()自动获取，et为虚拟表别名 (“et”).rowtime()自动获取，et为虚拟表别名(“ps”).proctime()表示系统处理时间，ps为虚拟表别名 .rowtime()，.proctime()值均是国际UTF时间 2 处理时间 在创建表的 DDL 中定义 CREATE TABLE EventTable( user STRING, url STRING, ts AS PROCTIME() ) WITH ( ... ); 在数据流转换为表时定义 DataStream&lt;Tuple2&lt;String, String&gt;&gt; stream = ...; // 声明一个额外的字段作为处理时间属性字段，$(&quot;ts&quot;).proctime()系统处理时间 Table table = tEnv.fromDataStream(stream, $(&quot;user&quot;), $(&quot;url&quot;), $(&quot;ts&quot;).proctime()); 1.1 案例1（DataStream SQL统计） 需求：将DataStream注册为Table和View并进行SQL统计。 代码如下： import lombok.AllArgsConstructor; import lombok.Data; import lombok.NoArgsConstructor; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.table.api.Table; import org.apache.flink.table.api.bridge.java.StreamTableEnvironment; import java.util.Arrays; import static org.apache.flink.table.api.Expressions.$; /** * 案例1：将DataStream注册为Table和View并进行SQL统计 */ public class Demo1 { public static void main(String[] args) throws Exception { //1.准备环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); //EnvironmentSettings settings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build(); //StreamTableEnvironment tEnv = StreamTableEnvironment.create(env, settings); StreamTableEnvironment tEnv = StreamTableEnvironment.create(env); //2.Source DataStream&lt;Order&gt; orderA = env.fromCollection(Arrays.asList( new Order(1L, &quot;beer&quot;, 3), new Order(1L, &quot;diaper&quot;, 4), new Order(3L, &quot;rubber&quot;, 2))); DataStream&lt;Order&gt; orderB = env.fromCollection(Arrays.asList( new Order(2L, &quot;pen&quot;, 3), new Order(2L, &quot;rubber&quot;, 3), new Order(4L, &quot;beer&quot;, 1))); //3.注册表 // convert DataStream to Table Table tableA = tEnv.fromDataStream(orderA, $(&quot;user&quot;), $(&quot;product&quot;), $(&quot;amount&quot;)); // register DataStream as Table tEnv.createTemporaryView(&quot;OrderB&quot;, orderB, $(&quot;user&quot;), $(&quot;product&quot;), $(&quot;amount&quot;)); //4.执行查询 System.out.println(tableA); // union the two tables Table resultTable = tEnv.sqlQuery( &quot;SELECT * FROM &quot; + tableA + &quot; WHERE amount &gt; 2 &quot; + &quot;UNION ALL &quot; + &quot;SELECT * FROM OrderB WHERE amount &lt; 2&quot; ); //5.输出结果 DataStream&lt;Order&gt; resultDS = tEnv.toAppendStream(resultTable, Order.class); resultDS.print(); env.execute(); } @Data @NoArgsConstructor @AllArgsConstructor public static class Order { public Long user; public String product; public int amount; } } 运行结果： 1.2 案例2（DataStream Table&amp;SQL统计） 需求：使用SQL和Table两种方式对DataStream中的单词进行统计。 示例代码如下（SQL方式）： import lombok.AllArgsConstructor; import lombok.Data; import lombok.NoArgsConstructor; import org.apache.flink.api.java.tuple.Tuple2; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.table.api.Table; import org.apache.flink.table.api.bridge.java.StreamTableEnvironment; import static org.apache.flink.table.api.Expressions.$; /** * 使用SQL和Table两种方式对DataStream中的单词进行统计。 * */ public class Demo02 { public static void main(String[] args) throws Exception { //1.准备环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); StreamTableEnvironment tEnv = StreamTableEnvironment.create(env); //2.Source DataStream&lt;WC&gt; input = env.fromElements( new WC(&quot;Hello&quot;, 1), new WC(&quot;World&quot;, 1), new WC(&quot;Hello&quot;, 1) ); //3.注册表 tEnv.createTemporaryView(&quot;WordCount&quot;, input, $(&quot;word&quot;), $(&quot;frequency&quot;)); //4.执行查询 Table resultTable = tEnv.sqlQuery(&quot;SELECT word, SUM(frequency) as frequency FROM WordCount GROUP BY word&quot;); //5.输出结果 //toAppendStream doesn't support consuming update changes which is produced by node GroupAggregate //DataStream&lt;WC&gt; resultDS = tEnv.toAppendStream(resultTable, WC.class); DataStream&lt;Tuple2&lt;Boolean, WC&gt;&gt; resultDS = tEnv.toRetractStream(resultTable, WC.class); resultDS.print(); env.execute(); } @Data @NoArgsConstructor @AllArgsConstructor public static class WC { public String word; public long frequency; } } 运行结果： 示例代码如下（Table方式）： import lombok.AllArgsConstructor; import lombok.Data; import lombok.NoArgsConstructor; import org.apache.flink.api.java.tuple.Tuple2; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.table.api.Table; import org.apache.flink.table.api.bridge.java.StreamTableEnvironment; import static org.apache.flink.table.api.Expressions.$; /** * 使用SQL和Table两种方式对DataStream中的单词进行统计 * */ public class Demo02Table { public static void main(String[] args) throws Exception { //1.准备环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); StreamTableEnvironment tEnv = StreamTableEnvironment.create(env); //2.Source DataStream&lt;WC&gt; input = env.fromElements( new WC(&quot;Hello&quot;, 1), new WC(&quot;World&quot;, 1), new WC(&quot;Hello&quot;, 1) ); //3.注册表 Table table = tEnv.fromDataStream(input); //4.执行查询 Table resultTable = table .groupBy($(&quot;word&quot;)) .select($(&quot;word&quot;), $(&quot;frequency&quot;).sum().as(&quot;frequency&quot;)) .filter($(&quot;frequency&quot;).isEqual(2)); //5.输出结果 DataStream&lt;Tuple2&lt;Boolean, WC&gt;&gt; resultDS = tEnv.toRetractStream(resultTable, WC.class); resultDS.print(); env.execute(); } @Data @NoArgsConstructor @AllArgsConstructor public static class WC { public String word; public long frequency; } } 1.4 案例4（SQL消费Kafka） 需求：从Kafka中消费数据并过滤出状态为success的数据再写入到Kafka {&quot;user_id&quot;: &quot;1&quot;, &quot;page_id&quot;:&quot;1&quot;, &quot;status&quot;: &quot;success&quot;} {&quot;user_id&quot;: &quot;1&quot;, &quot;page_id&quot;:&quot;1&quot;, &quot;status&quot;: &quot;success&quot;} {&quot;user_id&quot;: &quot;1&quot;, &quot;page_id&quot;:&quot;1&quot;, &quot;status&quot;: &quot;success&quot;} {&quot;user_id&quot;: &quot;1&quot;, &quot;page_id&quot;:&quot;1&quot;, &quot;status&quot;: &quot;success&quot;} {&quot;user_id&quot;: &quot;1&quot;, &quot;page_id&quot;:&quot;1&quot;, &quot;status&quot;: &quot;fail&quot;} /export/server/kafka/bin/kafka-topics.sh --create --zookeeper node1:2181 --replication-factor 2 --partitions 3 --topic input_kafka /export/server/kafka/bin/kafka-topics.sh --create --zookeeper node1:2181 --replication-factor 2 --partitions 3 --topic output_kafka /export/server/kafka/bin/kafka-console-producer.sh --broker-list node1:9092 --topic input_kafka /export/server/kafka/bin/kafka-console-consumer.sh --bootstrap-server node1:9092 --topic output_kafka --from-beginning 代码实现： https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/ https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/kafka.html import org.apache.flink.api.java.tuple.Tuple2; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.table.api.Table; import org.apache.flink.table.api.TableResult; import org.apache.flink.table.api.bridge.java.StreamTableEnvironment; import org.apache.flink.types.Row; /** * 从Kafka中消费数据并过滤出状态为success的数据再写入到Kafka */ public class Demo4 { public static void main(String[] args) throws Exception { //1.准备环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); StreamTableEnvironment tEnv = StreamTableEnvironment.create(env); //2.Source TableResult inputTable = tEnv.executeSql( &quot;CREATE TABLE input_kafka (\\n&quot; + &quot; `user_id` BIGINT,\\n&quot; + &quot; `page_id` BIGINT,\\n&quot; + &quot; `status` STRING\\n&quot; + &quot;) WITH (\\n&quot; + &quot; 'connector' = 'kafka',\\n&quot; + &quot; 'topic' = 'input_kafka',\\n&quot; + &quot; 'properties.bootstrap.servers' = 'node1:9092',\\n&quot; + &quot; 'properties.group.id' = 'testGroup',\\n&quot; + &quot; 'scan.startup.mode' = 'latest-offset',\\n&quot; + &quot; 'format' = 'json'\\n&quot; + &quot;)&quot; ); TableResult outputTable = tEnv.executeSql( &quot;CREATE TABLE output_kafka (\\n&quot; + &quot; `user_id` BIGINT,\\n&quot; + &quot; `page_id` BIGINT,\\n&quot; + &quot; `status` STRING\\n&quot; + &quot;) WITH (\\n&quot; + &quot; 'connector' = 'kafka',\\n&quot; + &quot; 'topic' = 'output_kafka',\\n&quot; + &quot; 'properties.bootstrap.servers' = 'node1:9092',\\n&quot; + &quot; 'format' = 'json',\\n&quot; + &quot; 'sink.partitioner' = 'round-robin'\\n&quot; + &quot;)&quot; ); String sql = &quot;select &quot; + &quot;user_id,&quot; + &quot;page_id,&quot; + &quot;status &quot; + &quot;from input_kafka &quot; + &quot;where status = 'success'&quot;; Table ResultTable = tEnv.sqlQuery(sql); DataStream&lt;Tuple2&lt;Boolean, Row&gt;&gt; resultDS = tEnv.toRetractStream(ResultTable, Row.class); resultDS.print(); tEnv.executeSql(&quot;insert into output_kafka select * from &quot; + ResultTable); //7.excute env.execute(); } } 2 Flink SQL常用算子 2.1 SELECT SELECT ：用于从 DataSet/DataStream 中选择数据，用于筛选出某些列。 示例： SELECT * FROM Table；// 取出表中的所有列 SELECT name，age FROM Table；// 取出表中 name 和 age两列 与此同时 SELECT语句中可以使用函数和别名，例如我们上面提到的 WordCount中： SELECT word, COUNT(word) FROM table GROUP BY word; 2.2 WHERE WHERE ：用于从数据集/流中过滤数据，与 SELECT 一起使用，用于根据某些条件对关系做水平分割，即选择符合条件的记录。 示例： SELECT name，age FROM Table where name LIKE ‘% 小明 %’； SELECT * FROM Table WHERE age = 20； WHERE是从原数据中进行过滤，那么在WHERE条件中，Flink SQL同样支持 =、&lt;、&gt;、&lt;&gt;、&gt;=、&lt;=，以及 AND、OR等表达式的组合，最终满足过滤条件的数据会被选择出来。并且 WHERE 可以结合IN、NOT IN联合使用。举个例子： SELECT name, age FROM Table WHERE name IN (SELECT name FROM Table2) 2.3 DISTINCT DISTINCT： 用于从数据集/流中去重根据 SELECT 的结果进行去重。 示例： SELECT DISTINCT name FROM Table; 对于流式查询，计算查询结果所需的 State可能会无限增长，用户需要自己控制查询的状态范围，以防止状态过大。 2.4 GROUP BY GROUP BY ：是对数据进行分组操作。例如我们需要计算成绩明细表中，每个学生的总分。 示例： SELECT name, SUM(score) as TotalScore FROM Table GROUP BY name; 2.5 UNION 和 UNION ALL UNION: 用于将两个结果集合并起来，要求两个结果集字段完全一致，包括字段类型、字段顺序。不同于 UNION ALL 的是，UNION 会对结果数据去重。 示例： SELECT * FROM T1 UNION (ALL) SELECT * FROM T2； 2.6 JOIN JOIN ：用于把来自两个表的数据联合起来形成结果表，Flink支持的JOIN 类型包括： JOIN - INNER JOIN LEFT JOIN - LEFT OUTER JOIN RIGHT JOIN - RIGHT OUTER JOIN FULL JOIN - FULL OUTER JOIN 这里的 JOIN的语义和我们在关系型数据库中使用的 JOIN 语义一致。 示例：JOIN(将订单表数据和商品表进行关联) SELECT * FROM Orders INNER JOIN Product ON Orders.productId = Product.id LEFT JOIN与 JOIN 的区别是当右表没有与左边相 JOIN 的数据时候，右边对应的字段补NULL输出，RIGHT JOIN 相当于LEFT JOIN左右两个表交互一下位置。FULL JOIN相当于RIGHT JOIN 和 LEFT JOIN之后进行UNION ALL 操作，示例： SELECT * FROM Orders LEFT JOIN Product ON Orders.productId = Product.id SELECT * FROM Orders RIGHT JOIN Product ON Orders.productId = Product.id SELECT * FROM Orders FULL OUTER JOIN Product ON Orders.productId = Product.id 3 窗口(window) 3 聚合（Aggregation）查询 3.1 TTL 在持续查询的过程中，由于用于分组的 key 可能会不断增加，因此计算结果所需要 维护的状态也会持续增长。为了防止状态无限增长耗尽资源 方式1 TableEnvironment tableEnv = ... // 获取表环境的配置 TableConfig tableConfig = tableEnv.getConfig(); // 配置状态保持时间 tableConfig.setIdleStateRetention(Duration.ofMinutes(60)); 方式2 TableEnvironment tableEnv = ... Configuration configuration = tableEnv.getConfig().getConfiguration(); configuration.setString(&quot;table.exec.state.ttl&quot;, &quot;60 min&quot;); 3.2 分组聚合 运用 Table eventCountTable = tableEnv.sqlQuery(&quot;SELECT user, COUNT(url) as cnt FROM EventTable GROUP BY user&quot;); csv数据 Mary,./home,1000 Alice,./cart,2000 Bob,./prod?id=100,3000 Bob,./cart,4000 Bob,./home,5000 Mary,./home,6000 Bob,./cart,7000 Bob,./home,8000 Bob,./prod?id=10,9000 Bob,./prod?id=10,11000 Bob,./prod?id=10,13000 Bob,./prod?id=10,15000 public class TimeAndWindowTest { public static void main(String[] args) throws Exception{ StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env); //1. 在创建表的DDL中直接定义时间属性 String createDDL = &quot;CREATE TABLE clickTable (&quot; + &quot; user_name STRING, &quot; + &quot; url STRING, &quot; + &quot; ts BIGINT,&quot; + &quot; et AS TO_TIMESTAMP(FROM_UNIXTIME(ts/1000)),&quot; +//TO_TIMESTAMP要String,使用FROM_UNIXTIME转成string传入，ts/1000是秒 &quot; WATERMARK FOR et AS et - INTERVAL '1' SECOND &quot; +//水位线 &quot; ) WITH (&quot; + &quot; 'connector' = 'filesystem',&quot; + &quot; 'path' = 'input/clicks.csv',&quot;+ &quot; 'format' = 'csv'&quot; + &quot;) &quot;; tableEnv.executeSql(createDDL); //2.在流转换成Table的时候定义时间属性 SingleOutputStreamOperator&lt;Event&gt; clickStream = env.addSource(new ClickSource()) .assignTimestampsAndWatermarks(WatermarkStrategy.&lt;Event&gt;forBoundedOutOfOrderness(Duration.ZERO) .withTimestampAssigner(new SerializableTimestampAssigner&lt;Event&gt;() { @Override public long extractTimestamp(Event element, long recordTimestamp) { return element.timestamp; } })); Table clickTable = tableEnv.fromDataStream(clickStream, $(&quot;user&quot;), $(&quot;url&quot;), $(&quot;timestamp&quot;).as(&quot;ts&quot;), $(&quot;et&quot;).rowtime()); //3.聚合查询转换 //3.1 分组聚合 Table aggTable = tableEnv.sqlQuery(&quot;select user_name,count(url) from clickTable group by user_name&quot;); //3.2 分组窗口聚合 Table groupWindowResultTable = tableEnv.sqlQuery(&quot;select &quot; + &quot; user_name,count(1) as cnt,&quot; + &quot; TUMBLE_END(et,INTERVAL '10' SECOND) as entT &quot; + &quot; from clickTable group by user_name,&quot; + &quot; TUMBLE(et,INTERVAL '10' SECOND)&quot;); //clickTable.printSchema(); tableEnv.toChangelogStream(aggTable).print(&quot;agg&quot;); tableEnv.toChangelogStream(groupWindowResultTable).print(&quot;group window&quot;); env.execute(); } agg结果 agg&gt; +I[Mary, 1] agg&gt; +I[Alice, 1] agg&gt; +I[Bob, 1] agg&gt; -U[Bob, 1] agg&gt; +U[Bob, 2] agg&gt; -U[Bob, 2] agg&gt; +U[Bob, 3] agg&gt; -U[Mary, 1] agg&gt; +U[Mary, 2] agg&gt; -U[Bob, 3] agg&gt; +U[Bob, 4] agg&gt; -U[Bob, 4] agg&gt; +U[Bob, 5] agg&gt; -U[Bob, 5] agg&gt; +U[Bob, 6] agg&gt; -U[Bob, 6] agg&gt; +U[Bob, 7] agg&gt; -U[Bob, 7] agg&gt; +U[Bob, 8] agg&gt; -U[Bob, 8] agg&gt; +U[Bob, 9] group window结果 group window:&gt; +I[Mary, 2, 1970-01-01T08:00:10] group window:&gt; +I[Alice, 1, 1970-01-01T08:00:10] group window:&gt; +I[Bob, 6, 1970-01-01T08:00:10] group window:&gt; +I[Bob, 3, 1970-01-01T08:00:20] 表示Mary在第一个十秒之内有2次点击，Bob6次，Alice1次 在第二个窗口十秒中，Bob有3次 3.3 分组窗口 在 Flink 1.12 之前的版本中，Table API 和 SQL 提供了一组“分组窗口”（Group Window）函数，常用的时间窗口如滚动窗口、滑动窗口、会话窗口都有对应的实现；具体在 SQL 中就是调用 TUMBLE()、HOP()、SESSION()，传入时间属性字段、窗口大小等参数就可以了。 3.3.1 老版本 Table result = tableEnv.sqlQuery( &quot;SELECT &quot; + &quot;user, &quot; + &quot;TUMBLE_END(ts, INTERVAL '1' HOUR) as endT, &quot; + &quot;COUNT(url) AS cnt &quot; + &quot;FROM EventTable &quot; + &quot;GROUP BY &quot; + // 使用窗口和用户名进行分组 &quot;user, &quot; + &quot;TUMBLE(ts, INTERVAL '1' HOUR)&quot; // 定义 1 小时滚动窗口 ); 这里定义了 1 小时的滚动窗口，将窗口和用户 user 一起作为分组的字段。用聚合函数COUNT()对分组数据的个数进行了聚合统计，并将结果字段重命名为cnt；用TUPMBLE_END()函数获取滚动窗口的结束时间，重命名为 endT 提取出来。 3.3.2 新版本（窗口表值函数 Windowing TVFs） 从 1.13 版本开始，Flink 开始使用窗口表值函数（Windowing table-valued functions，Windowing TVFs）来定义窗口。窗口表值函数是 Flink 定义的多态表函数（PTF），可以将表进行扩展后返回。表函数（table function）可以看作是返回一个表的函数 案例 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env); TableConfig config = tableEnv.getConfig(); config.setIdleStateRetention(Duration.ofMillis(60)); SingleOutputStreamOperator&lt;Event&gt; dataStream = env.addSource(new ClickSource()).assignTimestampsAndWatermarks(WatermarkStrategy.&lt;Event&gt;forBoundedOutOfOrderness(Duration.ZERO) .withTimestampAssigner(new SerializableTimestampAssigner&lt;Event&gt;() { @Override public long extractTimestamp(Event element, long recordTimestamp) { return element.getTimestamp(); } })); // 1. 注册虚拟表 tableEnv.createTemporaryView(&quot;table_click&quot;, dataStream, $(&quot;user&quot;), $(&quot;ts&quot;).rowtime()); // 2. 窗口聚合查询 老版本-滚动窗口 Table agg = tableEnv.sqlQuery(&quot;select &quot; + &quot; user,&quot; + &quot; count(1) AS ct,&quot; + &quot; TUMBLE_END(ts,INTERVAL '10' SECOND) AS endTime&quot; + &quot; from table_click &quot; + &quot; group by user, TUMBLE(ts, INTERVAL '10' SECOND)&quot;); // 滚动窗口 布长10 // 3. 窗口聚合查询 TVF-滚动窗口 Table tvfTumbleAgg = tableEnv.sqlQuery(&quot;select &quot; + &quot; user,&quot; + &quot; count(1) AS ct,&quot; + &quot; window_start,&quot; + &quot; window_end&quot; + &quot; from TABLE(&quot; + // 创建一个滚动窗口,参数1:数据来源的虚拟表,参数2:滚动日期参数，参数3：窗口布长 &quot; TUMBLE(table table_click, DESCRIPTOR(ts), INTERVAL '10' SECOND))&quot; + &quot; GROUP BY user, window_start, window_end&quot;); // group by window_start, window_end 固定写法 // 4. 窗口聚合查询 TVF-滑动窗口 Table tvfHopAgg = tableEnv.sqlQuery(&quot;select &quot; + &quot; user,&quot; + &quot; count(1) AS ct,&quot; + &quot; window_start,&quot; + &quot; window_end&quot; + &quot; from TABLE(&quot; + // 创建一个滚动窗口,参数1:数据来源的虚拟表,参数2:滚动日期参数，参数3：滑动补布长 参数4：窗口布长 &quot; HOP(table table_click, DESCRIPTOR(ts), INTERVAL '5' SECOND,INTERVAL '10' SECOND))&quot; + &quot; GROUP BY user, window_start, window_end&quot;); // group by window_start, window_end 固定写法 // 5. 累计窗口 Table tvfCumulateAgg = tableEnv.sqlQuery(&quot;select &quot; + &quot; CURRENT_TIME as cutTime,&quot; + &quot; user,&quot; + &quot; count(1) AS ct,&quot; + &quot; window_start,&quot; + &quot; window_end&quot; + &quot; from TABLE(&quot; + // 创建一个滚动窗口,参数1:数据来源的虚拟表,参数2:滚动日期参数，参数3：每隔多久计算输出一次 参数4：全窗口布长 &quot; CUMULATE(table table_click, DESCRIPTOR(ts), INTERVAL '5' SECOND,INTERVAL '10' SECOND))&quot; + &quot; GROUP BY user, window_start, window_end&quot;); // group by window_start, window_end 固定写法 dataStream.print(&quot;source:&quot;); //tableEnv.toChangelogStream(agg).print(&quot;agg:&quot;); //tableEnv.toChangelogStream(tvfTumbleAgg).print(&quot;tvfTumbleAgg:&quot;); //tableEnv.toChangelogStream(tvfHopAgg).print(&quot;tvfHotAgg:&quot;); tableEnv.toChangelogStream(tvfCumulateAgg).print(&quot;tvfCumulateAgg:&quot;); 注意：GROUP BY window_start, window_end 是固定写法 3.4 开窗（Over）聚合 Flink SQL 中的开窗函数也是通过 OVER 子句来实现的 3.4.1 语法 &lt;聚合函数&gt; OVER ( [PARTITION BY &lt;字段 1&gt;[, &lt;字段 2&gt;, ...]] ORDER BY &lt;时间属性字段&gt; &lt;开窗范围&gt;), ... FROM ... PARTITION BY（可选） 用来指定分区的键（key），类似于 GROUP BY 的分组，这部分是可选的 ORDER BY OVER 窗口是基于当前行扩展出的一段数据范围，选择的标准可以基于时间也可以基于数量。不论那种定义，数据都应该是以某种顺序排列好的；而表中的数据本身是无序的。所以在OVER 子句中必须用 ORDER BY 明确地指出数据基于那个字段排序。在 Flink 的流处理中，目前只支持按照时间属性的升序排列，所以这里 ORDER BY 后面的字段必须是定义好的时间属性。 开窗范围 对于开窗函数而言，还有一个必须要指定的就是开窗的范围，也就是到底要扩展多少行来做聚合。这个范围是由 BETWEEN &lt;下界&gt; AND &lt;上界&gt; 来定义的，也就是“从下界到上界”的范围。目前支持的上界只能是 CURRENT ROW，也就是定义一个“从之前某一行到当前行”的范围，所以一般的形式为： PRECEDING 指前面几个/前一段时间；CURRENT ROW：指到当前最新得行数 BETWEEN ... PRECEDING AND CURRENT ROW 范围间隔 范围间隔以 RANGE 为前缀，就是基于 ORDER BY 指定的时间字段去选取一个范围，一般就是当前行时间戳之前的一段时间。例如开窗范围选择当前行之前 1 小时的数据： RANGE BETWEEN INTERVAL '1' HOUR PRECEDING AND CURRENT ROW 行间隔 行间隔以 ROWS 为前缀，就是直接确定要选多少行，由当前行出发向前选取就可以了。 例如开窗范围选择当前行之前的 5 行数据（最终聚合会包括当前行，所以一共 6 条数据） ROWS BETWEEN 5 PRECEDING AND CURRENT ROW SELECT user, COUNT(url) OVER ( PARTITION BY user ORDER BY ts RANGE BETWEEN INTERVAL '1' HOUR PRECEDING AND CURRENT ROW ) AS cnt FROM EventTable 4 topN example import com.flink.dto.Event; import org.apache.flink.api.common.eventtime.SerializableTimestampAssigner; import org.apache.flink.api.common.eventtime.WatermarkStrategy; import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.table.api.Table; import org.apache.flink.table.api.bridge.java.StreamTableEnvironment; import static org.apache.flink.table.api.Expressions.$; public class WindowTopNExample { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); // 读取数据源，并分配时间戳、生成水位线 SingleOutputStreamOperator&lt;Event&gt; eventStream = env .fromElements( new Event(&quot;Alice&quot;, &quot;./home&quot;, 1000L), new Event(&quot;Bob&quot;, &quot;./cart&quot;, 1000L), new Event(&quot;Alice&quot;, &quot;./prod?id=1&quot;, 25 * 60 * 1000L), new Event(&quot;Alice&quot;, &quot;./prod?id=4&quot;, 55 * 60 * 1000L), new Event(&quot;Bob&quot;, &quot;./prod?id=5&quot;, 3600 * 1000L + 60 * 1000L), new Event(&quot;Cary&quot;, &quot;./home&quot;, 3600 * 1000L + 30 * 60 * 1000L), new Event(&quot;Cary&quot;, &quot;./prod?id=7&quot;, 3600 * 1000L + 59 * 60 * 1000L) ).assignTimestampsAndWatermarks(WatermarkStrategy.&lt;Event&gt;forMonotonousTimestamps() .withTimestampAssigner(new SerializableTimestampAssigner&lt;Event&gt;() { @Override public long extractTimestamp(Event element, long recordTimestamp) { return element.getTimestamp(); } })); // 创建表环境 StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env); // 将数据流转换成表，并指定时间属性 Table eventTable = tableEnv.fromDataStream( eventStream, $(&quot;user&quot;), $(&quot;url&quot;), $(&quot;timestamp&quot;).rowtime().as(&quot;ts&quot;) // 将 timestamp 指定为事件时间，并命名为 ts ); // 为方便在 SQL 中引用，在环境中注册表 EventTable tableEnv.createTemporaryView(&quot;EventTable&quot;, eventTable); // 定义子查询，进行窗口聚合，得到包含窗口信息、用户以及访问次数的结果表 String subQuery = &quot;SELECT window_start, window_end, user, COUNT(url) as cnt &quot; + &quot;FROM TABLE ( &quot; + &quot;TUMBLE( TABLE EventTable, DESCRIPTOR(ts), INTERVAL '1' HOUR )) &quot; + // 滚动窗口 布长1小时 &quot;GROUP BY window_start, window_end, user &quot;; // 定义 Top N 的外层查询 String topNQuery = &quot;SELECT * &quot; + &quot;FROM (&quot; + &quot;SELECT *, &quot; + &quot;ROW_NUMBER() OVER ( &quot; + &quot;PARTITION BY window_start, window_end &quot; + &quot;ORDER BY cnt desc &quot; + &quot;) AS row_num &quot; + &quot;FROM (&quot; + subQuery + &quot;)) &quot; + &quot;WHERE row_num &lt;= 2&quot;; // 执行 SQL 得到结果表 Table result = tableEnv.sqlQuery(topNQuery); tableEnv.toDataStream(result).print(); env.execute(); } } 5 联结查询 5.1 常规联结查询 等值内连接 等值外连接 这部分跟标准SQL一样呢，就不赘述啦 5.2 间隔联结查询 1.时间间隔限制 ltime BETWEEN rtime - INTERVAL '10' SECOND AND rtime + INTERVAL '5' SECOND SELECT * FROM Order o, Shipment s WHERE o.id = s.order_id AND o.order_time BETWEEN s.ship_time - INTERVAL '4' HOUR AND s.ship_time 在流处理中，间隔联结查询只支持具有时间属性的“仅追加”（Append-only）表。 ","link":"https://tianxiawuhao.github.io/ODDWghbcN/"},{"title":"第十章 Table API 与 SQL","content":"Table API &amp; SQL 介绍 1.1 Flink Table模块 参考：https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/ Flink的Table模块包括 Table API和 SQL： Table API： 是一种类SQL的API，通过Table API，用户可以像操作表一样操作数据，非常直观和方便； SQL：作为一种声明式语言，有着标准的语法和规范，用户可以不用关心底层实现即可进行数据的处理，非常易于上手。 Flink Table API 和 SQL 的实现上有80%左右的代码是公用的。作为一个流批统一的计算引擎，Flink 的 Runtime 层是统一的。 1.2 Table API &amp; SQL特点 Flink之所以选择将 Table API &amp; SQL作为未来的核心 API，是因为其具有一些非常重要的特点： 声明式：用户只关心做什么，不用关心怎么做 高性能：支持查询优化，可以获取更好的执行性能 批流统一：相同的统计逻辑，既可以流模式运行，也可以批模式运行 标准稳定：语义遵循SQL标准，不易变动 易理解：语义明确，所见即所得 使用举例： Table API SQL tab.groupBy(“word”).select(&quot;word,count(1) as count&quot;) SELECT word,COUNT(*) AS cnt FROM MyTable GROUP BY word 1.3 Table API&amp; SQL发展历程 自 2015 年开始，阿里巴巴开始调研开源流计算引擎，最终决定基于 Flink 打造新一代计算引擎，针对 Flink存在的不足进行优化和改进，并且在 2019 年初将最终代码开源，也就是Blink。 Blink 在原来的 Flink 基础上最显著的一个贡献就是 Flink SQL的实现！ 架构升级： 查询处理器的选择 ： Flink1.11之后Blink Query Processor查询处理器已经是默认的了。 03 开发准备 参考：https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/ 2.1 添加依赖 &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-table-api-scala-bridge_2.12&lt;/artifactId&gt; &lt;version&gt;${flink.version}&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-table-api-java-bridge_2.12&lt;/artifactId&gt; &lt;version&gt;${flink.version}&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- flink执行计划,这是1.9版本之前的--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-table-planner_2.12&lt;/artifactId&gt; &lt;version&gt;${flink.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!-- blink执行计划,1.11+默认的--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-table-planner-blink_2.12&lt;/artifactId&gt; &lt;version&gt;${flink.version}&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-table-common&lt;/artifactId&gt; &lt;version&gt;${flink.version}&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; 解析： flink-table-common：这个包中主要是包含 Flink Planner和 Blink Planner一些共用的代码。 flink-table-api-java：这部分是用户编程使用的 API，包含了大部分的API。 flink-table-api-scala：这里只是非常薄的一层，仅和 Table API的 Expression 和 DSL 相关。 两个 Planner：flink-table-planner和 flink-table-planner-blink。 两个 Bridge：flink-table-api-scala-bridge 和 flink-table-api-java-bridge Flink Planner 和 Blink Planner 都会依赖于具体的 JavaAPI，也会依赖于具体的 Bridge，通过Bridge可以将 API 操作相应的转化为Scala的 DataStream，或者转化为JAVA的 DataStream 2.2 程序结构 参考：https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/common.html#structure-of-table-api-and-sql-programs // create a TableEnvironment for specific planner batch or streaming TableEnvironment tableEnv = ...; // see &quot;Create a TableEnvironment&quot; section // create a Table tableEnv.connect(...).createTemporaryTable(&quot;table1&quot;); // register an output Table tableEnv.connect(...).createTemporaryTable(&quot;outputTable&quot;); // create a Table object from a Table API query Table tapiResult = tableEnv.from(&quot;table1&quot;).select(...); // create a Table object from a SQL query Table sqlResult = tableEnv.sqlQuery(&quot;SELECT ... FROM table1 ... &quot;); // emit a Table API result Table to a TableSink, same for SQL result TableResult tableResult = tapiResult.executeInsert(&quot;outputTable&quot;); tableResult... 2.3 API 2.3.1 获取环境 参考：https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/common.html#create-a-tableenvironment // ********************** // FLINK STREAMING QUERY // ********************** import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.table.api.EnvironmentSettings; import org.apache.flink.table.api.bridge.java.StreamTableEnvironment; EnvironmentSettings fsSettings = EnvironmentSettings.newInstance().useOldPlanner().inStreamingMode().build(); StreamExecutionEnvironment fsEnv = StreamExecutionEnvironment.getExecutionEnvironment(); StreamTableEnvironment fsTableEnv = StreamTableEnvironment.create(fsEnv, fsSettings); // or TableEnvironment fsTableEnv = TableEnvironment.create(fsSettings); // ****************** // FLINK BATCH QUERY // ****************** import org.apache.flink.api.java.ExecutionEnvironment; import org.apache.flink.table.api.bridge.java.BatchTableEnvironment; ExecutionEnvironment fbEnv = ExecutionEnvironment.getExecutionEnvironment(); BatchTableEnvironment fbTableEnv = BatchTableEnvironment.create(fbEnv); // ********************** // BLINK STREAMING QUERY // ********************** import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.table.api.EnvironmentSettings; import org.apache.flink.table.api.bridge.java.StreamTableEnvironment; StreamExecutionEnvironment bsEnv = StreamExecutionEnvironment.getExecutionEnvironment(); EnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build(); StreamTableEnvironment bsTableEnv = StreamTableEnvironment.create(bsEnv, bsSettings); // or TableEnvironment bsTableEnv = TableEnvironment.create(bsSettings); // ****************** // BLINK BATCH QUERY // ****************** import org.apache.flink.table.api.EnvironmentSettings; import org.apache.flink.table.api.TableEnvironment; EnvironmentSettings bbSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inBatchMode().build(); TableEnvironment bbTableEnv = TableEnvironment.create(bbSettings); 2.3.3 创建表 // get a TableEnvironment TableEnvironment tableEnv = ...; // see &quot;Create a TableEnvironment&quot; section // table is the result of a simple projection query Table projTable = tableEnv.from(&quot;X&quot;).select(...); // register the Table projTable as table &quot;projectedTable&quot; tableEnv.createTemporaryView(&quot;projectedTable&quot;, projTable); tableEnvironment .connect(...) .withFormat(...) .withSchema(...) .inAppendMode() .createTemporaryTable(&quot;MyTable&quot;) 2.3.4 查询表 Table API： // get a TableEnvironment TableEnvironment tableEnv = ...; // see &quot;Create a TableEnvironment&quot; section // register Orders table // scan registered Orders table Table orders = tableEnv.from(&quot;Orders&quot;);// compute revenue for all customers from France Table revenue = orders .filter($(&quot;cCountry&quot;) .isEqual(&quot;FRANCE&quot;)) .groupBy($(&quot;cID&quot;), $(&quot;cName&quot;) .select($(&quot;cID&quot;), $(&quot;cName&quot;), $(&quot;revenue&quot;) .sum() .as(&quot;revSum&quot;)); // emit or convert Table // execute query SQL： // get a TableEnvironment TableEnvironment tableEnv = ...; // see &quot;Create a TableEnvironment&quot; section // register Orders table // compute revenue for all customers from France Table revenue = tableEnv.sqlQuery( &quot;SELECT cID, cName, SUM(revenue) AS revSum &quot; + &quot;FROM Orders &quot; + &quot;WHERE cCountry = 'FRANCE' &quot; + &quot;GROUP BY cID, cName&quot; ); // emit or convert Table // execute query // get a TableEnvironment TableEnvironment tableEnv = ...; // see &quot;Create a TableEnvironment&quot; section // register &quot;Orders&quot; table // register &quot;RevenueFrance&quot; output table // compute revenue for all customers from France and emit to &quot;RevenueFrance&quot; tableEnv.executeSql( &quot;INSERT INTO RevenueFrance &quot; + &quot;SELECT cID, cName, SUM(revenue) AS revSum &quot; + &quot;FROM Orders &quot; + &quot;WHERE cCountry = 'FRANCE' &quot; + &quot;GROUP BY cID, cName&quot; ); 2.3.5 写出表 // get a TableEnvironment TableEnvironment tableEnv = ...; // see &quot;Create a TableEnvironment&quot; section // create an output Table final Schema schema = new Schema() .field(&quot;a&quot;, DataTypes.INT()) .field(&quot;b&quot;, DataTypes.STRING()) .field(&quot;c&quot;, DataTypes.BIGINT()); tableEnv.connect(new FileSystem().path(&quot;/path/to/file&quot;)) .withFormat(new Csv().fieldDelimiter('|').deriveSchema()) .withSchema(schema) .createTemporaryTable(&quot;CsvSinkTable&quot;); // compute a result Table using Table API operators and/or SQL queries Table result = ... // emit the result Table to the registered TableSink result.executeInsert(&quot;CsvSinkTable&quot;); 2.3.6 与DataStream集成 参考：https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/common.html#integration-with-datastream-and-dataset-api 3.3.6.1 从DataStream创建视图 Create a View from a DataStream or DataSet： // get StreamTableEnvironment // registration of a DataSet in a BatchTableEnvironment is equivalent StreamTableEnvironment tableEnv = ...; // see &quot;Create a TableEnvironment&quot; section DataStream&lt;Tuple2&lt;Long, String&gt;&gt; stream = ... // register the DataStream as View &quot;myTable&quot; with fields &quot;f0&quot;, &quot;f1&quot; tableEnv.createTemporaryView(&quot;myTable&quot;, stream); // register the DataStream as View &quot;myTable2&quot; with fields &quot;myLong&quot;, &quot;myString&quot; tableEnv.createTemporaryView(&quot;myTable2&quot;, stream, $(&quot;myLong&quot;), $(&quot;myString&quot;)); 2.3.6.2 转换DataStream到表 Convert a DataStream or DataSet into a Table： // get StreamTableEnvironment// registration of a DataSet in a BatchTableEnvironment is equivalent StreamTableEnvironment tableEnv = ...; // see &quot;Create a TableEnvironment&quot; section DataStream&lt;Tuple2&lt;Long, String&gt;&gt; stream = ... // Convert the DataStream into a Table with default fields &quot;f0&quot;, &quot;f1&quot; Table table1 = tableEnv.fromDataStream(stream); // Convert the DataStream into a Table with fields &quot;myLong&quot;, &quot;myString&quot; Table table2 = tableEnv.fromDataStream(stream, $(&quot;myLong&quot;), $(&quot;myString&quot;)); 2.3.6.3 转换表到DataStream Convert a Table into a DataStream： 追加模式（Append Mode）：只有当动态表仅通过插入更改进行修改时，才能使用此模式，即，它是仅追加模式，并且以前发出的结果从不更新； 撤回模式（Retract Mode）：此模式始终可用。它使用布尔标志对插入和删除更改进行编码。 // get StreamTableEnvironment. StreamTableEnvironment tableEnv = ...; // see &quot;Create a TableEnvironment&quot; section // Table with two fields (String name, Integer age) Table table = ... // convert the Table into an append DataStream of Row by specifying the class DataStream&lt;Row&gt; dsRow = tableEnv.toAppendStream(table, Row.class); // convert the Table into an append DataStream of Tuple2&lt;String, Integer&gt; // via a TypeInformation TupleTypeInfo&lt;Tuple2&lt;String, Integer&gt;&gt; tupleType = new TupleTypeInfo&lt;&gt;( Types.STRING(), Types.INT()); DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; dsTuple = tableEnv.toAppendStream(table, tupleType); // convert the Table into a retract DataStream of Row. // A retract stream of type X is a DataStream&lt;Tuple2&lt;Boolean, X&gt;&gt;. // The boolean field indicates the type of the change. // True is INSERT, false is DELETE. DataStream&lt;Tuple2&lt;Boolean, Row&gt;&gt; retractStream = tableEnv.toRetractStream(table, Row.class); 2.3.7 TableAPI 参考：https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/tableApi.html 2.3.8 SQL 参考：https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/sql/ 03 相关概念 参考：https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/streaming/dynamic_tables.html 3.1 动态表和连续查询 在Flink中，它把针对无界流的表称之为Dynamic Table（动态表）。它是Flink Table API和SQL的核心概念，顾名思义，它表示了Table是不断变化的。 我们可以这样来理解，当我们用Flink的API，建立一个表，其实把它理解为建立一个逻辑结构，这个逻辑结构需要映射到数据上去。 Flink source源源不断的流入数据，就好比每次都往表上新增一条数据。表中有了数据，我们就可以使用SQL去查询了。要注意一下，流处理中的数据是只有新增的，所以看起来数据会源源不断地添加到表中。 动态表也是一种表，既然是表，就应该能够被查询。我们来回想一下原先我们查询表的场景。 打开编译工具，编写一条SQL语句 将SQL语句放入到mysql的终端执行 查看结果 再编写一条SQL语句 再放入到终端执行 再查看结果 ……如此反复 而针对动态表，Flink的source端肯定是源源不断地会有数据流入，然后我们基于这个数据流建立了一张表，再编写SQL语句查询数据，进行处理。这个SQL语句一定是不断地执行的，而不是只执行一次。 注意：针对流处理的SQL绝对不会像批式处理一样，执行一次拿到结果就完了。而是会不停地执行，不断地查询获取结果处理。所以，官方给这种查询方式取了一个名字，叫Continuous Query，中文翻译过来叫连续查询。而且每一次查询出来的数据也是不断变化的。 该示意图描述了：我们通过建立动态表和连续查询来实现在无界流中的SQL操作。 大家也可以看到，在Continuous上面有一个State，表示查询出来的结果会存储在State中，再下来Flink最终还是使用流来进行处理。 所以，我们可以理解为Flink的Table API和SQL，是一个逻辑模型，通过该逻辑模型可以让我们的数据处理变得更加简单。 3.2 表与Stream的转换 3.2.1 表中的Update和Delete 我们前面提到的表示不断地 Append，表的数据是一直累加的，因为表示对接Source的，Source是不会有update的，但如果我们编写了一个SQL。这个SQL看起来是这样的： SELECT user, sum(money) FROM order GROUP BY user; 当执行一条SQL语句之后，这条语句的结果还是一个表，因为在Flink中执行的SQL是Continuous Query，这个表的数据是不断变化的。新创建的表存在Update的情况。仔细看下下面的示例，例如： 第一条数据，张三,2000，执行这条SQL语句的结果是，张三,2000 第二条数据，李四,1500，继续执行这条SQL语句，结果是，张三,2000 | 李四,1500 第三条数据，张三,300，继续执行这条SQL语句，结果是，张三,2300 | 李四,1500 …. 大家发现了吗，现在数据结果是有Update的，张三一开始是2000，但后面变成了2300。 那还有删除的情况吗？有的，看下面这条SQL语句： SELECT t1.`user`, SUM(t1.`money`) FROM t_order t1 WHERE NOT EXISTS (SELECT T2.`user`AS TOTAL_MONEY FROM t_order t2 WHERE T2.`user` = T1.`user` GROUP BY t2.`user` HAVING SUM(T2.`money`) &gt; 3000) GROUP BY t1.`user`GROUP BY t1.`user` 第一条数据，张三,2000，执行这条SQL语句的结果是，张三,2000 第二条数据，李四,1500，继续执行这条SQL语句，结果是，张三,2000 | 李四,1500 第三条数据，张三,300，继续执行这条SQL语句，结果是，张三,2300 | 李四,1500 第四条数据，张三,800，继续执行这条SQL语句，结果是，李四,1500 因为张三的消费的金额已经超过了3000，所以SQL执行完后，张三是被处理掉了。从数据的角度来看，它不就是被删除了吗？ 通过上面的两个示例，可以知道在Flink SQL中，对接Source的表都是Append-only的，不断地增加，执行一些SQL生成的表，这个表可能是要UPDATE的、也可能是要INSERT的。 3.2.2 对表的编码操作 我们前面说到过，表是一种逻辑结构，而Flink中的核心还是Stream，所以，Table最终还是会以Stream方式来继续处理，如果是以Stream方式处理，最终Stream中的数据有可能会写入到其他的外部系统中，例如：将Stream中的数据写入到MySQL中。 我们前面也看到了，表是有可能会UPDATE和DELETE的，那么如果是输出到MySQL中，就要执行UPDATE和DELETE语句了，而DataStream我们在学习Flink的时候就学习过了，DataStream是不能更新、删除事件的。 如果对表的操作是INSERT，这很好办，直接转换输出就好，因为DataStream数据也是不断递增的。但如果一个TABLE中的数据被UPDATE了、或者被DELETE了，如果用流来表达呢？ 因为流不可变的特征，我们肯定要对这种能够进行UPDATE/DELETE的TABLE做特殊操作。 解决方案：我们可以针对每一种操作，INSERT/UPDATE/DELETE都用一个或多个经过编码的事件来表示。例如： 针对UPDATE，我们用两个操作来表达，[DELETE]数据+ [INSERT]数据。也就是先把之前的数据删除，然后再插入一条新的数据。 针对DELETE，我们也可以对流中的数据进行编码，[DELETE]数据。 总体来说，我们通过对流数据进行编码，也可以告诉DataStream的下游，[DELETE]表示发出MySQL的DELETE操作，将数据删除。用[INSERT]表示插入新的数据。 3.2.3 将表转换为三种不同编码方式的流 Flink中的Table API或者SQL支持三种不同的编码方式，分别是： Append-only流 Retract流 Upsert流 3.2.3.1 Append-only流 跟INSERT操作对应。这种编码类型的流针对的是只会不断新增的Dynamic Table，这种方式好处理，不需要进行特殊处理，源源不断地往流中发送事件即可。 3.2.3.2 Retract流 这种流就和Append-only不太一样，上面的只能处理INSERT，如果表会发生DELETE或者UPDATE，Append-only编码方式的流就不合适了。 Retract流有几种类型的事件类型： ADD MESSAGE：这种消息对应的就是INSERT操作。 RETRACT MESSAGE：直译过来叫撤回消息，这种消息对应的就是DELETE操作。 我们可以看到通过ADD MESSAGE和RETRACT MESSAGE可以很好的向外部系统表达删除和插入操作，那如何进行UPDATE呢？其实RETRACT MESSAGE + ADD MESSAGE即可（先把之前的数据进行删除，然后插入一条新的）。 3.2.3.3 Upsert流 前面我们看到的RETRACT编码方式的流，实现UPDATE是使用DELETE + INSERT模式的。 大家想一下：在MySQL中我们更新数据的时候，肯定不会先DELETE掉一条数据，然后再插入一条数据，肯定是直接发出UPDATE语句执行更新。 而Upsert编码方式的流，是能够支持Update的，这种效率更高。它同样有两种类型的消息： UPSERT MESSAGE：这种消息可以表示要对外部系统进行Update或者INSERT操作 DELETE MESSAGE：这种消息表示DELETE操作。 Upsert流是要求必须指定Primary Key的，因为Upsert操作是要有Key的，Upsert流针对UPDATE操作用一个UPSERT MESSAGE就可以描述，所以效率会更高。 ","link":"https://tianxiawuhao.github.io/tloS_nSAM/"},{"title":"第九章 状态编程和容错机制","content":"流式计算分为无状态和有状态两种情况。 无状态的计算观察每个独立事件， 并根据最后一个事件输出结果。 例如， 流处理应用程序从传感器接收温度读数， 并在 温度超过 90 度时发出警告。 有状态的计算则会基于多个事件输出结果。 以下是一些 例子。 所有类型的窗口。 例如， 计算过去一小时的平均温度， 就是有状态的计算。 所有用于复杂事件处理的状态机。 例如， 若在一分钟内收到两个相差 20 度 以上的温度读数， 则发出警告， 这是有状态的计算。 流与流之间的所有关联操作， 以及流与静态表或动态表之间的关联操作， 都是有状态的计算。 下图展示了无状态流处理和有状态流处理的主要区别。 无状态流处理分别接收每条数据记录(图中的黑条)， 然后根据最新输入的数据生成输出数据(白条)。 有状态 流处理会维护状态(根据每条输入记录进行更新)， 并基于最新输入的记录和当前的 状态值生成输出记录(灰条)。 图 无状态和有状态的流处理 上图中输入数据由黑条表示。 无状态流处理每次只转换一条输入记录， 并且仅根据最新的输入记录输出结果(白条)。 有状态 流处理维护所有已处理记录的状态 值， 并根据每条新输入的记录更新状态， 因此输出记录(灰条)反映的是综合考虑多 个事件之后的结果。 尽管无状态的计算很重要， 但是流处理对有状态的计算更感兴趣。 事实上， 正确地实现有状态的计算比实现无状态的计算难得多。 旧的流处理系统并不支持有状 态的计算， 而新一代的流处理系统则将状态及其正确性视为重中之重。 有状态的算子和应用程序 Flink 内置的很多算子， 数据源 source， 数据存储 sink 都是有状态的， 流中的数据都是 buffer records， 会保存一定的元素或者元数据。 例如: ProcessWindowFunction会缓存输入流的数据， ProcessFunction 会保存设置的定时器信息等等。 在 Flink 中， 状态始终与特定算子相关联。 总的来说， 有两种类型的状态： 算子状态（ operator state） 键控状态（ keyed state） 算子状态（operator state） 算子状态的作用范围限定为算子任务。 这意味着由同一并行任务所处理的所有数据都可以访问到相同的状态， 状态对于同一任务而言是共享的。 算子状态不能由 相同或不同算子的另一个任务访问。 图 具有算子状态的任务 Flink 为算子状态提供三种基本数据结构： 列表状态（List state） 将状态表示为一组数据的列表。 联合列表状态（Union list state） 也将状态表示为数据的列表。 它与常规列表状态的区别在于， 在发生故障时， 或者从保 存点（savepoint） 启动应用程序时如何恢复。 广播状态（Broadcast state） 如果一个算子有多项任务， 而它的每项任务状态又都相同， 那么这种特殊情况最适合应 用广播状态。 键控状态（keyed state） 键控状态是根据输入数据流中定义的键（key） 来维护和访问的。 Flink 为每个键值维护一个状态实例， 并将具有相同键的所有数据， 都分区到同一个算子任务中， 这个任务会维护和处理这个 key 对应的状态。 当任务处理一条数据时， 它会自动将状态的访问范围限定为当 前数据的 key。 因此， 具有相同 key 的所有数据都会访问相同的状态。 Keyed State 很类似于 一个分布式的 key-value map 数据结构， 只能用于 KeyedStream（ keyBy 算子处理之后） 。 图 具有键控状态的任务 Flink 的 Keyed State 支持以下数据类型： ValueState[T]保存单个的值， 值的类型为 T。 1.1 get 操作: ValueState.value() 1.2 set 操作: ValueState.update(value: T) ListState[T]保存一个列表， 列表里的元素的数据类型为 T。 基本操作如下： 2.1 ListState.add(value: T) 2.2 ListState.addAll(values: java.util.List[T]) 2.3 ListState.get()返回 Iterable[T] 2.4 ListState.update(values: java.util.List[T]) MapState[K, V]保存 Key-Value 对。 3.1 MapState.get(key: K) 3.2 MapState.put(key: K, value: V) 3.3 MapState.contains(key: K) 3.4 MapState.remove(key: K) ReducingState[T] AggregatingState[I, O] State.clear()是清空操作。 通过 RuntimeContext 注册 StateDescriptor。 StateDescriptor 以状态 state 的名字和存储的数据类型为参数。 在 open()方法中创建 state 变量。 注意复习之前的 RichFunction 相关知识。接下来我们使用了 FlatMap with keyed ValueState 的快捷方式 flatMapWithState 实现以上需求。 状态一致性 当在分布式系统中引入状态时， 自然也引入了一致性问题。 一致性实际上是\"正确性级别\"的另一种说法， 也就是说在成功处理故障并恢复之后得到的结果， 与没 有发生任何故障时得到的结果相比， 前者到底有多正确？ 举例来说， 假设要对最近 一小时登录的用户计数。 在系统经历故障之后， 计数结果是多少？ 如果有偏差， 是 有漏掉的计数还是重复计数？ 一致性级别 在流处理中， 一致性可以分为 3 个级别： at-most-once: 这其实是没有正确性保障的委婉说法——故障发生之后， 计 数结果可能丢失。 同样的还有 udp。 at-least-once: 这表示计数结果可能大于正确值， 但绝不会小于正确值。 也 就是说， 计数程序在发生故障后可能多算， 但是绝不会少算。 exactly-once: 这指的是系统保证在发生故障后得到的计数结果与正确值一 致。 曾经， at-least-once 非常流行。 第一代流处理器(如 Storm 和 Samza)刚问世时只保证 at-least-once， 原因有二。 保证 exactly-once 的系统实现起来更复杂。 这在基础架构层(决定什么代表 正确， 以及 exactly-once 的范围是什么)和实现层都很有挑战性。 流处理系统的早期用户愿意接受框架的局限性， 并在应用层想办法弥补(例 如使应用程序具有幂等性， 或者用批量计算层再做一遍计算)。 最先保证 exactly-once 的系统(Storm Trident 和 Spark Streaming)在性能和表现力 这两个方面付出了很大的代价。 为了保证 exactly-once， 这些系统无法单独地对每条 记录运用应用逻辑， 而是同时处理多条(一批)记录， 保证对每一批的处理要么全部 成功， 要么全部失败。 这就导致在得到结果前， 必须等待一批记录处理结束。 因此， 用户经常不得不使用两个流处理框架(一个用来保证 exactly-once， 另一个用来对每 个元素做低延迟处理)， 结果使基础设施更加复杂。 曾经， 用户不得不在保证 exactly-once 与获得低延迟和效率之间权衡利弊。 Flink 避免了这种权衡。 Flink 的一个重大价值在于， 它既保证了 exactly-once， 也具有低延迟和高吞吐 的处理能力。 从根本上说， Flink 通过使自身满足所有需求来避免权衡， 它是业界的一次意义 重大的技术飞跃。 尽管这在外行看来很神奇， 但是一旦了解， 就会恍然大悟。 端到端（end-to-end） 状态一致性 目前我们看到的一致性保证都是由流处理器实现的， 也就是说都是在 Flink 流处理器内部保证的； 而在真实应用中， 流处理应用除了流处理器以外还包含了数据 源（ 例如 Kafka） 和输出到持久化系统。 端到端的一致性保证， 意味着结果的正确性贯穿了整个流处理应用的始终； 每一个组件都保证了它自己的一致性， 整个端到端的一致性级别取决于所有组件中一 致性最弱的组件。 具体可以划分如下： 内部保证 —— 依赖 checkpoint source 端 —— 需要外部源可重设数据的读取位置 sink 端 —— 需要保证从故障恢复时， 数据不会重复写入外部系统 而对于 sink 端， 又有两种具体的实现方式： 幂等（ Idempotent） 写入和事务性 （ Transactional） 写入。 幂等写入 所谓幂等操作， 是说一个操作， 可以重复执行很多次， 但只导致一次结果更改， 也就是说， 后面再重复执行就不起作用了。 事务写入 需要构建事务来写入外部系统， 构建的事务对应着 checkpoint， 等到 checkpoint 真正完成的时候， 才把所有对应的结果写入 sink 系统中。 对于事务性写入， 具体又有两种实现方式： 预写日志（ WAL） 和两阶段提交（ 2PC） 。 DataStream API 提供了 GenericWriteAheadSink 模板类和 TwoPhaseCommitSinkFunction 接口， 可以方便地实现这两种方式的事务性写入。 不同 Source 和 Sink 的一致性保证可以用下表说明： 检查点（checkpoint） Flink 具体如何保证 exactly-once 呢? 它使用一种被称为\"检查点\"（ checkpoint）的特性， 在出现故障时将系统重置回正确状态。 下面通过简单的类比来解释检查点 的作用。 假设你和两位朋友正在数项链上有多少颗珠子， 如下图所示。 你捏住珠子， 边数边拨， 每拨过一颗珠子就给总数加一。 你的朋友也这样数他们手中的珠子。 当你 分神忘记数到哪里时， 怎么办呢? 如果项链上有很多珠子， 你显然不想从头再数一 遍， 尤其是当三人的速度不一样却又试图合作的时候， 更是如此(比如想记录前一分 钟三人一共数了多少颗珠子， 回想一下一分钟滚动窗口)。 于是， 你想了一个更好的办法: 在项链上每隔一段就松松地系上一根有色皮筋， 将珠子分隔开; 当珠子被拨动的时候， 皮筋也可以被拨动; 然后， 你安排一个助手， 让他在你和朋友拨到皮筋时记录总数。 用这种方法， 当有人数错时， 就不必从头开 始数。 相反， 你向其他人发出错误警示， 然后你们都从上一根皮筋处开始重数， 助 手则会告诉每个人重数时的起始数值， 例如在粉色皮筋处的数值是多少。 Flink 检查点的作用就类似于皮筋标记。 数珠子这个类比的关键点是: 对于指定的皮筋而言， 珠子的相对位置是确定的; 这让皮筋成为重新计数的参考点。 总状态 (珠子的总数)在每颗珠子被拨动之后更新一次， 助手则会保存与每根皮筋对应的检 查点状态， 如当遇到粉色皮筋时一共数了多少珠子， 当遇到橙色皮筋时又是多少。 当问题出现时， 这种方法使得重新计数变得简单。 Flink 的检查点算法 Flink 检查点的核心作用是确保状态正确， 即使遇到程序中断， 也要正确。 记住这一基本点之后， 我们用一个例子来看检查点是如何运行的。 Flink 为用户提供了用 来定义状态的工具。 例如， 以下这个 Scala 程序按照输入记录的第一个字段(一个字 符串)进行分组并维护第二个字段的计数状态。 val stream: DataStream[(String, Int)] = ... val counts: DataStream[(String, Int)] = stream .keyBy(record =&gt; record._1) .mapWithState( (in: (String, Int), state: Option[Int]) =&gt; state match { case Some(c) =&gt; ( (in._1, c + in._2), Some(c + in._2) ) case None =&gt; ( (in._1, in._2), Some(in._2) ) }) 该程序有两个算子: keyBy 算子用来将记录按照第一个元素(一个字符串)进行分组， 根据该 key 将数据进行重新分区， 然后将记录再发送给下一个算子: 有状态的 map 算子(mapWithState)。 map 算子在接收到每个元素后， 将输入记录的第二个字段 的数据加到现有总数中， 再将更新过的元素发射出去。 下图表示程序的初始状态: 输 入流中的 6 条记录被检查点分割线(checkpoint barrier)隔开， 所有的 map 算子状态均 为 0(计数还未开始)。 所有 key 为 a 的记录将被顶层的 map 算子处理， 所有 key 为 b 的记录将被中间层的 map 算子处理， 所有 key 为 c 的记录则将被底层的 map 算子处 理。 图 按 key 累加计数程序初始状态 上图是程序的初始状态。 注意， a、 b、 c 三组的初始计数状态都是 0， 即三个圆柱上的值。 ckpt 表示检查点分割线（ checkpoint barriers） 。 每条记录在处理顺序上严格地遵守在检查点之前或之后的规定， 例如[\"b\",2]在检查点之前被处理， [\"a\",2] 则在检查点之后被处理。 当该程序处理输入流中的 6 条记录时， 涉及的操作遍布 3 个并行实例(节点、 CPU内核等)。 那么， 检查点该如何保证 exactly-once 呢? 检查点分割线和普通数据记录类似。 它们由算子处理， 但并不参与计算， 而是会触发与检查点相关的行为。 当读取输入流的数据源(在本例中与 keyBy 算子内联) 遇到检查点屏障时， 它将其在输入流中的位置保存到持久化存储中。 如果输入流来 自消息传输系统(Kafka)， 这个位置就是偏移量。 Flink 的存储机制是插件化的， 持久 化存储可以是分布式文件系统， 如 HDFS。 下图展示了这个过程。 图 遇到 checkpoint barrier 时， 保存其在输入流中的位置 当 Flink 数据源(在本例中与 keyBy 算子内联)遇到检查点分界线（ barrier） 时，它会将其在输入流中的位置保存到持久化存储中。 这让 Flink 可以根据该位置重启。检查点像普通数据记录一样在算子之间流动。 当 map 算子处理完前 3 条数据并 收到检查点分界线时， 它们会将状态以异步的方式写入持久化存储， 如下图所示。 图 保存 map 算子状态， 也就是当前各个 key 的计数值 位于检查点之前的所有记录([\"b\",2]、 [\"b\",3]和[\"c\",1])被 map 算子处理之后的情况。 此时， 持久化存储已经备份了检查点分界线在输入流中的位置(备份操作发生在barrier 被输入算子处理的时候)。 map 算子接着开始处理检查点分界线， 并触发将状 态异步备份到稳定存储中这个动作。 当 map 算子的状态备份和检查点分界线的位置备份被确认之后， 该检查点操作就可以被标记为完成， 如下图所示。 我们在无须停止或者阻断计算的条件下， 在一 个逻辑时间点(对应检查点屏障在输入流中的位置)为计算状态拍了快照。 通过确保 备份的状态和位置指向同一个逻辑时间点， 后文将解释如何基于备份恢复计算， 从 而保证 exactly-once。 值得注意的是， 当没有出现故障时， Flink 检查点的开销极小， 检查点操作的速度由持久化存储的可用带宽决定。 回顾数珠子的例子: 除了因为数 错而需要用到皮筋之外， 皮筋会被很快地拨过。 图 检查点操作完成， 继续处理数据 检查点操作完成， 状态和位置均已备份到稳定存储中。 输入流中的所有数据记录都已处理完成。 值得注意的是， 备份的状态值与实际的状态值是不同的。 备份反 映的是检查点的状态。 如果检查点操作失败， Flink 可以丢弃该检查点并继续正常执行， 因为之后的某一个检查点可能会成功。 虽然恢复时间可能更长， 但是对于状态的保证依旧很有力。 只有在一系列连续的检查点操作失败之后， Flink 才会抛出错误， 因为这通常预示着 发生了严重且持久的错误。 现在来看看下图所示的情况: 检查点操作已经完成， 但故障紧随其后。 图 故障紧跟检查点， 导致最底部的实例丢失 在这种情况下， Flink 会重新拓扑(可能会获取新的执行资源)， 将输入流倒回到上一个检查点， 然后恢复状态值并从该处开始继续计算。 在本例中， [\"a\",2]、 [\"a\",2]和[\"c\",2]这几条记录将被重播。 下图展示了这一重新处理过程。 从上一个检查点开始重新计算， 可以保证在剩下的记录被处理之后， 得到的 map 算子的状态值与没有发生故障时的状态值一致。 图 故障时的状态恢复 Flink 将输入流倒回到上一个检查点屏障的位置， 同时恢复 map 算子的状态值。然后， Flink 从此处开始重新处理。 这样做保证了在记录被处理之后， map 算子的状 态值与没有发生故障时的一致。 Flink 检查点算法的正式名称是异步分界线快照(asynchronous barrier snapshotting)。 该算法大致基于 Chandy-Lamport 分布式快照算法。 检查点是 Flink 最有价值的创新之一， 因为它使 Flink 可以保证 exactly-once， 并且不需要牺牲性能。 Flink+Kafka 如何实现端到端的 exactly-once 语义 我们知道， 端到端的状态一致性的实现， 需要每一个组件都实现， 对于 Flink +Kafka 的数据管道系统（ Kafka 进、 Kafka 出） 而言， 各组件怎样保证 exactly-once 语义呢？ 内部 —— 利用 checkpoint 机制， 把状态存盘， 发生故障的时候可以恢复， 保证内部的状态一致性 source —— kafka consumer 作为 source， 可以将偏移量保存下来， 如果后 续任务出现了故障， 恢复的时候可以由连接器重置偏移量， 重新消费数据， 保证一致性 sink —— kafka producer 作为 sink， 采用两阶段提交 sink， 需要实现一个 TwoPhaseCommitSinkFunction 内部的 checkpoint 机制我们已经有了了解， 那 source 和 sink 具体又是怎样运行的呢？ 接下来我们逐步做一个分析。 我们知道 Flink 由 JobManager 协调各个 TaskManager 进行 checkpoint 存储，checkpoint 保存在 StateBackend 中， 默认 StateBackend 是内存级的， 也可以改为文件级的进行持久化保存。 当 checkpoint 启动时， JobManager 会将检查点分界线（ barrier） 注入数据流；barrier 会在算子间传递下去。 每个算子会对当前的状态做个快照， 保存到状态后端。 对于 source 任务而言，就会把当前的 offset 作为状态保存起来。 下次从 checkpoint 恢复时， source 任务可以重新提交偏移量， 从上次保存的位置开始重新消费数据。 每个内部的 transform 任务遇到 barrier 时， 都会把状态存到 checkpoint 里。sink 任务首先把数据写入外部 kafka， 这些数据都属于预提交的事务（ 还不能 被消费） ； 当遇到 barrier 时， 把状态保存到状态后端， 并开启新的预提交事务。 当所有算子任务的快照完成， 也就是这次的 checkpoint 完成时， JobManager 会 向所有任务发通知， 确认这次 checkpoint 完成。 当 sink 任务收到确认通知， 就会正式提交之前的事务， kafka 中未确认的数据就改为“ 已确认” ， 数据就真正可以被消费了。 所以我们看到， 执行过程实际上是一个两段式提交， 每个算子执行完成， 会进行“ 预提交” ， 直到执行完 sink 操作， 会发起“ 确认提交” ， 如果执行失败， 预提 交会放弃掉。 具体的两阶段提交步骤总结如下： 第一条数据来了之后， 开启一个 kafka 的事务（ transaction） ， 正常写入 kafka 分区日志但标记为未提交， 这就是“ 预提交” jobmanager 触发 checkpoint 操作， barrier 从 source 开始向下传递， 遇到 barrier 的算子将状态存入状态后端， 并通知 jobmanager sink 连接器收到 barrier， 保存当前状态， 存入 checkpoint， 通知 jobmanager， 并开启下一阶段的事务， 用于提交下个检查点的数据 jobmanager 收到所有任务的通知， 发出确认信息， 表示 checkpoint 完成 sink 任务收到 jobmanager 的确认信息， 正式提交这段时间的数据 外部 kafka 关闭事务， 提交的数据可以正常消费了。 所以我们也可以看到， 如果宕机需要通过 StateBackend 进行恢复， 只能恢复所有确认提交的操作。 选择一个状态后端(state backend) MemoryStateBackend 内存级的状态后端， 会将键控状态作为内存中的对象进行管理， 将它们存储 在 TaskManager 的 JVM 堆上； 而将 checkpoint 存储在 JobManager 的内存中。 FsStateBackend 将 checkpoint 存到远程的持久化文件系统（ FileSystem） 上。 而对于本地状态， 跟 MemoryStateBackend 一样， 也会存在 TaskManager 的 JVM 堆上。 RocksDBStateBackend 将所有状态序列化后， 存入本地的 RocksDB 中存储。 注意： RocksDB 的支持并不直接包含在 flink 中， 需要引入依赖： &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-statebackend-rocksdb_2.12&lt;/artifactId&gt; &lt;version&gt;1.10.1&lt;/version&gt; &lt;/dependency&gt; 设置状态后端为 FsStateBackend： val env = StreamExecutionEnvironment.getExecutionEnvironment val checkpointPath: String = ??? val backend = new RocksDBStateBackend(checkpointPath) env.setStateBackend(backend) env.setStateBackend(new FsStateBackend(&quot;file:///tmp/checkpoints&quot;)) env.enableCheckpointing(1000) // 配置重启策略 env.setRestartStrategy(RestartStrategies.fixedDelayRestart(60, Time.of(10, TimeUnit.SECONDS))) ","link":"https://tianxiawuhao.github.io/rxadb61B8/"},{"title":"第八章 ProcessFunction API（底层 API）","content":"之前所介绍的流处理 API，无论是基本的转换、聚合，还是更为复杂的窗口操作，其实都是基于 DataStream 进行转换的；所以可以统称为 DataStream API，这也是 Flink 编程的核心。而我们知道，为了让代码有更强大的表现力和易用性，Flink 本身提供了多层 API，DataStream API 只是中间的一环 在更底层，我们可以不定义任何具体的算子（比如 map，filter，或者 window），而只是提炼出一个统一的“处理”（process）操作——它是所有转换算子的一个概括性的表达，可以自定义处理逻辑，所以这一层接口就被叫作“处理函数”（process function）。 在处理函数中，我们直面的就是数据流中最基本的元素：数据事件（event）、状态（state）以及时间（time）。这就相当于对流有了完全的控制权。处理函数比较抽象，没有具体的操作，所以对于一些常见的简单应用（比如求和、开窗口）会显得有些麻烦；不过正是因为它不限定具体做什么，所以理论上我们可以做任何事情，实现所有需求。所以可以说，处理函数是我们进行 Flink 编程的“大招”，轻易不用，一旦放出来必然会扫平一切。 基本处理函数（ProcessFunction） 处理函数主要是定义数据流的转换操作，所以也可以把它归到转换算子中。我们知道在Flink 中几乎所有转换算子都提供了对应的函数类接口，处理函数也不例外；它所对应的函数类，就叫作 ProcessFunction。 处理函数的功能和使用 我们之前学习的转换算子，一般只是针对某种具体操作来定义的，能够拿到的信息比较有限。比如 map 算子，我们实现的 MapFunction 中，只能获取到当前的数据，定义它转换之后的形式；而像窗口聚合这样的复杂操作，AggregateFunction 中除数据外，还可以获取到当前的状态（以累加器 Accumulator 形式出现）。另外我们还介绍过富函数类，比如 RichMapFunction，它提供了获取运行时上下文的方法 getRuntimeContext()，可以拿到状态，还有并行度、任务名称之类的运行时信息。 但是无论那种算子，如果我们想要访问事件的时间戳，或者当前的水位线信息，都是完全做不到的。在定义生成规则之后，水位线会源源不断地产生，像数据一样在任务间流动，可我们却不能像数据一样去处理它；跟时间相关的操作，目前我们只会用窗口来处理。而在很多应用需求中，要求我们对时间有更精细的控制，需要能够获取水位线，甚至要“把控时间”、定义什么时候做什么事，这就不是基本的时间窗口能够实现的了。 于是必须祭出大招——处理函数（ProcessFunction）了。处理函数提供了一个“定时服务”（TimerService），我们可以通过它访问流中的事件（event）、时间戳（timestamp）、水位线（watermark），甚至可以注册“定时事件”。而且处理函数继承了 AbstractRichFunction 抽象类，所以拥有富函数类的所有特性，同样可以访问状态（state）和其他运行时信息。此外，处理函数还可以直接将数据输出到侧输出流（side output）中。所以，处理函数是最为灵活的处理方法，可以实现各种自定义的业务逻辑；同时也是整个 DataStream API 的底层基础。 stream.process(new MyProcessFunction()) 这里 ProcessFunction 不是接口，而是一个抽象类，继承了 AbstractRichFunction；MyProcessFunction 是它的一个具体实现。所以所有的处理函数，都是富函数（RichFunction），富函数可以调用的东西这里同样都可以调用。 import org.apache.flink.api.common.eventtime.SerializableTimestampAssigner; import org.apache.flink.api.common.eventtime.WatermarkStrategy; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.ProcessFunction; import org.apache.flink.util.Collector; public class ProcessFunctionExample { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); env.addSource(new ClickSource()) .assignTimestampsAndWatermarks( WatermarkStrategy.&lt;Event&gt;forMonotonousTimestamps().withTimestampAssigner(new SerializableTimestampAssigner&lt;Event&gt;() { @Override public long extractTimestamp(Event event, long l) { return event.timestamp; } }) ) .process(new ProcessFunction&lt;Event, String&gt;() { @Override public void processElement(Event value, Context ctx, Collector&lt;String&gt; out) throws Exception { if (value.user.equals(&quot;Mary&quot;)) { out.collect(value.user); } else if (value.user.equals(&quot;Bob&quot;)) { out.collect(value.user); out.collect(value.user); } System.out.println(ctx.timerService().currentWatermark()); } }) .print(); env.execute(); } } 这里我们在 ProcessFunction 中重写了.processElement()方法，自定义了一种处理逻辑：当数据的 user 为“Mary”时，将其输出一次；而如果为“Bob”时，将 user 输出两次。这里的输 出 ， 是 通 过 调 用 out.collect() 来实现的。另外我们还可以调用ctx.timerService().currentWatermark() 来 获 取 当 前 的 水 位 线 打 印 输 出 。 所 以 可 以 看 到 ，ProcessFunction 函数有点像 FlatMapFunction 的升级版。可以实现 Map、Filter、FlatMap 的所有功能。很明显，处理函数非常强大，能够做很多之前做不到的事情。 ProcessFunction 解析 在源码中我们可以看到，抽象类 ProcessFunction 继承了 AbstractRichFunction，有两个泛型类型参数：I 表示 Input，也就是输入的数据类型；O 表示 Output，也就是处理完成之后输出的数据类型。 内部单独定义了两个方法：一个是必须要实现的抽象方法.processElement()；另一个是非抽象方法.onTimer()。 public abstract class ProcessFunction&lt;I, O&gt; extends AbstractRichFunction { ... public abstract void processElement(I value, Context ctx, Collector&lt;O&gt; out)throws Exception; public void onTimer(long timestamp, OnTimerContext ctx, Collector&lt;O&gt; out) throws Exception {} ... } 抽象方法.processElement() 用于“处理元素”，定义了处理的核心逻辑。这个方法对于流中的每个元素都会调用一次，参数包括三个：输入数据值 value，上下文 ctx，以及“收集器”（Collector）out。方法没有返回值，处理之后的输出数据是通过收集器 out 来定义的。 value：当前流中的输入元素，也就是正在处理的数据，类型与流中数据类型一致。 ctx：类型是 ProcessFunction 中定义的内部抽象类 Context，表示当前运行的上下文，可以获取到当前的时间戳，并提供了用于查询时间和注册定时器的“定时服务”(TimerService)，以及可以将数据发送到“侧输出流”（side output）的方法.output()。Context 抽象类定义如下： public abstract class Context { public abstract Long timestamp(); public abstract TimerService timerService(); public abstract &lt;X&gt; void output(OutputTag&lt;X&gt; outputTag, X value); } out：“收集器”（类型为 Collector），用于返回输出数据。使用方式与 flatMap算子中的收集器完全一样，直接调用 out.collect()方法就可以向下游发出一个数据。这个方法可以多次调用，也可以不调用。 通过几个参数的分析不难发现，ProcessFunction 可以轻松实现 flatMap 这样的基本转换功能（当然 map、filter 更不在话下）；而通过富函数提供的获取上下文方法.getRuntimeContext()，也可以自定义状态（state）进行处理，这也就能实现聚合操作的功能了。 非抽象方法.onTimer() 用于定义定时触发的操作，这是一个非常强大、也非常有趣的功能。这个方法只有在注册好的定时器触发的时候才会调用，而定时器是通过“定时服务”TimerService 来注册的。打个比方，注册定时器（timer）就是设了一个闹钟，到了设定时间就会响；而.onTimer()中定义的，就是闹钟响的时候要做的事。所以它本质上是一个基于时间的“回调”（callback）方法，通过时间的进展来触发；在事件时间语义下就是由水位线（watermark）来触发了。 与.processElement()类似，定时方法.onTimer()也有三个参数：时间戳（timestamp），上下文（ctx），以及收集器（out）。这里的 timestamp 是指设定好的触发时间，事件时间语义下当然就是水位线了。另外这里同样有上下文和收集器，所以也可以调用定时服务（TimerService），以及任意输出处理之后的数据。 既然有.onTimer()方法做定时触发，我们用 ProcessFunction 也可以自定义数据按照时间分组、定时触发计算输出结果；这其实就实现了窗口（window）的功能。所以说 ProcessFunction是真正意义上的终极奥义，用它可以实现一切功能。 我们也可以看到，处理函数都是基于事件触发的。水位线就如同插入流中的一条数据一样；只不过处理真正的数据事件调用的是.processElement()方法，而处理水位线事件调用的是.onTimer()。 这里需要注意的是，上面的.onTimer()方法只是定时器触发时的操作，而定时器（timer）真正的设置需要用到上下文 ctx 中的定时服务。在 Flink 中，只有“按键分区流”KeyedStream才支持设置定时器的操作，所以之前的代码中我们并没有使用定时器。所以基于不同类型的流，可以使用不同的处理函数，它们之间还是有一些微小的区别的。接下来我们就介绍一下处理函数的分类。 处理函数的分类 Flink 中的处理函数其实是一个大家族，ProcessFunction 只是其中一员。 我们知道，DataStream 在调用一些转换方法之后，有可能生成新的流类型；例如调用.keyBy()之后得到 KeyedStream，进而再调用.window()之后得到 WindowedStream。对于不同类型的流，其实都可以直接调用.process()方法进行自定义处理，这时传入的参数就都叫作处理函数。当然，它们尽管本质相同，都是可以访问状态和时间信息的底层 API，可彼此之间也会有所差异。 Flink 提供了 8 个不同的处理函数： （1）ProcessFunction 最基本的处理函数，基于 DataStream 直接调用.process()时作为参数传入。 （2）KeyedProcessFunction 对流按键分区后的处理函数，基于 KeyedStream 调用.process()时作为参数传入。要想使用定时器，比如基于 KeyedStream。 （3）ProcessWindowFunction 开窗之后的处理函数，也是全窗口函数的代表。基于 WindowedStream 调用.process()时作 为参数传入。 （4）ProcessAllWindowFunction 同样是开窗之后的处理函数，基于 AllWindowedStream 调用.process()时作为参数传入。 （5）CoProcessFunction 合并（connect）两条流之后的处理函数，基于 ConnectedStreams 调用.process()时作为参 数传入。关于流的连接合并操作 （6）ProcessJoinFunction 间隔连接（interval join）两条流之后的处理函数，基于 IntervalJoined 调用.process()时作为 参数传入。 （7）BroadcastProcessFunction 广播连接流处理函数，基于 BroadcastConnectedStream 调用.process()时作为参数传入。这里的“广播连接流”BroadcastConnectedStream，是一个未 keyBy 的普通 DataStream 与一个广播流（BroadcastStream）做连接（conncet）之后的产物。 （8）KeyedBroadcastProcessFunction 按键分区的广播连接流处理函数，同样是基于 BroadcastConnectedStream 调用.process()时作为参数传入。与 BroadcastProcessFunction 不同的是，这时的广播连接流，是一个 KeyedStream与广播流（BroadcastStream）做连接之后的产物。 接下来，我们就对 KeyedProcessFunction 和 ProcessWindowFunction 的具体用法展开详细说明。 按键分区处理函数（KeyedProcessFunction） 在 Flink 程序中，为了实现数据的聚合统计，或者开窗计算之类的功能，我们一般都要先用 keyBy 算子对数据流进行“按键分区”，得到一个 KeyedStream。也就是指定一个键（key），按照它的哈希值（hash code）将数据分成不同的“组”，然后分配到不同的并行子任务上执行计算；这相当于做了一个逻辑分流的操作，从而可以充分利用并行计算的优势实时处理海量数据。 另外我们在上节中也提到，只有在 KeyedStream 中才支持使用 TimerService 设置定时器的操作。所以一般情况下，我们都是先做了 keyBy 分区之后，再去定义处理操作；代码中更加常见的处理函数是 KeyedProcessFunction，最基本的 ProcessFunction 反而出镜率没那么高。 定时器（Timer）和定时服务（TimerService） KeyedProcessFunction 的一个特色，就是可以灵活地使用定时器。 定时器（timers）是处理函数中进行时间相关操作的主要机制。在.onTimer()方法中可以实现定时处理的逻辑，而它能触发的前提，就是之前曾经注册过定时器、并且现在已经到了触发时间。注册定时器的功能，是通过上下文中提供的“定时服务”（TimerService）来实现的。 定时服务与当前运行的环境有关。前面已经介绍过，ProcessFunction 的上下文（Context）中提供了.timerService()方法，可以直接返回一个 TimerService 对象： public abstract TimerService timerService(); //TimerService 是 Flink 关于时间和定时器的基础服务接口，包含以下六个方法： // 获取当前的处理时间 long currentProcessingTime(); // 获取当前的水位线（事件时间） long currentWatermark(); // 注册处理时间定时器，当处理时间超过 time 时触发 void registerProcessingTimeTimer(long time); // 注册事件时间定时器，当水位线超过 time 时触发 void registerEventTimeTimer(long time); // 删除触发时间为 time 的处理时间定时器 void deleteProcessingTimeTimer(long time); // 删除触发时间为 time 的处理时间定时器 void deleteEventTimeTimer(long time); 六个方法可以分成两大类：基于处理时间和基于事件时间。而对应的操作主要有三个：获取当前时间，注册定时器，以及删除定时器。需要注意，尽管处理函数中都可以直接访问TimerService，不过只有基于 KeyedStream 的处理函数，才能去调用注册和删除定时器的方法；未作按键分区的 DataStream 不支持定时器操作，只能获取当前时间。 对于处理时间和事件时间这两种类型的定时器，TimerService 内部会用一个优先队列将它们的时间戳（timestamp）保存起来，排队等待执行。可以认为，定时器其实是 KeyedStream上处理算子的一个状态，它以时间戳作为区分。所以 TimerService 会以键（key）和时间戳为标准，对定时器进行去重；也就是说对于每个 key 和时间戳，最多只有一个定时器，如果注册了多次，onTimer()方法也将只被调用一次。这样一来，我们在代码中就方便了很多，可以肆无忌惮地对一个 key 注册定时器，而不用担心重复定义——因为一个时间戳上的定时器只会触发一次。 基于 KeyedStream 注册定时器时，会传入一个定时器触发的时间戳，这个时间戳的定时器对于每个 key 都是有效的。这样，我们的代码并不需要做额外的处理，底层就可以直接对不同key 进行独立的处理操作了。 利用这个特性，有时我们可以故意降低时间戳的精度，来减少定时器的数量，从而提高处理性能。比如我们可以在设置定时器时只保留整秒数，那么定时器的触发频率就是最多 1 秒一次。 long coalescedTime = time / 1000 * 1000; ctx.timerService().registerProcessingTimeTimer(coalescedTime); 这里注意定时器的时间戳必须是毫秒数，所以我们得到整秒之后还要乘以 1000。定时器默认的区分精度是毫秒。 另外 Flink 对.onTimer()和.processElement()方法是同步调用的（synchronous），所以也不会出现状态的并发修改。 Flink 的定时器同样具有容错性，它和状态一起都会被保存到一致性检查点（checkpoint）中。当发生故障时，Flink 会重启并读取检查点中的状态，恢复定时器。如果是处理时间的定时器，有可能会出现已经“过期”的情况，这时它们会在重启时被立刻触发。 KeyedProcessFunction 的使用 KeyedProcessFunction 可以说是处理函数中的“嫡系部队”，可以认为是 ProcessFunction 的一个扩展。我们只要基于 keyBy 之后的 KeyedStream，直接调用.process()方法，这时需要传入的参数就是 KeyedProcessFunction 的实现类。 stream.keyBy( t -&gt; t.f0 ) .process(new MyKeyedProcessFunction()) 类似地，KeyedProcessFunction 也是继承自 AbstractRichFunction 的一个抽象类，源码中定义如下： public abstract class KeyedProcessFunction&lt;K, I, O&gt; extends AbstractRichFunction { ... public abstract void processElement(I value, Context ctx, Collector&lt;O&gt; out) throws Exception; public void onTimer(long timestamp, OnTimerContext ctx, Collector&lt;O&gt; out) throws Exception {} public abstract class Context {...} ... } 可以看到与 ProcessFunction 的定义几乎完全一样，区别只是在于类型参数多了一个 K，这是当前按键分区的 key 的类型。同样地，我们必须实现一个.processElement()抽象方法，用来处理流中的每一个数据；另外还有一个非抽象方法.onTimer()，用来定义定时器触发时的回调操作。由于定时器只能在 KeyedStream 上使用，所以到了 KeyedProcessFunction 这里，我们才真正对时间有了精细的控制，定时方法.onTimer()才真正派上了用场。 import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.KeyedProcessFunction; import org.apache.flink.util.Collector; import java.sql.Timestamp; public class ProcessingTimeTimerTest { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); // 处理时间语义，不需要分配时间戳和 watermark SingleOutputStreamOperator&lt;Event&gt; stream = env.addSource(new ClickSource()); // 要用定时器，必须基于 KeyedStream stream.keyBy(data -&gt; true) .process(new KeyedProcessFunction&lt;Boolean, Event, String&gt;() { @Override public void processElement(Event value, Context ctx, Collector&lt;String&gt; out) throws Exception { Long currTs = ctx.timerService().currentProcessingTime(); out.collect(&quot;数据到达，到达时间：&quot; + new Timestamp(currTs)); // 注册一个 10 秒后的定时器 ctx.timerService().registerProcessingTimeTimer(currTs + 10 * 1000L); } @Override public void onTimer(long timestamp, OnTimerContext ctx, Collector&lt;String&gt; out) throws Exception { out.collect(&quot;定时器触发，触发时间：&quot; + new Timestamp(timestamp)); } }).print(); env.execute(); } } 在上面的代码中，由于定时器只能在 KeyedStream 上使用，所以先要进行 keyBy；这里的.keyBy(data -&gt; true)是将所有数据的 key 都指定为了 true，其实就是所有数据拥有相同的 key，会分配到同一个分区。 之后我们自定义了一个 KeyedProcessFunction，其中.processElement()方法是每来一个数据都会调用一次，主要是定义了一个 10 秒之后的定时器；而.onTimer()方法则会在定时器触发时调用。所以我们会看到，程序运行后先在控制台输出“数据到达”的信息，等待 10 秒之后，又会输出“定时器触发”的信息，打印出的时间间隔正是 10 秒。 当然，上面的例子是处理时间的定时器，所以我们是真的需要等待 10 秒才会看到结果。事件时间语义下，又会有什么不同呢？我们可以对上面的代码略作修改，做一个测试： import org.apache.flink.api.common.eventtime.SerializableTimestampAssigner; import org.apache.flink.api.common.eventtime.WatermarkStrategy; import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.KeyedProcessFunction; import org.apache.flink.streaming.api.functions.source.SourceFunction; import org.apache.flink.util.Collector; public class EventTimeTimerTest { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); SingleOutputStreamOperator&lt;Event&gt; stream = env.addSource(new CustomSource()) .assignTimestampsAndWatermarks(WatermarkStrategy.&lt;Event&gt;forMonotonousTimestamps() .withTimestampAssigner(new SerializableTimestampAssigner&lt;Event&gt;() { @Override public long extractTimestamp(Event element, long recordTimestamp) { return element.timestamp; } })); // 基于 KeyedStream 定义事件时间定时器 stream.keyBy(data -&gt; true) .process(new KeyedProcessFunction&lt;Boolean, Event, String&gt;() { @Override public void processElement(Event value, Context ctx, Collector&lt;String&gt; out) throws Exception { out.collect(&quot;数据到达，时间戳为：&quot; + ctx.timestamp()); out.collect(&quot; 数据到达，水位线为： &quot; + ctx.timerService().currentWatermark() + &quot;\\n -------分割线-------&quot;); // 注册一个 10 秒后的定时器 ctx.timerService().registerEventTimeTimer(ctx.timestamp() + 10 * 1000L); } @Override public void onTimer(long timestamp, OnTimerContext ctx, Collector&lt;String&gt; out) throws Exception { out.collect(&quot;定时器触发，触发时间：&quot; + timestamp); } }) .print(); env.execute(); } // 自定义测试数据源 public static class CustomSource implements SourceFunction&lt;Event&gt; { @Override public void run(SourceContext&lt;Event&gt; ctx) throws Exception { // 直接发出测试数据 ctx.collect(new Event(&quot;Mary&quot;, &quot;./home&quot;, 1000L)); // 为了更加明显，中间停顿 5 秒钟 Thread.sleep(5000L); // 发出 10 秒后的数据 ctx.collect(new Event(&quot;Mary&quot;, &quot;./home&quot;, 11000L)); Thread.sleep(5000L); // 发出 10 秒+1ms 后的数据 ctx.collect(new Event(&quot;Alice&quot;, &quot;./cart&quot;, 11001L)); Thread.sleep(5000L); } @Override public void cancel() { } } } 由于是事件时间语义，所以我们必须从数据中提取出数据产生的时间戳。这里为了更清楚地看到程序行为，我们自定义了一个数据源，发出三条测试数据，时间戳分别为 1000、11000和 11001，并且发出数据后都会停顿 5 秒。 在代码中，我们依然将所有数据分到同一分区，然后在自定义的 KeyedProcessFunction 中使用定时器。同样地，每来一条数据，我们就将当前的数据时间戳和水位线信息输出，并注册一个 10 秒后（以当前数据时间戳为基准）的事件时间定时器。执行程序结果如下： 数据到达，时间戳为：1000 数据到达，水位线为：-9223372036854775808 -------分割线------- 数据到达，时间戳为：11000 数据到达，水位线为：999 -------分割线------- 数据到达，时间戳为：11001 数据到达，水位线为：10999 -------分割线------- 定时器触发，触发时间：11000 定时器触发，触发时间：21000 定时器触发，触发时间：21001 每来一条数据，都会输出两行“数据到达”的信息，并以分割线隔开；两条数据到达的时间间隔为 5 秒。当第三条数据到达后，随后立即输出一条定时器触发的信息；再过 5 秒之后，剩余两条定时器信息输出，程序运行结束。 我们可以发现，数据到来之后，当前的水位线与时间戳并不是一致的。当第一条数据到来，时间戳为 1000，可水位线的生成是周期性的（默认 200ms 一次），不会立即发生改变，所以依然是最小值 Long.MIN_VALUE；随后只要到了水位线生成的时间点（200ms 到了），就会依据当前的最大时间戳 1000 来生成水位线了。这里我们没有设置水位线延迟，默认需要减去 1 毫秒，所以水位线推进到了 999。而当时间戳为 11000 的第二条数据到来之后，水位线同样没有立即改变，仍然是 999，就好像总是“滞后”数据一样。 这样程序的行为就可以得到合理解释了。事件时间语义下，定时器触发的条件就是水位线推进到设定的时间。第一条数据到来后，设定的定时器时间为 1000 + 10 * 1000 = 11000；而当时间戳为 11000 的第二条数据到来，水位线还处在 999 的位置，当然不会立即触发定时器；而之后水位线会推进到 10999，同样是无法触发定时器的。必须等到第三条数据到来，将水位线真正推进到 11000，就可以触发第一个定时器了。第三条数据发出后再过 5 秒，没有更多的数据生成了，整个程序运行结束将要退出，此时 Flink 会自动将水位线推进到长整型的最大值（Long.MAX_VALUE）。于是所有尚未触发的定时器这时就统一触发了，我们就在控制台看到了后两个定时器的触发信息。 窗口处理函数 除 了 KeyedProcessFunction ， 另 外 一 大 类 常 用 的 处 理 函 数 ， 就 是 基 于 窗 口 的ProcessWindowFunction 和 ProcessAllWindowFunction 了。 窗口处理函数的使用 进行窗口计算，我们可以直接调用现成的简单聚合方法（sum/max/min）,也可以通过调用.reduce()或.aggregate()来自定义一般的增量聚合函数（ReduceFunction/AggregateFucntion）；而对于更加复杂、需要窗口信息和额外状态的一些场景，我们还可以直接使用全窗口函数、把数据全部收集保存在窗口内，等到触发窗口计算时再统一处理。窗口处理函数就是一种典型的全窗口函数。 窗 口 处 理 函 数 ProcessWindowFunction 的 使 用 与 其 他 窗 口 函 数 类 似 ， 也 是 基 于WindowedStream 直接调用方法就可以，只不过这时调用的是.process()。 stream.keyBy( t -&gt; t.f0 ) .window( TumblingEventTimeWindows.of(Time.seconds(10)) ) .process(new MyProcessWindowFunction()) ProcessWindowFunction 解析 ProcessWindowFunction 既是处理函数又是全窗口函数。从名字上也可以推测出，它的本质似乎更倾向于“窗口函数”一些。事实上它的用法也确实跟其他处理函数有很大不同。我们可以从源码中的定义看到这一点： public abstract class ProcessWindowFunction&lt;IN, OUT, KEY, W extends Window&gt; extends AbstractRichFunction { ... public abstract void process( KEY key, Context context, Iterable&lt;IN&gt; elements, Collector&lt;OUT&gt; out) throws Exception; public void clear(Context context) throws Exception {} public abstract class Context implements java.io.Serializable {...} } ProcessWindowFunction 依然是一个继承了 AbstractRichFunction 的抽象类，它有四个类型参数： IN：input，数据流中窗口任务的输入数据类型。 OUT：output，窗口任务进行计算之后的输出数据类型。 KEY：数据中键 key 的类型。 W：窗口的类型，是 Window 的子类型。一般情况下我们定义时间窗口，W就是 TimeWindow。 而内部定义的方法，跟我们之前熟悉的处理函数就有所区别了。因为全窗口函数不是逐个处理元素的，所以处理数据的方法在这里并不是.processElement()，而是改成了.process()。方法包含四个参数。 key：窗口做统计计算基于的键，也就是之前 keyBy 用来分区的字段。 context：当前窗口进行计算的上下文，它的类型就是 ProcessWindowFunction内部定义的抽象类 Context。 elements：窗口收集到用来计算的所有数据，这是一个可迭代的集合类型。 out：用来发送数据输出计算结果的收集器，类型为 Collector。 可以明显看出，这里的参数不再是一个输入数据，而是窗口中所有数据的集合。而上下文context 所包含的内容也跟其他处理函数有所差别： public abstract class Context implements java.io.Serializable { public abstract W window(); public abstract long currentProcessingTime(); public abstract long currentWatermark(); public abstract KeyedStateStore windowState(); public abstract KeyedStateStore globalState(); public abstract &lt;X&gt; void output(OutputTag&lt;X&gt; outputTag, X value); } 除了可以通过.output()方法定义侧输出流不变外，其他部分都有所变化。这里不再持有TimerService 对象，只能通过 currentProcessingTime()和 currentWatermark()来获取当前时间，所以失去了设置定时器的功能；另外由于当前不是只处理一个数据，所以也不再提供.timestamp()方法。与此同时，也增加了一些获取其他信息的方法：比如可以通过.window()直接获取到当前的窗口对象，也可以通过.windowState()和.globalState()获取到当前自定义的窗口状态和全局状态。注意这里的“窗口状态”是自定义的，不包括窗口本身已经有的状态，针对当前 key、当前窗口有效；而“全局状态”同样是自定义的状态，针对当前 key 的所有窗口有效。 所以我们会发现，ProcessWindowFunction 中除了.process()方法外，并没有.onTimer()方法，而是多出了一个.clear()方法。从名字就可以看出，这主要是方便我们进行窗口的清理工作。如果我们自定义了窗口状态，那么必须在.clear()方法中进行显式地清除，避免内存溢出。 这里有一个问题：没有了定时器，那窗口处理函数就失去了一个最给力的武器，如果我们希望有一些定时操作又该怎么做呢？其实仔细思考会发现，对于窗口而言，它本身的定义就包含了一个触发计算的时间点，其实一般情况下是没有必要再去做定时操作的。如果非要这么干，Flink也提供了另外的途径——使用窗口触发器（Trigger）。在触发器中也有一个TriggerContext，它可以起到类似 TimerService 的作用：获取当前时间、注册和删除定时器，另外还可以获取当前的状态。这样设计无疑会让处理流程更加清晰——定时操作也是一种“触发”，所以我们就让所有的触发操作归触发器管，而所有处理数据的操作则归窗口函数管。 至于另一种窗口处理函数 ProcessAllWindowFunction，它的用法非常类似。区别在于它基于的是 AllWindowedStream，相当于对没有 keyBy 的数据流直接开窗并调用.process()方法: stream.windowAll( TumblingEventTimeWindows.of(Time.seconds(10)) ) .process(new MyProcessAllWindowFunction()) 应用案例——Top N 窗口的计算处理，在实际应用中非常常见。对于一些比较复杂的需求，如果增量聚合函数无法满足，我们就需要考虑使用窗口处理函数这样的“大招”了。 网站中一个非常经典的例子，就是实时统计一段时间内的热门 url。例如，需要统计最近10 秒钟内最热门的两个 url 链接，并且每 5 秒钟更新一次。我们知道，这可以用一个滑动窗口来实现，而“热门度”一般可以直接用访问量来表示。于是就需要开滑动窗口收集 url 的访问数据，按照不同的 url 进行统计，而后汇总排序并最终输出前两名。这其实就是著名的“Top N”问题。 很显然，简单的增量聚合可以得到 url 链接的访问量，但是后续的排序输出 Top N 就很难实现了。所以接下来我们用窗口处理函数进行实现。 使用 ProcessAllWindowFunction 一种最简单的想法是，我们干脆不区分 url 链接，而是将所有访问数据都收集起来，统一进行统计计算。所以可以不做 keyBy，直接基于 DataStream 开窗，然后使用全窗口函数ProcessAllWindowFunction 来进行处理。 在窗口中可以用一个 HashMap 来保存每个 url 的访问次数，只要遍历窗口中的所有数据，自然就能得到所有 url 的热门度。最后把 HashMap 转成一个列表 ArrayList，然后进行排序、取出前两名输出就可以了。 import org.apache.flink.api.common.eventtime.SerializableTimestampAssigner; import org.apache.flink.api.common.eventtime.WatermarkStrategy; import org.apache.flink.api.common.functions.MapFunction; import org.apache.flink.api.java.tuple.Tuple2; import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.windowing.ProcessAllWindowFunction; import org.apache.flink.streaming.api.windowing.assigners.SlidingEventTimeWindows; import org.apache.flink.streaming.api.windowing.time.Time; import org.apache.flink.streaming.api.windowing.windows.TimeWindow; import org.apache.flink.util.Collector; import java.sql.Timestamp; import java.util.ArrayList; import java.util.Comparator; import java.util.HashMap; public class ProcessAllWindowTopN { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); SingleOutputStreamOperator&lt;Event&gt; eventStream = env.addSource(new ClickSource()) .assignTimestampsAndWatermarks(WatermarkStrategy.&lt;Event&gt;forMonotonousTimestamps() .withTimestampAssigner(new SerializableTimestampAssigner&lt;Event&gt;() { @Override public long extractTimestamp(Event element, long recordTimestamp) { return element.timestamp; } }) ); // 只需要 url 就可以统计数量，所以转换成 String 直接开窗统计 SingleOutputStreamOperator&lt;String&gt; result = eventStream .map(new MapFunction&lt;Event, String&gt;() { @Override public String map(Event value) throws Exception { return value.url; } }) .windowAll(SlidingEventTimeWindows.of(Time.seconds(10), Time.seconds(5))) // 开滑动窗口 .process(new ProcessAllWindowFunction&lt;String, String, TimeWindow&gt;() { @Override public void process(Context context, Iterable&lt;String&gt; elements, Collector&lt;String&gt; out) throws Exception { HashMap&lt;String, Long&gt; urlCountMap = new HashMap&lt;&gt;(); // 遍历窗口中数据，将浏览量保存到一个 HashMap 中 for (String url : elements) { if (urlCountMap.containsKey(url)) { long count = urlCountMap.get(url); urlCountMap.put(url, count + 1L); } else { urlCountMap.put(url, 1L); } } ArrayList&lt;Tuple2&lt;String, Long&gt;&gt; mapList = new ArrayList&lt;Tuple2&lt;String, Long&gt;&gt;(); // 将浏览量数据放入 ArrayList，进行排序 for (String key : urlCountMap.keySet()) { mapList.add(Tuple2.of(key, urlCountMap.get(key))); } mapList.sort(new Comparator&lt;Tuple2&lt;String, Long&gt;&gt;() { @Override public int compare(Tuple2&lt;String, Long&gt; o1, Tuple2&lt;String, Long&gt; o2) { return o2.f1.intValue() - o1.f1.intValue(); } }); // 取排序后的前两名，构建输出结果 StringBuilder result = new StringBuilder(); result.append(&quot;========================================\\n&quot;); for (int i = 0; i &lt; 2; i++) { Tuple2&lt;String, Long&gt; temp = mapList.get(i); String info = &quot;浏览量 No.&quot; + (i + 1) + &quot; url：&quot; + temp.f0 + &quot; 浏览量：&quot; + temp.f1 + &quot; 窗 口 结 束 时 间 ： &quot; + new Timestamp(context.window().getEnd()) + &quot;\\n&quot;; result.append(info); } result.append(&quot;========================================\\n&quot;); out.collect(result.toString()); } }); result.print(); env.execute(); } } 运行结果如下所示： ======================================== 浏览量 No.1 url：./prod?id=1 浏览量：2 窗口结束时间：2021-07-01 15:24:25.0 浏览量 No.2 url：./cart 浏览量：1 窗口结束时间：2021-07-01 15:24:25.0 ======================================== 使用 KeyedProcessFunction 在上一小节的实现过程中，我们没有进行按键分区，直接将所有数据放在一个分区上进行了开窗操作。这相当于将并行度强行设置为 1，在实际应用中是要尽量避免的，所以 Flink 官方也并不推荐使用 AllWindowedStream 进行处理。另外，我们在全窗口函数中定义了 HashMap 来统计 url 链接的浏览量，计算过程是要先收集齐所有数据、然后再逐一遍历更新 HashMap，这显然不够高效。如果我们可以利用增量聚合函数的特性，每来一条数据就更新一次对应 url的浏览量，那么到窗口触发计算时只需要做排序输出就可以了。 基于这样的想法，我们可以从两个方面去做优化：一是对数据进行按键分区，分别统计浏览量；二是进行增量聚合，得到结果最后再做排序输出。所以，我们可以使用增量聚合函数AggregateFunction 进行浏览量的统计，然后结合 ProcessWindowFunction 排序输出来实现 Top N的需求。 具体实现思路就是，先按照 url 对数据进行 keyBy 分区，然后开窗进行增量聚合。这里就会发现一个问题：我们进行按键分区之后，窗口的计算就会只针对当前 key 有效了；也就是说，每个窗口的统计结果中，只会有一个 url 的浏览量，这是无法直接用 ProcessWindowFunction进行排序的。所以我们只能分成两步：先对每个 url 链接统计出浏览量，然后再将统计结果收集起来，排序输出最终结果。因为最后的排序还是基于每个时间窗口的，所以为了让输出的统计结果中包含窗口信息，我们可以借用第六章中定义的 POJO 类 UrlViewCount 来表示，它包含了 url、浏览量（count）以及窗口的起始结束时间。之后对 UrlViewCount 的处理，可以先按窗口分区，然后用 KeyedProcessFunction 来实现。 总结处理流程如下： 读取数据源； 筛选浏览行为（pv）； 提取时间戳并生成水位线； 按照 url 进行 keyBy 分区操作； 开长度为 1 小时、步长为 5 分钟的事件时间滑动窗口； 使用增量聚合函数 AggregateFunction，并结合全窗口函数 WindowFunction 进行窗口聚合，得到每个 url、在每个统计窗口内的浏览量，包装成 UrlViewCount； 按照窗口进行 keyBy 分区操作； 对同一窗口的统计结果数据，使用 KeyedProcessFunction 进行收集并排序输出。 糟糕的是，这里又会带来另一个问题。最后我们用 KeyedProcessFunction 来收集数据做排序，这时面对的就是窗口聚合之后的数据流，而窗口已经不存在了；那到底什么时候会收集齐所有数据呢？这问题听起来似乎有些没道理。我们统计浏览量的窗口已经关闭，就说明了当前已经到了要输出结果的时候，直接输出不就行了吗？ 没有这么简单。因为数据流中的元素是逐个到来的，所以即使理论上我们应该“同时”收到很多 url 的浏览量统计结果，实际也是有先后的、只能一条一条处理。下游任务（就是我们定义的 KeyedProcessFunction）看到一个 url 的统计结果，并不能保证这个时间段的统计数据不会再来了，所以也不能贸然进行排序输出。解决的办法，自然就是要等所有数据到齐了——这很容易让我们联想起水位线设置延迟时间的方法。这里我们也可以“多等一会儿”，等到水位线真正超过了窗口结束时间，要统计的数据就肯定到齐了。 具体实现上，可以采用一个延迟触发的事件时间定时器。基于窗口的结束时间来设定延迟，其实并不需要等太久——因为我们是靠水位线的推进来触发定时器，而水位线的含义就是“之前的数据都到齐了”。所以我们只需要设置 1 毫秒的延迟，就一定可以保证这一点。 而在等待过程中，之前已经到达的数据应该缓存起来，我们这里用一个自定义的“列表状态”（ListState）来进行存储，如图所示。这个状态需要使用富函数类的 getRuntimeContext()方法获取运行时上下文来定义，我们一般把它放在 open()生命周期方法中。之后每来一个UrlViewCount，就把它添加到当前的列表状态中，并注册一个触发时间为窗口结束时间加 1毫秒（windowEnd + 1）的定时器。待到水位线到达这个时间，定时器触发，我们可以保证当前窗口所有 url 的统计结果 UrlViewCount 都到齐了；于是从状态中取出进行排序输出。 import org.apache.flink.api.common.eventtime.SerializableTimestampAssigner; import org.apache.flink.api.common.eventtime.WatermarkStrategy; import org.apache.flink.api.common.functions.AggregateFunction; import org.apache.flink.api.common.state.ListState; import org.apache.flink.api.common.state.ListStateDescriptor; import org.apache.flink.api.common.typeinfo.Types; import org.apache.flink.configuration.Configuration; import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.KeyedProcessFunction; import org.apache.flink.streaming.api.functions.windowing.ProcessWindowFunction; import org.apache.flink.streaming.api.windowing.assigners.SlidingEventTimeWindows; import org.apache.flink.streaming.api.windowing.time.Time; import org.apache.flink.streaming.api.windowing.windows.TimeWindow; import org.apache.flink.util.Collector; import java.sql.Timestamp; import java.util.ArrayList; import java.util.Comparator; public class KeyedProcessTopN { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); // 从自定义数据源读取数据 SingleOutputStreamOperator&lt;Event&gt; eventStream = env.addSource(new ClickSource()) .assignTimestampsAndWatermarks(WatermarkStrategy.&lt;Event&gt;forMonotonousTimestamps() .withTimestampAssigner(new SerializableTimestampAssigner&lt;Event&gt;() { @Override public long extractTimestamp(Event element, long recordTimestamp) { return element.timestamp; } })); // 需要按照 url 分组，求出每个 url 的访问量 SingleOutputStreamOperator&lt;UrlViewCount&gt; urlCountStream = eventStream.keyBy(data -&gt; data.url) .window(SlidingEventTimeWindows.of(Time.seconds(10), Time.seconds(5))) .aggregate(new UrlViewCountAgg(), new UrlViewCountResult()); // 对结果中同一个窗口的统计数据，进行排序处理 SingleOutputStreamOperator&lt;String&gt; result = urlCountStream.keyBy(data -&gt; data.windowEnd) .process(new TopN(2)); result.print(&quot;result&quot;); env.execute(); } // 自定义增量聚合 public static class UrlViewCountAgg implements AggregateFunction&lt;Event, Long, Long&gt; { @Override public Long createAccumulator() { return 0L; } @Override public Long add(Event value, Long accumulator) { return accumulator + 1; } @Override public Long getResult(Long accumulator) { return accumulator; } @Override public Long merge(Long a, Long b) { return null; } } // 自定义全窗口函数，只需要包装窗口信息 public static class UrlViewCountResult extends ProcessWindowFunction&lt;Long, UrlViewCount, String, TimeWindow&gt; { @Override public void process(String url, Context context, Iterable&lt;Long&gt; elements, Collector&lt;UrlViewCount&gt; out) throws Exception { // 结合窗口信息，包装输出内容 Long start = context.window().getStart(); Long end = context.window().getEnd(); out.collect(new UrlViewCount(url, elements.iterator().next(), start, end)); } } // 自定义处理函数，排序取 top n public static class TopN extends KeyedProcessFunction&lt;Long, UrlViewCount, String&gt; { // 将 n 作为属性 private Integer n; // 定义一个列表状态 private ListState&lt;UrlViewCount&gt; urlViewCountListState; public TopN(Integer n) { this.n = n; } @Override public void open(Configuration parameters) throws Exception { // 从环境中获取列表状态句柄 urlViewCountListState = getRuntimeContext().getListState( new ListStateDescriptor&lt;UrlViewCount&gt;(&quot;url-view-count-list&quot;, Types.POJO(UrlViewCount.class))); } @Override public void processElement(UrlViewCount value, Context ctx, Collector&lt;String&gt; out) throws Exception { // 将 count 数据添加到列表状态中，保存起来 urlViewCountListState.add(value); // 注册 window end + 1ms 后的定时器，等待所有数据到齐开始排序 ctx.timerService().registerEventTimeTimer(ctx.getCurrentKey() + 1); } @Override public void onTimer(long timestamp, OnTimerContext ctx, Collector&lt;String&gt; out) throws Exception { // 将数据从列表状态变量中取出，放入 ArrayList，方便排序 ArrayList&lt;UrlViewCount&gt; urlViewCountArrayList = new ArrayList&lt;&gt;(); for (UrlViewCount urlViewCount : urlViewCountListState.get()) { urlViewCountArrayList.add(urlViewCount); } // 清空状态，释放资源 urlViewCountListState.clear(); // 排序 urlViewCountArrayList.sort(new Comparator&lt;UrlViewCount&gt;() { @Override public int compare(UrlViewCount o1, UrlViewCount o2) { return o2.count.intValue() - o1.count.intValue(); } }); // 取前两名，构建输出结果 StringBuilder result = new StringBuilder(); result.append(&quot;========================================\\n&quot;); result.append(&quot;窗口结束时间：&quot; + new Timestamp(timestamp - 1) + &quot;\\n&quot;); for (int i = 0; i &lt; this.n; i++) { UrlViewCount UrlViewCount = urlViewCountArrayList.get(i); String info = &quot;No.&quot; + (i + 1) + &quot; &quot; + &quot;url：&quot; + UrlViewCount.url + &quot; &quot; + &quot;浏览量：&quot; + UrlViewCount.count + &quot;\\n&quot;; result.append(info); } result.append(&quot;========================================\\n&quot;); out.collect(result.toString()); } } } 代码中，我们还利用了定时器的特性：针对同一 key、同一时间戳会进行去重。所以对于同一个窗口而言，我们接到统计结果数据后设定的 windowEnd + 1 的定时器都是一样的，最终只会触发一次计算。而对于不同的 key（这里 key 是 windowEnd），定时器和状态都是独立的，所以我们也不用担心不同窗口间数据的干扰。 我们在上面的代码中使用了后面要讲解的 ListState。这里可以先简单说明一下。我们先声明一个列表状态变量: private ListState&lt;Event&gt; UrlViewCountListState; 然后在 open 方法中初始化了列表状态变量，我们初始化的时候使用了 ListStateDescriptor描述符，这个描述符用来告诉 Flink 列表状态变量的名字和类型。列表状态变量是单例，也就是说只会被实例化一次。这个列表状态变量的作用域是当前 key 所对应的逻辑分区。我们使用add 方法向列表状态变量中添加数据，使用 get 方法读取列表状态变量中的所有元素。 侧输出流（Side Output） 处理函数还有另外一个特有功能，就是将自定义的数据放入“侧输出流”（side output）输出。这个概念我们并不陌生，之前在讲到窗口处理迟到数据时，最后一招就是输出到侧输出流。而这种处理方式的本质，其实就是处理函数的侧输出流功能。 我们之前讲到的绝大多数转换算子，输出的都是单一流，流里的数据类型只能有一种。而侧输出流可以认为是“主流”上分叉出的“支流”，所以可以由一条流产生出多条流，而且这些流中的数据类型还可以不一样。利用这个功能可以很容易地实现“分流”操作。 具体应用时，只要在处理函数的.processElement()或者.onTimer()方法中，调用上下文的.output()方法就可以了。 DataStream&lt;Integer&gt; stream = env.addSource(...); SingleOutputStreamOperator&lt;Long&gt; longStream = stream.process(new ProcessFunction&lt;Integer, Long&gt;() { @Override public void processElement( Integer value, Context ctx, Collector&lt;Integer&gt; out) throws Exception { // 转换成 Long，输出到主流中 out.collect(Long.valueOf(value)); // 转换成 String，输出到侧输出流中 ctx.output(outputTag, &quot;side-output: &quot; + String.valueOf(value)); } }); 这里 output()方法需要传入两个参数，第一个是一个“输出标签”OutputTag，用来标识侧输出流，一般会在外部统一声明；第二个就是要输出的数据。 我们可以在外部先将 OutputTag 声明出来： OutputTag&lt;String&gt; outputTag = new OutputTag&lt;String&gt;(&quot;side-output&quot;) {}; 如果想要获取这个侧输出流，可以基于处理之后的 DataStream 直接调用.getSideOutput()方法，传入对应的 OutputTag，这个方式与窗口 API 中获取侧输出流是完全一样的。 DataStream&lt;String&gt; stringStream = longStream.getSideOutput(outputTag); ","link":"https://tianxiawuhao.github.io/cctea1w9G/"},{"title":"第七章 flink流合并","content":"多流转换 无论是基本的简单转换和聚合， 还是基于窗口的计算，都是针对一条流上的数据进行 处理的。在实际应用中， 可能需要将不同来源的数据连接合并在一起处理， 也有可能需要将 一条流拆分开， 所以经常会有对多条流进行处理的场景。简单划分，多流转换可以分为“分流”和“合流”两大类。目前分流的操作一般是通 过侧输出流(side output) 来实现，而合流的算子比较丰富，根据不同的需求可以调用 union、 connect、join 以及 coGroup 等接口进行连接合并操作。 侧输出流 简单来说，只需要调用上下文 ctx的output()方法，就可以输出任意类型的数据了。而侧输出流的标记和提取， 都离不开一个“输出标签”(OutputTag)，指定了侧输出流的 id 和类型。 代码示例： package com.company.flink.demo; import com.company.flink.data.ClickSource; import com.company.flink.entity.Event; import org.apache.flink.api.java.tuple.Tuple3; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.ProcessFunction; import org.apache.flink.util.Collector; import org.apache.flink.util.OutputTag; public class SplitStreamDemo { // 定义侧输出流 private static OutputTag&lt;Tuple3&lt;String, String, Long&gt;&gt; zhangsanTag = new OutputTag&lt;Tuple3&lt;String, String, Long&gt;&gt;(&quot;zhangsan-pv&quot;){}; private static OutputTag&lt;Tuple3&lt;String, String, Long&gt;&gt; lisiTag = new OutputTag&lt;Tuple3&lt;String, String, Long&gt;&gt;(&quot;lisi-pv&quot;){}; public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); SingleOutputStreamOperator&lt;Event&gt; stream = env .addSource(new ClickSource()); SingleOutputStreamOperator&lt;Event&gt; outPutStream = stream.process( new ProcessFunction&lt;Event, Event&gt;() { @Override public void processElement(Event value, Context ctx, Collector&lt;Event&gt; out) { if (value.user.equals(&quot;zhangsan&quot;)) { ctx.output(zhangsanTag, new Tuple3&lt;&gt;(value.user, value.url, value.timestamp)); } else if (value.user.equals(&quot;lisi&quot;)) { ctx.output(lisiTag, new Tuple3&lt;&gt;(value.user, value.url, value.timestamp)); } else { out.collect(value); } } }); DataStream&lt;Tuple3&lt;String, String, Long&gt;&gt; zhangsanSideOutput = outPutStream.getSideOutput(zhangsanTag); zhangsanSideOutput.print(&quot;zhangsan pv&quot;); DataStream&lt;Tuple3&lt;String, String, Long&gt;&gt; lisiSideOutput = outPutStream.getSideOutput(lisiTag); lisiSideOutput.print(&quot;lisi pv&quot;); outPutStream.print(&quot;else&quot;); env.execute(); } } 合流操作 Flink 中合流的操作会更加普遍，对应的 API 也更加丰富。 1）联合(Union) ​ 最简单的合流操作， 就是直接将多条流合在一起，叫作流“联合”(union)。联合操作要求必须流中的数据类型必须相同，合并之后的新流会包括所有流中的元素， 数据类型不变。 2）连接（Connect） ​ 流的联合虽然简单，不过受限于数据类型不能改变，灵活性不足，实践中较少使用。除了联合（union），Flink 还提供了另外一种合流操作就是连接（connect）。这种操作就是直接把两条流像接线一样对接起来。 ​ 为了处理更加灵活，连接操作允许流的数据类型不同。但一个 DataStream 中的数据类型是唯一的（所以需要（co-process）转换操作） 基于时间的合流——双流联结(Join) 对于两条流的合并，很多情况并不是简单地将所有数据放在一起，而是希望根据某个字段的值将它们联结起来“配对”做处理。 这种需求与关系型数据库中表的join 操作非常相似。Flink 中两条流 的 connect 操作，就可以通过 keyBy 指定键进行分组后合并，实现了类似于 SQL 中的 join 操作； 另外 connect 支持处理函数，可以使用自定义状态和 TimerService 灵活实现各种需求。 1）窗口联结(Window Join) ​ 窗口联结首先需要调用 DataStream 的.join()方法来合并两条流， 得到一 个 、JoinedStreams ；接着通过 .where() 和.equalTo() 方法指定两条流中联结的 key ；然后通 过.window()开窗口， 并调用.apply()传入联结窗口函数进行处理计算。调用形式如下： SingleOutputStreamOperator&lt;Event&gt; eventStream = env.addSource(new ClickSource()) .join(DataStream otherStream) .where(KeySelector keySelector) .equalTo(KeySelector keySelector) .window(WindowAssigner assigner) .apply(JoinFunction function) 上面代码中.where()的参数是键选择器(KeySelector)， 用来指定第一条流中的 key ；而.equalTo()传入的 KeySelector 则指定了第二条流中的 key。两者相同的元素，如果在同一窗口中， 就可以匹配起来， 并通过一个“联结函数”(JoinFunction) 进行处理。这里需要注意， JoinFunciton 并不是真正的“窗口函数”，它只是定义了窗口函数在调用时 对匹配数据的具体处理逻辑。 两条流的数据到来之后， 首先会按照 key 分组、进入对应的窗口中存储；当到达窗口结束时间时，算子会先统计出窗口内两条流的数据的所有组合，也就是对两条流中的数据做一个笛卡尔积(相当于表的交叉连接， cross join)，然后进行遍历，把每一对匹配的数据， 作为参数 (first ，second)传入 JoinFunction 的.join()方法进行计算处理。所以窗口中每有一对数据成功联结匹配， JoinFunction 的.join()方法就会被调用一次， 并输 出一个结果。 窗口join的调用和SQL中表的join非常相似。SQL的 inner join ... on本身表示的是两张表基于 id 的“内连接”(inner join)。而 Flink 中的 window join，同样类似于 inner join。也就是说，最后 处理输出的，只有两条流中数据按 key 配对成功的那些；如果某个窗口中一条流的数据没有任 何另一条流的数据匹配， 那么就不会调用 JoinFunction 的.join()方法， 也就没有任何输出。 代码示列： package com.company.flink.demo; import org.apache.flink.api.common.eventtime.SerializableTimestampAssigner; import org.apache.flink.api.common.eventtime.WatermarkStrategy; import org.apache.flink.api.common.functions.JoinFunction; import org.apache.flink.api.java.tuple.Tuple2; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows; import org.apache.flink.streaming.api.windowing.time.Time; public class FlinkWindowJoinDemo { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); DataStream&lt;Tuple2&lt;String, Long&gt;&gt; stream1 = env .fromElements( Tuple2.of(&quot;a&quot;, 1000L), Tuple2.of(&quot;a&quot;, 2000L), Tuple2.of(&quot;b&quot;, 1000L), Tuple2.of(&quot;b&quot;, 2000L), Tuple2.of(&quot;c&quot;, 5000L) ) .assignTimestampsAndWatermarks( WatermarkStrategy .&lt;Tuple2&lt;String, Long&gt;&gt;forMonotonousTimestamps().withTimestampAssigner( (SerializableTimestampAssigner&lt;Tuple2&lt;String, Long&gt;&gt;) (stringLongTuple2, l) -&gt; stringLongTuple2.f1 ) ); DataStream&lt;Tuple2&lt;String, Long&gt;&gt; stream2 = env .fromElements( Tuple2.of(&quot;a&quot;, 3000L), Tuple2.of(&quot;a&quot;, 4000L), Tuple2.of(&quot;b&quot;, 3000L), Tuple2.of(&quot;b&quot;, 4000L) ) .assignTimestampsAndWatermarks( WatermarkStrategy .&lt;Tuple2&lt;String, Long&gt;&gt;forMonotonousTimestamps().withTimestampAssigner( (SerializableTimestampAssigner&lt;Tuple2&lt;String, Long&gt;&gt;) (stringLongTuple2, l) -&gt; stringLongTuple2.f1 ) ); stream1 .join(stream2) .where(r -&gt; r.f0) .equalTo(r -&gt; r.f0) .window(TumblingEventTimeWindows.of(Time.seconds(5))) .apply((JoinFunction&lt;Tuple2&lt;String, Long&gt;, Tuple2&lt;String, Long&gt;, String&gt;) (left, right) -&gt; left + &quot;=&gt;&quot; + right) .print(); env.execute(); } } 2）间隔联结(Interval Join) 在一些场景下， 处理的时间间隔可能并不是固定的。比如， 在交易系统中， 需要实时地对每一笔交易进行核验，保证两个账户转入转出数额相等，也就是所谓的“实时对账”。 两次转账的数据可能写入了不同的日志流，它们的时间戳相差不大，所以可以考虑只统计一段时间内是否有出账入账的数据匹配。这时显然不能用滚动窗口或滑动窗口来处理因为匹配的两个数据有可能刚好在窗口边缘两侧，这时窗口内就都没有匹配了；会话窗口虽然时间不固定，但也明显不适合这个场景。Flink 为这种场景提供了一种叫作“间隔联结”(interval join) 的合流操作，间隔联结的思路就是针对一条流的每条数据，开辟出其时间戳前后的一段时间间隔， 看这期间是否有来自另一条流的数据与之匹配。 间隔连接原理： 给定两个时间点，分别叫作间隔的“上界”(upperBound) 和“下界”(lowerBound)；对于一条流A 中的任意一个数据元素 a，就可以 开辟一段时间间隔： [a.timestamp + lowerBound, a.timestamp + upperBound）把这段时间作为可以匹配另一条流数据 的“窗口”范围。所以对于另一条流 B 中的数据元素 b，如果它的时间戳落在了这 个区间范围内， a 和 b 就可以成功配对，进而进行计算输出结果。所以匹配的条件为：a.timestamp + lowerBound &lt;= b.timestamp &lt;= a.timestamp + upperBound 这里需要注意， 做间隔联结的两条流 A 和 B，也必须基于相同的 key； 间隔联结目前只支持事件时间语义。如图所示： 可以看到，间隔联结同样是一种内连接(inner join)。与窗口联结不同的是，interval join 做匹配的时间段是基于流中数据的并不确定； 而且流 B 中的数据可以不只在一个区间内被匹配。 代码示列： package com.company.flink.demo; import com.company.flink.entity.Event; import org.apache.flink.api.common.eventtime.SerializableTimestampAssigner; import org.apache.flink.api.common.eventtime.WatermarkStrategy; import org.apache.flink.api.java.tuple.Tuple3; import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.co.ProcessJoinFunction; import org.apache.flink.streaming.api.windowing.time.Time; import org.apache.flink.util.Collector; // 基于间隔的 join public class FlinkIntervalJoinDemo { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); SingleOutputStreamOperator&lt;Tuple3&lt;String, String, Long&gt;&gt; orderStream = env.fromElements( Tuple3.of(&quot;Mary&quot;, &quot;order-1&quot;, 5000L), Tuple3.of(&quot;Alice&quot;, &quot;order-2&quot;, 5000L), Tuple3.of(&quot;Bob&quot;, &quot;order-3&quot;, 20000L), Tuple3.of(&quot;Alice&quot;, &quot;order-4&quot;, 20000L), Tuple3.of(&quot;Cary&quot;, &quot;order-5&quot;, 51000L) ).assignTimestampsAndWatermarks(WatermarkStrategy.&lt;Tuple3&lt;String, String, Long&gt;&gt;forMonotonousTimestamps() .withTimestampAssigner((SerializableTimestampAssigner&lt;Tuple3&lt;String, String, Long&gt;&gt;) (element, recordTimestamp) -&gt; element.f2) ); SingleOutputStreamOperator&lt;Event&gt; clickStream = env.fromElements( new Event(&quot;Bob&quot;, &quot;./cart&quot;, 2000L), new Event(&quot;Alice&quot;, &quot;./prod?id=100&quot;, 3000L), new Event(&quot;Alice&quot;, &quot;./prod?id=200&quot;, 3500L), new Event(&quot;Bob&quot;, &quot;./prod?id=2&quot;, 2500L), new Event(&quot;Alice&quot;, &quot;./prod?id=300&quot;, 36000L), new Event(&quot;Bob&quot;, &quot;./home&quot;, 30000L), new Event(&quot;Bob&quot;, &quot;./prod?id=1&quot;, 23000L), new Event(&quot;Bob&quot;, &quot;./prod?id=3&quot;, 33000L) ).assignTimestampsAndWatermarks(WatermarkStrategy.&lt;Event&gt;forMonotonousTimestamps() .withTimestampAssigner((SerializableTimestampAssigner&lt;Event&gt;) (element, recordTimestamp) -&gt; element.timestamp) ); orderStream.keyBy(data -&gt; data.f0) .intervalJoin(clickStream.keyBy(data -&gt; data.user)) .between(Time.seconds(-5), Time.seconds(10)) .process(new ProcessJoinFunction&lt;Tuple3&lt;String, String, Long&gt;, Event, String&gt;() { @Override public void processElement(Tuple3&lt;String, String, Long&gt; left, Event right, Context ctx, Collector&lt;String&gt; out) throws Exception { out.collect(right + &quot; =&gt; &quot; + left); } }) .print(); env.execute(); } } 3）窗口同组联结(Window CoGroup) 窗口同组联结(window coGroup)的用法跟 window join 非常类似， 也是将两条流合并之后开窗处理匹配的元素，调用时只需要将.join()换为.coGroup()就可以了。 与 window join的区别在于，调用 .apply() 方法定义 具体操作 时，传入的是一个CoGroupFunction。它是一个函数类接口 public interface CoGroupFunction&lt;IN1, IN2, O&gt; extends Function, Serializable { void coGroup(Iterable&lt;IN1&gt; first, Iterable&lt;IN2&gt; second, Collector&lt;O&gt; out) throws Exception; } coGroup()方法，有些类似于 FlatJoinFunction中 join()的形式， 同样有三个参数， 分别代表两条流中的数据以及用于输出的收集器(Collector)。不同的是，这里的前两个参数 不再是单独的每一组“配对”数据了， 而是传入了可遍历的数据集合。也就是说不会再去计算窗口中两条流数据集的笛卡尔积，而是直接把收集到的所有数据一次性传入，至于要怎样配对完全是自定义。这样coGroup()方法只会被调用一次， 而且即使一条流的数据没有任何另一条流的数据匹配， 也可以出现在集合中、当然也可以定义输出结果了。能够看出 coGroup 操作比窗口的 join 更加通用，不仅可以实现类似 SQL 中的“内 连接”(inner join)，也可以实现左外连接(left outer join)、右外连接(right outer join) 和全外 连接(full outer join)。事实上， 窗口join 的底层，也是通过 coGroup 来实现的。 代码示例： package com.company.flink.demo; import org.apache.flink.api.common.eventtime.SerializableTimestampAssigner; import org.apache.flink.api.common.eventtime.WatermarkStrategy; import org.apache.flink.api.common.functions.CoGroupFunction; import org.apache.flink.api.java.tuple.Tuple2; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows; import org.apache.flink.streaming.api.windowing.time.Time; import org.apache.flink.util.Collector; public class FlinkCoGroupDemo { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); DataStream&lt;Tuple2&lt;String, Long&gt;&gt; stream1 = env .fromElements( Tuple2.of(&quot;a&quot;, 1000L), Tuple2.of(&quot;a&quot;, 2000L), Tuple2.of(&quot;b&quot;, 1000L), Tuple2.of(&quot;b&quot;, 2000L) ) .assignTimestampsAndWatermarks( WatermarkStrategy .&lt;Tuple2&lt;String, Long&gt;&gt;forMonotonousTimestamps() .withTimestampAssigner( (SerializableTimestampAssigner&lt;Tuple2&lt;String, Long&gt;&gt;) (stringLongTuple2, l) -&gt; stringLongTuple2.f1 ) ); DataStream&lt;Tuple2&lt;String, Long&gt;&gt; stream2 = env .fromElements( Tuple2.of(&quot;a&quot;, 3000L), Tuple2.of(&quot;b&quot;, 3000L), Tuple2.of(&quot;b&quot;, 4000L), Tuple2.of(&quot;a&quot;, 4000L), Tuple2.of(&quot;c&quot;, 4000L) ) .assignTimestampsAndWatermarks( WatermarkStrategy .&lt;Tuple2&lt;String, Long&gt;&gt;forMonotonousTimestamps() .withTimestampAssigner( (SerializableTimestampAssigner&lt;Tuple2&lt;String, Long&gt;&gt;) (stringLongTuple2, l) -&gt; stringLongTuple2.f1 ) ); stream1 .coGroup(stream2) .where(r -&gt; r.f0) .equalTo(r -&gt; r.f0) .window(TumblingEventTimeWindows.of(Time.seconds(5))) .apply(new CoGroupFunction&lt;Tuple2&lt;String, Long&gt;, Tuple2&lt;String, Long&gt;, String&gt;() { @Override public void coGroup(Iterable&lt;Tuple2&lt;String, Long&gt;&gt; iter1, Iterable&lt;Tuple2&lt;String, Long&gt;&gt; iter2, Collector&lt;String&gt; collector) { collector.collect(iter1 + &quot;=&gt;&quot; + iter2); } }) .print(); env.execute(); } } ","link":"https://tianxiawuhao.github.io/6pSug_PPu/"},{"title":"第七章 时间语义与 Wartermark","content":"Flink 中的时间语义 在 Flink 的流式处理中， 会涉及到时间的不同概念， 如下图所示： 图 Flink 时间概念 Event Time： 是事件创建的时间。 它通常由事件中的时间戳描述， 例如采集的 日志数据中， 每一条日志都会记录自己的生成时间， Flink 通过时间戳分配器访问事 件时间戳。 Ingestion Time： 是数据进入 Flink 的时间。 Processing Time： 是每一个执行基于时间操作的算子的本地系统时间， 与机器 相关， 默认的时间属性就是 Processing Time。 一个例子——电影《 星球大战》 ： 例如， 一条日志进入 Flink 的时间为 2020-11-12 10:00:00.123， 到达 Window 的 系统时间为 2020-11-12 10:00:01.234， 日志的内容如下： 2020-11-02 18:37:15.624 INFO Fail over to rm2 对于业务来说， 要统计 1min 内的故障日志个数， 哪个时间是最有意义的？ —— eventTime， 因为我们要根据日志的生成时间进行统计。 EventTime 的引入 在 Flink 的流式处理中， 绝大部分的业务都会使用 eventTime， 一般只在eventTime 无法使用时， 才会被迫使用 ProcessingTime 或者 IngestionTime。 如果要使用 EventTime， 那么需要引入 EventTime 的时间属性， 引入方式如下所 示： // 创建 execution environment StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 告诉系统按照 EventTime 处理 env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime); Watermark 基本概念 我们知道， 流处理从事件产生， 到流经 source， 再到 operator， 中间是有一个过程和时间的， 虽然大部分情况下， 流到 operator 的数据都是按照事件产生的时间顺序来的， 但是也不排除由于网络、 分布式等原因， 导致乱序的产生， 所谓乱序， 就 是指 Flink 接收到的事件的先后顺序不是严格按照事件的 Event Time 顺序排列的。 图 数据的乱序 那么此时出现一个问题， 一旦出现乱序， 如果只根据 eventTime 决定 window 的运行， 我们不能明确数据是否全部到位， 但又不能无限期的等下去， 此时必须要有 个机制来保证一个特定的时间后， 必须触发 window 去进行计算了， 这个特别的机 制， 就是 Watermark。 Watermark 是一种衡量 Event Time 进展的机制。 Watermark 是用于处理乱序事件的， 而正确的处理乱序事件， 通常用 Watermark 机制结合 window 来实现。 数据流中的 Watermark 用于表示 timestamp 小于 Watermark 的数据， 都已经 到达了， 因此， window 的执行也是由 Watermark 触发的。 Watermark 可以理解成一个延迟触发机制， 我们可以设置 Watermark 的延时 时长 t， 每次系统会校验已经到达的数据中最大的 maxEventTime， 然后认定 eventTime 小于 maxEventTime - t 的所有数据都已经到达， 如果有窗口的停止时间等于 maxEventTime – t， 那么这个窗口被触发执行。 有序流的 Watermarker 如下图所示： （ Watermark 设置为 0） 图 有序数据的 Watermark 乱序流的 Watermarker 如下图所示： （ Watermark 设置为 2） 图 无序数据的 Watermark 当 Flink 接收到数据时， 会按照一定的规则去生成 Watermark， 这条 Watermark 就等于当前所有到达数据中的 maxEventTime - 延迟时长， 也就是说， Watermark 是 基于数据携带的时间戳生成的， 一旦 Watermark 比当前未触发的窗口的停止时间要 晚， 那么就会触发相应窗口的执行。 由于 event time 是由数据携带的， 因此， 如果 运行过程中无法获取新的数据， 那么没有被触发的窗口将永远都不被触发。 上图中， 我们设置的允许最大延迟到达时间为 2s， 所以时间戳为 7s 的事件对应 的 Watermark 是 5s， 时间戳为 12s 的事件的 Watermark 是 10s， 如果我们的窗口 1 是 1s~5s， 窗口 2 是 6s~10s， 那么时间戳为 7s 的事件到达时的 Watermarker 恰好触 发窗口 1， 时间戳为 12s 的事件到达时的 Watermark 恰好触发窗口 2。Watermark 就是触发前一窗口的“ 关窗时间” ， 一旦触发关门那么以当前时刻为准在窗口范围内的所有所有数据都会收入窗中。 只要没有达到水位那么不管现实中的时间推进了多久都不会触发关窗。 Watermark 的引入 watermark 的引入很简单， 对于乱序数据， 最常见的引用方式如下： // 抽取出时间和生成 watermark dataStream.assignTimestampsAndWatermarks(new AscendingTimestampExtractor&lt;UserBehavior&gt;() { @Override public long extractAscendingTimestamp(UserBehavior userBehavior) { // 原始数据单位秒，将其转成毫秒 return userBehavior.timestamp * 1000; } }) Event Time 的使用一定要指定数据源中的时间戳。 否则程序无法知道事件的事 件时间是什么(数据源里的数据没有时间戳的话， 就只能使用 Processing Time 了)。 我们看到上面的例子中创建了一个看起来有点复杂的类， 这个类实现的其实就 是分配时间戳的接口。 Flink 暴露了 TimestampAssigner 接口供我们实现， 使我们可 以自定义如何从事件数据中抽取时间戳。 // 创建 execution environment final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 通过连接 socket 获取输入数据，这里连接到本地9000端口，如果9000端口已被占用，请换一个端口 DataStream&lt;String&gt; text = env.socketTextStream(&quot;localhost&quot;, 9000, &quot;\\n&quot;); text.assignTimestampsAndWatermarks(new MyAssigner()) MyAssigner 有两种类型 AssignerWithPeriodicWatermarks AssignerWithPunctuatedWatermarks 以上两个接口都继承自 TimestampAssigner。 Assigner with periodic watermarks 周期性的生成 watermark： 系统会周期性的将 watermark 插入到流中(水位线也 是一种特殊的事件!)。 默认周期是 200 毫秒。 可以使用 ExecutionConfig.setAutoWatermarkInterval()方法进行设置。 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime) // 每隔 5 秒产生一个 watermark env.getConfig.setAutoWatermarkInterval(5000) 产生 watermark 的逻辑： 每隔 5 秒钟， Flink 会调用 AssignerWithPeriodicWatermarks 的 getCurrentWatermark()方法。 如果方法返回一个 时间戳大于之前水位的时间戳， 新的 watermark 会被插入到流中。 这个检查保证了 水位线是单调递增的。 如果方法返回的时间戳小于等于之前水位的时间戳， 则不会 产生新的 watermark。 例子， 自定义一个周期性的时间戳抽取： public class PeriodicAssigner extends AssignerWithPeriodicWatermarks&lt;SensorReading&gt; { Long bound= 60 * 1000 // 延时为 1 分钟 Long maxTs = Long.MinValue // 观察到的最大时间戳 public Watermark getCurrentWatermark() { new Watermark(maxTs - bound) } public void extractTimestamp(SensorReading r,Long previousTS) = { maxTs = maxTs.max(r.timestamp) r.timestamp } } 一种简单的特殊情况是， 如果我们事先得知数据流的时间戳是单调递增的， 也就是说没有乱序， 那我们可以使用 assignAscendingTimestamps， 这个方法会直接使 用数据的时间戳生成 watermark。 DataStream&lt;SensorReading&gt; stream: = ... DataStream&lt;SensorReading&gt; withTimestampsAndWatermarks = stream .assignAscendingTimestamps(e =&gt; e.timestamp) &gt;&gt; result: E(1), W(1), E(2), W(2), ... 而对于乱序数据流， 如果我们能大致估算出数据流中的事件的最大延迟时间， 就可以使用如下代码： DataStream&lt;SensorReading&gt; stream: = ... DataStream&lt;SensorReading&gt; withTimestampsAndWatermarks = stream.assignTimestampsAndWatermarks( new SensorTimeAssigner() ) public class SensorTimeAssigner extends BoundedOutOfOrdernessTimestampExtractor&lt;SensorReading&gt;(Time.seconds(5)) { // 抽取时间戳 public Long extractTimestamp(SensorReading r){ return r.timestamp } } &gt;&gt; relust: E(10), W(0), E(8), E(7), E(11), W(1), ... Assigner with punctuated watermarks 间断式地生成 watermark。 和周期性生成的方式不同， 这种方式不是固定时间的，而是可以根据需要对每条数据进行筛选和处理。 直接上代码来举个例子， 我们只给 sensor_1 的传感器的数据流插入 watermark： public class PunctuatedAssigner extends AssignerWithPunctuatedWatermarks&lt;SensorReading&gt; { Long bound=60 * 1000 public Watermark checkAndGetNextWatermark(SensorReading r , Long extractedTS) { if (r.id == &quot;sensor_1&quot;) { new Watermark(extractedTS - bound) } else { null } } public Long extractTimestamp(SensorReading r, Long previousTS){ return r.timestamp } } EvnetTime 在 window 中的使用 滚动窗口（TumblingEventTimeWindows） public class EventTimeTumblingWindowAllDemo { public static void main(String[] args) throws Exception { // 2021-03-06 21:00:00,1 // 2021-03-06 21:00:05,2 // 结果 ： 2&gt; 1 // 3&gt; 2 StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(new Configuration()); // 老版本必须要设置时间标准 （1.20 之前的） env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime); DataStreamSource&lt;String&gt; lines = env.socketTextStream(&quot;linux01&quot;, 8888); // flink里面的时间都精确到毫秒 // 时间提取器 BoundedOutOfOrdernessTimestampExtractor : 允许时间乱序，并且可以指定窗口延时 // WaterMark （水位线，可以让窗口延迟触发的一种机制） // 一个窗口中的一个分区的水位线 = 当前窗口当前分区最大的 EventTime - 延迟时间 Time.seconds(0) SingleOutputStreamOperator&lt;String&gt; linesWithWaterMark = lines.assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor&lt;String&gt;(Time.seconds(0)) { private SimpleDateFormat dateFormat = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;) ; @Override public long extractTimestamp(String s) { String strings = s.split(&quot;,&quot;)[0]; long timestamp = 0; try { Date date = dateFormat.parse(strings); timestamp = date.getTime(); } catch (ParseException e) { e.printStackTrace(); timestamp = System.currentTimeMillis(); } return timestamp; } }); SingleOutputStreamOperator&lt;Integer&gt; nums = linesWithWaterMark.map(new MapFunction&lt;String, Integer&gt;() { @Override public Integer map(String s) throws Exception { int i = Integer.parseInt(s.split(&quot;,&quot;)[1]); return i; } }); // 划分窗口 AllWindowedStream&lt;Integer, TimeWindow&gt; window = nums.windowAll(TumblingProcessingTimeWindows.of(Time.seconds(5))); // 对 window 中的数据 进行聚合 SingleOutputStreamOperator&lt;Integer&gt; sum = window.sum(0); sum.print(); env.execute() ; } } 结果是按照 Event Time 的时间窗口计算得出的， 而无关系统的时间（ 包括输入的快慢） 。 滑动窗口（SlidingEventTimeWindows） public class EventTimeSlidingWindowDemo { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(new Configuration()); // 两秒 调一次方法 env.getConfig().setAutoWatermarkInterval(1000); // 老版本必须要设置时间标准 （1.20 之前的） env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime); DataStreamSource&lt;String&gt; lines = env.socketTextStream(&quot;linux01&quot;, 8888); // flink里面的时间都精确到毫秒 // 时间提取器 BoundedOutOfOrdernessTimestampExtractor : 允许时间乱序，并且可以指定窗口延时 // WaterMark （水位线，可以让窗口延迟触发的一种机制） // 一个窗口中的一个分区的水位线 = 当前窗口当前分区最大的 EventTime - 延迟时间 Time.seconds(0) SingleOutputStreamOperator&lt;String&gt; linesWithWaterMark = lines.assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor&lt;String&gt;(Time.seconds(0)) { @Override public long extractTimestamp(String s) { return Long.parseLong(s.split(&quot;,&quot;)[0]); // EventTime } }); // 提取完 EventTime 后生成 WaterMark ，但数据还是原来的老样子 // 1000,spark,1 --&gt; spark,1 SingleOutputStreamOperator &lt;Tuple2&lt;String,Integer&gt;&gt; WordAndCount = linesWithWaterMark.map(new MapFunction&lt;String, Tuple2&lt;String,Integer&gt;&gt;() { @Override public Tuple2&lt;String,Integer&gt; map(String s) throws Exception { String[] split = s.split(&quot;,&quot;); return Tuple2.of(split[1],Integer.parseInt(split[2])); } }); // 先 keyBy，再划分窗口 KeyedStream&lt;Tuple2&lt;String, Integer&gt;, String&gt; keyed = WordAndCount.keyBy(t -&gt; t.f0); // 划分窗口 WindowedStream&lt;Tuple2&lt;String, Integer&gt;, String, TimeWindow&gt; window = keyed.window(SlidingEventTimeWindows.of(Time.seconds(10), Time.seconds(5))); // 对窗口里面的数据进行 sum SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; sum = window.sum(1); sum.print(); env.execute() ; } } 会话窗口（EventTimeSessionWindows） 相邻两次数据的 EventTime 的时间差超过指定的时间间隔就会触发执行。 如果加入 Watermark， 会在符合窗口触发的情况下进行延迟。 到达延迟水位再进行窗口 触发。 public class EventTimeSessionWindowDemo { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(new Configuration()); // 两秒 调一次方法 env.getConfig().setAutoWatermarkInterval(1000); // 老版本必须要设置时间标准 （1.20 之前的） env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime); DataStreamSource&lt;String&gt; lines = env.socketTextStream(&quot;linux01&quot;, 8888); // flink里面的时间都精确到毫秒 // 时间提取器 BoundedOutOfOrdernessTimestampExtractor : 允许时间乱序，并且可以指定窗口延时 // WaterMark （水位线，可以让窗口延迟触发的一种机制） // 一个窗口中的一个分区的水位线 = 当前窗口当前分区最大的 EventTime - 延迟时间 Time.seconds(0) SingleOutputStreamOperator&lt;String&gt; linesWithWaterMark = lines.assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor&lt;String&gt;(Time.seconds(0)) { @Override public long extractTimestamp(String s) { return Long.parseLong(s.split(&quot;,&quot;)[0]); // EventTime } }); // 提取完 EventTime 后生成 WaterMark ，但数据还是原来的老样子 // 1000,spark,1 --&gt; spark,1 SingleOutputStreamOperator &lt;Tuple2&lt;String,Integer&gt;&gt; WordAndCount = linesWithWaterMark.map(new MapFunction&lt;String, Tuple2&lt;String,Integer&gt;&gt;() { @Override public Tuple2&lt;String,Integer&gt; map(String s) throws Exception { String[] split = s.split(&quot;,&quot;); return Tuple2.of(split[1],Integer.parseInt(split[2])); } }); // 先 keyBy，再划分窗口 KeyedStream&lt;Tuple2&lt;String, Integer&gt;, String&gt; keyed = WordAndCount.keyBy(t -&gt; t.f0); // 划分窗口 WindowedStream&lt;Tuple2&lt;String, Integer&gt;, String, TimeWindow&gt; window = keyed.window(EventTimeSessionWindows.withGap(Time.seconds(5))); // 对窗口里面的数据进行 sum SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; sum = window.sum(1); sum.print(); env.execute() ; } } ","link":"https://tianxiawuhao.github.io/GzZxLdaVh/"},{"title":"第六章 Flink 的Window 操作","content":"窗口函数（Window Functions） 定义了窗口分配器，我们只是知道了数据属于哪个窗口，可以将数据收集起来了；至于收集起来到底要做什么，其实还完全没有头绪。所以在窗口分配器之后，必须再接上一个定义窗口如何进行计算的操作，这就是所谓的“窗口函数”（window functions）。 经窗口分配器处理之后，数据可以分配到对应的窗口中，而数据流经过转换得到的数据类型是 WindowedStream。这个类型并不是 DataStream，所以并不能直接进行其他转换，而必须进一步调用窗口函数，对收集到的数据进行处理计算之后，才能最终再次得到 DataStream。 窗口函数定义了要对窗口中收集的数据做的计算操作，根据处理的方式可以分为两类：增量聚合函数和全窗口函数。下面我们来进行分别讲解。 增量聚合函数（incremental aggregation functions） 窗口将数据收集起来，最基本的处理操作当然就是进行聚合。窗口对无限流的切分，可以看作得到了一个有界数据集。如果我们等到所有数据都收集齐，在窗口到了结束时间要输出结果的一瞬间再去进行聚合，显然就不够高效了——这相当于真的在用批处理的思路来做实时流处理。 为了提高实时性，我们可以再次将流处理的思路发扬光大：就像 DataStream 的简单聚合一样，每来一条数据就立即进行计算，中间只要保持一个简单的聚合状态就可以了；区别只是在于不立即输出结果，而是要等到窗口结束时间。等到窗口到了结束时间需要输出计算结果的时候，我们只需要拿出之前聚合的状态直接输出，这无疑就大大提高了程序运行的效率和实时性。 典型的增量聚合函数有两个：ReduceFunction 和 AggregateFunction。 归约函数（ReduceFunction） 最基本的聚合方式就是归约（reduce）。我们在基本转换的聚合算子中介绍过 reduce 的用法，窗口的归约聚合也非常类似，就是将窗口中收集到的数据两两进行归约。当我们进行流处理时，就是要保存一个状态；每来一个新的数据，就和之前的聚合状态做归约，这样就实现了增量式的聚合。 窗口函数中也提供了 ReduceFunction：只要基于 WindowedStream 调用.reduce()方法，然后传入 ReduceFunction 作为参数，就可以指定以归约两个元素的方式去对窗口中数据进行聚合了。这里的 ReduceFunction 其实与简单聚合时用到的 ReduceFunction 是同一个函数类接口，所以使用方式也是完全一样的。 我们回忆一下，ReduceFunction 中需要重写一个 reduce 方法，它的两个参数代表输入的两个元素，而归约最终输出结果的数据类型，与输入的数据类型必须保持一致。也就是说，中间聚合的状态和输出的结果，都和输入的数据类型是一样的。 public class ClickSource implements SourceFunction&lt;Event&gt; { // 声明一个布尔变量，作为控制数据生成的标识位 private Boolean running = true; public void run(SourceContext&lt;Event&gt; sourceContext) throws Exception { Random random = new Random(); // 在指定的数据集中随机选取数据 String[] users = {&quot;Mary&quot;, &quot;Alice&quot;, &quot;Bob&quot;, &quot;Cary&quot;}; String[] urls = {&quot;./home&quot;, &quot;./cart&quot;, &quot;./fav&quot;, &quot;./prod?id=1&quot;, &quot;./prod?id=2&quot;}; while (running) { sourceContext.collect(new Event( users[random.nextInt(users.length)], urls[random.nextInt(urls.length)], Calendar.getInstance().getTimeInMillis() )); // 隔 1 秒生成一个点击事件，方便观测 Thread.sleep(1000); } } public void cancel() { running = false; } } import org.apache.flink.api.common.eventtime.SerializableTimestampAssigner; import org.apache.flink.api.common.eventtime.WatermarkStrategy; import org.apache.flink.api.common.functions.MapFunction; import org.apache.flink.api.common.functions.ReduceFunction; import org.apache.flink.api.java.tuple.Tuple2; import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows; import org.apache.flink.streaming.api.windowing.time.Time; import java.time.Duration; public class WindowReduceExample { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); // 从自定义数据源读取数据，并提取时间戳、生成水位线 SingleOutputStreamOperator&lt;Event&gt; stream = env.addSource(new ClickSource()) .assignTimestampsAndWatermarks(WatermarkStrategy.&lt;Event&gt;forBoundedOutOfOrderness(Duration.ZERO) .withTimestampAssigner(new SerializableTimestampAssigner&lt;Event&gt;() { @Override public long extractTimestamp(Event element, long recordTimestamp) { return element.timestamp; } })); stream.map(new MapFunction&lt;Event, Tuple2&lt;String, Long&gt;&gt;() { @Override public Tuple2&lt;String, Long&gt; map(Event value) throws Exception { // 将数据转换成二元组，方便计算 return Tuple2.of(value.user, 1L); } }) .keyBy(r -&gt; r.f0) // 设置滚动事件时间窗口 .window(TumblingEventTimeWindows.of(Time.seconds(5))) .reduce(new ReduceFunction&lt;Tuple2&lt;String, Long&gt;&gt;() { @Override public Tuple2&lt;String, Long&gt; reduce(Tuple2&lt;String, Long&gt; value1, Tuple2&lt;String, Long&gt; value2) throws Exception { // 定义累加规则，窗口闭合时，向下游发送累加结果 return Tuple2.of(value1.f0, value1.f1 + value2.f1); } }) .print(); env.execute(); } } 运行结果：每五秒钟输出一次 (Bob,1) (Alice,2) (Mary,2) ... 代码中我们对每个用户的行为数据进行了开窗统计。与 word count 逻辑类似，首先将数据转换成(user, count)的二元组形式（类型为 Tuple2&lt;String, Long&gt;），每条数据对应的初始 count值都是 1；然后按照用户 id 分组，在处理时间下开滚动窗口，统计每 5 秒内的用户行为数量。对于窗口的计算，我们用 ReduceFunction 对 count 值做了增量聚合：窗口中会将当前的总 count值保存成一个归约状态，每来一条数据，就会调用内部的 reduce 方法，将新数据中的 count值叠加到状态上，并得到新的状态保存起来。等到了 5 秒窗口的结束时间，就把归约好的状态直接输出。 这里需要注意，我们经过窗口聚合转换输出的数据，数据类型依然是二元组 Tuple2&lt;String, Long&gt;。 聚合函数（AggregateFunction） ReduceFunction 可以解决大多数归约聚合的问题，但是这个接口有一个限制，就是聚合状态的类型、输出结果的类型都必须和输入数据类型一样。这就迫使我们必须在聚合前，先将数据转换（map）成预期结果类型；而在有些情况下，还需要对状态进行进一步处理才能得到输出结果，这时它们的类型可能不同，使用 ReduceFunction 就会非常麻烦。 例如，如果我们希望计算一组数据的平均值，应该怎样做聚合呢？很明显，这时我们需要计算两个状态量：数据的总和（sum），以及数据的个数（count），而最终输出结果是两者的商（sum/count）。如果用 ReduceFunction，那么我们应该先把数据转换成二元组(sum, count)的形式，然后进行归约聚合，最后再将元组的两个元素相除转换得到最后的平均值。本来应该只是一个任务，可我们却需要 map-reduce-map 三步操作，这显然不够高效。 于是自然可以想到，如果取消类型一致的限制，让输入数据、中间状态、输出结果三者类型都可以不同，不就可以一步直接搞定了吗？ Flink 的 Window API 中的 aggregate 就提供了这样的操作。直接基于 WindowedStream 调 用.aggregate()方法，就可以定义更加灵活的口聚合操作。这个方法需要传入一个AggregateFunction 的实现类作为参数。AggregateFunction 在源码中的定义如下 public interface AggregateFunction&lt;IN, ACC, OUT&gt; extends Function, Serializable { ACC createAccumulator(); ACC add(IN value, ACC accumulator); OUT getResult(ACC accumulator); ACC merge(ACC a, ACC b); } AggregateFunction 可以看作是 ReduceFunction 的通用版本，这里有三种类型：输入类型（IN）、累加器类型（ACC）和输出类型（OUT）。输入类型 IN 就是输入流中元素的数据类型；累加器类型 ACC 则是我们进行聚合的中间状态类型；而输出类型当然就是最终计算结果的类型了。 接口中有四个方法： createAccumulator()：创建一个累加器，这就是为聚合创建了一个初始状态，每个聚合任务只会调用一次。 add()：将输入的元素添加到累加器中。这就是基于聚合状态，对新来的数据进行进一步聚合的过程。方法传入两个参数：当前新到的数据 value，和当前的累加器accumulator；返回一个新的累加器值，也就是对聚合状态进行更新。每条数据到来之后都会调用这个方法。 getResult()：从累加器中提取聚合的输出结果。也就是说，我们可以定义多个状态，然后再基于这些聚合的状态计算出一个结果进行输出。比如之前我们提到的计算平均值，就可以把 sum 和 count 作为状态放入累加器，而在调用这个方法时相除得到最终结果。这个方法只在窗口要输出结果时调用。 merge()：合并两个累加器，并将合并后的状态作为一个累加器返回。这个方法只在需要合并窗口的场景下才会被调用；最常见的合并窗口（Merging Window）的场景就是会话窗口（Session Windows）。 所以可以看到，AggregateFunction 的工作原理是：首先调用 createAccumulator()为任务初始化一个状态(累加器)；而后每来一个数据就调用一次 add()方法，对数据进行聚合，得到的结果保存在状态中；等到了窗口需要输出时，再调用 getResult()方法得到计算结果。很明显，与 ReduceFunction 相同，AggregateFunction 也是增量式的聚合；而由于输入、中间状态、输出的类型可以不同，使得应用更加灵活方便。 下面来看一个具体例子。我们知道，在电商网站中，PV（页面浏览量）和 UV（独立访客数）是非常重要的两个流量指标。一般来说，PV 统计的是所有的点击量；而对用户 id 进行去重之后，得到的就是 UV。所以有时我们会用 PV/UV 这个比值，来表示“人均重复访问量”，也就是平均每个用户会访问多少次页面，这在一定程度上代表了用户的粘度。 import org.apache.flink.api.common.eventtime.SerializableTimestampAssigner; import org.apache.flink.api.common.eventtime.WatermarkStrategy; import org.apache.flink.api.common.functions.AggregateFunction; import org.apache.flink.api.java.tuple.Tuple2; import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.windowing.assigners.SlidingEventTimeWindows; import org.apache.flink.streaming.api.windowing.time.Time; import java.util.HashSet; public class WindowAggregateFunctionExample { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); SingleOutputStreamOperator&lt;Event&gt; stream = env.addSource(new ClickSource()) .assignTimestampsAndWatermarks(WatermarkStrategy.&lt;Event&gt;forMonotonousTimestamps() .withTimestampAssigner(new SerializableTimestampAssigner&lt;Event&gt;() { @Override public long extractTimestamp(Event element, long recordTimestamp) { return element.timestamp; } })); // 所有数据设置相同的 key，发送到同一个分区统计 PV 和 UV，再相除 stream.keyBy(data -&gt; true) .window(SlidingEventTimeWindows.of(Time.seconds(10), Time.seconds(2))) .aggregate(new AvgPv()) .print(); env.execute(); } public static class AvgPv implements AggregateFunction&lt;Event, Tuple2&lt;HashSet&lt;String&gt;, Long&gt;, Double&gt; { @Override public Tuple2&lt;HashSet&lt;String&gt;, Long&gt; createAccumulator() { // 创建累加器 return Tuple2.of(new HashSet&lt;String&gt;(), 0L); } @Override public Tuple2&lt;HashSet&lt;String&gt;, Long&gt; add(Event value, Tuple2&lt;HashSet&lt;String&gt;, Long&gt; accumulator) { // 属于本窗口的数据来一条累加一次，并返回累加器 accumulator.f0.add(value.user); return Tuple2.of(accumulator.f0, accumulator.f1 + 1L); } @Override public Double getResult(Tuple2&lt;HashSet&lt;String&gt;, Long&gt; accumulator) { // 窗口闭合时，增量聚合结束，将计算结果发送到下游 return (double) accumulator.f1 / accumulator.f0.size(); } @Override public Tuple2&lt;HashSet&lt;String&gt;, Long&gt; merge(Tuple2&lt;HashSet&lt;String&gt;, Long&gt; a, Tuple2&lt;HashSet&lt;String&gt;, Long&gt; b) { return null; } } } 输出结果： 1.0 1.6666666666666667 ... 代码中我们创建了事件时间滑动窗口，统计 10 秒钟的“人均 PV”，每 2 秒统计一次。由于聚合的状态还需要做处理计算，因此窗口聚合时使用了更加灵活的 AggregateFunction。为了统计 UV，我们用一个 HashSet 保存所有出现过的用户 id，实现自动去重；而 PV 的统计则类似一个计数器，每来一个数据加一就可以了。所以这里的状态，定义为包含一个 HashSet 和一个 count 值的二元组（Tuple2&lt;HashSet, Long&gt;），每来一条数据，就将 user 存入 HashSet，同时 count 加 1。这里的 count 就是 PV，而 HashSet 中元素的个数（size）就是 UV；所以最终窗口的输出结果，就是它们的比值。 这里没有涉及会话窗口，所以 merge()方法可以不做任何操作。 另外，Flink 也为窗口的聚合提供了一系列预定义的简单聚合方法，可以直接基于WindowedStream 调用。主要包括.sum()/max()/maxBy()/min()/minBy()，与 KeyedStream 的简单聚合非常相似。它们的底层，其实都是通过 AggregateFunction 来实现的。 通过 ReduceFunction 和 AggregateFunction 我们可以发现，增量聚合函数其实就是在用流处理的思路来处理有界数据集，核心是保持一个聚合状态，当数据到来时不停地更新状态。这就是 Flink 所谓的“有状态的流处理”，通过这种方式可以极大地提高程序运行的效率，所以 在实际应用中最为常见。 全窗口函数（full window functions） 窗口操作中的另一大类就是全窗口函数。与增量聚合函数不同，全窗口函数需要先收集窗口中的数据，并在内部缓存起来，等到窗口要输出结果的时候再取出数据进行计算。 很明显，这就是典型的批处理思路了——先攒数据，等一批都到齐了再正式启动处理流程。这样做毫无疑问是低效的：因为窗口全部的计算任务都积压在了要输出结果的那一瞬间，而在之前收集数据的漫长过程中却无所事事。这就好比平时不用功，到考试之前通宵抱佛脚，肯定不如把工夫花在日常积累上。 那为什么还需要有全窗口函数呢？这是因为有些场景下，我们要做的计算必须基于全部的数据才有效，这时做增量聚合就没什么意义了；另外，输出的结果有可能要包含上下文中的一些信息（比如窗口的起始时间），这是增量聚合函数做不到的。所以，我们还需要有更丰富的窗口计算方式，这就可以用全窗口函数来实现。 在 Flink 中，全窗口函数也有两种：WindowFunction 和 ProcessWindowFunction。 窗口函数（WindowFunction） WindowFunction 字面上就是“窗口函数”，它其实是老版本的通用窗口函数接口。我们可以基于 WindowedStream 调用.apply()方法，传入一个 WindowFunction 的实现类。 stream .keyBy(&lt;key selector&gt;) .window(&lt;window assigner&gt;) .apply(new MyWindowFunction()); 这个类中可以获取到包含窗口所有数据的可迭代集合（Iterable），还可以拿到窗口（Window）本身的信息。WindowFunction 接口在源码中实现如下： public interface WindowFunction&lt;IN, OUT, KEY, W extends Window&gt; extends Function, Serializable { void apply(KEY key, W window, Iterable&lt;IN&gt; input, Collector&lt;OUT&gt; out) throws Exception; } 当窗口到达结束时间需要触发计算时，就会调用这里的 apply 方法。我们可以从 input 集合中取出窗口收集的数据，结合 key 和 window 信息，通过收集器（Collector）输出结果。这里 Collector 的用法，与 FlatMapFunction 中相同。 不过我们也看到了，WindowFunction 能提供的上下文信息较少，也没有更高级的功能。事实上，它的作用可以被 ProcessWindowFunction 全覆盖，所以之后可能会逐渐弃用。一般在实际应用，直接使用ProcessWindowFunction 就可以了。 处理窗口函数（ProcessWindowFunction） ProcessWindowFunction 是 Window API 中最底层的通用窗口函数接口。之所以说它“最底层”，是因为除了可以拿到窗口中的所有数据之外，ProcessWindowFunction 还可以获取到一个“上下文对象”（Context）。这个上下文对象非常强大，不仅能够获取窗口信息，还可以访问当前的时间和状态信息。这里的时间就包括了处理时间（processing time）和事件时间水位线（eventtime watermark）。这就使得 ProcessWindowFunction 更加灵活、功能更加丰富。事实上，ProcessWindowFunction 是 Flink 底层 API——处理函数（process function）中的一员，关于处理函数我们会在后续章节展开讲解。 当 然 ， 这 些 好 处 是 以 牺 牲 性 能 和 资 源 为 代 价 的 。 作 为 一 个 全 窗 口 函 数 ，ProcessWindowFunction 同样需要将所有数据缓存下来、等到窗口触发计算时才使用。它其实就是一个增强版的 WindowFunction。 具体使用跟 WindowFunction 非常类似，我们可以基于 WindowedStream 调用.process()方法，传入一个 ProcessWindowFunction 的实现类。下面是一个电商网站统计每小时 UV 的例子： import org.apache.flink.api.common.eventtime.SerializableTimestampAssigner; import org.apache.flink.api.common.eventtime.WatermarkStrategy; import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.windowing.ProcessWindowFunction; import org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows; import org.apache.flink.streaming.api.windowing.time.Time; import org.apache.flink.streaming.api.windowing.windows.TimeWindow; import org.apache.flink.util.Collector; import java.sql.Timestamp; import java.time.Duration; import java.util.HashSet; public class UvCountByWindowExample { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); SingleOutputStreamOperator&lt;Event&gt; stream = env.addSource(new ClickSource()) .assignTimestampsAndWatermarks(WatermarkStrategy.&lt;Event&gt;forBoundedOutOfOrderness(Duration.ZERO) .withTimestampAssigner(new SerializableTimestampAssigner&lt;Event&gt;() { @Override public long extractTimestamp(Event element, long recordTimestamp) { return element.timestamp; } })); // 将数据全部发往同一分区，按窗口统计 UV stream.keyBy(data -&gt; true) .window(TumblingEventTimeWindows.of(Time.seconds(10))) .process(new UvCountByWindow()) .print(); env.execute(); } // 自定义窗口处理函数 public static class UvCountByWindow extends ProcessWindowFunction&lt;Event, String, Boolean, TimeWindow&gt; { @Override public void process(Boolean aBoolean, Context context, Iterable&lt;Event&gt; elements, Collector&lt;String&gt; out) throws Exception { HashSet&lt;String&gt; userSet = new HashSet&lt;&gt;(); // 遍历所有数据，放到 Set 里去重 for (Event event : elements) { userSet.add(event.user); } // 结合窗口信息，包装输出内容 Long start = context.window().getStart(); Long end = context.window().getEnd(); out.collect(&quot; 窗 口 : &quot; + new Timestamp(start) + &quot; ~ &quot; + new Timestamp(end) + &quot; 的独立访客数量是：&quot; + userSet.size()); } } } 这里我们使用的是事件时间语义。定义 10 秒钟的滚动事件窗口后，直接使用ProcessWindowFunction 来定义处理的逻辑。我们可以创建一个 HashSet，将窗口所有数据的userId 写入实现去重，最终得到 HashSet 的元素个数就是 UV 值。 当然，这里我们并没有用到下文中其他信息 ， 所以其实没有必要使用ProcessWindowFunction。全窗口函数因为运行效率较低，很少直接单独使用，往往会和增量聚合函数结合在一起，共同实现窗口的处理计算。 增量聚合和全窗口函数的结合使用 我们已经了解了 Window API 中两类窗口函数的用法，下面我们先来做个简单的总结。 增量聚合函数处理计算会更高效。举一个最简单的例子，对一组数据求和。大量的数据连续不断到来，全窗口函数只是把它们收集缓存起来，并没有处理；到了窗口要关闭、输出结果的时候，再遍历所有数据依次叠加，得到最终结果。而如果我们采用增量聚合的方式，那么只需要保存一个当前和的状态，每个数据到来时就会做一次加法，更新状态；到了要输出结果的时候，只要将当前状态直接拿出来就可以了。增量聚合相当于把计算量“均摊”到了窗口收集数据的过程中，自然就会比全窗口聚合更加高效、输出更加实时。 而全窗口函数的优势在于提供了更多的信息，可以认为是更加“通用”的窗口操作。它只负责收集数据、提供上下文相关信息，把所有的原材料都准备好，至于拿来做什么我们完全可以任意发挥。这就使得窗口计算更加灵活，功能更加强大。 所以在实际应用中，我们往往希望兼具这两者的优点，把它们结合在一起使用。Flink 的Window API 就给我们实现了这样的用法。、、 我们之前在调用 WindowedStream 的.reduce()和.aggregate()方法时，只是简单地直接传入了一个 ReduceFunction 或 AggregateFunction 进行增量聚合。除此之外，其实还可以传入第二个参数：一个全窗口函数，可以是 WindowFunction 或者 ProcessWindowFunction。 // ReduceFunction 与 WindowFunction 结合 public &lt;R&gt; SingleOutputStreamOperator&lt;R&gt; reduce(ReduceFunction&lt;T&gt; reduceFunction, WindowFunction&lt;T, R, K, W&gt; function) // ReduceFunction 与 ProcessWindowFunction 结合 public &lt;R&gt; SingleOutputStreamOperator&lt;R&gt; reduce(ReduceFunction&lt;T&gt; reduceFunction, ProcessWindowFunction&lt;T, R, K, W&gt; function) // AggregateFunction 与 WindowFunction 结合 public &lt;ACC, V, R&gt; SingleOutputStreamOperator&lt;R&gt; aggregate(AggregateFunction&lt;T, ACC, V&gt; aggFunction, WindowFunction&lt;V, R, K, W&gt; windowFunction) // AggregateFunction 与 ProcessWindowFunction 结合 public &lt;ACC, V, R&gt; SingleOutputStreamOperator&lt;R&gt; aggregate(AggregateFunction&lt;T, ACC, V&gt; aggFunction,ProcessWindowFunction&lt;V, R, K, W&gt; windowFunction) 这样调用的处理机制是：基于第一个参数（增量聚合函数）来处理窗口数据，每来一个数据就做一次聚合；等到窗口需要触发计算时，则调用第二个参数（全窗口函数）的处理逻辑输出结果。需要注意的是，这里的全窗口函数就不再缓存所有数据了，而是直接将增量聚合函数的结果拿来当作了 Iterable 类型的输入。一般情况下，这时的可迭代集合中就只有一个元素了。 下面我们举一个具体的实例来说明。在网站的各种统计指标中，一个很重要的统计指标就是热门的链接；想要得到热门的 url，前提是得到每个链接的“热门度”。一般情况下，可以用url 的浏览量（点击量）表示热门度。我们这里统计 10 秒钟的 url 浏览量，每 5 秒钟更新一次；另外为了更加清晰地展示，还应该把窗口的起始结束时间一起输出。我们可以定义滑动窗口，并结合增量聚合函数和全窗口函数来得到统计结果。 import org.apache.flink.api.common.eventtime.SerializableTimestampAssigner; import org.apache.flink.api.common.eventtime.WatermarkStrategy; import org.apache.flink.api.common.functions.AggregateFunction; import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.windowing.ProcessWindowFunction; import org.apache.flink.streaming.api.windowing.assigners.SlidingEventTimeWindows; import org.apache.flink.streaming.api.windowing.time.Time; import org.apache.flink.streaming.api.windowing.windows.TimeWindow; import org.apache.flink.util.Collector; public class UrlViewCountExample { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); SingleOutputStreamOperator&lt;Event&gt; stream = env.addSource(new ClickSource()) .assignTimestampsAndWatermarks(WatermarkStrategy.&lt;Event&gt;forMonotonousTimestamps() .withTimestampAssigner(new SerializableTimestampAssigner&lt;Event&gt;() { @Override public long extractTimestamp(Event element, long recordTimestamp) { return element.timestamp; } })); // 需要按照 url 分组，开滑动窗口统计 stream.keyBy(data -&gt; data.url) .window(SlidingEventTimeWindows.of(Time.seconds(10), Time.seconds(5))) // 同时传入增量聚合函数和全窗口函数 .aggregate(new UrlViewCountAgg(), new UrlViewCountResult()) .print(); env.execute(); } // 自定义增量聚合函数，来一条数据就加一 public static class UrlViewCountAgg implements AggregateFunction&lt;Event, Long, Long&gt; { @Override public Long createAccumulator() { return 0L; } @Override public Long add(Event value, Long accumulator) { return accumulator + 1; } @Override public Long getResult(Long accumulator) { return accumulator; } @Override public Long merge(Long a, Long b) { return null; } } // 自定义窗口处理函数，只需要包装窗口信息 public static class UrlViewCountResult extends ProcessWindowFunction&lt;Long, UrlViewCount, String, TimeWindow&gt; { @Override public void process(String url, Context context, Iterable&lt;Long&gt; elements, Collector&lt;UrlViewCount&gt; out) throws Exception { // 结合窗口信息，包装输出内容 Long start = context.window().getStart(); Long end = context.window().getEnd(); // 迭代器中只有一个元素，就是增量聚合函数的计算结果 out.collect(new UrlViewCount(url, elements.iterator().next(), start, end)); } } } import java.sql.Timestamp; public class UrlViewCount { public String url; public Long count; public Long windowStart; public Long windowEnd; public UrlViewCount() { } public UrlViewCount(String url, Long count, Long windowStart, Long windowEnd) { this.url = url; this.count = count; this.windowStart = windowStart; this.windowEnd = windowEnd; } @Override public String toString() { return &quot;UrlViewCount{&quot; + &quot;url='&quot; + url + '\\'' + &quot;, count=&quot; + count + &quot;, windowStart=&quot; + new Timestamp(windowStart) + &quot;, windowEnd=&quot; + new Timestamp(windowEnd) + '}'; } } 代码中用一个 AggregateFunction 来实现增量聚合，每来一个数据就计数加一；得到的结果交给 ProcessWindowFunction，结合窗口信息包装成我们想要的 UrlViewCount，最终输出统计结果。 注：ProcessWindowFunction 是处理函数中的一种，后面我们会详细讲解。这里只用它来将增量聚合函数的输出结果包裹一层窗口信息。 窗口处理的主体还是增量聚合，而引入全窗口函数又可以获取到更多的信息包装输出，这样的结合兼具了两种窗口函数的优势，在保证处理性能和实时性的同时支持了更加丰富的应用场景。 测试水位线和窗口的使用 之前讲过，当水位线到达窗口结束时间时，窗口就会闭合不再接收迟到的数据，因为根据水位线的定义，所有小于等于水位线的数据都已经到达，所以显然 Flink 会认为窗口中的数据都到达了（尽管可能存在迟到数据，也就是时间戳小于当前水位线的数据）。我们可以在之前生成水位线代码 WatermarkTest 的基础上，增加窗口应用做一下测试： import org.apache.flink.api.common.eventtime.SerializableTimestampAssigner; import org.apache.flink.api.common.eventtime.WatermarkStrategy; import org.apache.flink.api.common.functions.MapFunction; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.windowing.ProcessWindowFunction; import org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows; import org.apache.flink.streaming.api.windowing.time.Time; import org.apache.flink.streaming.api.windowing.windows.TimeWindow; import org.apache.flink.util.Collector; import java.time.Duration; public class WatermarkTest { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); // 将数据源改为 socket 文本流，并转换成 Event 类型 env.socketTextStream(&quot;localhost&quot;, 7777) .map(new MapFunction&lt;String, Event&gt;() { @Override public Event map(String value) throws Exception { String[] fields = value.split(&quot;,&quot;); return new Event(fields[0].trim(), fields[1].trim(), Long.valueOf(fields[2].trim())); } }) // 插入水位线的逻辑 .assignTimestampsAndWatermarks( // 针对乱序流插入水位线，延迟时间设置为 5s WatermarkStrategy.&lt;Event&gt;forBoundedOutOfOrderness(Duration.ofSeconds(5)) .withTimestampAssigner(new SerializableTimestampAssigner&lt;Event&gt;() { // 抽取时间戳的逻辑 @Override public long extractTimestamp(Event element, long recordTimestamp) { return element.timestamp; } }) ) // 根据 user 分组，开窗统计 .keyBy(data -&gt; data.user) .window(TumblingEventTimeWindows.of(Time.seconds(10))) .process(new WatermarkTestResult()) .print(); env.execute(); } // 自定义处理窗口函数，输出当前的水位线和窗口信息 public static class WatermarkTestResult extends ProcessWindowFunction&lt;Event, String, String, TimeWindow&gt; { @Override public void process(String s, Context context, Iterable&lt;Event&gt; elements, Collector&lt;String&gt; out) throws Exception { Long start = context.window().getStart(); Long end = context.window().getEnd(); Long currentWatermark = context.currentWatermark(); Long count = elements.spliterator().getExactSizeIfKnown(); out.collect(&quot;窗口&quot; + start + &quot; ~ &quot; + end + &quot;中共有&quot; + count + &quot;个元素，窗口闭合计算时，水位线处于：&quot; + currentWatermark); } } } 我们这里设置的最大延迟时间是 5 秒，所以当我们在终端启动 nc 程序，也就是 nc –lk 7777 然后输入如下数据时： Alice, ./home, 1000 Alice, ./cart, 2000 Alice, ./prod?id=100, 10000 Alice, ./prod?id=200, 8000 Alice, ./prod?id=300, 15000 我们会看到如下结果： 窗口 0 ~ 10000 中共有 3 个元素，窗口闭合计算时，水位线处于：9999 我们就会发现，当最后输入[Alice, ./prod?id=300, 15000]时，流中会周期性地（默认 200毫秒）插入一个时间戳为 15000L – 5 * 1000L – 1L = 9999 毫秒的水位线，已经到达了窗口[0,10000)的结束时间，所以会触发窗口的闭合计算。而后面再输入一条[Alice, ./prod?id=200, 9000]时，将不会有任何结果；因为这是一条迟到数据，它所属于的窗口已经触发计算然后销毁了（窗口默认被销毁），所以无法再进入到窗口中，自然也就无法更新计算结果了。窗口中的迟到数据默认会被丢弃，这会导致计算结果不够准确。 其他 API 对于一个窗口算子而言，窗口分配器和窗口函数是必不可少的。除此之外，Flink 还提供了其他一些可选的 API，让我们可以更加灵活地控制窗口行为。 触发器（Trigger） 触发器主要是用来控制窗口什么时候触发计算。所谓的“触发计算”，本质上就是执行窗口函数，所以可以认为是计算得到结果并输出的过程。 基于 WindowedStream 调用.trigger()方法，就可以传入一个自定义的窗口触发器（Trigger）。 stream.keyBy(...) .window(...) .trigger(new MyTrigger()) Trigger 是窗口算子的内部属性，每个窗口分配器（WindowAssigner）都会对应一个默认的触发器；对于 Flink 内置的窗口类型，它们的触发器都已经做了实现。例如，所有事件时间窗口，默认的触发器都是 EventTimeTrigger；类似还有 ProcessingTimeTrigger 和 CountTrigger。所以一般情况下是不需要自定义触发器的，不过我们依然有必要了解它的原理。Trigger 是一个抽象类，自定义时必须实现下面四个抽象方法： onElement()：窗口中每到来一个元素，都会调用这个方法。 onEventTime()：当注册的事件时间定时器触发时，将调用这个方法。 onProcessingTime ()：当注册的处理时间定时器触发时，将调用这个方法。 clear()：当窗口关闭销毁时，调用这个方法。一般用来清除自定义的状态。 可以看到，除了 clear()比较像生命周期方法，其他三个方法其实都是对某种事件的响应。onElement()是对流中数据元素到来的响应；而另两个则是对时间的响应。这几个方法的参数中都有一个“触发器上下文”（TriggerContext）对象，可以用来注册定时器回调（callback）。这里提到的“定时器”（Timer），其实就是我们设定的一个“闹钟”，代表未来某个时间点会执行的事件；当时间进展到设定的值时，就会执行定义好的操作。很明显，对于时间窗口（TimeWindow）而言，就应该是在窗口的结束时间设定了一个定时器，这样到时间就可以触发窗口的计算输出了。关于定时器的内容，我们在后面讲解处理函数（process function）时还会提到。 上面的前三个方法可以响应事件，那它们又是怎样跟窗口操作联系起来的呢？这就需要了解一下它们的返回值。这三个方法返回类型都是 TriggerResult，这是一个枚举类型（enum），其中定义了对窗口进行操作的四种类型。 CONTINUE（继续）：什么都不做 FIRE（触发）：触发计算，输出结果 PURGE（清除）：清空窗口中的所有数据，销毁窗口 FIRE_AND_PURGE（触发并清除）：触发计算输出结果，并清除窗口 我们可以看到，Trigger 除了可以控制触发计算，还可以定义窗口什么时候关闭（销毁）。上面的四种类型，其实也就是这两个操作交叉配对产生的结果。一般我们会认为，到了窗口的结束时间，那么就会触发计算输出结果，然后关闭窗口——似乎这两个操作应该是同时发生的；但 TriggerResult 的定义告诉我们，两者可以分开。稍后我们就会看到它们分开操作的场景。 下面我们举一个例子。在日常业务场景中，我们经常会开比较大的窗口来计算每个窗口的pv 或者 uv 等数据。但窗口开的太大，会使我们看到计算结果的时间间隔变长。所以我们可以使用触发器，来隔一段时间触发一次窗口计算。我们在代码中计算了每个 url 在 10 秒滚动窗口的 pv 指标，然后设置了触发器，每隔 1 秒钟触发一次窗口的计算。 import org.apache.flink.api.common.eventtime.SerializableTimestampAssigner; import org.apache.flink.api.common.eventtime.WatermarkStrategy; import org.apache.flink.api.common.state.ValueState; import org.apache.flink.api.common.state.ValueStateDescriptor; import org.apache.flink.api.common.typeinfo.Types; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.windowing.ProcessWindowFunction; import org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows; import org.apache.flink.streaming.api.windowing.time.Time; import org.apache.flink.streaming.api.windowing.triggers.Trigger; import org.apache.flink.streaming.api.windowing.triggers.TriggerResult; import org.apache.flink.streaming.api.windowing.windows.TimeWindow; import org.apache.flink.util.Collector; public class TriggerExample { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); env.addSource(new ClickSource()) .assignTimestampsAndWatermarks(WatermarkStrategy.&lt;Event&gt;forMonotonousTimestamps() .withTimestampAssigner(new SerializableTimestampAssigner&lt;Event&gt;() { @Override public long extractTimestamp(Event event, long l) { return event.timestamp; } }) ) .keyBy(r -&gt; r.url) .window(TumblingEventTimeWindows.of(Time.seconds(10))) .trigger(new MyTrigger()) .process(new WindowResult()) .print(); env.execute(); } public static class WindowResult extends ProcessWindowFunction&lt;Event, UrlViewCount, String, TimeWindow&gt; { @Override public void process(String s, Context context, Iterable&lt;Event&gt; iterable, Collector&lt;UrlViewCount&gt; collector) throws Exception { collector.collect( new UrlViewCount( s, // 获取迭代器中的元素个数 iterable.spliterator().getExactSizeIfKnown(), context.window().getStart(), context.window().getEnd() ) ); } } public static class MyTrigger extends Trigger&lt;Event, TimeWindow&gt; { @Override public TriggerResult onElement(Event event, long l, TimeWindow timeWindow, TriggerContext triggerContext) throws Exception { ValueState&lt;Boolean&gt; isFirstEvent = triggerContext.getPartitionedState( new ValueStateDescriptor&lt;Boolean&gt;(&quot;first-event&quot;, Types.BOOLEAN) ); if (isFirstEvent.value() == null) { for (long i = timeWindow.getStart(); i &lt; timeWindow.getEnd(); i = i + 1000L) { triggerContext.registerEventTimeTimer(i); } isFirstEvent.update(true); } return TriggerResult.CONTINUE; } @Override public TriggerResult onEventTime(long l, TimeWindow timeWindow, TriggerContext triggerContext) throws Exception { return TriggerResult.FIRE; } @Override public TriggerResult onProcessingTime(long l, TimeWindow timeWindow, TriggerContext triggerContext) throws Exception { return TriggerResult.CONTINUE; } @Override public void clear(TimeWindow timeWindow, TriggerContext triggerContext) throws Exception { ValueState&lt;Boolean&gt; isFirstEvent = triggerContext.getPartitionedState( new ValueStateDescriptor&lt;Boolean&gt;(&quot;first-event&quot;, Types.BOOLEAN) ); isFirstEvent.clear(); } } } 输出结果如下： UrlViewCount{url='./prod?id=1', count=1, windowStart=2021-07-01 14:44:10.0, windowEnd=2021-07-01 14:44:20.0} UrlViewCount{url='./prod?id=1', count=1, windowStart=2021-07-01 14:44:10.0, windowEnd=2021-07-01 14:44:20.0} UrlViewCount{url='./prod?id=1', count=1, windowStart=2021-07-01 14:44:10.0, windowEnd=2021-07-01 14:44:20.0} UrlViewCount{url='./prod?id=1', count=1, windowStart=2021-07-01 14:44:10.0, windowEnd=2021-07-01 14:44:20.0} 移除器（Evictor） 移除器主要用来定义移除某些数据的逻辑。基于 WindowedStream 调用.evictor()方法，就可以传入一个自定义的移除器（Evictor）。Evictor 是一个接口，不同的窗口类型都有各自预实现的移除器。 stream.keyBy(...) .window(...) .evictor(new MyEvictor()) Evictor 接口定义了两个方法： evictBefore()：定义执行窗口函数之前的移除数据操作 evictAfter()：定义执行窗口函数之后的以处数据操作 默认情况下，预实现的移除器都是在执行窗口函数（window fucntions）之前移除数据的。 允许延迟（Allowed Lateness） 在事件时间语义下，窗口中可能会出现数据迟到的情况。这是因为在乱序流中，水位线（watermark）并不一定能保证时间戳更早的所有数据不会再来。当水位线已经到达窗口结束时间时，窗口会触发计算并输出结果，这时一般也就要销毁窗口了；如果窗口关闭之后，又有本属于窗口内的数据姗姗来迟，默认情况下就会被丢弃。这也很好理解：窗口触发计算就像发车，如果要赶的车已经开走了，又不能坐其他的车（保证分配窗口的正确性），那就只好放弃坐班车了。 不过在多数情况下，直接丢弃数据也会导致统计结果不准确，我们还是希望该上车的人都能上来。为了解决迟到数据的问题，Flink 提供了一个特殊的接口，可以为窗口算子设置一个“允许的最大延迟”（Allowed Lateness）。也就是说，我们可以设定允许延迟一段时间，在这段时间内，窗口不会销毁，继续到来的数据依然可以进入窗口中并触发计算。直到水位线推进到了 窗口结束时间 + 延迟时间，才真正将窗口的内容清空，正式关闭窗口。 基于 WindowedStream 调用.allowedLateness()方法，传入一个 Time 类型的延迟时间，就可以表示允许这段时间内的延迟数据。 stream.keyBy(...) .window(TumblingEventTimeWindows.of(Time.hours(1))) .allowedLateness(Time.minutes(1)) 比如上面的代码中，我们定义了 1 小时的滚动窗口，并设置了允许 1 分钟的延迟数据。也就是说，在不考虑水位线延迟的情况下，对于 8 点~9 点的窗口，本来应该是水位线到达 9 点整就触发计算并关闭窗口；现在允许延迟 1 分钟，那么 9 点整就只是触发一次计算并输出结果，并不会关窗。后续到达的数据，只要属于 8 点~9 点窗口，依然可以在之前统计的基础上继续叠加，并且再次输出一个更新后的结果。直到水位线到达了 9 点零 1 分，这时就真正清空状态、关闭窗口，之后再来的迟到数据就会被丢弃了。 从这里我们就可以看到，窗口的触发计算（Fire）和清除（Purge）操作确实可以分开。不过在默认情况下，允许的延迟是 0，这样一旦水位线到达了窗口结束时间就会触发计算并清除窗口，两个操作看起来就是同时发生了。当窗口被清除（关闭）之后，再来的数据就会被丢弃。 将迟到的数据放入侧输出流 我们自然会想到，即使可以设置窗口的延迟时间，终归还是有限的，后续的数据还是会被丢弃。如果不想丢弃任何一个数据，又该怎么做呢？ Flink 还提供了另外一种方式处理迟到数据。我们可以将未收入窗口的迟到数据，放入“侧输出流”（side output）进行另外的处理。所谓的侧输出流，相当于是数据流的一个“分支”，这个流中单独放置那些错过了该上的车、本该被丢弃的数据。 基于 WindowedStream 调用.sideOutputLateData() 方法，就可以实现这个功能。方法需要传入一个“输出标签”（OutputTag），用来标记分支的迟到数据流。因为保存的就是流中的原始数据，所以 OutputTag 的类型与流中数据类型相同。 DataStream&lt;Event&gt; stream = env.addSource(...); OutputTag&lt;Event&gt; outputTag = new OutputTag&lt;Event&gt;(&quot;late&quot;) {}; stream.keyBy(...) .window(TumblingEventTimeWindows.of(Time.hours(1))) .sideOutputLateData(outputTag) 将迟到数据放入侧输出流之后，还应该可以将它提取出来。基于窗口处理完成之后的DataStream，调用.getSideOutput()方法，传入对应的输出标签，就可以获取到迟到数据所在的流了。 SingleOutputStreamOperator&lt;AggResult&gt; winAggStream = stream.keyBy(...) .window(TumblingEventTimeWindows.of(Time.hours(1))) .sideOutputLateData(outputTag) .aggregate(new MyAggregateFunction()) DataStream&lt;Event&gt; lateStream = winAggStream.getSideOutput(outputTag); 这里注意，getSideOutput()是 SingleOutputStreamOperator 的方法，获取到的侧输出流数据类型应该和 OutputTag 指定的类型一致，与窗口聚合之后流中的数据类型可以不同。 窗口的生命周期 熟悉了窗口 API 的使用，我们再回头梳理一下窗口本身的生命周期，这也是对窗口所有操作的一个总结。 窗口的创建 窗口的类型和基本信息由窗口分配器（window assigners）指定，但窗口不会预先创建好，而是由数据驱动创建。当第一个应该属于这个窗口的数据元素到达时，就会创建对应的窗口。 窗口计算的触发 除了窗口分配器，每个窗口还会有自己的窗口函数（window functions）和触发器（trigger）。窗口函数可以分为增量聚合函数和全窗口函数，主要定义了窗口中计算的逻辑；而触发器则是指定调用窗口函数的条件。 对于不同的窗口类型，触发计算的条件也会不同。例如，一个滚动事件时间窗口，应该在水位线到达窗口结束时间的时候触发计算，属于“定点发车”；而一个计数窗口，会在窗口中元素数量达到定义大小时触发计算，属于“人满就发车”。所以 Flink 预定义的窗口类型都有对应内置的触发器。 对于事件时间窗口而言，除去到达结束时间的“定点发车”，还有另一种情形。当我们设置了允许延迟，那么如果水位线超过了窗口结束时间、但还没有到达设定的最大延迟时间，这期间内到达的迟到数据也会触发窗口计算。这类似于没有准时赶上班车的人又追上了车，这时车要再次停靠、开门，将新的数据整合统计进来。 窗口的销毁 一般情况下，当时间达到了结束点，就会直接触发计算输出结果、进而清除状态销毁窗口。这时窗口的销毁可以认为和触发计算是同一时刻。这里需要注意，Flink 中只对时间窗口（TimeWindow）有销毁机制；由于计数窗口（CountWindow）是基于全局窗口（GlobalWindw）实现的，而全局窗口不会清除状态，所以就不会被销毁。 在特殊的场景下，窗口的销毁和触发计算会有所不同。事件时间语义下，如果设置了允许延迟，那么在水位线到达窗口结束时间时，仍然不会销毁窗口；窗口真正被完全删除的时间点，是窗口的结束时间加上用户指定的允许延迟时间。 窗口 API 调用总结 到目前为止，我们已经彻底明白了 Flink 中窗口的概念和 Window API 的调用，我们再用一张图做一个完整总结 Window API 首先按照时候按键分区分成两类。keyBy 之后的 KeyedStream，可以调用.window()方法声明按键分区窗口（Keyed Windows）；而如果不做 keyBy，DataStream 也可以直接调用.windowAll()声明非按键分区窗口。之后的方法调用就完全一样了。 接下来首先是通过.window()/.windowAll()方法定义窗口分配器，得到 WindowedStream；然 后 通 过 各 种 转 换 方 法 （ reduce/aggregate/apply/process ） 给 出 窗 口 函 数(ReduceFunction/AggregateFunction/ProcessWindowFunction)，定义窗口的具体计算处理逻辑，转换之后重新得到 DataStream。这两者必不可少，是窗口算子（WindowOperator）最重要的组成部分。 此外，在这两者之间，还可以基于 WindowedStream 调用.trigger()自定义触发器、调用.evictor()定义移除器、调用.allowedLateness()指定允许延迟时间、调用.sideOutputLateData()将迟到数据写入侧输出流，这些都是可选的 API，一般不需要实现。而如果定义了侧输出流，可以基于窗口聚合之后的 DataStream 调用.getSideOutput()获取侧输出流。 迟到数据的处理 有了事件时间、水位线和窗口的相关知识，现在就可以系统性地讨论一下怎样处理迟到数据了。我们知道，所谓的“迟到数据”（late data），是指某个水位线之后到来的数据，它的时间戳其实是在水位线之前的。所以只有在事件时间语义下，讨论迟到数据的处理才是有意义的。 事件时间里用来表示时钟进展的就是水位线（watermark）。对于乱序流，水位线本身就可以设置一个延迟时间；而做窗口计算时，我们又可以设置窗口的允许延迟时间；另外窗口还有将迟到数据输出到测输出流的用法。所有的这些方法，它们之间有什么关系，我们又该怎样合理利用呢？这一节我们就来讨论这个问题。 设置水位线延迟时间 水位线是事件时间的进展，它是我们整个应用的全局逻辑时钟。水位线生成之后，会随着数据在任务间流动，从而给每个任务指明当前的事件时间。所以从这个意义上讲，水位线是一个覆盖万物的存在，它并不只针对事件时间窗口有效。 之前我们讲到触发器时曾提到过“定时器”，时间窗口的操作底层就是靠定时器来控制触发的。既然是底层机制，定时器自然就不可能是窗口的专利了；事实上它是 Flink 底层 API— —处理函数（process function）的重要部分。 所以水位线其实是所有事件时间定时器触发的判断标准。那么水位线的延迟，当然也就是全局时钟的滞后，相当于是上帝拨动了琴弦，所有人的表都变慢了。 既然水位线这么重要，那一般情况就不应该把它的延迟设置得太大，否则流处理的实时性就会大大降低。因为水位线的延迟主要是用来对付分布式网络传输导致的数据乱序，而网络传输的乱序程度一般并不会很大，大多集中在几毫秒至几百毫秒。所以实际应用中，我们往往会给水位线设置一个“能够处理大多数乱序数据的小延迟”，视需求一般设在毫秒~秒级。 当我们设置了水位线延迟时间后，所有定时器就都会按照延迟后的水位线来触发。如果一个数据所包含的时间戳，小于当前的水位线，那么它就是所谓的“迟到数据”。 允许窗口处理迟到数据 水位线延迟设置的比较小，那之后如果仍有数据迟到该怎么办？对于窗口计算而言，如果水位线已经到了窗口结束时间，默认窗口就会关闭，那么之后再来的数据就要被丢弃了。 自然想到，Flink 的窗口也是可以设置延迟时间，允许继续处理迟到数据的。 这种情况下，由于大部分乱序数据已经被水位线的延迟等到了，所以往往迟到的数据不会太多。这样，我们会在水位线到达窗口结束时间时，先快速地输出一个近似正确的计算结果；然后保持窗口继续等到延迟数据，每来一条数据，窗口就会再次计算，并将更新后的结果输出。这样就可以逐步修正计算结果，最终得到准确的统计值了。 类比班车的例子，我们可以这样理解：大多数人是在发车时刻前后到达的，所以我们只要把表调慢，稍微等一会儿，绝大部分人就都上车了，这个把表调慢的时间就是水位线的延迟；到点之后，班车就准时出发了，不过可能还有该来的人没赶上。于是我们就先慢慢往前开，这段时间内，如果迟到的人抓点紧还是可以追上的；如果有人追上来了，就停车开门让他上来，然后车继续向前开。当然我们的车不能一直慢慢开，需要有一个时间限制，这就是窗口的允许延迟时间。一旦超过了这个时间，班车就不再停留，开上高速疾驰而去了。 所以我们将水位线的延迟和窗口的允许延迟数据结合起来，最后的效果就是先快速实时地输出一个近似的结果，而后再不断调整，最终得到正确的计算结果。回想流处理的发展过程，这不就是著名的 Lambda 架构吗？原先需要两套独立的系统来同时保证实时性和结果的最终正确性，如今 Flink 一套系统就全部搞定了。 将迟到数据放入窗口侧输出流 即使我们有了前面的双重保证，可窗口不能一直等下去，最后总要真正关闭。窗口一旦关闭，后续的数据就都要被丢弃了。那如果真的还有漏网之鱼又该怎么办呢？ 那就要用到最后一招了：用窗口的侧输出流来收集关窗以后的迟到数据。这种方式是最后“兜底”的方法，只能保证数据不丢失；因为窗口已经真正关闭，所以是无法基于之前窗口的结果直接做更新的。我们只能将之前的窗口计算结果保存下来，然后获取侧输出流中的迟到数据，判断数据所属的窗口，手动对结果进行合并更新。尽管有些烦琐，实时性也不够强，但能够保证最终结果一定是正确的。 如果还用赶班车来类比，那就是车已经上高速开走了，这班车是肯定赶不上了。不过我们还留下了行进路线和联系方式，迟到的人如果想办法辗转到了目的地，还是可以和大部队会合的。最终，所有该到的人都会在目的地出现。 所以总结起来，Flink 处理迟到数据，对于结果的正确性有三重保障：水位线的延迟，窗口允许迟到数据，以及将迟到数据放入窗口侧输出流。我们可以回忆一下之前 6.3.5 小节统计每个 url 浏览次数的代码 UrlViewCountExample，稍作改进，增加处理迟到数据的功能。具体代码如下。 import org.apache.flink.api.common.eventtime.SerializableTimestampAssigner; import org.apache.flink.api.common.eventtime.WatermarkStrategy; import org.apache.flink.api.common.functions.AggregateFunction; import org.apache.flink.api.common.functions.MapFunction; import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.windowing.ProcessWindowFunction; import org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows; import org.apache.flink.streaming.api.windowing.time.Time; import org.apache.flink.streaming.api.windowing.windows.TimeWindow; import org.apache.flink.util.Collector; import org.apache.flink.util.OutputTag; import java.time.Duration; public class ProcessLateDataExample { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); // 读取 socket 文本流 SingleOutputStreamOperator&lt;Event&gt; stream = env.socketTextStream(&quot;localhost&quot;, 7777) .map(new MapFunction&lt;String, Event&gt;() { @Override public Event map(String value) throws Exception { String[] fields = value.split(&quot; &quot;); return new Event(fields[0].trim(), fields[1].trim(), Long.valueOf(fields[2].trim())); } }) // 方式一：设置 watermark 延迟时间，2 秒钟 .assignTimestampsAndWatermarks(WatermarkStrategy.&lt;Event&gt;forBoundedOutOfOrderness(Duration.ofSeconds(2)) .withTimestampAssigner(new SerializableTimestampAssigner&lt;Event&gt;() { @Override public long extractTimestamp(Event element, long recordTimestamp) { return element.timestamp; } })); // 定义侧输出流标签 OutputTag&lt;Event&gt; outputTag = new OutputTag&lt;Event&gt;(&quot;late&quot;) {}; SingleOutputStreamOperator&lt;UrlViewCount&gt; result = stream.keyBy(data -&gt; data.url) .window(TumblingEventTimeWindows.of(Time.seconds(10))) // 方式二：允许窗口处理迟到数据，设置 1 分钟的等待时间 .allowedLateness(Time.minutes(1)) // 方式三：将最后的迟到数据输出到侧输出流 .sideOutputLateData(outputTag) .aggregate(new UrlViewCountAgg(), new UrlViewCountResult()); result.print(&quot;result&quot;); result.getSideOutput(outputTag).print(&quot;late&quot;); // 为方便观察，可以将原始数据也输出 stream.print(&quot;input&quot;); env.execute(); } public static class UrlViewCountAgg implements AggregateFunction&lt;Event, Long, Long&gt; { @Override public Long createAccumulator() { return 0L; } @Override public Long add(Event value, Long accumulator) { return accumulator + 1; } @Override public Long getResult(Long accumulator) { return accumulator; } @Override public Long merge(Long a, Long b) { return null; } } public static class UrlViewCountResult extends ProcessWindowFunction&lt;Long, UrlViewCount, String, TimeWindow&gt; { @Override public void process(String url, Context context, Iterable&lt;Long&gt; elements, Collector&lt;UrlViewCount&gt; out) throws Exception { // 结合窗口信息，包装输出内容 Long start = context.window().getStart(); Long end = context.window().getEnd(); out.collect(new UrlViewCount(url, elements.iterator().next(), start, end)); } } } 我们还是先启动 nc –lk 7777，然后依次输入以下数据： Alice, ./home, 1000 Alice, ./home, 2000 Alice, ./home, 10000 Alice, ./home, 9000 Alice, ./cart, 12000 Alice, ./prod?id=100, 15000 Alice, ./home, 9000 Alice, ./home, 8000 Alice, ./prod?id=200, 70000 Alice, ./home, 8000 Alice, ./prod?id=300, 72000 Alice, ./home, 8000 下面我们来分析一下程序的运行过程。当输入数据[Alice, ./home, 10000]时，时间戳为10000，由于设置了 2 秒钟的水位线延迟时间，所以此时水位线到达了 8 秒（事实上是 7999毫秒，这里不再追究减 1 的细节），并没有触发 [0, 10s) 窗口的计算；所以接下来时间戳为 9000的数据到来，同样可以直接进入窗口做增量聚合。当时间戳为 12000 的数据到来时（无所谓url 是什么，所有数据都可以推动水位线前进），水位线到达了 12000 – 2 * 1000 = 10000，所以触发了[0, 10s) 窗口的计算，第一次输出了窗口统计结果，如下所示： result&gt; UrlViewCount{url='./home,', count=3, windowStart=1970-01-01 08:00:00.0, windowEnd=1970-01-01 08:00:10.0} 这里 count 值为 3，就包括了之前输入的时间戳为 1000、2000、9000 的三条数据。 不过窗口触发计算之后并没有关闭销毁，而是继续等待迟到数据。之后时间戳为 15000的数据继续推进水位线，此时时钟已经进展到了 13000ms；此时再来一条时间戳为 9000 的数据，我们会发现立即输出了一条统计结果： result&gt; UrlViewCount{url='./home,', count=4, windowStart=1970-01-01 08:00:00.0, windowEnd=1970-01-01 08:00:10.0} 很明显，这仍然是[0, 10s) 的窗口，在之前计数值 3 的基础上继续叠加，更新统计结果为4。所以允许窗口处理迟到数据之后，相当于窗口有了一段等待时间，在这期间所有的迟到数据都会立即触发窗口计算，更新之前的结果。 因此，之后时间戳为 8000 的数据到来，同样会立即输出： result&gt; UrlViewCount{url='./home,', count=5, windowStart=1970-01-01 08:00:00.0, windowEnd=1970-01-01 08:00:10.0} 我们设置窗口等待的时间为 1 分钟，所以当时间推进到 10000 + 60 * 1000 = 70000 时，窗口就会真正被销毁。此前的所有迟到数据可以直接更新窗口的计算结果，而之后的迟到数据已经无法整合进窗口，就只能用侧输出流来捕获了。需要注意的是，这里的“时间”依然是由水 位线来指示的，所以时间戳为 70000 的数据到来，并不会触发窗口的销毁；当时间戳为 72000的数据到来，水位线推进到了 72000 – 2 * 1000 = 70000，此时窗口真正销毁关闭，之后再来的迟到数据就会输出到侧输出流了： late&gt; Event{user='Alice,', url='./home,', timestamp=1970-01-01 08:00:08.0} ","link":"https://tianxiawuhao.github.io/5uA_yv-aO/"},{"title":"第六章 Flink 中的 Window","content":"Window Window 概述 streaming 流式计算是一种被设计用于处理无限数据集的数据处理引擎， 而无限数据集是指一种不断增长的本质上无限的数据集， 而 window 是一种切割无限数据 为有限块进行处理的手段。 Window 是无限数据流处理的核心， Window 将一个无限的 stream 拆分成有限大小的” buckets” 桶， 我们可以在这些桶上做计算操作。 Window 类型 Window 可以分成两类： CountWindow： 按照指定的数据条数生成一个 Window， 与时间无关。 TimeWindow： 按照时间生成 Window。 对于 TimeWindow， 可以根据窗口实现原理的不同分成三类： 滚动窗口（ TumblingWindow） 、 滑动窗口（ Sliding Window）、 和会话窗口（ Session Window） 。 滚动窗口（Tumbling Windows） 将数据依据固定的窗口长度对数据进行切片。 特点： 时间对齐， 窗口长度固定， 没有重叠。 滚动窗口分配器将每个元素分配到一个指定窗口大小的窗口中， 滚动窗口有一个固定的大小， 并且不会出现重叠。 例如： 如果你指定了一个 5 分钟大小的滚动窗 口， 窗口的创建如下图所示： 图 滚动窗口 适用场景： 适合做 BI 统计等（ 做每个时间段的聚合计算） 。 滑动窗口（Sliding Windows） 滑动窗口是固定窗口的更广义的一种形式， 滑动窗口由固定的窗口长度和滑动间隔组成。 特点：时间对齐， 窗口长度固定， 可以有重叠。 滑动窗口分配器将元素分配到固定长度的窗口中， 与滚动窗口类似， 窗口的大小由窗口大小参数来配置， 另一个窗口滑动参数控制滑动窗口开始的频率。 因此， 滑动窗口如果滑动参数小于窗口大小的话， 窗口是可以重叠的， 在这种情况下元素会被分配到多个窗口中。 例如， 你有 10 分钟的窗口和 5 分钟的滑动， 那么每个窗口中 5 分钟的窗口里包含着上个 10 分钟产生的数据， 如下图所示： 图 滑动窗口 适用场景： 对最近一个时间段内的统计（ 求某接口最近 5min 的失败率来决定是 否要报警） 。 会话窗口（Session Windows） 由一系列事件组合一个指定时间长度的 timeout 间隙组成， 类似于 web 应用的session， 也就是一段时间没有接收到新数据就会生成新的窗口。 特点： 时间无对齐。 session 窗口分配器通过 session 活动来对元素进行分组， session 窗口跟滚动窗口和滑动窗口相比， 不会有重叠和固定的开始时间和结束时间的情况， 相反， 当它在一个固定的时间周期内不再收到元素， 即非活动间隔产生， 那个这个窗口就会关 闭。 一个 session 窗口通过一个 session 间隔来配置， 这个 session 间隔定义了非活跃 周期的长度， 当这个非活跃周期产生， 那么当前的 session 将关闭并且后续的元素将 被分配到新的 session 窗口中去。 图 会话窗口 Window API TimeWindow TimeWindow 是将指定时间范围内的所有数据组成一个 window， 一次对一个 window 里面的所有数据进行计算。 滚动窗口 Flink 默认的时间窗口根据 Processing Time 进行窗口的划分， 将 Flink 获取到的 数据根据进入 Flink 的时间划分到不同的窗口中。 时间间隔可以通过 Time.milliseconds(x)， Time.seconds(x)， Time.minutes(x)等其 中的一个来指定。 import com.tan.flink.bean.SensorReading; import com.tan.flink.source.SourceFromCustom; import org.apache.flink.streaming.api.datastream.DataStreamSource; import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.windowing.time.Time; public class TimeWindow_Tumbling { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStreamSource&lt;SensorReading&gt; inputDataStream = env.addSource(new SourceFromCustom.CustomSource()); SingleOutputStreamOperator&lt;SensorReading&gt; resultDataStream = inputDataStream.keyBy(&quot;id&quot;) .timeWindow(Time.seconds(5L)) // 滚动时间窗口大小 .maxBy(&quot;temperature&quot;); resultDataStream.print(); env.execute(); } } 滑动窗口（SlidingEventTimeWindows） 滑动窗口和滚动窗口的函数名是完全一致的， 只是在传参数时需要传入两个参 数， 一个是 window_size， 一个是 sliding_size。下面代码中的 sliding_size 设置为了 5s， 也就是说， 每 5s 就计算输出结果一次，每一次计算的 window 范围是 15s 内的所有元素。 时间间隔可以通过 Time.milliseconds(x)， Time.seconds(x)， Time.minutes(x)等其 中的一个来指定。 import com.tan.flink.bean.SensorReading; import com.tan.flink.source.SourceFromCustom; import org.apache.flink.streaming.api.datastream.DataStreamSource; import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.windowing.time.Time; public class TimeWindow_Sliding { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStreamSource&lt;SensorReading&gt; inputDataStream = env.addSource(new SourceFromCustom.CustomSource()); SingleOutputStreamOperator&lt;SensorReading&gt; resultDataStream = inputDataStream.keyBy(&quot;id&quot;) .timeWindow(Time.seconds(15L), Time.seconds(5)) // 窗口大小15秒 滑动大小5秒 .minBy(&quot;temperature&quot;); resultDataStream.print(); env.execute(); } } CountWindow CountWindow 根据窗口中相同 key 元素的数量来触发执行， 执行时只计算元素数量达到窗口大小的 key 对应的结果。 注意： CountWindow 的 window_size 指的是相同 Key 的元素的个数， 不是输入 的所有元素的总数。 1 滚动窗口 默认的 CountWindow 是一个滚动窗口， 只需要指定窗口大小即可， 当元素数量达到窗口大小时， 就会触发窗口的执行。 import com.tan.flink.bean.SensorReading; import com.tan.flink.source.SourceFromCustom; import org.apache.flink.streaming.api.datastream.DataStreamSource; import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; public class CountWindow_Tumbling { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStreamSource&lt;SensorReading&gt; inputDataStream = env.addSource(new SourceFromCustom.CustomSource()); SingleOutputStreamOperator&lt;SensorReading&gt; resultDataStream = inputDataStream.keyBy(&quot;id&quot;) .countWindow(3) .maxBy(&quot;temperature&quot;); resultDataStream.print(); env.execute(); } } 2 滑动窗口 滑动窗口和滚动窗口的函数名是完全一致的， 只是在传参数时需要传入两个参数， 一个是 window_size， 一个是 sliding_size。 下面代码中的 sliding_size 设置为了 2， 也就是说， 每收到两个相同 key 的数据就计算一次， 每一次计算的 window 范围是 10 个元素。 import com.tan.flink.bean.SensorReading; import com.tan.flink.source.SourceFromCustom; import org.apache.flink.streaming.api.datastream.DataStreamSource; import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; public class CountWindow_Sliding { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStreamSource&lt;SensorReading&gt; inputDataStream = env.addSource(new SourceFromCustom.CustomSource()); SingleOutputStreamOperator&lt;SensorReading&gt; resultDataStream = inputDataStream.keyBy(&quot;id&quot;) .countWindow(10, 2) .minBy(&quot;temperature&quot;); resultDataStream.print(); env.execute(); } } window function window function 定义了要对窗口中收集的数据做的计算操作， 主要可以分为两 类： 增量聚合函数（ incremental aggregation functions） 每条数据到来就进行计算， 保持一个简单的状态。 典型的增量聚合函数有 ReduceFunction, AggregateFunction。 全窗口函数（ full window functions） 先把窗口所有数据收集起来， 等到计算的时候会遍历所有数据。 ProcessWindowFunction 就是一个全窗口函数。 其它可选 API trigger() —— 触发器 定义 window 什么时候关闭， 触发计算并输出结果 evitor() —— 移除器,定义移除某些数据的逻辑 allowedLateness() —— 允许处理迟到的数据 sideOutputLateData() —— 将迟到的数据放入侧输出流 getSideOutput() —— 获取侧输出 ","link":"https://tianxiawuhao.github.io/FnbZ5kiHQ/"},{"title":"第五章 Flink 流处理 API","content":"Environment getExecutionEnvironment 创建一个执行环境， 表示当前执行程序的上下文。 如果程序是独立调用的， 则 此方法返回本地执行环境； 如果从命令行客户端调用程序以提交到集群， 则此方法 返回此集群的执行环境， 也就是说， getExecutionEnvironment 会根据查询运行的方 式决定返回什么样的运行环境， 是最常用的一种创建执行环境的方式。 // 批处理环境 ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment() // 流式数据处理环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment() 如果没有设置并行度， 会以 flink-conf.yaml 中的配置为准， 默认是 1。 createLocalEnvironment 返回本地执行环境， 需要在调用时指定默认的并行度。 LocalStreamEnvironment localEnvironment = StreamExecutionEnvironment.createLocalEnvironment(1); createRemoteEnvironment 返回集群执行环境， 将 Jar 提交到远程服务器。 需要在调用时指定 JobManager的 IP 和端口号， 并指定要在集群中运行的 Jar 包。 final ExecutionEnvironment env = ExecutionEnvironment.createRemoteEnvironment( cluster.getHostname(), cluster.getPort(), config ); scala val env = ExecutionEnvironment.createRemoteEnvironment(&quot;jobmanage-hostname&quot;, 6123,&quot;YOURPATH//wordcount.jar&quot;) setParallelism // 为了打印到控制台的结果不乱序，我们配置全局的并发为1，改变并发对结果正确性没有影响 env.setParallelism(1); Source 从集合读取数据 package myflink; import lombok.*; //传感器温度读数的数据类型 @Data @NoArgsConstructor @AllArgsConstructor @Builder @ToString public class SensorReading { //属性 id,时间戳,温度值 private String id; private Long timestamp; private Double temperature; } public class SourceReading_Collection { public static void main(String[] args) throws Exception { //创建执行环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); //设置并行调度 //env.setParallelism(1); //从集合中读取数据 //属性 id,时间戳,温度值 DataStream&lt;SensorReading&gt; dataStream = env.fromCollection(Arrays.asList(new SensorReading(&quot;sensor_1&quot;, 1537718199L, 35.8), new SensorReading(&quot;sensor_6&quot;, 1547718201L, 15.4), new SensorReading(&quot;sensor_7&quot;, 1547718202L, 6.7), new SensorReading(&quot;sensor_10&quot;, 1547718205L, 38.1))); DataStream&lt;Integer&gt; integerDataStream = env.fromElements(1, 2, 4, 67, 189); //打印输出 dataStream.print(&quot;data&quot;); integerDataStream.print(&quot;int&quot;); //执行 Flink的jobName env.execute(&quot;SensorReading&quot;); } } 从文件读取数据 public class SourceReading_File { public static void main(String[] args) throws Exception { //创建执行环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); //读取文件 DataStream&lt;String&gt; dataStream = env.readTextFile(filePath,charsetName); //打印输出 dataStream.print(&quot;data&quot;); //执行 Flink的jobName env.execute(&quot;SensorReading&quot;); } } 以 kafka 消息队列的数据作为来源 需要引入 kafka 连接器的依赖： pom.xml &lt;!-- https://mvnrepository.com/artifact/org.apache.flink/flink-connector-kafka-0.11 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-kafka-0.11_2.12&lt;/artifactId&gt; &lt;version&gt;1.10.1&lt;/version&gt; &lt;/dependency&gt; //1. 创建一个Topic名为“test20201217”的主题 kafka-topics.bat --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test20201217 //2. 创建producer(生产者)，生产主题的消息 kafka-console-producer.bat --broker-list localhost:9092 --topic test20201217 //3. 创建consumer(消费者)，消费主题消息 kafka-console-consumer.bat --bootstrap-server localhost:9092 --topic test20201217 具体代码如下： /** * kafkaSource * * 從指定的offset出消费kafka */ public class StreamingKafkaSource { public static void main(String[] args) throws Exception { // 创建流处理的执行环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 配置KafKa //配置KafKa和Zookeeper的ip和端口 Properties properties = new Properties(); properties.setProperty(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;); properties.setProperty(&quot;zookeeper.connect&quot;, &quot;localhost:2181&quot;); properties.setProperty(&quot;group.id&quot;, &quot;consumer-group&quot;); //将kafka和zookeeper配置信息加载到Flink的执行环境当中StreamExecutionEnvironment FlinkKafkaConsumer011&lt;String&gt; myConsumer = new FlinkKafkaConsumer011&lt;String&gt;(&quot;test20201217&quot;, new SimpleStringSchema(), properties); //添加数据源，此处选用数据流的方式，将KafKa中的数据转换成Flink的DataStream类型 DataStream&lt;String&gt; stream = env.addSource(myConsumer); //打印输出 stream.print(); //执行Job，Flink执行环境必须要有job的执行步骤，而以上的整个过程就是一个Job env.execute(&quot;kafka sink test&quot;); } } 自定义 Source 除了以上的 source 数据来源， 我们还可以自定义 source。 需要做的， 只是传入一个 SourceFunction 就可以。 具体调用如下： env.addSource( new MySensorSource() ) 我们希望可以随机生成传感器数据， MySensorSource 具体的代码实现如下： import org.apache.flink.api.common.functions.MapFunction; import org.apache.flink.api.java.tuple.Tuple; import org.apache.flink.api.java.tuple.Tuple2; import org.apache.flink.streaming.api.datastream.DataStreamSource; import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator; import org.apache.flink.streaming.api.datastream.WindowedStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.source.SourceFunction; import org.apache.flink.streaming.api.functions.windowing.WindowFunction; import org.apache.flink.streaming.api.windowing.time.Time; import org.apache.flink.streaming.api.windowing.windows.TimeWindow; import org.apache.flink.util.Collector; import org.apache.log4j.Level; import org.apache.log4j.Logger; import org.apache.lucene.analysis.CachingTokenFilter; import java.util.Random; public class MySelfSourceTest01 { public static void main(String[] args) { Logger.getLogger(&quot;org&quot;).setLevel(Level.OFF); StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStreamSource&lt;String&gt; dataStreamSource = env.addSource(new SourceFunction&lt;String&gt;() { @Override public void run(SourceContext&lt;String&gt; ctx) throws Exception { Random random = new Random(); // 循环可以不停的读取静态数据 while (true) { int nextInt = random.nextInt(100); ctx.collect(&quot;random : &quot; + nextInt); Thread.sleep(1000); } } @Override public void cancel() { } }); WindowedStream&lt;Tuple2&lt;String, Integer&gt;, Tuple, TimeWindow&gt; window = dataStreamSource.map(new MapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() { @Override public Tuple2&lt;String, Integer&gt; map(String value) throws Exception { String[] sps = value.split(&quot;:&quot;); return new Tuple2&lt;&gt;(value, Integer.parseInt(sps[1].trim())); } }).keyBy(0).timeWindow(Time.seconds(5)); SingleOutputStreamOperator&lt;String&gt; apply = window.apply(new WindowFunction&lt;Tuple2&lt;String, Integer&gt;, String, Tuple, TimeWindow&gt;() { @Override public void apply(Tuple tuple, TimeWindow window, Iterable&lt;Tuple2&lt;String, Integer&gt;&gt; input, Collector&lt;String&gt; out) throws Exception { input.forEach(x -&gt; { System.out.println(&quot;apply function -&gt; &quot; + x.f0); out.collect(x.f0); }); } }); apply.print(); try { env.execute(&quot;myself_source_test01&quot;); } catch (Exception e) { e.printStackTrace(); } } } Transform 转换算子 map stream.map { x =&gt; x * 2 } // 1.map.把string转化成长度输出 //参数T R //T就是传的数据，什么类型都行 //R就是返回的类型 DataStream&lt;Integer&gt; mapStream = stringDataStream.map(new MapFunction&lt;String, Integer&gt;() { @Override public Integer map(String value) throws Exception { return value.length(); } }); flatMap flatMap 的函数签名： def flatMap[A,B](as: List[A])(f: A ⇒ List[B]): List[B] 例如: flatMap(List(1,2,3))(i ⇒ List(i,i)) 结果是 List(1,1,2,2,3,3), 而 List(&quot;a b&quot;, &quot;c d&quot;).flatMap(line ⇒ line.split(&quot; &quot;)) 结果是 List(a, b, c, d)。 stream.flatMap{ x =&gt; x.split(&quot; &quot;) } //2、flatmap,按照逗号分隔 DataStream&lt;String&gt; flatMapStream = stringDataStream.flatMap(new FlatMapFunction&lt;String, String&gt;() { @Override public void flatMap(String value, Collector&lt;String&gt; out) throws Exception { String[] fields = value.split(&quot;,&quot;); for(String field:fields) out.collect(field); } }); Filter stream.filter{ x =&gt; x == 1 } //3.filter,筛选sensor_1开头的的id对应的数据 DataStream&lt;String&gt; filterStream = stringDataStream.filter(new FilterFunction&lt;String&gt;() { @Override public boolean filter(String value) throws Exception { return value.startsWith(&quot;sensor_1&quot;); } }); KeyBy DataStream → KeyedStream： 逻辑地将一个流拆分成不相交的分区， 每个分区包含具有相同 key 的元素， 在内部以 hash 的形式实现的。 DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; windowCounts = text .flatMap(new FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() { @Override public void flatMap(String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out) { for (String word : value.split(&quot;\\\\s&quot;)) { out.collect(Tuple2.of(word, 1)); } } }) //按Tuple2的第一个属性进行分区 .keyBy(0) //或者根据对象属性进行分区 KeyedStream&lt;SensorReading, String&gt; tempkeyedStream = mapDataStream.keyBy(mpdata -&gt; mpdata.getId()); 滚动聚合算子（Rolling Aggregation） 这些算子可以针对 KeyedStream 的每一个支流做聚合。 sum() min() max() minBy() maxBy() Reduce KeyedStream → DataStream： 一个分组数据流的聚合操作， 合并当前的元素和上次聚合的结果， 产生一个新的值， 返回的流中包含每一次聚合的结果， 而不是只返回最后一次聚合的最终结果。 DataStream&lt;String&gt; stream2 = env.readTextFile(&quot;YOUR_PATH\\\\sensor.txt&quot;) .map( data =&gt; { String[] dataArray = data.split(&quot;,&quot;) SensorReading(dataArray(0).trim, dataArray(1).trim.toLong, dataArray(2).trim.toDouble) }) .keyBy(&quot;id&quot;) .reduce( (x, y) =&gt; SensorReading(x.id, x.timestamp + 1, y.temperature) ) Split 和 Select Split 图 Split DataStream → SplitStream： 根据某些特征把一个 DataStream 拆分成两个或者 多个 DataStream。 Select 图 Select SplitStream→ DataStream： 从一个 SplitStream 中获取一个或者多个 DataStream。 需求： 传感器数据按照温度高低（ 以 30 度为界） ， 拆分成两个流。 public static void main(String[] args) throws Exception { final StreamExecutionEnvironment env=StreamExecutionEnvironment.getExecutionEnvironment(); DataStream&lt;Long&gt; input=env.generateSequence(0,10); SplitStream&lt;Long&gt; splitStream = input.split(new OutputSelector&lt;Long&gt;(){ @Override public Iterable&lt;String&gt; select(Long value) { List&lt;String&gt; output = new ArrayList&lt;String&gt;(); if (value % 2 == 0) { output.add(&quot;even&quot;); }else { output.add(&quot;odd&quot;); } return output; } }); //splitStream.print(); DataStream&lt;Long&gt; even = splitStream.select(&quot;even&quot;); DataStream&lt;Long&gt; odd = splitStream.select(&quot;odd&quot;); DataStream&lt;Long&gt; all = splitStream.select(&quot;even&quot;,&quot;odd&quot;); //even.print(); odd.print(); //all.print(); env.execute(); } Connect 和 CoMap 图 Connect 算子 DataStream,DataStream → ConnectedStreams： 连接两个保持他们类型的数 据流， 两个数据流被 Connect 之后， 只是被放在了一个同一个流中， 内部依然保持 各自的数据和形式不发生任何变化， 两个流相互独立。 CoMap,CoFlatMap 图 CoMap/CoFlatMap ConnectedStreams → DataStream： 作用于 ConnectedStreams 上， 功能与 map 和 flatMap 一样， 对 ConnectedStreams 中的每一个 Stream 分别进行 map 和 flatMap 处理。 DataStream warning = high.map( sensorData =&gt; (sensorData.id, sensorData.temperature) ) ConnectedStreams connected = warning.connect(low) DataStream coMap = connected.map( warningData =&gt; (warningData._1, warningData._2, &quot;warning&quot;), lowData =&gt; (lowData.id, &quot;healthy&quot;) ) Union 图 Union DataStream → DataStream： 对两个或者两个以上的 DataStream 进行 union 操 作， 产生一个包含所有 DataStream 元素的新 DataStream。 //合并以后打印 DataStream&lt;StartUpLog&gt; unionStream = appStoreStream.union(otherStream) unionStream.print(&quot;union:::&quot;) Connect 与 Union 区别： Union 之前两个流的类型必须是一样， Connect 可以不一样， 在之后的 coMap 中再去调整成为一样的。 Connect 只能操作两个流， Union 可以操作多个。 支持的数据类型 Flink 流应用程序处理的是以数据对象表示的事件流。 所以在 Flink 内部， 我们 需要能够处理这些对象。 它们需要被序列化和反序列化， 以便通过网络传送它们； 或者从状态后端、 检查点和保存点读取它们。 为了有效地做到这一点， Flink 需要明 确知道应用程序所处理的数据类型。 Flink 使用类型信息的概念来表示数据类型， 并 为每个数据类型生成特定的序列化器、 反序列化器和比较器。 Flink 还具有一个类型提取系统， 该系统分析函数的输入和返回类型， 以自动获 取类型信息， 从而获得序列化器和反序列化器。 但是， 在某些情况下， 例如 lambda 函数或泛型类型， 需要显式地提供类型信息， 才能使应用程序正常工作或提高其性 能。 Flink 支持 Java 和 Scala 中所有常见数据类型。 使用最广泛的类型有以下几种。 基础数据类型 Flink 支持所有的 Java 和 Scala 基础数据类型， Int, Double, Long, String, …​ DataStream&lt;Long&gt; numbers = env.fromElements(1L, 2L, 3L, 4L) numbers.map( n =&gt; n + 1 ) Java 和 Scala 元组（Tuples） DataStream&lt;String, Integer&gt; persons= env.fromElements( (&quot;Adam&quot;, 17), (&quot;Sarah&quot;, 23) ) persons.filter(p =&gt; p._2 &gt; 18) Scala 样例类（case classes） case class Person(name: String, age: Int) val persons: DataStream[Person] = env.fromElements( Person(&quot;Adam&quot;, 17), Person(&quot;Sarah&quot;, 23) ) persons.filter(p =&gt; p.age &gt; 18) Java 简单对象（POJOs） public class Person { public String name; public int age; public Person() {} public Person(String name, int age) { this.name = name; this.age = age; } } DataStream&lt;Person&gt; persons = env.fromElements( new Person(&quot;Alex&quot;, 42), new Person(&quot;Wendy&quot;, 23)); 其它（Arrays, Lists, Maps, Enums, 等等） Flink 对 Java 和 Scala 中的一些特殊目的的类型也都是支持的， 比如 Java 的 ArrayList， HashMap， Enum 等等。 实现 UDF 函数——更细粒度的控制流 函数类（Function Classes） Flink 暴露了所有 udf 函数的接口(实现方式为接口或者抽象类)。 例如 MapFunction, FilterFunction, ProcessFunction 等等。 下面例子实现了 FilterFunction 接口： public class CustomFilterFunction implements FilterFunction&lt;SensorReading&gt; { @Override public boolean filter(SensorReading sensorReading) throws Exception { return sensorReading.temperature&gt;30.0; } } public static void main(String[] args) throws Exception { // 创建流处理的执行环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 从文件中读取数据 String inputPath = &quot;F:\\\\Projects\\\\BigData\\\\Flink\\\\FlinkTutorial\\\\src\\\\main\\\\resources\\\\sensor.txt&quot;; // 获取数据 DataStreamSource&lt;String&gt; dataStream = env.readTextFile(inputPath); // 1、先转换成SensorReading类型（简单转换操作） DataStream&lt;SensorReading&gt; stream = dataStream.map(new MapFunction&lt;String, SensorReading&gt;() { @Override public SensorReading map(String data) throws Exception { String[] arr = data.split(&quot;,&quot;); return new SensorReading(arr[0], arr[1], Double.valueOf(arr[2].toString())); } }); // 调用自定义CustomFilterFunction类的，实现过滤 DataStream&lt;SensorReading&gt; dataStreamFilter = stream.filter(new CustomFilterFunction()); dataStreamFilter .print(&quot;CustomFilterFunction&quot;); env.execute(&quot;Function test&quot;); } 还可以将函数实现成匿名类 DataStream&lt;SensorReading&gt; dataStreamFilter = stream.filter(new FilterFunction&lt;SensorReading&gt;() { @Override public boolean filter(SensorReading sensorReading) throws Exception { return sensorReading.temperature&gt;30.0; } ); 我们 filter 的字符串&quot;flink&quot;还可以当作参数传进去。 DataStream&lt;String&gt; tweets = ... DataStream&lt;String&gt; flinkTweets = tweets.filter(new KeywordFilter(&quot;flink&quot;)) public class KeywordFilter(String keyWord) implements FilterFunction&lt;String&gt; { @Override public boolean filter(String value) throws Exception { return value.contains(keyWord) } } 匿名函数（Lambda Functions） DataStream&lt;String&gt; tweets = ... DataStream&lt;String&gt; flinkTweets = tweets.filter((value) -&gt;value.contains(&quot;flink&quot;)) 富函数（Rich Functions） “ 富函数” 是 DataStream API 提供的一个函数类的接口， 所有 Flink 函数类都 有其 Rich 版本。 它与常规函数的不同在于， 可以获取运行环境的上下文， 并拥有一 些生命周期方法， 所以可以实现更复杂的功能。 RichMapFunction RichFlatMapFunction RichFilterFunction …​ Rich Function 有一个生命周期的概念。 典型的生命周期方法有： open()方法是 rich function 的初始化方法， 当一个算子例如 map 或者 filter 被调用之前 open()会被调用。 close()方法是生命周期中的最后一个调用的方法， 做一些清理工作。 getRuntimeContext()方法提供了函数的 RuntimeContext 的一些信息， 例如函 数执行的并行度， 任务的名字， 以及 state 状态 public class MyFlatMap extends RichFlatMapFunction&lt;Tuple2&lt;Integer, Integer&gt;, Tuple2&lt;Integer, Integer&gt;&gt; { Integer subTaskIndex = 0 StreamingRuntimeContext context = (StreamingRuntimeContext) getRuntimeContext() public void open(Configuration parameters) throws Exception { subTaskIndex = context.getIndexOfThisSubtask // 以下可以做一些初始化工作， 例如建立一个和 HDFS 的连接 } @Override public void flatMap(Tuple2&lt;Integer, Integer&gt; integerIntegerTuple2, Collector&lt;Tuple2&lt;Integer, Integer&gt;&gt; collector) throws Exception { if (in % 2 == subTaskIndex) { out.collect((subTaskIndex, in)) } } public void close() throws Exception { // 以下做一些清理工作， 例如断开和 HDFS 的连接。 } } Sink Flink 没有类似于 spark 中 foreach 方法， 让用户进行迭代的操作。 虽有对外的 输出操作都要利用 Sink 完成。 最后通过类似如下方式完成整个任务最终输出操作。 stream.addSink(new MySink(xxxx)) 官方提供了一部分的框架的 sink。 除此以外， 需要用户自定义实现 sink。 Kafka pom.xml &lt;!-- https://mvnrepository.com/artifact/org.apache.flink/flink-connector-kafka-0.11 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-kafka-0.11_2.12&lt;/artifactId&gt; &lt;version&gt;1.10.1&lt;/version&gt; &lt;/dependency&gt; 主函数中添加 sink： DataStream&lt;String&gt; union = high.union(low).map(item-&gt;item.temperature.toString) union.addSink(new FlinkKafkaProducer&lt;String&gt;(&quot;localhost:9092&quot;, &quot;test&quot;, new SimpleStringSchema())) Redis pom.xml &lt;!-- https://mvnrepository.com/artifact/org.apache.bahir/flink-connector-redis --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.bahir&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-redis_2.11&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;/dependency&gt; 定义一个 redis 的 mapper 类， 用于定义保存到 redis 时调用的命令： public class MyRedisMapper extends RedisMapper&lt;SensorReading&gt;{ public RedisCommandDescription getCommandDescription{ new RedisCommandDescription(RedisCommand.HSET, &quot;sensor_temperature&quot;) } public String getValueFromData(SensorReading t){ t.temperature.toString String getKeyFromData(SensorReading t) { return t.id } } } 在主函数中调用： dataStream.addSink( new RedisSink&lt;SensorReading&gt;( new FlinkJedisPoolConfig.Builder().setHost(&quot;localhost&quot;).setPort(6379).build(), new MyRedisMapper) ) Elasticsearch pom.xml &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-elasticsearch6_2.12&lt;/artifactId&gt; &lt;version&gt;1.10.1&lt;/version&gt; &lt;/dependency&gt; 在主函数中调用： List httpHosts = new ArrayList&lt;HttpHost&gt;() httpHosts.add(new HttpHost(&quot;localhost&quot;, 9200)) DataStream esSinkBuilder = new ElasticsearchSink.Builder&lt;SensorReading&gt;( httpHosts,new ElasticsearchSinkFunction&lt;SensorReading&gt; { public Unit process(SensorReading t, RuntimeContext runtimeContext, RequestIndexer requestIndexer ) { println(&quot;saving data: &quot; + t) map json = new util.HashMap&lt;String, String&gt;() json.put(&quot;data&quot;, t.toString) IndexRequest indexRequest = Requests.indexRequest().index(&quot;sensor&quot;).`type`(&quot;readingData&quot;).source(json) requestIndexer.add(indexRequest) println(&quot;saved successfully&quot;) } } ) dataStream.addSink( esSinkBuilder.build() ) JDBC 自定义 sink &lt;!-- https://mvnrepository.com/artifact/mysql/mysql-connector-java --&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.44&lt;/version&gt; &lt;/dependency&gt; 添加 MyJdbcSink public class MyJdbcSink() extends RichSinkFunction&lt;SensorReading&gt;{ Connection conn; PreparedStatement insertStmt; PreparedStatement updateStmt; // open 主要是创建连接 public Unit open(Configuration parameters) = { super.open(parameters) conn = DriverManager.getConnection(&quot;jdbc:mysql://localhost:3306/test&quot;, &quot;root&quot;, &quot;123456&quot;) insertStmt = conn.prepareStatement(&quot;INSERT INTO temperatures (sensor, temp) VALUES (?, ?)&quot;) updateStmt = conn.prepareStatement(&quot;UPDATE temperatures SET temp = ? WHERE sensor = ?&quot;) } //调用连接， 执行 sql public Unit invoke(SensorReading value , SinkFunction.Context[] context）{ updateStmt.setDouble(1, value.temperature) updateStmt.setString(2, value.id) updateStmt.execute() if (updateStmt.getUpdateCount == 0) { insertStmt.setString(1, value.id) insertStmt.setDouble(2, value.temperature) insertStmt.execute() } } public Unit close() { insertStmt.close() updateStmt.close() conn.close() } } 在 main 方法中增加， 把明细保存到 mysql 中 dataStream.addSink(new MyJdbcSink()) ","link":"https://tianxiawuhao.github.io/lvf921hnA/"},{"title":"第四章 Flink 运行架构","content":"Flink 运行时的组件 Flink 运行时架构主要包括四个不同的组件， 它们会在运行流处理应用程序时协同工作： 作业管理器（JobManager） 、 资源管理器（ResourceManager） 、 任务管理器（TaskManager），以及分发器（Dispatcher） 。 因为 Flink 是用 Java 和 Scala 实现的， 所以所有组件都会运行在Java 虚拟机上。 每个组件的职责如下： 1. 作业管理器（JobManager） 控制一个应用程序执行的主进程， 也就是说， 每个应用程序都会被一个不同的 JobManager 所控制执行。 JobManager 会先接收到要执行的应用程序， 这个应用程序会包括： 作业图（JobGraph） 、 逻辑数据流图（logical dataflow graph） 和打包了所有的类、 库和其它 资源的 JAR 包。 JobManager 会把 JobGraph 转换成一个物理层面的数据流图， 这个图被叫做 “执行图” （ExecutionGraph） ， 包含了所有可以并发执行的任务。 JobManager 会向资源管 理器（ResourceManager） 请求执行任务必要的资源， 也就是任务管理器（TaskManager） 上 的插槽（ slot） 。 一旦它获取到了足够的资源， 就会将执行图分发到真正运行它们的TaskManager 上。 而在运行过程中， JobManager 会负责所有需要中央协调的操作， 比如说检 查点（checkpoints） 的协调。 2. 资源管理器（ResourceManager） 主要负责管理任务管理器（TaskManager） 的插槽（slot） ， TaskManger 插槽是 Flink 中 定义的处理资源单元。 Flink 为不同的环境和资源管理工具提供了不同资源管理器， 比如 YARN、 Mesos、 K8s， 以及 standalone 部署。 当 JobManager 申请插槽资源时， ResourceManager会将有空闲插槽的 TaskManager 分配给 JobManager。 如果 ResourceManager 没有足够的插槽来满足 JobManager 的请求， 它还可以向资源提供平台发起会话， 以提供启动 TaskManager进程的容器。 另外， ResourceManager 还负责终止空闲的TaskManager， 释放计算资源。 3. 任务管理器（TaskManager） Flink 中的工作进程。 通常在 Flink 中会有多个 TaskManager 运行， 每一个 TaskManager 都包含了一定数量的插槽（slots） 。 插槽的数量限制了 TaskManager 能够执行的任务数量。 启动之后， TaskManager 会向资源管理器注册它的插槽； 收到资源管理器的指令后，TaskManager 就会将一个或者多个插槽提供给 JobManager 调用。 JobManager 就可以向插槽分配任务（tasks） 来执行了。 在执行过程中， 一个 TaskManager 可以跟其它运行同一应用程序的 TaskManager 交换数据。 4. 分发器（Dispatcher） 可以跨作业运行， 它为应用提交提供了 REST 接口。 当一个应用被提交执行时， 分发器 就会启动并将应用移交给一个 JobManager。 由于是 REST 接口， 所以 Dispatcher 可以作为集 群的一个 HTTP 接入点， 这样就能够不受防火墙阻挡。 Dispatcher 也会启动一个 Web UI， 用 来方便地展示和监控作业执行的信息。 Dispatcher 在架构中可能并不是必需的， 这取决于应 用提交运行的方式。 任务提交流程 我们来看看当一个应用提交执行时， Flink 的各个组件是如何交互协作的： 图 任务提交和组件交互流程 上图是从一个较为高层级的视角， 来看应用中各组件的交互协作。 如果部署的集群环境 不同（例如 YARN， Mesos， Kubernetes， standalone 等） ， 其中一些步骤可以被省略， 或是 有些组件会运行在同一个 JVM 进程中。 具体地， 如果我们将 Flink 集群部署到 YARN 上， 那么就会有如下的提交流程： 图 Yarn 模式任务提交流程 Flink 任 务 提 交 后 ， Client 向 HDFS 上 传 Flink 的 Jar 包 和 配 置 ， 之 后 向 YarnResourceManager 提 交 任 务 ， ResourceManager 分 配 Container 资 源 并 通 知 对 应 的NodeManager 启动 ApplicationMaster， ApplicationMaster 启动后加载 Flink 的 Jar 包和配置构建环境， 然后启动 JobManager， 之后 ApplicationMaster 向 ResourceManager申 请 资 源 启 动 TaskManager ， ResourceManager 分 配 Container 资 源 后 ， 由ApplicationMaster 通 知 资 源 所 在 节 点 的 NodeManager 启 动 TaskManager ，NodeManager 加载 Flink 的 Jar 包和配置构建环境并启动 TaskManager， TaskManager启动后向 JobManager 发送心跳包， 并等待 JobManager 向其分配任务。 任务调度原理 图 任务调度原理 客 户 端 不 是 运 行 时 和 程 序 执 行 的 一 部 分 ， 但 它 用 于 准 备 并 发 送 dataflow(JobGraph)给 Master(JobManager)， 然后， 客户端断开连接或者维持连接以 等待接收计算结果。 当 Flink 集 群 启 动 后 ， 首 先 会 启 动 一 个 JobManger 和 一 个 或 多 个 的TaskManager。 由 Client 提交任务给 JobManager， JobManager 再调度任务到各个TaskManager 去执行， 然后 TaskManager 将心跳和统计信息汇报给 JobManager。TaskManager 之间以流的形式进行数据的传输。 上述三者均为独立的 JVM 进程。 Client 为提交 Job 的客户端， 可以是运行在任何机器上（ 与 JobManager 环境连通即可） 。 提交 Job 后， Client 可以结束进程（ Streaming 的任务） ， 也可以不结束并等待结果返回。 JobManager`主 要 负 责 调 度 Job 并 协 调 Task 做 checkpoint ， 职 责 上 很 像Storm 的 Nimbus。 从 Client 处接收到 Job 和 JAR 包等资源后， 会生成优化后的执行计划， 并以 Task 的单元调度到各个 TaskManager 去执行。 TaskManager`在启动的时候就设置好了槽位数（ Slot） ， 每个 slot 能启动一个Task， Task 为线程。 从 JobManager 处接收需要部署的 Task， 部署启动后， 与自己的上游建立 Netty 连接， 接收数据并处理。 TaskManger 与 Slots Flink 中每一个 worker(TaskManager)都是一个 `JVM 进程`， 它可能会在`独立的线 程`上执行一个或多个 subtask。 为了控制一个 worker 能接收多少个 task， worker 通 过 task slot 来进行控制（ 一个 worker 至少有一个 task slot） 。 每个 task slot 表示 TaskManager 拥有资源的`一个固定大小的子集` 。 假如一个 TaskManager 有三个 slot， 那么它会将其管理的内存分成三份给各个 slot。 资源 slot 化意味着一个 subtask 将不需要跟来自其他 job 的 subtask 竞争被管理的内存， 取而 代之的是它将拥有一定数量的内存储备。 需要注意的是， 这里不会涉及到 CPU 的隔 离， slot 目前仅仅用来隔离 task 的受管理的内存。 通过调整 task slot 的数量， 允许用户定义 subtask 之间如何互相隔离。 如果一个 TaskManager 一个 slot， 那将意味着每个 task group 运行在独立的 JVM 中（ 该 JVM 可能是通过一个特定的容器启动的） ， 而一个 TaskManager 多个 slot 意味着更多的 subtask 可以共享同一个 JVM。 而在同一个 JVM 进程中的 task 将共享 TCP 连接（ 基 于多路复用） 和心跳消息。 它们也可能共享数据集和数据结构， 因此这减少了每个 task 的负载。 图 TaskManager 与 Slot 图 子任务共享 默认情况下， Flink 允许子任务共享 slot， 即使它们是不同任务的子任务（ 前提是它们来自同一个 job） 。 这样的结果是， 一个 slot 可以保存作业的整个管道。 Task Slot 是静态的概念， 是指 TaskManager 具有的并发执行能力， 可以通过参数 taskmanager.numberOfTaskSlots 进行配置； 而并行度 parallelism 是动态概念，即 TaskManager 运行程序时实际使用的并发能力， 可以通过参数 parallelism.default 进行配置。 也就是说， 假设一共有 3 个 TaskManager， 每一个 TaskManager 中的分配 3 个TaskSlot， 也就是每个 TaskManager 可以接收 3 个 task， 一共 9 个 TaskSlot， 如果我们设置 parallelism.default=1， 即运行程序默认的并行度为 1， 9 个 TaskSlot 只用了 1个， 有 8 个空闲， 因此， 设置合适的并行度才能提高效率。 程序与数据流（DataFlow） 所有的 Flink 程序都是由三部分组成的： `Source` 、 `Transformation` 和 `Sink`。 Source 负责读取数据源， Transformation 利用各种算子进行处理加工， Sink 负责输出。 在运行时， Flink 上运行的程序会被映射成“ 逻辑数据流” （ dataflows） ， 它包含了这三部分。 每一个 dataflow 以一个或多个 sources 开始以一个或多个 sinks 结束。 dataflow 类似于任意的有向无环图（ DAG） 。 在大部分情况下， 程序中的转换 运算（ transformations） 跟 dataflow 中的算子（ operator） 是一一对应的关系， 但有 时候， 一个 transformation 可能对应多个 operator。 图 程序与数据流 执行图（ExecutionGraph） 由 Flink 程序直接映射成的数据流图是 StreamGraph， 也被称为逻辑流图， 因为它们表示的是计算逻辑的高级视图。 为了执行一个流处理程序， Flink 需要将逻辑流 图转换为物理数据流图（ 也叫执行图） ， 详细说明程序的执行方式。 Flink 中的执行图可以分成四层： StreamGraph -> JobGraph -> ExecutionGraph -> 物理执行图。 StreamGraph： 是根据用户通过 Stream API 编写的代码生成的最初的图。 用来表示程序的拓扑结构。 JobGraph： StreamGraph 经过优化后生成了 JobGraph， 提交给 JobManager 的数据结构。 主要的优化为， 将多个符合条件的节点 chain 在一起作为一个节点， 这样可以减少数据在节点之间流动所需要的序列化/反序列化/传输消耗。 ExecutionGraph ： JobManager 根 据 JobGraph 生 成 ExecutionGraph 。ExecutionGraph 是 JobGraph 的并行化版本， 是调度层最核心的数据结构。 物理执行图： JobManager 根据 ExecutionGraph 对 Job 进行调度后， 在各个TaskManager 上部署 Task 后形成的“ 图” ， 并不是一个具体的数据结构。 并行度（Parallelism） Flink 程序的执行具有并行、 分布式的特性。 在执行过程中， 一个流（ stream） 包含一个或多个分区（ stream partition） ， 而 每一个算子（ operator） 可以包含一个或多个子任务（ operator subtask） ， 这些子任 务在不同的线程、 不同的物理机或不同的容器中彼此互不依赖地执行。 一个特定算子的子任务（ subtask） 的个数被称之为其并行度（ parallelism） 。 一般情况下， 一个流程序的并行度， 可以认为就是其所有算子中最大的并行度。 一 个程序中， 不同的算子可能具有不同的并行度。 图 并行数据流 Stream 在算子之间传输数据的形式可以是 one-to-one(forwarding)的模式也可以是 redistributing 的模式， 具体是哪一种形式， 取决于算子的种类。 One-to-one： stream(比如在 source 和 map operator 之间)维护着分区以及元素的 顺序。 那意味着 map 算子的子任务看到的元素的个数以及顺序跟 source 算子的子 任务生产的元素的个数、 顺序相同， map、 fliter、 flatMap 等算子都是 one-to-one 的 对应关系。 类似于 spark 中的窄依赖 Redistributing： stream(map()跟 keyBy/window 之间或者 keyBy/window 跟 sink之间)的分区会发生改变。 每一个算子的子任务依据所选择的 transformation 发送数据到不同的目标任务。 例如， keyBy() 基于 hashCode 重分区、 broadcast 和 rebalance会随机重新分区， 这些算子都会引起 redistribute 过程， 而 redistribute 过程就类似于Spark 中的 shuffle 过程。 类似于 spark 中的宽依赖 任务链（Operator Chains） 相同并行度的 one to one 操作， Flink 这样相连的算子链接在一起形成一个 task，原来的算子成为里面的一部分。 将算子链接成 task 是非常有效的优化： 它能减少线程之间的切换和基于缓存区的数据交换， 在减少时延的同时提升吞吐量。 链接的行 为可以在编程 API 中进行指定。 图 task 与 operator chains ","link":"https://tianxiawuhao.github.io/4_EhGqVjY/"},{"title":"第三章 Flink 部署","content":"Standalone 模式 安装 解压缩 flink-1.10.1-bin-scala_2.12.tgz， 进入 conf 目录中。 修改 flink/conf/flink-conf.yaml 文件： 修改 /conf/slaves 文件： 分发给另外两台机子： 启动 访问 http://localhost:8081 可以对 flink 集群和任务进行监控管理。 提交任务 准备数据文件（ 如果需要） 把含数据文件的文件夹， 分发到 taskmanage 机器中 如 果 从 文 件 中 读 取 数 据 ， 由 于 是 从 本 地 磁 盘 读 取 ， 实 际 任 务 会 被 分 发 到taskmanage 的机器中， 所以要把目标文件分发。 执行程序 ./flink run -c com.atguigu.wc.StreamWordCount –p 2 FlinkTutorial-1.0-SNAPSHOT-jar-with-dependencies.jar --host lcoalhost –port 7777 到目标文件夹中查看计算结果 注 意 ： 如 果 计 算 结 果 输 出 到 文 件 ， 会 保 存 到 taskmanage 的 机 器 下 ， 不 会 在jobmanage 下。 在 webui 控制台查看计算过程 Yarn 模式 以 Yarn 模式部署 Flink 任务时， 要求 Flink 是有 Hadoop 支持的版本， Hadoop环境需要保证版本在 2.2 以上， 并且集群中安装有 HDFS 服务 Flink on Yarn Flink 提供了两种在 yarn 上运行的模式， 分别为 Session-Cluster 和 Per-Job-Cluster模式。 Session-cluster 模式： Session-Cluster 模式需要先启动集群， 然后再提交作业， 接着会向 yarn 申请一块空间后， 资源永远保持不变。 如果资源满了， 下一个作业就无法提交， 只能等到 yarn 中的其中一个作业执行完成后， 释放了资源， 下个作业才会正常提交。 所有作 业共享 Dispatcher 和 ResourceManager； 共享资源； 适合规模小执行时间短的作业。 在 yarn 中初始化一个 flink 集群， 开辟指定的资源， 以后提交任务都向这里提交。 这个 flink 集群会常驻在 yarn 集群中， 除非手工停止。 Per-Job-Cluster 模式： 一个 Job 会对应一个集群， 每提交一个作业会根据自身的情况， 都会单独向 yarn申请资源， 直到作业执行完成， 一个作业的失败与否并不会影响下一个作业的正常 提交和运行。 独享 Dispatcher 和 ResourceManager， 按需接受资源申请； 适合规模大 长时间运行的作业。 每次提交都会创建一个新的 flink 集群， 任务之间互相独立， 互不影响， 方便管理。 任务执行完成之后创建的集群也会消失。 Session Cluster 启动 hadoop 集群（ 略） 启动 yarn-session ./yarn-session.sh -n 2 -s 2 -jm 1024 -tm 1024 -nm test -d 其中： -n(--container)： TaskManager 的数量。 -s(--slots)： 每个 TaskManager 的 slot 数量， 默认一个 slot 一个 core， 默认每个 taskmanager 的 slot 的个数为 1， 有时可以多一些 taskmanager， 做冗余。 -jm： JobManager 的内存（ 单位 MB)。 -tm： 每个 taskmanager 的内存（ 单位 MB)。 -nm： yarn 的 appName(现在 yarn 的 ui 上的名字)。 -d： 后台执行。 执行任务 ./flink run -c com.atguigu.wc.StreamWordCount FlinkTutorial-1.0-SNAPSHOT-jar-with-dependencies.jar --host lcoalhost –port 7777 去 yarn 控制台查看任务状态 取消 yarn-session yarn application --kill application_1577588252906_0001 3.2.2 Per Job Cluster 启动 hadoop 集群（ 略） 不启动 yarn-session， 直接执行 job ./flink run –m yarn-cluster -c com.atguigu.wc.StreamWordCount FlinkTutorial-1.0-SNAPSHOT-jar-with-dependencies.jar --host lcoalhost –port 7777 Kubernetes 部署 容器化部署时目前业界很流行的一项技术， 基于 Docker 镜像运行能够让用户更加 方 便 地 对 应 用 进 行 管 理 和 运 维 。 容 器 管 理 工 具 中 最 为 流 行 的 就 是 Kubernetes（ k8s） ， 而 Flink 也在最近的版本中支持了 k8s 部署模式。 搭建 Kubernetes 集群（ 略） 配置各组件的 yaml 文件 在 k8s 上构建 Flink Session Cluster， 需要将 Flink 集群的组件对应的 docker 镜像分别在 k8s 上启动， 包括 JobManager、 TaskManager、 JobManagerService 三个镜像服务。 每个镜像服务都可以从中央镜像仓库中获取。 启动 Flink Session Cluster // 启动 jobmanager-service 服务 kubectl create -f jobmanager-service.yaml // 启动 jobmanager-deployment 服务 kubectl create -f jobmanager-deployment.yaml // 启动 taskmanager-deployment 服务 kubectl create -f taskmanager-deployment.yaml 访问 Flink UI 页面 集群启动后， 就可以通过 JobManagerServicers 中配置的 WebUI 端口， 用浏览器输入以下 url 来访问 Flink UI 页面了： http://{JobManagerHost:Port}/api/v1/namespaces/default/services/flink-jobmanager:ui/proxy ","link":"https://tianxiawuhao.github.io/aPsJFkgVZ/"},{"title":"第二章 快速上手","content":"搭建 maven 工程 FlinkTutorial pom 文件 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;org.example&lt;/groupId&gt; &lt;artifactId&gt;flink-test&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;properties&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;scala.version&gt;2.11&lt;/scala.version&gt; &lt;flink.version&gt;1.10.0&lt;/flink.version&gt; &lt;hadoop.version&gt;2.7.7&lt;/hadoop.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-java&lt;/artifactId&gt; &lt;version&gt;${flink.version}&lt;/version&gt; &lt;!--&lt;scope&gt;provided&lt;/scope&gt;--&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-java_${scala.version}&lt;/artifactId&gt; &lt;version&gt;${flink.version}&lt;/version&gt; &lt;!--&lt;scope&gt;provided&lt;/scope&gt;--&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/project&gt; 批处理 wordcount WordCount 程序是大数据处理框架的入门程序，俗称“单词计数”。用来统计一段文字每个单词的出现次数，该程序主要分为两个部分：一部分是将文字拆分成单词；另一部分是单词进行分组计数并打印输出结果。 public static void main(String[] args) throws Exception { // 创建Flink运行的上下文环境 final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); // 创建DataSet，这里我们的输入是一行一行的文本 DataSet&lt;String&gt; text = env.fromElements( &quot;Flink Spark Storm&quot;, &quot;Flink Flink Flink&quot;, &quot;Spark Spark Spark&quot;, &quot;Storm Storm Storm&quot; ); // 通过Flink内置的转换函数进行计算 DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; counts = text.flatMap(new LineSplitter()) .groupBy(0) .sum(1); //结果打印 counts.printToErr(); } public static final class LineSplitter implements FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt; { @Override public void flatMap(String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out) { // 将文本分割 String[] tokens = value.toLowerCase().split(&quot;\\\\W+&quot;); for (String token : tokens) { if (token.length() &gt; 0) { out.collect(new Tuple2&lt;String, Integer&gt;(token, 1)); } } } } 实现的整个过程中分为以下几个步骤。 首先，我们需要创建 Flink 的上下文运行环境： 复制ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); 然后，使用 fromElements 函数创建一个 DataSet 对象，该对象中包含了我们的输入，使用 FlatMap、GroupBy、SUM 函数进行转换。 最后，直接在控制台打印输出。 我们可以直接右键运行一下 main 方法，在控制台会出现我们打印的计算结果： 流处理 wordcount 为了模仿一个流式计算环境，我们选择监听一个本地的 Socket 端口，并且使用 Flink 中的滚动窗口，每 5 秒打印一次计算结果。代码如下： public class StreamingJob { public static void main(String[] args) throws Exception { // 创建Flink的流式计算环境 final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 监听本地9000端口 DataStream&lt;String&gt; text = env.socketTextStream(&quot;127.0.0.1&quot;, 9000, &quot;\\n&quot;); // 将接收的数据进行拆分，分组，窗口计算并且进行聚合输出 DataStream&lt;WordWithCount&gt; windowCounts = text .flatMap(new FlatMapFunction&lt;String, WordWithCount&gt;() { @Override public void flatMap(String value, Collector&lt;WordWithCount&gt; out) { for (String word : value.split(&quot;\\\\s&quot;)) { out.collect(new WordWithCount(word, 1L)); } } }) .keyBy(&quot;word&quot;) .timeWindow(Time.seconds(5), Time.seconds(1)) .reduce(new ReduceFunction&lt;WordWithCount&gt;() { @Override public WordWithCount reduce(WordWithCount a, WordWithCount b) { return new WordWithCount(a.word, a.count + b.count); } }); // 打印结果 windowCounts.print().setParallelism(1); env.execute(&quot;Socket Window WordCount&quot;); } // Data type for words with count public static class WordWithCount { public String word; public long count; public WordWithCount() {} public WordWithCount(String word, long count) { this.word = word; this.count = count; } @Override public String toString() { return word + &quot; : &quot; + count; } } } 整个流式计算的过程分为以下几步。 首先创建一个流式计算环境： 复制StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); 然后进行监听本地 9000 端口，将接收的数据进行拆分、分组、窗口计算并且进行聚合输出。代码中使用了 Flink 的窗口函数，我们在后面的课程中将详细讲解。 我们在本地使用 netcat 命令启动一个端口： nc -lk 9000 然后直接运行我们的 main 方法： 测试——在 linux 系统中用 netcat 命令进行发送测试。 在 nc 中输入： $ nc -lk 9000 Flink Flink Flink Flink Spark Storm 可以在控制台看到： Flink : 4 Spark : 1 Storm : 1 ","link":"https://tianxiawuhao.github.io/pQQ9j5lFD/"},{"title":"第一章 Flink 简介","content":"初识 Flink Flink 起源于 Stratosphere 项目， Stratosphere 是在 2010~2014 年由 3 所地处柏林的大学和欧洲的一些其他的大学共同进行的研究项目， 2014 年 4 月 Stratosphere 的代 码 被 复 制 并 捐 赠 给 了 Apache 软 件 基 金 会 ， 参 加 这 个 孵 化 项 目 的 初 始 成 员 是Stratosphere 系统的核心开发人员， 2014 年 12 月， Flink 一跃成为 Apache 软件基金会的顶级项目。 在德语中， Flink 一词表示快速和灵巧， 项目采用一只松鼠的彩色图案作为 logo，这不仅是因为松鼠具有快速和灵巧的特点， 还因为柏林的松鼠有一种迷人的红棕色， 而 Flink 的松鼠 logo 拥有可爱的尾巴， 尾巴的颜色与 Apache 软件基金会的 logo 颜 色相呼应， 也就是说， 这是一只 Apache 风格的松鼠。 Flink 项目的理念是：“ Apache Flink 是为分布式、 高性能、 随时可用以及准确的流处理应用程序打造的开源流处理框架”。 Apache Flink 是一个框架和分布式处理引擎， 用于对无界和有界数据流进行有状态计算。 Flink 被设计在所有常见的集群环境中运行， 以内存执行速度和任意规模 来执行计算。 Flink 的重要特点 事件驱动型(Event-driven) 事件驱动型应用是一类具有状态的应用， 它从一个或多个事件流提取数据， 并根据到来的事件触发计算、 状态更新或其他外部动作。 比较典型的就是以 kafka 为代表的消息队列几乎都是事件驱动型应用。 与之不同的就是 SparkStreaming 微批次， 如图： 事件驱动型： 流与批的世界观 批处理的特点是有界、 持久、 大量， 非常适合需要访问全套记录才能完成的计算工作， 一般用于离线统计。 流处理的特点是无界、 实时, 无需针对整个数据集执行操作， 而是对通过系统传输的每个数据项执行操作， 一般用于实时统计。在 spark 的世界观中， 一切都是由批次组成的， 离线数据是一个大批次， 而实时数据是由一个一个无限的小批次组成的。而在 flink 的世界观中， 一切都是由流组成的， 离线数据是有界限的流， 实时数据是一个没有界限的流， 这就是所谓的有界流和无界流。 无界数据流： 无界数据流有一个开始但是没有结束， 它们不会在生成时终止并提供数据， 必须连续处理无界流， 也就是说必须在获取后立即处理 event。 对于无界 数据流我们无法等待所有数据都到达， 因为输入是无界的， 并且在任何时间点都不会完成。 处理无界数据通常要求以特定顺序（ 例如事件发生的顺序） 获取 event， 以便能够推断结果完整性。 有界数据流： 有界数据流有明确定义的开始和结束， 可以在执行任何计算之前通过获取所有数据来处理有界流， 处理有界流不需要有序获取， 因为可以始终对有 界数据集进行排序， 有界流的处理也称为批处理。 这种以流为世界观的架构， 获得的最大好处就是具有极低的延迟。 分层 api 最底层级的抽象仅仅提供了有状态流， 它将通过过程函数（ Process Function）被嵌入到 DataStream API 中。 底层过程函数（ Process Function） 与 DataStream API相集成， 使其可以对某些特定的操作进行底层的抽象， 它允许用户可以自由地处理 来自一个或多个数据流的事件， 并使用一致的容错的状态。 除此之外， 用户可以注册事件时间并处理时间回调， 从而使程序可以处理复杂的计算。 实际上， 大多数应用并不需要上述的底层抽象， 而是针对核心 API（ Core APIs）进行编程， 比如 DataStream API（ 有界或无界流数据） 以及 DataSet API（ 有界数据集） 。 这些 API 为数据处理提供了通用的构建模块， 比如由用户定义的多种形式的 转换（ transformations） ， 连接（ joins） ， 聚合（ aggregations） ， 窗口操作（ windows）等等。 DataSet API 为有界数据集提供了额外的支持， 例如循环与迭代。 这些 API 处理的数据类型以类（ classes） 的形式由各自的编程语言所表示。 Table API 是以表为中心的声明式编程， 其中表可能会动态变化（ 在表达流数据时） 。 Table API 遵循（ 扩展的） 关系模型： 表有二维数据结构（ schema） （ 类似于关系数据库中的表）， 同时 API 提供可比较的操作， 例如 select、 project、 join、group-by、aggregate 等。 Table API 程序声明式地定义了什么逻辑操作应该执行， 而不是准确地 确定这些操作代码的看上去如何。 尽管 Table API 可以通过多种类型的用户自定义函数（ UDF） 进行扩展， 其仍不如核心 API 更具表达能力， 但是使用起来却更加简洁（ 代码量更少） 。 除此之外， Table API 程序在执行之前会经过内置优化器进行优化。 你可以在表与 DataStream/DataSet 之间无缝切换， 以允许程序将 Table API 与DataStream 以及 DataSet 混合使用。 Flink 提 供 的 最高 层 级 的 抽 象 是 SQL 。 这 一 层抽 象 在 语 法 与 表 达能 力 上 与Table API 类似， 但是是以 SQL 查询表达式的形式表现程序。 SQL 抽象与 Table API。交互密切， 同时 SQL 查询可以直接在 Table API 定义的表上执行。 目前 Flink 作为批处理还不是主流， 不如 Spark 成熟， 所以 DataSet 使用的并不是很多。 Flink Table API 和 Flink SQL 也并不完善， 大多都由各大厂商自己定制。 所以我们主要学习 DataStream API 的使用。 实际上 Flink 作为最接近 Google DataFlow模型的实现， 是流批统一的观点， 所以基本上使用 DataStream 就可以了。 Flink 几大模块 Flink Table &amp; SQL(还没开发完) Flink Gelly(图计算) Flink CEP(复杂事件处理) ","link":"https://tianxiawuhao.github.io/gam9PGQCS/"},{"title":"Flink 零基础实战教程：如何计算实时热门商品","content":"在上一篇入门教程中，我们已经能够快速构建一个基础的 Flink 程序了。本文会一步步地带领你实现一个更复杂的 Flink 应用程序：实时热门商品。在开始本文前我们建议你先实践一遍上篇文章，因为本文会沿用上文的my-flink-project项目框架。 通过本文你将学到： 1. 如何基于 EventTime 处理，如何指定 Watermark 2. 如何使用 Flink 灵活的 Window API 3. 何时需要用到 State，以及如何使用 4. 如何使用 ProcessFunction 实现 TopN 功能 实战案例介绍 本案例将实现一个“实时热门商品”的需求，我们可以将“实时热门商品”翻译成程序员更好理解的需求：每隔5分钟输出最近一小时内点击量最多的前 N 个商品。将这个需求进行分解我们大概要做这么几件事情： * 抽取出业务时间戳，告诉 Flink 框架基于业务时间做窗口 * 过滤出点击行为数据 * 按一小时的窗口大小，每5分钟统计一次，做滑动窗口聚合（Sliding Window） * 按每个窗口聚合，输出每个窗口中点击量前N名的商品 数据准备 这里我们准备了一份淘宝用户行为数据集（来自阿里云天池公开数据集，特别感谢）。本数据集包含了淘宝上某一天随机一百万用户的所有行为（包括点击、购买、加购、收藏）。数据集的组织形式和MovieLens-20M类似，即数据集的每一行表示一条用户行为，由用户ID、商品ID、商品类目ID、行为类型和时间戳组成，并以逗号分隔。关于数据集中每一列的详细描述如下： 列名称 说明 用户ID 整数类型，加密后的用户ID 商品ID 整数类型，加密后的商品ID 商品类目ID 整数类型，加密后的商品所属类目ID 行为类型 字符串，枚举类型，包括(‘pv’, ‘buy’, ‘cart’, ‘fav’) 时间戳 行为发生的时间戳，单位秒 你可以通过下面的命令下载数据集到项目的 resources 目录下： $ cd my-flink-project/src/main/resources $ curl https://raw.githubusercontent.com/wuchong/my-flink-project/master/src/main/resources/UserBehavior.csv &gt; UserBehavior.csv 这里是否使用 curl 命令下载数据并不重要，你也可以使用 wget 命令或者直接访问链接下载数据。关键是，将数据文件保存到项目的 resources 目录下，方便应用程序访问。 编写程序 在 src/main/java/myflink 下创建 HotItems.java 文件： package myflink; public class HotItems { public static void main(String[] args) throws Exception { } } 与上文一样，我们会一步步往里面填充代码。第一步仍然是创建一个 StreamExecutionEnvironment，我们把它添加到 main 函数中。 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 为了打印到控制台的结果不乱序，我们配置全局的并发为1，这里改变并发对结果正确性没有影响 env.setParallelism(1); 创建模拟数据源 在数据准备章节，我们已经将测试的数据集下载到本地了。由于是一个csv文件，我们将使用 CsvInputFormat 创建模拟数据源。 注：虽然一个流式应用应该是一个一直运行着的程序，需要消费一个无限数据源。但是在本案例教程中，为了省去构建真实数据源的繁琐，我们使用了文件来模拟真实数据源，这并不影响下文要介绍的知识点。这也是一种本地验证 Flink 应用程序正确性的常用方式。 我们先创建一个 UserBehavior 的 POJO 类（所有成员变量声明成public便是POJO类），强类型化后能方便后续的处理。 /** 用户行为数据结构 **/ public static class UserBehavior { public long userId; // 用户ID public long itemId; // 商品ID public int categoryId; // 商品类目ID public String behavior; // 用户行为, 包括(&quot;pv&quot;, &quot;buy&quot;, &quot;cart&quot;, &quot;fav&quot;) public long timestamp; // 行为发生的时间戳，单位秒 } 接下来我们就可以创建一个 PojoCsvInputFormat 了， 这是一个读取 csv 文件并将每一行转成指定 POJO 类型（在我们案例中是 UserBehavior）的输入器。 // UserBehavior.csv 的本地文件路径 URL fileUrl = HotItems2.class.getClassLoader().getResource(&quot;UserBehavior.csv&quot;); Path filePath = Path.fromLocalFile(new File(fileUrl.toURI())); // 抽取 UserBehavior 的 TypeInformation，是一个 PojoTypeInfo PojoTypeInfo&lt;UserBehavior&gt; pojoType = (PojoTypeInfo&lt;UserBehavior&gt;) TypeExtractor.createTypeInfo(UserBehavior.class); // 由于 Java 反射抽取出的字段顺序是不确定的，需要显式指定下文件中字段的顺序 String[] fieldOrder = new String[]{&quot;userId&quot;, &quot;itemId&quot;, &quot;categoryId&quot;, &quot;behavior&quot;, &quot;timestamp&quot;}; // 创建 PojoCsvInputFormat PojoCsvInputFormat&lt;UserBehavior&gt; csvInput = new PojoCsvInputFormat&lt;&gt;(filePath, pojoType, fieldOrder); 下一步我们用 PojoCsvInputFormat 创建输入源。 DataStream&lt;UserBehavior&gt; dataSource = env.createInput(csvInput, pojoType); 这就创建了一个 UserBehavior 类型的 DataStream。 EventTime 与 Watermark 当我们说“统计过去一小时内点击量”，这里的“一小时”是指什么呢？ 在 Flink 中它可以是指 ProcessingTime ，也可以是 EventTime，由用户决定。 ProcessingTime：事件被处理的时间。也就是由机器的系统时间来决定。 EventTime：事件发生的时间。一般就是数据本身携带的时间。 在本案例中，我们需要统计业务时间上的每小时的点击量，所以要基于 EventTime 来处理。那么如果让 Flink 按照我们想要的业务时间来处理呢？这里主要有两件事情要做。 第一件是告诉 Flink 我们现在按照 EventTime 模式进行处理，Flink 默认使用 ProcessingTime 处理，所以我们要显式设置下。 env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime); 第二件事情是指定如何获得业务时间，以及生成 Watermark。Watermark 是用来追踪业务事件的概念，可以理解成 EventTime 世界中的时钟，用来指示当前处理到什么时刻的数据了。由于我们的数据源的数据已经经过整理，没有乱序，即事件的时间戳是单调递增的，所以可以将每条数据的业务时间就当做 Watermark。这里我们用 AscendingTimestampExtractor 来实现时间戳的抽取和 Watermark 的生成。 注：真实业务场景一般都是存在乱序的，所以一般使用 BoundedOutOfOrdernessTimestampExtractor。 DataStream&lt;UserBehavior&gt; timedData = dataSource .assignTimestampsAndWatermarks(new AscendingTimestampExtractor&lt;UserBehavior&gt;() { @Override public long extractAscendingTimestamp(UserBehavior userBehavior) { // 原始数据单位秒，将其转成毫秒 return userBehavior.timestamp * 1000; } }); 这样我们就得到了一个带有时间标记的数据流了，后面就能做一些窗口的操作。 过滤出点击事件 在开始窗口操作之前，先回顾下需求“每隔5分钟输出过去一小时内点击量最多的前 N 个商品”。由于原始数据中存在点击、加购、购买、收藏各种行为的数据，但是我们只需要统计点击量，所以先使用 FilterFunction 将点击行为数据过滤出来。 DataStream&lt;UserBehavior&gt; pvData = timedData .filter(new FilterFunction&lt;UserBehavior&gt;() { @Override public boolean filter(UserBehavior userBehavior) throws Exception { // 过滤出只有点击的数据 return userBehavior.behavior.equals(&quot;pv&quot;); } }); 窗口统计点击量 由于要每隔5分钟统计一次最近一小时每个商品的点击量，所以窗口大小是一小时，每隔5分钟滑动一次。即分别要统计 [09:00, 10:00), [09:05, 10:05), [09:10, 10:10)… 等窗口的商品点击量。是一个常见的滑动窗口需求（Sliding Window）。 DataStream&lt;ItemViewCount&gt; windowedData = pvData .keyBy(&quot;itemId&quot;) .timeWindow(Time.minutes(60), Time.minutes(5)) .aggregate(new CountAgg(), new WindowResultFunction()); 我们使用.keyBy(&quot;itemId&quot;)对商品进行分组，使用.timeWindow(Time size, Time slide)对每个商品做滑动窗口（1小时窗口，5分钟滑动一次）。然后我们使用.aggregate(AggregateFunction af, WindowFunction wf)做增量的聚合操作，它能使用AggregateFunction提前聚合掉数据，减少 state 的存储压力。较之.apply(WindowFunction wf)会将窗口中的数据都存储下来，最后一起计算要高效地多。aggregate()方法的第一个参数用于 这里的CountAgg实现了AggregateFunction接口，功能是统计窗口中的条数，即遇到一条数据就加一。 /** COUNT 统计的聚合函数实现，每出现一条记录加一 */ public static class CountAgg implements AggregateFunction&lt;UserBehavior, Long, Long&gt; { @Override public Long createAccumulator() { return 0L; } @Override public Long add(UserBehavior userBehavior, Long acc) { return acc + 1; } @Override public Long getResult(Long acc) { return acc; } @Override public Long merge(Long acc1, Long acc2) { return acc1 + acc2; } } .aggregate(AggregateFunction af, WindowFunction wf) 的第二个参数WindowFunction将每个 key每个窗口聚合后的结果带上其他信息进行输出。我们这里实现的WindowResultFunction将主键商品ID，窗口，点击量封装成了ItemViewCount进行输出。 /** 用于输出窗口的结果 */ public static class WindowResultFunction implements WindowFunction&lt;Long, ItemViewCount, Tuple, TimeWindow&gt; { @Override public void apply( Tuple key, // 窗口的主键，即 itemId TimeWindow window, // 窗口 Iterable&lt;Long&gt; aggregateResult, // 聚合函数的结果，即 count 值 Collector&lt;ItemViewCount&gt; collector // 输出类型为 ItemViewCount ) throws Exception { Long itemId = ((Tuple1&lt;Long&gt;) key).f0; Long count = aggregateResult.iterator().next(); collector.collect(ItemViewCount.of(itemId, window.getEnd(), count)); } } /** 商品点击量(窗口操作的输出类型) */ public static class ItemViewCount { public long itemId; // 商品ID public long windowEnd; // 窗口结束时间戳 public long viewCount; // 商品的点击量 public static ItemViewCount of(long itemId, long windowEnd, long viewCount) { ItemViewCount result = new ItemViewCount(); result.itemId = itemId; result.windowEnd = windowEnd; result.viewCount = viewCount; return result; } } 现在我们得到了每个商品在每个窗口的点击量的数据流。 TopN 计算最热门商品 为了统计每个窗口下最热门的商品，我们需要再次按窗口进行分组，这里根据ItemViewCount中的windowEnd进行keyBy()操作。然后使用 ProcessFunction 实现一个自定义的 TopN 函数 TopNHotItems 来计算点击量排名前3名的商品，并将排名结果格式化成字符串，便于后续输出。 DataStream&lt;String&gt; topItems = windowedData .keyBy(&quot;windowEnd&quot;) .process(new TopNHotItems(3)); // 求点击量前3名的商品 ProcessFunction 是 Flink 提供的一个 low-level API，用于实现更高级的功能。它主要提供了定时器 timer 的功能（支持EventTime或ProcessingTime）。本案例中我们将利用 timer 来判断何时收齐了某个 window 下所有商品的点击量数据。由于 Watermark 的进度是全局的， 在 processElement 方法中，每当收到一条数据（ItemViewCount），我们就注册一个 windowEnd+1 的定时器（Flink 框架会自动忽略同一时间的重复注册）。windowEnd+1 的定时器被触发时，意味着收到了windowEnd+1的 Watermark，即收齐了该windowEnd下的所有商品窗口统计值。我们在onTimer() 中处理将收集的所有商品及点击量进行排序，选出 TopN，并将排名信息格式化成字符串后进行输出。 这里我们还使用了 ListState&lt;ItemViewCount&gt; 来存储收到的每条 ItemViewCount 消息，保证在发生故障时，状态数据的不丢失和一致性。ListState 是 Flink 提供的类似 Java List 接口的 State API，它集成了框架的 checkpoint 机制，自动做到了 exactly-once 的语义保证。 /** 求某个窗口中前 N 名的热门点击商品，key 为窗口时间戳，输出为 TopN 的结果字符串 */ public static class TopNHotItems extends KeyedProcessFunction&lt;Tuple, ItemViewCount, String&gt; { private final int topSize; public TopNHotItems(int topSize) { this.topSize = topSize; } // 用于存储商品与点击数的状态，待收齐同一个窗口的数据后，再触发 TopN 计算 private ListState&lt;ItemViewCount&gt; itemState; @Override public void open(Configuration parameters) throws Exception { super.open(parameters); // 状态的注册 ListStateDescriptor&lt;ItemViewCount&gt; itemsStateDesc = new ListStateDescriptor&lt;&gt;( &quot;itemState-state&quot;, ItemViewCount.class); itemState = getRuntimeContext().getListState(itemsStateDesc); } @Override public void processElement( ItemViewCount input, Context context, Collector&lt;String&gt; collector) throws Exception { // 每条数据都保存到状态中 itemState.add(input); // 注册 windowEnd+1 的 EventTime Timer, 当触发时，说明收齐了属于windowEnd窗口的所有商品数据 context.timerService().registerEventTimeTimer(input.windowEnd + 1); } @Override public void onTimer( long timestamp, OnTimerContext ctx, Collector&lt;String&gt; out) throws Exception { // 获取收到的所有商品点击量 List&lt;ItemViewCount&gt; allItems = new ArrayList&lt;&gt;(); for (ItemViewCount item : itemState.get()) { allItems.add(item); } // 提前清除状态中的数据，释放空间 itemState.clear(); // 按照点击量从大到小排序 allItems.sort(new Comparator&lt;ItemViewCount&gt;() { @Override public int compare(ItemViewCount o1, ItemViewCount o2) { return (int) (o2.viewCount - o1.viewCount); } }); // 将排名信息格式化成 String, 便于打印 StringBuilder result = new StringBuilder(); result.append(&quot;====================================\\n&quot;); result.append(&quot;时间: &quot;).append(new Timestamp(timestamp-1)).append(&quot;\\n&quot;); for (int i=0;i&lt;topSize;i++) { ItemViewCount currentItem = allItems.get(i); // No1: 商品ID=12224 浏览量=2413 result.append(&quot;No&quot;).append(i).append(&quot;:&quot;) .append(&quot; 商品ID=&quot;).append(currentItem.itemId) .append(&quot; 浏览量=&quot;).append(currentItem.viewCount) .append(&quot;\\n&quot;); } result.append(&quot;====================================\\n\\n&quot;); out.collect(result.toString()); } } 打印输出 最后一步我们将结果打印输出到控制台，并调用env.execute执行任务。 topItems.print(); env.execute(&quot;Hot Items Job&quot;); 运行程序 直接运行 main 函数，就能看到不断输出的每个时间点的热门商品ID。 更换 Kafka 作为数据源 实际生产环境中， 我们的数据流往往是从 Kafka 获取到的。 如果要让代码更贴 近生产实际， 我们只需将 source 更换为 Kafka 即可： val properties = new Properties() properties.setProperty(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;) properties.setProperty(&quot;group.id&quot;, &quot;consumer-group&quot;) properties.setProperty(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;) properties.setProperty(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;) properties.setProperty(&quot;auto.offset.reset&quot;, &quot;latest&quot;) val env = StreamExecutionEnvironment.getExecutionEnvironment env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime) env.setParallelism(1) val stream = env .addSource(new FlinkKafkaConsumer[String](&quot;hotitems&quot;, new SimpleStringSchema(), properties)) 当然， 根据实际的需要， 我们还可以将 Sink 指定为 Kafka、 ES、 Redis 或其它 存储， 这里就不一一展开实现了 总结 本文的完整代码可以通过 GitHub 访问到。本文通过实现一个“实时热门商品”的案例，学习和实践了 Flink 的多个核心概念和 API 用法。包括 EventTime、Watermark 的使用，State 的使用，Window API 的使用，以及 TopN 的实现。希望本文能加深大家对 Flink 的理解，帮助大家解决实战上遇到的问题。 ","link":"https://tianxiawuhao.github.io/use-flink-calculate-hot-items/"},{"title":"从零构建第一个 Flink 应用","content":" 原文：http://wuchong.me/blog/2018/11/07/5-minutes-build-first-flink-application/ 作者：云邪（Jark） 在本文中，我们将从零开始，教您如何构建第一个 Flink 应用程序。 开发环境准备 Flink 可以运行在 Linux, Max OS X, 或者是 Windows 上。为了开发 Flink 应用程序，在本地机器上需要有 Java 8.x 和 maven 环境。 如果有 Java 8 环境，运行下面的命令会输出如下版本信息： $ java -version java version &quot;1.8.0_65&quot; Java(TM) SE Runtime Environment (build 1.8.0_65-b17) Java HotSpot(TM) 64-Bit Server VM (build 25.65-b01, mixed mode) 如果有 maven 环境，运行下面的命令会输出如下版本信息： $ mvn -version Apache Maven 3.5.4 (1edded0938998edf8bf061f1ceb3cfdeccf443fe; 2018-06-18T02:33:14+08:00) Maven home: /Users/wuchong/dev/maven Java version: 1.8.0_65, vendor: Oracle Corporation, runtime: /Library/Java/JavaVirtualMachines/jdk1.8.0_65.jdk/Contents/Home/jre Default locale: zh_CN, platform encoding: UTF-8 OS name: &quot;mac os x&quot;, version: &quot;10.13.6&quot;, arch: &quot;x86_64&quot;, family: &quot;mac&quot; 另外我们推荐使用 ItelliJ IDEA （社区免费版已够用）作为 Flink 应用程序的开发 IDE。Eclipse 虽然也可以，但是 Eclipse 在 Scala 和 Java 混合型项目下会有些已知问题，所以不太推荐 Eclipse。下一章节，我们会介绍如何创建一个 Flink 工程并将其导入 ItelliJ IDEA。 创建 Maven 项目 我们将使用 Flink Maven Archetype 来创建我们的项目结构和一些初始的默认依赖。在你的工作目录下，运行如下命令来创建项目： mvn archetype:generate \\ -DarchetypeGroupId=org.apache.flink \\ -DarchetypeArtifactId=flink-quickstart-java \\ -DarchetypeVersion=1.6.1 \\ -DgroupId=my-flink-project \\ -DartifactId=my-flink-project \\ -Dversion=0.1 \\ -Dpackage=myflink \\ -DinteractiveMode=false 你可以编辑上面的 groupId, artifactId, package 成你喜欢的路径。使用上面的参数，Maven 将自动为你创建如下所示的项目结构： $ tree my-flink-project my-flink-project ├── pom.xml └── src └── main ├── java │ └── myflink │ ├── BatchJob.java │ └── StreamingJob.java └── resources └── log4j.properties 我们的 pom.xml 文件已经包含了所需的 Flink 依赖，并且在 src/main/java 下有几个示例程序框架。接下来我们将开始编写第一个 Flink 程序。 编写 Flink 程序 启动 IntelliJ IDEA，选择 “Import Project”（导入项目），选择 my-flink-project 根目录下的 pom.xml。根据引导，完成项目导入。 在 src/main/java/myflink 下创建 SocketWindowWordCount.java 文件： package myflink; public class SocketWindowWordCount { public static void main(String[] args) throws Exception { } } 现在这程序还很基础，我们会一步步往里面填代码。注意下文中我们不会将 import 语句也写出来，因为 IDE 会自动将他们添加上去。在本节末尾，我会将完整的代码展示出来，如果你想跳过下面的步骤，可以直接将最后的完整代码粘到编辑器中。 Flink 程序的第一步是创建一个 StreamExecutionEnvironment 。这是一个入口类，可以用来设置参数和创建数据源以及提交任务。所以让我们把它添加到 main 函数中： StreamExecutionEnvironment see = StreamExecutionEnvironment.getExecutionEnvironment(); 下一步我们将创建一个从本地端口号 9000 的 socket 中读取数据的数据源： DataStream&lt;String&gt; text = env.socketTextStream(&quot;localhost&quot;, 9000, &quot;\\n&quot;); 这创建了一个字符串类型的 DataStream。DataStream 是 Flink 中做流处理的核心 API，上面定义了非常多常见的操作（如，过滤、转换、聚合、窗口、关联等）。在本示例中，我们感兴趣的是每个单词在特定时间窗口中出现的次数，比如说5秒窗口。为此，我们首先要将字符串数据解析成单词和次数（使用Tuple2&lt;String, Integer&gt;表示），第一个字段是单词，第二个字段是次数，次数初始值都设置成了1。我们实现了一个 flatmap 来做解析的工作，因为一行数据中可能有多个单词。 DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; wordCounts = text .flatMap(new FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() { @Override public void flatMap(String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out) { for (String word : value.split(&quot;\\\\s&quot;)) { out.collect(Tuple2.of(word, 1)); } } }); 接着我们将数据流按照单词字段（即0号索引字段）做分组，这里可以简单地使用 keyBy(int index) 方法，得到一个以单词为 key 的Tuple2&lt;String, Integer&gt;数据流。然后我们可以在流上指定想要的窗口，并根据窗口中的数据计算结果。在我们的例子中，我们想要每5秒聚合一次单词数，每个窗口都是从零开始统计的：。 DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; windowCounts = wordCounts .keyBy(0) .timeWindow(Time.seconds(5)) .sum(1); 第二个调用的.timeWindow()指定我们想要5秒的翻滚窗口（Tumble）。第三个调用为每个key每个窗口指定了sum聚合函数，在我们的例子中是按照次数字段（即1号索引字段）相加。得到的结果数据流，将每5秒输出一次这5秒内每个单词出现的次数。 最后一件事就是将数据流打印到控制台，并开始执行： windowCounts.print().setParallelism(1); env.execute(&quot;Socket Window WordCount&quot;); 最后的 env.execute 调用是启动实际Flink作业所必需的。所有算子操作（例如创建源、聚合、打印）只是构建了内部算子操作的图形。只有在execute()被调用时才会在提交到集群上或本地计算机上执行。 下面是完整的代码，部分代码经过简化（代码在 GitHub 上也能访问到）： package myflink; import org.apache.flink.api.common.functions.FlatMapFunction; import org.apache.flink.api.java.tuple.Tuple2; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.windowing.time.Time; import org.apache.flink.util.Collector; public class SocketWindowWordCount { public static void main(String[] args) throws Exception { // 创建 execution environment final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 通过连接 socket 获取输入数据，这里连接到本地9000端口，如果9000端口已被占用，请换一个端口 DataStream&lt;String&gt; text = env.socketTextStream(&quot;localhost&quot;, 9000, &quot;\\n&quot;); // 解析数据，按 word 分组，开窗，聚合 DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; windowCounts = text .flatMap(new FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() { @Override public void flatMap(String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out) { for (String word : value.split(&quot;\\\\s&quot;)) { out.collect(Tuple2.of(word, 1)); } } }) .keyBy(0) .timeWindow(Time.seconds(5)) .sum(1); // 将结果打印到控制台，注意这里使用的是单线程打印，而非多线程 windowCounts.print().setParallelism(1); env.execute(&quot;Socket Window WordCount&quot;); } } 运行程序 要运行示例程序，首先我们在终端启动 netcat 获得输入流： nc -lk 9000 如果是 Windows 平台，可以通过 https://nmap.org/ncat/ 安装 ncat 然后运行： ncat -lk 9000 然后直接运行SocketWindowWordCount的 main 方法。 只需要在 netcat 控制台输入单词，就能在 SocketWindowWordCount 的输出控制台看到每个单词的词频统计。如果想看到大于1的计数，请在5秒内反复键入相同的单词。 Cheers! 🎉 ","link":"https://tianxiawuhao.github.io/build-first-flink-application/"}]}